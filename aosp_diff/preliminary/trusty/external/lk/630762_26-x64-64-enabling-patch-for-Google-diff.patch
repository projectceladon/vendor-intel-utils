From 6cbff469fb024bfa8dfd45885df9c124cce4ba67 Mon Sep 17 00:00:00 2001
From: "Zhong,Fangjian" <fangjian.zhong@intel.com>
Date: Mon, 23 Apr 2018 08:20:33 +0800
Subject: [PATCH] x64-64 enabling patch for Google diff

Change-Id: I2b4c0d513d24b6d913d0273a31ee5f2485d9a76b
Tracked-On: https://jira01.devtools.intel.com/browse/OAM-63646
Signed-off-by: Zhong,Fangjian <fangjian.zhong@intel.com>
---
 arch/x86/32/mmu.c                             |   8 +-
 arch/x86/64/exceptions.S                      |  21 +-
 arch/x86/64/kernel.ld                         |   4 +-
 arch/x86/64/mmu.c                             | 131 +++++++---
 arch/x86/64/ops.S                             |  29 ++-
 arch/x86/64/start.S                           |  21 +-
 arch/x86/64/usercopy.c                        | 119 ++++++++++
 arch/x86/arch.c                               | 224 ++++++++++++++----
 arch/x86/cache.c                              |  13 +
 arch/x86/descriptor.c                         |  78 +++---
 arch/x86/faults.c                             |   8 +-
 arch/x86/fpu.c                                |  59 ++---
 arch/x86/gdt.S                                |  63 +++--
 arch/x86/include/arch/arch_ops.h              |  55 ++++-
 arch/x86/include/arch/arch_thread.h           |  23 +-
 arch/x86/include/arch/aspace.h                |   6 +-
 .../x86/include/arch/local_apic.h             |  28 +--
 arch/x86/include/arch/spinlock.h              |   5 +
 arch/x86/include/arch/x86.h                   |  87 +++++--
 arch/x86/include/arch/x86/descriptor.h        |  35 ++-
 arch/x86/include/arch/x86/mmu.h               |   6 +-
 arch/x86/include/arch/x86/mp.h                |  59 +++++
 arch/x86/local_apic.c                         | 190 +++++++++++++++
 arch/x86/rules.mk                             |  40 +++-
 .../arch/x86/memset.S => arch/x86/stack_chk.c |  21 +-
 arch/x86/thread.c                             |  47 ++--
 arch/x86/toolchain.mk                         |  19 +-
 engine.mk                                     |  12 +-
 include/arch/hidden.h                         |   5 +
 include/arch/ops.h                            |   6 +
 include/arch/usercopy.h                       |   3 +-
 lib/libc/include/endian.h                     |   4 +
 lib/libc/include/string.h                     |   2 +
 .../string/arch/{x86-64 => x86/64}/memcpy.S   |   0
 lib/libc/string/arch/x86/64/memcpy_s.c        |  41 ++++
 .../string/arch/{x86-64 => x86/64}/memset.S   |   0
 .../string/arch/{x86-64 => x86/64}/rules.mk   |   5 +-
 lib/libc/string/arch/x86/rules.mk             |  12 +-
 make/compile.mk                               |  18 +-
 make/module.mk                                |  20 ++
 40 files changed, 1195 insertions(+), 332 deletions(-)
 create mode 100644 arch/x86/64/usercopy.c
 rename lib/libc/string/arch/x86/memcpy.S => arch/x86/include/arch/local_apic.h (74%)
 create mode 100644 arch/x86/include/arch/x86/mp.h
 create mode 100644 arch/x86/local_apic.c
 rename lib/libc/string/arch/x86/memset.S => arch/x86/stack_chk.c (83%)
 create mode 100644 include/arch/hidden.h
 rename lib/libc/string/arch/{x86-64 => x86/64}/memcpy.S (100%)
 create mode 100644 lib/libc/string/arch/x86/64/memcpy_s.c
 rename lib/libc/string/arch/{x86-64 => x86/64}/memset.S (100%)
 rename lib/libc/string/arch/{x86-64 => x86/64}/rules.mk (59%)

diff --git a/arch/x86/32/mmu.c b/arch/x86/32/mmu.c
index 4ff7daad..253626ff 100644
--- a/arch/x86/32/mmu.c
+++ b/arch/x86/32/mmu.c
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  * Copyright (c) 2016 Travis Geiselbrecht
  *
  * Permission is hereby granted, free of charge, to any person obtaining
@@ -623,9 +623,9 @@ void x86_mmu_early_init(void)
     x86_set_cr4(cr4);
 
     /* Set NXE bit in MSR_EFER*/
-    efer_msr = read_msr(x86_MSR_EFER);
-    efer_msr |= x86_EFER_NXE;
-    write_msr(x86_MSR_EFER, efer_msr);
+    efer_msr = read_msr(X86_MSR_EFER);
+    efer_msr |= X86_EFER_NXE;
+    write_msr(X86_MSR_EFER, efer_msr);
 #endif
 
     /* unmap the lower identity mapping */
diff --git a/arch/x86/64/exceptions.S b/arch/x86/64/exceptions.S
index a8aaf6df..d81c6e8c 100644
--- a/arch/x86/64/exceptions.S
+++ b/arch/x86/64/exceptions.S
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  * Copyright (c) 2016 Travis Geiselbrecht
  *
  * Permission is hereby granted, free of charge, to any person obtaining
@@ -25,7 +25,7 @@
 #include <asm.h>
 #include <arch/x86/descriptor.h>
 
-#define NUM_INT 0x31
+#define NUM_INT 0x100
 #define NUM_EXC 0x14
 
 .text
@@ -38,11 +38,11 @@ _isr:
 .set isr_stub_start, .
 
 .if i == 8 || (i >= 10 && i <= 14) || i == 17
-        nop                                     /* error code pushed by exception */
-        nop                                     /* 2 nops are the same length as push byte */
+.align 16
         pushq $i                                /* interrupt number */
         jmp interrupt_common
 .else
+.align 16
         pushq $0                                /* fill in error code in iframe */
         pushq $i                                /* interrupt number */
         jmp interrupt_common
@@ -58,7 +58,12 @@ _isr:
 .fill 256
 
 interrupt_common:
+    /* Check if from user space */
+    testb $3, 0x18(%rsp)
+    jz .Lskip_swapgs_in
+    swapgs
 
+.Lskip_swapgs_in:
     /* save general purpose registers */
     pushq %r15
     pushq %r14
@@ -97,6 +102,12 @@ interrupt_common:
     popq %r14
     popq %r15
 
+    /* check if back to user space */
+    testb $3, 0x18(%rsp)
+    jz .Lskip_swapgs_out
+    swapgs
+
+.Lskip_swapgs_out:
     /* drop vector number and error code*/
     addq $16, %rsp
     iretq
@@ -137,7 +148,7 @@ DATA(_idtr)
 DATA(_idt)
 
 .set i, 0
-.rept NUM_INT-1
+.rept NUM_INT
     .short 0        /* low 16 bits of ISR offset (_isr#i & 0FFFFh) */
     .short CODE_64_SELECTOR   /* selector */
     .byte  0
diff --git a/arch/x86/64/kernel.ld b/arch/x86/64/kernel.ld
index f06d1a3d..27d280c4 100644
--- a/arch/x86/64/kernel.ld
+++ b/arch/x86/64/kernel.ld
@@ -1,7 +1,7 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
  * Copyright (c) 2013 Travis Geiselbrecht
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -23,7 +23,7 @@
  * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
 
-ENTRY(_start)
+ENTRY(_start_pa)
 SECTIONS
 {
     . = %KERNEL_BASE% + %KERNEL_LOAD_OFFSET%;
diff --git a/arch/x86/64/mmu.c b/arch/x86/64/mmu.c
index 60a10c8d..006abf3d 100644
--- a/arch/x86/64/mmu.c
+++ b/arch/x86/64/mmu.c
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  * Copyright (c) 2016 Travis Geiselbrecht
  *
  * Permission is hereby granted, free of charge, to any person obtaining
@@ -43,16 +43,20 @@
 uint8_t g_vaddr_width = 0;
 uint8_t g_paddr_width = 0;
 
+paddr_t x86_kernel_page_table = 0;
+
 /* top level kernel page tables, initialized in start.S */
 map_addr_t pml4[NO_OF_PT_ENTRIES] __ALIGNED(PAGE_SIZE);
-map_addr_t pdp[NO_OF_PT_ENTRIES] __ALIGNED(PAGE_SIZE); /* temporary */
-map_addr_t pte[NO_OF_PT_ENTRIES] __ALIGNED(PAGE_SIZE);
+/* temporary low 512GB mapping, removed at x86_mmu_early_init */
+map_addr_t pdp[NO_OF_PT_ENTRIES] __ALIGNED(PAGE_SIZE);
+/* Enlarge to map 4GB, for MMIO mapping */
+map_addr_t pte[NO_OF_PT_ENTRIES * 4] __ALIGNED(PAGE_SIZE);
 
 /* top level pdp needed to map the -512GB..0 space */
 map_addr_t pdp_high[NO_OF_PT_ENTRIES] __ALIGNED(PAGE_SIZE);
 
 /* a big pile of page tables needed to map 64GB of memory into kernel space using 2MB pages */
-map_addr_t linear_map_pdp[(64ULL*GB) / (2*MB)];
+map_addr_t linear_map_pdp[(64ULL*GB) / (2*MB)] __ALIGNED(PAGE_SIZE);
 
 /**
  * @brief  check if the virtual address is aligned and canonical
@@ -146,7 +150,11 @@ static inline uint64_t get_pfn_from_pte(uint64_t pte)
 {
     uint64_t pfn;
 
+    /* Clear low 12 bits */
     pfn = (pte & X86_PG_FRAME);
+
+    /* Clear high 12 bits */
+    pfn &= X86_PHY_ADDR_MASK;
     return pfn;
 }
 
@@ -174,8 +182,14 @@ arch_flags_t get_x86_arch_flags(arch_flags_t flags)
     if (flags & ARCH_MMU_FLAG_PERM_USER)
         arch_flags |= X86_MMU_PG_U;
 
-    if (flags & ARCH_MMU_FLAG_UNCACHED)
-        arch_flags |= X86_MMU_CACHE_DISABLE;
+    /*
+     * for cached, PAT:PCD:PWT is 000 (default)
+     * for uncached, PAT:PCD:PWT is 011
+     */
+    if (flags & ARCH_MMU_FLAG_UNCACHED) {
+        arch_flags |= X86_MMU_PG_PCD;
+        arch_flags |= X86_MMU_PG_PWT;
+    }
 
     if (flags & ARCH_MMU_FLAG_PERM_NO_EXECUTE)
         arch_flags |= X86_MMU_PG_NX;
@@ -196,8 +210,13 @@ uint get_arch_mmu_flags(arch_flags_t flags)
     if (flags & X86_MMU_PG_U)
         mmu_flags |= ARCH_MMU_FLAG_PERM_USER;
 
-    if (flags & X86_MMU_CACHE_DISABLE)
+    /* Default memory type is CACHED/WB */
+    if ((flags & X86_MMU_PG_PCD) &&
+        (flags & X86_MMU_PG_PWT) &&
+        !(flags & X86_MMU_PG_PTE_PAT))
         mmu_flags |= ARCH_MMU_FLAG_UNCACHED;
+    else
+        mmu_flags |= ARCH_MMU_FLAG_CACHED;
 
     if (flags & X86_MMU_PG_NX)
         mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
@@ -397,7 +416,7 @@ status_t x86_mmu_add_mapping(map_addr_t pml4, map_addr_t paddr,
                              vaddr_t vaddr, arch_flags_t mmu_flags)
 {
     uint32_t pd_new = 0, pdp_new = 0;
-    uint64_t pml4e, pdpe, pde;
+    uint64_t pml4e, pdpe, pde = 0;
     map_addr_t *m = NULL;
     status_t ret = NO_ERROR;
 
@@ -443,8 +462,9 @@ status_t x86_mmu_add_mapping(map_addr_t pml4, map_addr_t paddr,
     if (!pd_new)
         pde = get_pd_entry_from_pd_table(vaddr, pdpe);
 
-    if (pd_new || (pde & X86_MMU_PG_P) == 0) {
+    if (pd_new || (pde & X86_MMU_PG_P) == 0 || (pde & X86_MMU_PG_PS) != 0) {
         /* Creating a new pt */
+        bool need_expand = !!(pde & X86_MMU_PG_PS);
         m  = _map_alloc_page();
         if (m == NULL) {
             ret = ERR_NO_MEMORY;
@@ -453,6 +473,22 @@ status_t x86_mmu_add_mapping(map_addr_t pml4, map_addr_t paddr,
             goto clean;
         }
 
+        /* If preview table is 2M page leaf, expand to 4K page leaves */
+        if (need_expand) {
+            uint64_t pd_paddr = X86_VIRT_TO_PHYS(pde);
+            arch_flags_t flag = get_x86_arch_flags(pde);
+            uint64_t *pt_table = (uint64_t *)((uint64_t)m & X86_PG_FRAME);
+            uint32_t index;
+
+            for (index = 0; index < 512; index++) {
+                pt_table[index] = (uint64_t)pd_paddr;
+                pt_table[index] |= flag | X86_MMU_PG_P;
+                if (!(flag & X86_MMU_PG_U))
+                    pt_table[index] |= X86_MMU_PG_G;
+                pd_paddr += 4096;
+            }
+        }
+
         update_pd_entry(vaddr, pdpe, X86_VIRT_TO_PHYS(m), get_x86_arch_flags(mmu_flags));
         pde = (uint64_t)m;
     }
@@ -481,7 +517,7 @@ clean:
 static void x86_mmu_unmap_entry(vaddr_t vaddr, int level, vaddr_t table_entry)
 {
     uint32_t offset = 0, next_level_offset = 0;
-    vaddr_t *table, *next_table_addr, value;
+    vaddr_t *table, *next_table_addr;
 
     LTRACEF("vaddr 0x%lx level %d table_entry 0x%lx\n", vaddr, level, table_entry);
 
@@ -545,12 +581,10 @@ static void x86_mmu_unmap_entry(vaddr_t vaddr, int level, vaddr_t table_entry)
         }
         pmm_free_page(paddr_to_vm_page(X86_VIRT_TO_PHYS(next_table_addr)));
     }
-    /* All present bits for all entries in next level table for this address are 0 */
+    /* zero PTE to avoid L1TF issue */
     if ((X86_PHYS_TO_VIRT(table[offset]) & X86_MMU_PG_P) != 0) {
         arch_disable_ints();
-        value = table[offset];
-        value = value & X86_PTE_NOT_PRESENT;
-        table[offset] = value;
+        table[offset] = 0;
         arch_enable_ints();
     }
 }
@@ -569,6 +603,11 @@ status_t x86_mmu_unmap(map_addr_t pml4, vaddr_t vaddr, uint count)
     next_aligned_v_addr = vaddr;
     while (count > 0) {
         x86_mmu_unmap_entry(next_aligned_v_addr, X86_PAGING_LEVELS, pml4);
+        /*
+         * Flush page mapping in TLB when unmapping pages,
+         * need to invalid page to avoid data loss.
+         */
+        __asm__ __volatile__ ("invlpg (%0)": : "r" (next_aligned_v_addr) : "memory");
         next_aligned_v_addr += PAGE_SIZE;
         count--;
     }
@@ -589,8 +628,8 @@ int arch_mmu_unmap(arch_aspace_t *aspace, vaddr_t vaddr, uint count)
     if (count == 0)
         return NO_ERROR;
 
-    DEBUG_ASSERT(x86_get_cr3());
-    current_cr3_val = (addr_t)x86_get_cr3();
+    current_cr3_val = aspace->page_table;
+    DEBUG_ASSERT(current_cr3_val);
 
     return (x86_mmu_unmap(X86_PHYS_TO_VIRT(current_cr3_val), vaddr, count));
 }
@@ -648,17 +687,15 @@ status_t arch_mmu_query(arch_aspace_t *aspace, vaddr_t vaddr, paddr_t *paddr, ui
 
     DEBUG_ASSERT(aspace);
 
-    if (!paddr)
-        return ERR_INVALID_ARGS;
-
-    DEBUG_ASSERT(x86_get_cr3());
-    current_cr3_val = (addr_t)x86_get_cr3();
+    current_cr3_val = aspace->page_table;
+    DEBUG_ASSERT(current_cr3_val);
 
     stat = x86_mmu_get_mapping(X86_PHYS_TO_VIRT(current_cr3_val), vaddr, &ret_level, &ret_flags, &last_valid_entry);
     if (stat)
         return stat;
 
-    *paddr = (paddr_t)(last_valid_entry);
+    if (paddr)
+        *paddr = (paddr_t)(last_valid_entry);
     LTRACEF("paddr 0x%llx\n", last_valid_entry);
 
     /* converting x86 arch specific flags to arch mmu flags */
@@ -686,8 +723,8 @@ int arch_mmu_map(arch_aspace_t *aspace, vaddr_t vaddr, paddr_t paddr, uint count
     if (count == 0)
         return NO_ERROR;
 
-    DEBUG_ASSERT(x86_get_cr3());
-    current_cr3_val = (addr_t)x86_get_cr3();
+    current_cr3_val = aspace->page_table;
+    DEBUG_ASSERT(current_cr3_val);
 
     range.start_vaddr = vaddr;
     range.start_paddr = paddr;
@@ -714,9 +751,9 @@ void x86_mmu_early_init(void)
     x86_set_cr4(cr4);
 
     /* Set NXE bit in MSR_EFER*/
-    efer_msr = read_msr(x86_MSR_EFER);
-    efer_msr |= x86_EFER_NXE;
-    write_msr(x86_MSR_EFER, efer_msr);
+    efer_msr = read_msr(X86_MSR_EFER);
+    efer_msr |= X86_EFER_NXE;
+    write_msr(X86_MSR_EFER, efer_msr);
 
     /* getting the address width from CPUID instr */
     /* Bits 07-00: Physical Address width info */
@@ -730,6 +767,8 @@ void x86_mmu_early_init(void)
     /* unmap the lower identity mapping */
     pml4[0] = 0;
 
+    x86_kernel_page_table = x86_get_cr3();
+
     /* tlb flush */
     x86_set_cr3(x86_get_cr3());
 }
@@ -738,16 +777,29 @@ void x86_mmu_init(void)
 {
 }
 
-/*
- * x86-64 does not support multiple address spaces at the moment, so fail if these apis
- * are used for it.
- */
+static paddr_t x86_create_page_table(void)
+{
+    addr_t *new_table = NULL;
+
+    new_table = (addr_t *)_map_alloc_page();
+    ASSERT(new_table);
+    new_table[511] = pml4[511];
+
+    return (paddr_t)X86_VIRT_TO_PHYS(new_table);
+}
+
 status_t arch_mmu_init_aspace(arch_aspace_t *aspace, vaddr_t base, size_t size, uint flags)
 {
     DEBUG_ASSERT(aspace);
 
-    if ((flags & ARCH_ASPACE_FLAG_KERNEL) == 0) {
-        return ERR_NOT_SUPPORTED;
+    aspace->size = size;
+    aspace->base = base;
+
+    if ((flags & ARCH_ASPACE_FLAG_KERNEL)) {
+        aspace->page_table = x86_kernel_page_table;
+    } else {
+        if (0 == aspace->page_table)
+            aspace->page_table = x86_create_page_table();
     }
 
     return NO_ERROR;
@@ -755,13 +807,20 @@ status_t arch_mmu_init_aspace(arch_aspace_t *aspace, vaddr_t base, size_t size,
 
 status_t arch_mmu_destroy_aspace(arch_aspace_t *aspace)
 {
+    DEBUG_ASSERT(aspace);
+
+    aspace->size = 0;
+    aspace->base = 0;
+    aspace->page_table = 0;
+
     return NO_ERROR;
 }
 
 void arch_mmu_context_switch(arch_aspace_t *aspace)
 {
-    if (aspace != NULL) {
-        PANIC_UNIMPLEMENTED;
-    }
+    if (NULL == aspace || 0 == aspace->page_table)
+        return;
+
+    x86_set_cr3(aspace->page_table);
 }
 
diff --git a/arch/x86/64/ops.S b/arch/x86/64/ops.S
index 10b55bd5..bacb89e8 100644
--- a/arch/x86/64/ops.S
+++ b/arch/x86/64/ops.S
@@ -37,12 +37,12 @@
 
 /* int _atomic_and(int *ptr, int val); */
 FUNCTION(_atomic_and)
-    movq (%rdi), %rax
+    mov (%rdi), %eax
 0:
-    movq %rax, %rcx
-    andq %rsi, %rcx
+    mov %eax, %ecx
+    and %esi, %ecx
     lock
-    cmpxchgq %rcx, (%rdi)
+    cmpxchg %ecx, (%rdi)
     jnz 1f                  /* static prediction: branch forward not taken */
     ret
 1:
@@ -52,12 +52,12 @@ FUNCTION(_atomic_and)
 /* int _atomic_or(int *ptr, int val); */
 FUNCTION(_atomic_or)
 
-    movq (%rdi), %rax
+    mov (%rdi), %eax
 0:
-    movq %rax, %rcx
-    orq %rsi, %rcx
+    mov %eax, %ecx
+    or  %esi, %ecx
     lock
-    cmpxchgq %rcx, (%rdi)
+    cmpxchg %ecx, (%rdi)
     jnz 1f                  /* static prediction: branch forward not taken */
     ret
 1:
@@ -74,3 +74,16 @@ FUNCTION(arch_idle)
 1:
     ret
 
+#ifdef STACK_PROTECTOR
+FUNCTION(get_rand_64)
+    mov $10, %rdx
+1:
+    rdrand %rax
+    jc 2f            #jump short if carry (CF=1)
+    dec %rdx
+    jnz 1b
+    jz 3f
+2:  mov %rax, (%rdi)
+3:  mov %rdx, %rax
+    ret
+#endif
\ No newline at end of file
diff --git a/arch/x86/64/start.S b/arch/x86/64/start.S
index edf4e113..f9a5d332 100644
--- a/arch/x86/64/start.S
+++ b/arch/x86/64/start.S
@@ -39,8 +39,11 @@
 /* The magic number passed by a Multiboot-compliant boot loader. */
 #define MULTIBOOT_BOOTLOADER_MAGIC 0x2BADB002
 
-#define MSR_EFER 0xc0000080
-#define EFER_LME 0x00000100
+#define MSR_EFER    0xc0000080
+#define EFER_LME    0x00000100
+#define MSR_GS_BASE 0xc0000101
+#define PAT_MSR     0x277
+#define CACHE_MODE  0x70106
 
 #define PHYS_LOAD_ADDRESS (MEMBASE + KERNEL_LOAD_OFFSET)
 #define PHYS_ADDR_DELTA (KERNEL_BASE + KERNEL_LOAD_OFFSET - PHYS_LOAD_ADDRESS)
@@ -134,6 +137,12 @@ paging_setup:
     orl $EFER_LME,%eax
     wrmsr
 
+    /* setting the PAT MSRs */
+    movl $PAT_MSR, %ecx
+    movl $CACHE_MODE, %eax
+    movl $CACHE_MODE, %edx
+    wrmsr
+
     /* Setting the First PML4E with a PDP table reference*/
     movl $PHYS(pdp), %eax
     orl  $X86_KERNEL_PD_FLAGS, %eax
@@ -224,6 +233,14 @@ highaddr:
     /* reload the gdtr */
     lgdt _gdtr
 
+    /* set up gs base */
+    leaq global_states(%rip), %rax
+
+    movq %rax, %rdx
+    shr  $32, %rdx
+    movq $MSR_GS_BASE, %rcx
+    wrmsr
+
     /* set up the idt */
     call setup_idt
 
diff --git a/arch/x86/64/usercopy.c b/arch/x86/64/usercopy.c
new file mode 100644
index 00000000..4a3a7570
--- /dev/null
+++ b/arch/x86/64/usercopy.c
@@ -0,0 +1,119 @@
+/*
+ * Copyright (c) 2018 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files
+ * (the "Software"), to deal in the Software without restriction,
+ * including without limitation the rights to use, copy, modify, merge,
+ * publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so,
+ * subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include <arch/usercopy.h>
+#include <string.h>
+#include <kernel/vm.h>
+#include <err.h>
+
+static vmm_region_t *get_region(user_addr_t vaddr)
+{
+    vmm_aspace_t *aspace = vaddr_to_aspace((void *)(uint64_t)vaddr);
+    vmm_region_t *region = NULL;
+
+    if (NULL == aspace) {
+        return NULL;
+    }
+
+    list_for_every_entry(&aspace->region_list, region, vmm_region_t, node) {
+        if ((region->base <= vaddr) &&
+                (vaddr < (region->base + region->size))) {
+            return region;
+        }
+    }
+
+    return NULL;
+}
+
+status_t arch_copy_from_user(void *kdest, user_addr_t usrc, size_t len)
+{
+    vmm_region_t *region = get_region(usrc);
+    status_t ret = 0;
+
+    if (0 == len)
+        return NO_ERROR;
+
+    if (NULL == kdest || 0 == usrc || NULL == region)
+        return ERR_FAULT;
+
+    if (usrc + len > region->size + region->base)
+        return ERR_FAULT;
+
+    __asm__ volatile("stac");
+    ret = memcpy_s(kdest, len, (void *)(uint64_t)usrc, len);
+    __asm__ volatile("clac");
+
+    if(ret)
+        return ERR_FAULT;
+
+    return NO_ERROR;
+}
+
+status_t arch_copy_to_user(user_addr_t udest, const void *ksrc, size_t len)
+{
+    vmm_region_t *region = get_region(udest);
+    status_t ret = 0;
+
+    if (0 == len)
+        return NO_ERROR;
+
+    if (NULL == ksrc || 0 == udest || NULL == region)
+        return ERR_FAULT;
+
+    if (udest + len > region->size + region->base)
+        return ERR_FAULT;
+
+    __asm__ volatile("stac");
+    ret = memcpy_s((void *)(uint64_t)udest, len, ksrc, len);
+    __asm__ volatile("clac");
+
+    if(ret)
+        return ERR_FAULT;
+
+    return NO_ERROR;
+}
+
+
+ssize_t arch_strlcpy_from_user(char *kdst, user_addr_t usrc, size_t len)
+{
+    size_t max_len = 0;
+    size_t act_len = len;
+    char  *ksrc = (char *)(uint64_t)usrc;
+    vmm_region_t *region = get_region(usrc);
+
+    if (NULL == region || 0 == len)
+        return 0;
+
+    max_len = region->size - (usrc - region->base);
+
+    if (len > max_len)
+        return (size_t)ERR_INVALID_ARGS;
+
+    __asm__ volatile("stac");
+    while (act_len-- && (*kdst++ = *ksrc++) != '\0')
+        ;
+    __asm__ volatile("clac");
+
+    act_len = len - act_len;
+
+    return act_len;
+}
diff --git a/arch/x86/arch.c b/arch/x86/arch.c
index 721c57eb..df79d35b 100644
--- a/arch/x86/arch.c
+++ b/arch/x86/arch.c
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -27,42 +27,124 @@
 #include <arch/ops.h>
 #include <arch/x86.h>
 #include <arch/x86/mmu.h>
+#include <arch/x86/mp.h>
 #include <arch/x86/descriptor.h>
 #include <arch/fpu.h>
 #include <arch/mmu.h>
 #include <platform.h>
 #include <sys/types.h>
 #include <string.h>
+#include <assert.h>
+#ifdef STACK_PROTECTOR
+#include <kernel/vm.h>
+#include <arch/usercopy.h>
+#include <err.h>
+#endif
 
 /* early stack */
-uint8_t _kstack[PAGE_SIZE] __ALIGNED(8);
+uint8_t _kstack[PAGE_SIZE * SMP_MAX_CPUS] __ALIGNED(8);
 
 /* save a pointer to the multiboot information coming in from whoever called us */
 /* make sure it lives in .data to avoid it being wiped out by bss clearing */
 __SECTION(".data") void *_multiboot_info;
 
 /* main tss */
-static tss_t system_tss;
+static tss_t system_tss[SMP_MAX_CPUS];
+x86_global_states_t global_states[SMP_MAX_CPUS];
+volatile int cpu_waken_up = 0;
+extern uint64_t __tss_end;
+extern void x86_syscall(void);
+
+#ifdef STACK_PROTECTOR
+typedef struct stack_guard_segment {
+    /* clang hardcodes the stack cookie offset as 0x28 on x86-64 */
+    uint8_t padding[40];
+    uint64_t stack_guard;
+} stack_guard_segment_t;
+
+#define STACK_GUARD_SEGMENT_SIZE 0x30
+void *start_fs_base = NULL;
+#endif
+
+static void init_global_state(x86_global_states_t *states, uint cpu)
+{
+    states->cur_thread    = NULL;
+    states->syscall_stack = 0;
+
+    write_msr(X86_MSR_GS_BASE, (uint64_t)states);
+}
+
+void *get_tss_base(void)
+{
+    volatile uint cpu = arch_curr_cpu_num();
+    ASSERT(cpu < SMP_MAX_CPUS);
+
+    if (cpu < SMP_MAX_CPUS)
+        return &system_tss[cpu];
+    else
+        return NULL;
+}
 
+static void arch_set_tss_segment_percpu(void)
+{
+    uint64_t addr;
+    /* Get system tss segment */
+    tss_t *tss_base = get_tss_base();
+    uint cpu_id = arch_curr_cpu_num();
+
+    ASSERT(tss_base);
+    ASSERT(cpu_id < SMP_MAX_CPUS);
+
+    addr = (uint64_t)&__tss_end - (uint64_t)(cpu_id * PAGE_SIZE);
+
+    tss_base->rsp0 = addr;
+    x86_set_syscall_stack(addr);
+}
+
+static void arch_setup_syscall_percpu(void)
+{
+    /* msr_id,low,hi */
+    write_msr(SYSENTER_CS_MSR, CODE_64_SELECTOR);
+    write_msr(SYSENTER_ESP_MSR, x86_get_syscall_stack());
+    write_msr(SYSENTER_EIP_MSR, (uint64_t)(x86_syscall));
+}
 void arch_early_init(void)
 {
+    seg_sel_t sel = 0;
+    uint cpu_id = 1;
+
+    __asm__ __volatile__ (
+            "lock xaddq %%rax, (%%rdx)"
+            : "=a" (cpu_id)
+            : "a" (cpu_id), "d"(&cpu_waken_up));
+
+    /*
+     * At this point, BSP has set up current thread in global state,
+     * init global states of AP(s) only.
+     */
+    if (0 != cpu_id)
+        init_global_state(&global_states[cpu_id], cpu_id);
+
+    if (check_fsgsbase_avail())
+        x86_set_cr4(x86_get_cr4() | X86_CR4_FSGSBASE);
+    sel = (seg_sel_t)(cpu_id << 4);
+    sel += TSS_SELECTOR;
+
     /* enable caches here for now */
     clear_in_cr0(X86_CR0_NW | X86_CR0_CD);
 
-    memset(&system_tss, 0, sizeof(system_tss));
+    memset(&system_tss[cpu_id], 0, sizeof(tss_t));
 
-#if ARCH_X86_32
-    system_tss.esp0 = 0;
-    system_tss.ss0 = DATA_SELECTOR;
-    system_tss.ss1 = 0;
-    system_tss.ss2 = 0;
-    system_tss.eflags = 0x00003002;
-    system_tss.bitmap = offsetof(tss_32_t, tss_bitmap);
-    system_tss.trace = 1; // trap on hardware task switch
-#endif
-
-    set_global_desc(TSS_SELECTOR, &system_tss, sizeof(system_tss), 1, 0, 0, SEG_TYPE_TSS, 0, 0);
-    x86_ltr(TSS_SELECTOR);
+    set_global_desc(sel,
+            &system_tss[cpu_id],
+            sizeof(tss_t),
+            1,
+            0,
+            0,
+            SEG_TYPE_TSS,
+            0,
+            0);
+    x86_ltr(sel);
 
     x86_mmu_early_init();
 }
@@ -71,6 +153,9 @@ void arch_init(void)
 {
     x86_mmu_init();
 
+    arch_set_tss_segment_percpu();
+    arch_setup_syscall_percpu();
+
 #ifdef X86_WITH_FPU
     fpu_init();
 #endif
@@ -83,37 +168,84 @@ void arch_chain_load(void *entry, ulong arg0, ulong arg1, ulong arg2, ulong arg3
 
 void arch_enter_uspace(vaddr_t entry_point, vaddr_t user_stack_top, uint32_t flags, ulong arg0)
 {
-    PANIC_UNIMPLEMENTED;
-#if 0
-    DEBUG_ASSERT(IS_ALIGNED(user_stack_top, 16));
+    register uint64_t sp_usr  = user_stack_top;
+    register uint64_t entry = entry_point;
+    register uint64_t code_seg = USER_CODE_64_SELECTOR | USER_RPL;
+    register uint64_t data_seg = USER_DATA_64_SELECTOR | USER_RPL;
+    register uint64_t usr_flags = USER_EFLAGS;
+#ifdef STACK_PROTECTOR
+    static uint32_t tid = 0; /* trusty thread count */
+    thread_t *cur_thread = get_current_thread();
+    status_t status;
+    int ret = 0;
+    stack_guard_segment_t stack_guard_segment;
+
+    /* allocate one page memory for fs base. Each TA gets its
+       fs base with the offset by tid */
+    if(!start_fs_base) {
+        start_fs_base = memalign(PAGE_SIZE, PAGE_SIZE);
+    }
+
+    status = vmm_alloc_physical(cur_thread->aspace, "fs_base",
+                             PAGE_SIZE, (void **)&cur_thread->arch.fs_base,
+                             PAGE_SIZE_SHIFT, vaddr_to_paddr(start_fs_base),
+                             0,
+                             ARCH_MMU_FLAG_PERM_NO_EXECUTE |
+                             ARCH_MMU_FLAG_PERM_USER);
+    if(status != NO_ERROR) {
+        dprintf(0, "arch_enter_uspace: failed to alloc fs_base\n");
+    } else {
+        assert(tid * STACK_GUARD_SEGMENT_SIZE < PAGE_SIZE);
+        cur_thread->arch.fs_base += tid++ * STACK_GUARD_SEGMENT_SIZE;
+
+        ret = get_rand_64(&stack_guard_segment.stack_guard);
+        /* in case get_rand_64 fail, use the address instead */
+        if(ret == 0) {
+            stack_guard_segment.stack_guard = (uint64_t)&stack_guard_segment.stack_guard;
+        }
+
+        status = arch_copy_to_user(cur_thread->arch.fs_base,
+                                &stack_guard_segment, sizeof(stack_guard_segment));
+        if(status) {
+            dprintf(CRITICAL, "failed to copy to fs base!\n");
+        }
+    }
+
+    write_msr(X86_MSR_FS_BASE, (uint64_t)cur_thread->arch.fs_base);
+#endif
 
-    thread_t *ct = get_current_thread();
+    __asm__ __volatile__ (
+            "pushq %0   \n\t"
+            "pushq %1   \n\t"
+            "pushq %2   \n\t"
+            "pushq %3   \n\t"
+            "pushq %4   \n\t"
+            "pushq %0   \n\t"
+            "popq %%rax \n\t"
+            "swapgs \n\t"
+            "movw %%ax, %%ds    \n\t"
+            "movw %%ax, %%es    \n\t"
+#if !defined(STACK_PROTECTOR)
+            "movw %%ax, %%fs    \n\t"
+#endif
+            "movw %%ax, %%gs    \n\t"
+            "iretq"
+            :
+            :"r"(data_seg),"r"(sp_usr),"r"(usr_flags),"r"(code_seg),"r"(entry));
+    __UNREACHABLE;
+}
 
-    vaddr_t kernel_stack_top = (uintptr_t)ct->stack + ct->stack_size;
-    kernel_stack_top = ROUNDDOWN(kernel_stack_top, 16);
+void arch_syscall_stack_switch(vaddr_t new_thread_syscall)
+{
+    tss_t *tss_base= get_tss_base();
 
-    /* set up a default spsr to get into 64bit user space:
-     * zeroed NZCV
-     * no SS, no IL, no D
-     * all interrupts enabled
-     * mode 0: EL0t
-     */
-    uint32_t spsr = 0;
-
-    arch_disable_ints();
-
-    asm volatile(
-        "mov    sp, %[kstack];"
-        "msr    sp_el0, %[ustack];"
-        "msr    elr_el1, %[entry];"
-        "msr    spsr_el1, %[spsr];"
-        "eret;"
-        :
-        : [ustack]"r"(user_stack_top),
-        [kstack]"r"(kernel_stack_top),
-        [entry]"r"(entry_point),
-        [spsr]"r"(spsr)
-        : "memory");
-    __UNREACHABLE;
-#endif
+    ASSERT(tss_base);
+
+    if (0 == new_thread_syscall) {
+        tss_base->rsp0 = x86_get_syscall_stack();
+    } else {
+        tss_base->rsp0 = new_thread_syscall;
+    }
+
+    write_msr(SYSENTER_ESP_MSR, tss_base->rsp0);
 }
diff --git a/arch/x86/cache.c b/arch/x86/cache.c
index 8f5cf5b9..1e24b64a 100644
--- a/arch/x86/cache.c
+++ b/arch/x86/cache.c
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
+ * Copyright (c) 2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -26,3 +27,15 @@
 void arch_sync_cache_range(addr_t start, size_t len)
 {
 }
+
+void arch_clean_cache_range(addr_t start, size_t len)
+{
+}
+
+void arch_clean_invalidate_cache_range(addr_t start, size_t len)
+{
+}
+
+void arch_invalidate_cache_range(addr_t start, size_t len)
+{
+}
diff --git a/arch/x86/descriptor.c b/arch/x86/descriptor.c
index b9424f4a..08d8d0f1 100644
--- a/arch/x86/descriptor.c
+++ b/arch/x86/descriptor.c
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
+ * Copyright (c) 2017-2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -24,45 +25,64 @@
 #include <compiler.h>
 #include <arch/x86/descriptor.h>
 
-/* not the best way to do this, but easy for now */
-typedef struct {
-    uint16_t limit_15_0;
-    uint16_t base_15_0;
-    uint8_t base_23_16;
+typedef union {
+    struct {
+        uint16_t limit_15_0;
+        uint16_t base_15_0;
+        uint8_t base_23_16;
 
-    uint8_t type : 4;
-    uint8_t s : 1;
-    uint8_t dpl : 2;
-    uint8_t p : 1;
+        uint8_t type : 4;
+        uint8_t s : 1;
+        uint8_t dpl : 2;
+        uint8_t p : 1;
 
-    uint8_t limit_19_16 : 4;
-    uint8_t avl : 1;
-    uint8_t reserved_0 : 1;
-    uint8_t d_b : 1;
-    uint8_t g : 1;
+        uint8_t limit_19_16 : 4;
+        uint8_t avl : 1;
+        uint8_t reserved_0 : 1;
+        uint8_t d_b : 1;
+        uint8_t g : 1;
 
-    uint8_t base_31_24;
+        uint8_t base_31_24;
+    } __PACKED seg_desc_legacy;
+    struct {
+        uint32_t base_32_63;
+        uint16_t rsvd_1;
+        uint16_t rsvd_2;
+    } __PACKED seg_desc_64_t;
 } __PACKED seg_desc_t;
 
 extern seg_desc_t _gdt[];
 
-void set_global_desc(seg_sel_t sel, void *base, uint32_t limit,
-                     uint8_t present, uint8_t ring, uint8_t sys, uint8_t type, uint8_t gran, uint8_t bits)
+void set_global_desc(seg_sel_t sel,
+        void *base,
+        uint32_t limit,
+        uint8_t present,
+        uint8_t ring,
+        uint8_t sys,
+        uint8_t type,
+        uint8_t gran,
+        uint8_t bits)
 {
-    // convert selector into index
+    /* convert selector into index */
     uint16_t index = sel >> 3;
 
-    _gdt[index].limit_15_0 = limit & 0x0000ffff;
-    _gdt[index].limit_19_16 = (limit & 0x000f0000) >> 16;
+    _gdt[index].seg_desc_legacy.limit_15_0  = limit & 0x0000ffff;
+    _gdt[index].seg_desc_legacy.limit_19_16 = (limit & 0x000f0000) >> 16;
 
-    _gdt[index].base_15_0 = ((uintptr_t) base) & 0x0000ffff;
-    _gdt[index].base_23_16 = (((uintptr_t) base) & 0x00ff0000) >> 16;
-    _gdt[index].base_31_24 = ((uintptr_t) base) >> 24;
+    _gdt[index].seg_desc_legacy.base_15_0   = ((uint64_t) base) & 0x0000ffff;
+    _gdt[index].seg_desc_legacy.base_23_16  = (((uint64_t) base) & 0x00ff0000) >> 16;
+    _gdt[index].seg_desc_legacy.base_31_24  = ((uint64_t) base) >> 24;
 
-    _gdt[index].type = type & 0x0f; // segment type
-    _gdt[index].p = present != 0;   // present
-    _gdt[index].dpl = ring & 0x03;  // descriptor privilege level
-    _gdt[index].g = gran != 0;      // granularity
-    _gdt[index].s = sys != 0;       // system / non-system
-    _gdt[index].d_b = bits != 0;    // 16 / 32 bit
+    _gdt[index].seg_desc_legacy.type = type & 0x0f;    // segment type
+    _gdt[index].seg_desc_legacy.p    = present != 0;   // present
+    _gdt[index].seg_desc_legacy.dpl  = ring & 0x03;    // descriptor privilege level
+    _gdt[index].seg_desc_legacy.g    = gran != 0;      // granularity
+    _gdt[index].seg_desc_legacy.s    = sys != 0;       // system / non-system
+    _gdt[index].seg_desc_legacy.d_b  = bits != 0;      // 16 / 32 bit
+
+    /* high bits of TSS */
+    if ((sel >= TSS_SELECTOR) && (sel <= TSS_SELECTOR + (SMP_MAX_CPUS - 1) * 0x10)) {
+        index = (sel + 8) >> 3;
+        _gdt[index].seg_desc_64_t.base_32_63 = ((uint64_t)base >> 32) & 0xffffffff;
+    }
 }
diff --git a/arch/x86/faults.c b/arch/x86/faults.c
index b617eb56..7b82bc5e 100644
--- a/arch/x86/faults.c
+++ b/arch/x86/faults.c
@@ -77,7 +77,7 @@ static void dump_fault_frame(x86_iframe_t *frame)
 
 static void exception_die(x86_iframe_t *frame, const char *msg)
 {
-    dprintf(CRITICAL, msg);
+    dprintf(CRITICAL, "%s\n", msg);
     dump_fault_frame(frame);
 
     for (;;) {
@@ -196,12 +196,6 @@ void x86_exception_handler(x86_iframe_t *frame)
             x86_pfe_handler(frame);
             break;
 
-        case INT_DEV_NA_EX:
-#if X86_WITH_FPU
-            fpu_dev_na_handler();
-#endif
-            break;
-
         case INT_MF: { /* x87 floating point math fault */
             uint16_t fsw;
             __asm__ __volatile__("fnstsw %0" : "=m" (fsw));
diff --git a/arch/x86/fpu.c b/arch/x86/fpu.c
index 414fbfa9..ad38d740 100644
--- a/arch/x86/fpu.c
+++ b/arch/x86/fpu.c
@@ -39,6 +39,7 @@
 #define ECX_SSSE3   (0x00000001 << 9)
 #define ECX_SSE4_1  (0x00000001 << 19)
 #define ECX_SSE4_2  (0x00000001 << 20)
+#define ECX_OSXSAVE (0x00000001 << 27)
 #define EDX_FXSR    (0x00000001 << 24)
 #define EDX_SSE     (0x00000001 << 25)
 #define EDX_SSE2    (0x00000001 << 26)
@@ -53,11 +54,16 @@
 
 #define FXSAVE_CAP(ecx, edx) ((edx & EDX_FXSR) != 0)
 
-static int fp_supported;
-static thread_t *fp_owner;
+#define OSXSAVE_CAP(ecx, edx) ((ecx & ECX_OSXSAVE) !=0 )
+
+static int fp_supported = 0;
 
 /* FXSAVE area comprises 512 bytes starting with 16-byte aligned */
-static uint8_t __ALIGNED(16) fpu_init_states[512]= {0};
+typedef struct _fpu_init_state {
+    uint8_t fpu_states[512];
+}fpu_init_states_t;
+
+static fpu_init_states_t __ALIGNED(16) fpu_init_states[SMP_MAX_CPUS];
 
 static void get_cpu_cap(uint32_t *ecx, uint32_t *edx)
 {
@@ -72,6 +78,7 @@ void fpu_init(void)
     uint32_t ecx = 0, edx = 0;
     uint16_t fcw;
     uint32_t mxcsr;
+    uint     cpu_id = arch_curr_cpu_num();
 
 #ifdef ARCH_X86_64
     uint64_t x;
@@ -79,9 +86,6 @@ void fpu_init(void)
     uint32_t x;
 #endif
 
-    fp_supported = 0;
-    fp_owner = NULL;
-
     get_cpu_cap(&ecx, &edx);
 
     if (!FPU_CAP(ecx, edx) || !SSE_CAP(ecx, edx) || !FXSAVE_CAP(ecx, edx))
@@ -113,7 +117,9 @@ void fpu_init(void)
     x = x86_get_cr4();
     x |= X86_CR4_OSXMMEXPT;
     x |= X86_CR4_OSFXSR;
-    x &= ~X86_CR4_OSXSAVE;
+    if(OSXSAVE_CAP(ecx, edx)) {
+        x |= X86_CR4_OSXSAVE;
+    }
     x86_set_cr4(x);
 
     __asm__ __volatile__("stmxcsr %0" : "=m" (mxcsr));
@@ -127,16 +133,21 @@ void fpu_init(void)
     __asm__ __volatile__("ldmxcsr %0" : : "m" (mxcsr));
 
     /* save fpu initial states, and used when new thread creates */
-    __asm__ __volatile__("fxsave %0" : "=m" (fpu_init_states));
+    __asm__ __volatile__("fxsave %0" : "=m" (fpu_init_states[cpu_id].fpu_states));
 
-    x86_set_cr0(x86_get_cr0() | X86_CR0_TS);
     return;
 }
 
 void fpu_init_thread_states(thread_t *t)
 {
+    uint cpu_id = arch_curr_cpu_num();
+    status_t ret = 0;
+
     t->arch.fpu_states = (vaddr_t *)ROUNDUP(((vaddr_t)t->arch.fpu_buffer), 16);
-    memcpy(t->arch.fpu_states, fpu_init_states, sizeof(fpu_init_states));
+    ret = memcpy_s(t->arch.fpu_states, sizeof(fpu_init_states_t),
+                   (const void *)fpu_init_states[cpu_id].fpu_states, sizeof(fpu_init_states_t));
+    if(ret)
+        panic("memcpy_s fails in fpu_init_thread_states()\n");
 }
 
 void fpu_context_switch(thread_t *old_thread, thread_t *new_thread)
@@ -144,34 +155,14 @@ void fpu_context_switch(thread_t *old_thread, thread_t *new_thread)
     if (fp_supported == 0)
         return;
 
-    if (new_thread != fp_owner)
-        x86_set_cr0(x86_get_cr0() | X86_CR0_TS);
-    else
-        x86_set_cr0(x86_get_cr0() & ~X86_CR0_TS);
-
-    return;
-}
-
-void fpu_dev_na_handler(void)
-{
-    thread_t *self;
-
-    x86_set_cr0(x86_get_cr0() & ~X86_CR0_TS);
-
-    if (fp_supported == 0)
-        return;
-
-    self = get_current_thread();
-
-    LTRACEF("owner %p self %p\n", fp_owner, self);
-    if ((fp_owner != NULL) && (fp_owner != self)) {
-        __asm__ __volatile__("fxsave %0" : "=m" (*fp_owner->arch.fpu_states));
-        __asm__ __volatile__("fxrstor %0" : : "m" (*self->arch.fpu_states));
+    if (old_thread) {
+        __asm__ __volatile__("fxsave %0" : "=m" (*old_thread->arch.fpu_states));
     }
+    __asm__ __volatile__("fxrstor %0" : : "m" (*new_thread->arch.fpu_states));
 
-    fp_owner = self;
     return;
 }
+
 #endif
 
 /* End of file */
diff --git a/arch/x86/gdt.S b/arch/x86/gdt.S
index c70dfe4b..c37e20de 100644
--- a/arch/x86/gdt.S
+++ b/arch/x86/gdt.S
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  * Copyright (c) 2016 Travis Geiselbrecht
  *
  * Permission is hereby granted, free of charge, to any person obtaining
@@ -51,79 +51,96 @@ DATA(_gdt)
     .int 0
     .int 0
 
-/* ring 0 descriptors */
 .set codesel_32, . - _gdt
 _code_32_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b10011010       /* P(1) DPL(00) S(1) 1 C(0) R(1) A(0) */
     .byte  0b11001111       /* G(1) D(1) 0 0 limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
 
 .set datasel, . - _gdt
 _data_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b10010010       /* P(1) DPL(00) S(1) 0 E(0) W(1) A(0) */
     .byte  0b11001111       /* G(1) B(1) 0 0 limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
 
 .set user_codesel_32, . - _gdt
 _user_code_32_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b11111010       /* P(1) DPL(11) S(1) 1 C(0) R(1) A(0) */
     .byte  0b11001111       /* G(1) D(1) 0 0 limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
 
 
 .set user_datasel, . - _gdt
 _user_data_32_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b11110010       /* P(1) DPL(11) S(1) 0 E(0) W(1) A(0) */
     .byte  0b11001111       /* G(1) B(1) 0 0 limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
 
 .set codesel_64, . - _gdt
 _code_64_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b10011010       /* P(1) DPL(00) S(1) 1 C(0) R(1) A(0) */
     .byte  0b10101111       /* G(1) D(0) L(1) AVL(0) limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
 
 .set datasel_64, . - _gdt
 _data_64_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
-    .byte  0b10010010       /* P(1) DPL(00) S(1) 1 C(0) R(1) A(0) */
+    .byte  0x00             /* base 23:16 */
+    .byte  0b10010010       /* P(1) DPL(00) S(1) 1 E(0) W(1) A(0) */
     .byte  0b11001111       /* G(1) B(1) 0 AVL(0) limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
+
+.set user_codesel_compat, . - _gdt
+_user_code_compat_gde:
+    .short 0xffff           /* limit 15:00 */
+    .short 0x0000           /* base 15:00 */
+    .byte  0x00             /* base 23:16 */
+    .byte  0b11111010       /* P(1) DPL(11) S(1) 1 C(0) R(1) A(0) */
+    .byte  0b11001111       /* G(1) D(1) L(0) AVL(0) limit 19:16 */
+    .byte  0x0              /* base 31:24 */
+
+.set user_datasel_compat, . - _gdt
+_user_data_compat_gde:
+    .short 0xffff           /* limit 15:00 */
+    .short 0x0000           /* base 15:00 */
+    .byte  0x00             /* base 23:16 */
+    .byte  0b11110010       /* P(1) DPL(11) S(1) 0 E(0) W(1) A(0) */
+    .byte  0b11001111       /* G(1) B(1) 0 0 limit 19:16 */
+    .byte  0x0              /* base 31:24 */
 
 .set user_codesel_64, . - _gdt
 _user_code_64_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b11111010       /* P(1) DPL(11) S(1) 1 C(0) R(1) A(0) */
-    .byte  0b10101111       /* G(1) D(1) L(0) AVL(0) limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0b10101111       /* G(1) D(0) L(1) AVL(0) limit 19:16 */
+    .byte  0x0              /* base 31:24 */
 
 .set user_datasel_64, . - _gdt
 _user_data_64_gde:
     .short 0xffff           /* limit 15:00 */
     .short 0x0000           /* base 15:00 */
-    .byte  0x00         /* base 23:16 */
+    .byte  0x00             /* base 23:16 */
     .byte  0b11110010       /* P(1) DPL(11) S(1) 0 E(0) W(1) A(0) */
     .byte  0b11001111       /* G(1) B(1) 0 0 limit 19:16 */
-    .byte  0x0          /* base 31:24 */
+    .byte  0x0              /* base 31:24 */
 
 /* TSS descriptor */
 .set tsssel, . - _gdt
@@ -131,9 +148,9 @@ _tss_gde:
     .short 0                /* limit 15:00 */
     .short 0                /* base 15:00 */
     .byte  0                /* base 23:16 */
-    .byte  0x89             /* P(1) DPL(11) 0 10 B(0) 1 */
-    .byte  0x80             /* G(0) 0 0 AVL(0) limit 19:16 */
-    .byte  0               /* base 31:24 */
+    .byte  0x89             /* P(1) DPL(0) S(0) TYPE(9) */
+    .byte  0x80             /* G(1) 0 0 AVL(0) limit 19:16 */
+    .byte  0                /* base 31:24 */
     .quad  0x0000000000000000
 
 DATA(_gdt_end)
diff --git a/arch/x86/include/arch/arch_ops.h b/arch/x86/include/arch/arch_ops.h
index 5ad97b13..b7860752 100644
--- a/arch/x86/include/arch/arch_ops.h
+++ b/arch/x86/include/arch/arch_ops.h
@@ -1,6 +1,7 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
  * Copyright (c) 2014 Travis Geiselbrecht
+ * Copyright (c) 2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -28,6 +29,7 @@
 #ifndef ASSEMBLY
 
 #include <arch/x86.h>
+#include <arch/x86/mp.h>
 
 /* override of some routines */
 static inline void arch_enable_ints(void)
@@ -42,6 +44,16 @@ static inline void arch_disable_ints(void)
     CF;
 }
 
+static inline void arch_enable_fiqs(void)
+{
+    CF;
+}
+
+static inline void arch_disable_fiqs(void)
+{
+    CF;
+}
+
 static inline bool arch_ints_disabled(void)
 {
     x86_flags_t state;
@@ -89,9 +101,30 @@ static inline int atomic_swap(volatile int *ptr, int val)
 }
 
 
+#if ARCH_X86_32
 static inline int atomic_and(volatile int *ptr, int val) { return _atomic_and(ptr, val); }
 static inline int atomic_or(volatile int *ptr, int val) { return _atomic_or(ptr, val); }
 static inline int atomic_cmpxchg(volatile int *ptr, int oldval, int newval) { return _atomic_cmpxchg(ptr, oldval, newval); }
+#elif ARCH_X86_64
+static inline int atomic_and(volatile int *ptr, long val) { return _atomic_and(ptr, val); }
+static inline int atomic_or(volatile int *ptr, long val) { return _atomic_or(ptr, val); }
+static inline int atomic_cmpxchg(volatile int *ptr, int oldval, uint64_t newval)
+{
+#if USE_GCC_ATOMICS
+    __atomic_compare_exchange_n(ptr, &oldval, newval, false,
+            __ATOMIC_RELAXED, __ATOMIC_RELAXED);
+
+#else
+    __asm__ volatile(
+        "lock cmpxchgq  %[newval], %[ptr];"
+        : "=a" (oldval),  "=m" (*ptr)
+        : "a" (oldval), [newval]"r" (newval), [ptr]"m" (*ptr)
+        : "memory"
+    );
+#endif
+    return oldval;
+}
+#endif
 
 static inline uint32_t arch_cycle_count(void)
 {
@@ -101,17 +134,14 @@ static inline uint32_t arch_cycle_count(void)
     return timestamp;
 }
 
-/* use a global pointer to store the current_thread */
-extern struct thread *_current_thread;
-
 static inline struct thread *get_current_thread(void)
 {
-    return _current_thread;
+    return (struct thread *)x86_get_current_thread();
 }
 
 static inline void set_current_thread(struct thread *t)
 {
-    _current_thread = t;
+    x86_set_current_thread((void *)t);
 }
 
 static inline uint arch_curr_cpu_num(void)
@@ -119,4 +149,19 @@ static inline uint arch_curr_cpu_num(void)
     return 0;
 }
 
+#define mb()        __asm__ volatile ("mfence":::"memory");
+#define wmb()       __asm__ volatile ("sfence":::"memory");
+#define rmb()       __asm__ volatile ("lfence":::"memory");
+
+#ifdef WITH_SMP
+#define smp_mb()    mb()
+#define smp_wmb()   wmb()
+#define smp_rmb()   rmb()
+#else
+#define smp_mb()    mb()
+#define smp_wmb()   wmb()
+#define smp_rmb()   rmb()
+#endif
+
+
 #endif // !ASSEMBLY
diff --git a/arch/x86/include/arch/arch_thread.h b/arch/x86/include/arch/arch_thread.h
index bd970a47..45cb4bdf 100644
--- a/arch/x86/include/arch/arch_thread.h
+++ b/arch/x86/include/arch/arch_thread.h
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -25,11 +25,32 @@
 
 #include <sys/types.h>
 
+#define CF_MASK         0x0000
+#define PA_MASK         0x0001
+#define PF_MASK         0x0020
+#define TF_MASK         0x0100
+#define IF_MASK         0x0200
+#define IOPL_MASK       0x3000
+#define NTi_MASK        0x4000
+#define RSVD            0x0002
+
+/* 0x3202 */
+#define USER_EFLAGS (IF_MASK|IOPL_MASK|RSVD)
+
+/* SYSCALL Handling */
+#define SYSENTER_CS_MSR    0x174
+#define SYSENTER_ESP_MSR   0x175
+#define SYSENTER_EIP_MSR   0x176
+
 struct arch_thread {
     vaddr_t sp;
 #if X86_WITH_FPU
     vaddr_t *fpu_states;
     uint8_t fpu_buffer[512 + 16];
 #endif
+#ifdef STACK_PROTECTOR
+    /* fs base of TA. It's user address */
+    vaddr_t fs_base;
+#endif
 };
 
diff --git a/arch/x86/include/arch/aspace.h b/arch/x86/include/arch/aspace.h
index 3c82778c..a494a661 100644
--- a/arch/x86/include/arch/aspace.h
+++ b/arch/x86/include/arch/aspace.h
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2016 Travis Geiselbrecht
+ * Copyright (c) 2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -23,11 +24,14 @@
 #pragma once
 
 #include <compiler.h>
+#include <sys/types.h>
 
 __BEGIN_CDECLS
 
 struct arch_aspace {
-    // nothing for now, does not support address spaces other than the kernel
+    paddr_t page_table;
+    vaddr_t base;
+    size_t  size;
 };
 
 __END_CDECLS
diff --git a/lib/libc/string/arch/x86/memcpy.S b/arch/x86/include/arch/local_apic.h
similarity index 74%
rename from lib/libc/string/arch/x86/memcpy.S
rename to arch/x86/include/arch/local_apic.h
index 84aec44a..9c38bea9 100644
--- a/lib/libc/string/arch/x86/memcpy.S
+++ b/arch/x86/include/arch/local_apic.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2009 Corey Tabaka
+ * Copyright (c) 2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -20,19 +20,17 @@
  * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
  * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
-#include <asm.h>
+#pragma once
 
-/* TODO: */
-
-.text
-.align 2
-
-/* void bcopy(const void *src, void *dest, size_t n); */
-FUNCTION(bcopy)
-    ret
-
-/* void *memcpy(void *dest, const void *src, size_t n); */
-FUNCTION(memmove)
-FUNCTION(memcpy)
-    ret
+#include <compiler.h>
+#include <sys/types.h>
+#include <stdlib.h>
+#include <stdbool.h>
 
+void lapic_id_init(void);
+void local_apic_init(void);
+void lapic_eoi(void);
+void lapic_software_disable(void);
+bool send_self_ipi(uint32_t vector);
+void local_apic_reinit(void);
+bool local_apic_vector_in_service(uint32_t vector);
diff --git a/arch/x86/include/arch/spinlock.h b/arch/x86/include/arch/spinlock.h
index b6411132..203c80a7 100644
--- a/arch/x86/include/arch/spinlock.h
+++ b/arch/x86/include/arch/spinlock.h
@@ -1,5 +1,6 @@
 /*
  * Copyright (c) 2015 Travis Geiselbrecht
+ * Copyright (c) 2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -28,6 +29,10 @@
 
 #define SPIN_LOCK_INITIAL_VALUE (0)
 
+/* flags are unused on x86 */
+#define ARCH_DEFAULT_SPIN_LOCK_FLAG_INTERRUPTS  0
+#define SPIN_LOCK_FLAG_IRQ    ARCH_DEFAULT_SPIN_LOCK_FLAG_INTERRUPTS
+#define SPIN_LOCK_FLAG_IRQ_FIQ    ARCH_DEFAULT_SPIN_LOCK_FLAG_INTERRUPTS
 typedef unsigned long spin_lock_t;
 
 typedef x86_flags_t spin_lock_saved_state_t;
diff --git a/arch/x86/include/arch/x86.h b/arch/x86/include/arch/x86.h
index 2e7087e9..4f07a393 100644
--- a/arch/x86/include/arch/x86.h
+++ b/arch/x86/include/arch/x86.h
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  * Copyright (c) 2016 Travis Geiselbrecht
  *
  * Permission is hereby granted, free of charge, to any person obtaining
@@ -136,24 +136,28 @@ typedef tss_32_t tss_t;
 typedef tss_64_t tss_t;
 #endif
 
-#define X86_CR0_PE 0x00000001 /* protected mode enable */
-#define X86_CR0_MP 0x00000002 /* monitor coprocessor */
-#define X86_CR0_EM 0x00000004 /* emulation */
-#define X86_CR0_TS 0x00000008 /* task switched */
-#define X86_CR0_NE 0x00000020 /* enable x87 exception */
-#define X86_CR0_WP 0x00010000 /* supervisor write protect */
-#define X86_CR0_NW 0x20000000 /* not write-through */
-#define X86_CR0_CD 0x40000000 /* cache disable */
-#define X86_CR0_PG 0x80000000 /* enable paging */
-#define X86_CR4_PAE 0x00000020 /* PAE paging */
-#define X86_CR4_OSFXSR 0x00000200 /* os supports fxsave */
-#define X86_CR4_OSXMMEXPT 0x00000400 /* os supports xmm exception */
-#define X86_CR4_OSXSAVE 0x00040000 /* os supports xsave */
-#define X86_CR4_SMEP 0x00100000 /* SMEP protection enabling */
-#define X86_CR4_SMAP 0x00200000 /* SMAP protection enabling */
-#define x86_EFER_NXE 0x00000800 /* to enable execute disable bit */
-#define x86_MSR_EFER 0xc0000080 /* EFER Model Specific Register id */
-#define X86_CR4_PSE 0xffffffef /* Disabling PSE bit in the CR4 */
+#define X86_CR0_PE              0x00000001 /* protected mode enable */
+#define X86_CR0_MP              0x00000002 /* monitor coprocessor */
+#define X86_CR0_EM              0x00000004 /* emulation */
+#define X86_CR0_TS              0x00000008 /* task switched */
+#define X86_CR0_NE              0x00000020 /* enable x87 exception */
+#define X86_CR0_WP              0x00010000 /* supervisor write protect */
+#define X86_CR0_NW              0x20000000 /* not write-through */
+#define X86_CR0_CD              0x40000000 /* cache disable */
+#define X86_CR0_PG              0x80000000 /* enable paging */
+#define X86_CR4_PAE             0x00000020 /* PAE paging */
+#define X86_CR4_OSFXSR          0x00000200 /* os supports fxsave */
+#define X86_CR4_OSXMMEXPT       0x00000400 /* os supports xmm exception */
+#define X86_CR4_FSGSBASE        0x00010000 /* FSGSBASE enable bit */
+#define X86_CR4_OSXSAVE         0x00040000 /* os supports xsave */
+#define X86_CR4_SMEP            0x00100000 /* SMEP protection enabling */
+#define X86_CR4_SMAP            0x00200000 /* SMAP protection enabling */
+#define X86_EFER_NXE            0x00000800 /* to enable execute disable bit */
+#define X86_MSR_EFER            0xc0000080 /* EFER Model Specific Register id */
+#define X86_MSR_FS_BASE         0xc0000100 /* Map of base address of FS */
+#define X86_MSR_GS_BASE         0xc0000101 /* Map of base address of GS */
+#define X86_MSR_KRNL_GS_BASE    0xc0000102 /* Swap target of base address of GS */
+#define X86_CR4_PSE             0xffffffef /* Disabling PSE bit in the CR4 */
 
 #if ARCH_X86_32
 static inline void set_in_cr0(uint32_t mask)
@@ -497,7 +501,7 @@ static inline uint32_t check_smep_avail(void)
         "cpuid \n\t"
         :"=b" (reg_b)
         :"a" (reg_a),"c" (reg_c));
-    return ((reg_b>>0x06) & 0x1);
+    return ((reg_b>>0x07) & 0x1);
 }
 
 static inline uint32_t check_smap_avail(void)
@@ -509,7 +513,7 @@ static inline uint32_t check_smap_avail(void)
         "cpuid \n\t"
         :"=b" (reg_b)
         :"a" (reg_a),"c" (reg_c));
-    return ((reg_b>>0x13) & 0x1);
+    return ((reg_b>>0x14) & 0x1);
 }
 #endif // ARCH_X86_32
 
@@ -828,7 +832,7 @@ static inline uint64_t check_smep_avail(void)
         "cpuid \n\t"
         :"=b" (reg_b)
         :"a" (reg_a),"c" (reg_c));
-    return ((reg_b>>0x06) & 0x1);
+    return ((reg_b>>0x07) & 0x1);
 }
 
 static inline uint64_t check_smap_avail(void)
@@ -840,9 +844,46 @@ static inline uint64_t check_smap_avail(void)
         "cpuid \n\t"
         :"=b" (reg_b)
         :"a" (reg_a),"c" (reg_c));
-    return ((reg_b>>0x13) & 0x1);
+    return ((reg_b>>0x14) & 0x1);
 }
 
+static inline uint64_t check_fsgsbase_avail(void)
+{
+    uint64_t reg_a = 0x07;
+    uint64_t reg_b = 0x0;
+    uint64_t reg_c = 0x0;
+    __asm__ __volatile__ (
+        "cpuid \n\t"
+        :"=b" (reg_b)
+        :"a" (reg_a),"c" (reg_c));
+    return (reg_b & 0x1);
+}
+
+static inline uint64_t x86_read_gs_with_offset(uintptr_t offset)
+{
+    uint64_t ret;
+    __asm__ __volatile__ (
+        "movq  %%gs:%1, %0"
+        :"=r" (ret)
+        :"m" (*(uint64_t *)offset));
+    return ret;
+}
+
+static inline void x86_write_gs_with_offset(uint64_t offset, uint64_t val)
+{
+    __asm__ __volatile__ (
+        "movq  %0, %%gs:%1"
+        :
+        :"ir" (val), "m" (*(uint64_t *)offset)
+        :"memory");
+}
+
+void *get_tss_base(void);
+
+#ifdef STACK_PROTECTOR
+int get_rand_64(uint64_t* val_ret);
+#endif
+
 #endif // ARCH_X86_64
 
 __END_CDECLS
diff --git a/arch/x86/include/arch/x86/descriptor.h b/arch/x86/include/arch/x86/descriptor.h
index 832c2c41..91e7c446 100644
--- a/arch/x86/include/arch/x86/descriptor.h
+++ b/arch/x86/include/arch/x86/descriptor.h
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
- * Copyright (c) 2014 Intel Corporation
+ * Copyright (c) 2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -29,18 +29,20 @@
 #define NULL_SELECTOR       0x00
 
 /********* x86 selectors *********/
-#define CODE_SELECTOR       0x08
-#define DATA_SELECTOR       0x10
-#define USER_CODE_32_SELECTOR   0x18
-#define USER_DATA_32_SELECTOR   0x20
+#define CODE_SELECTOR                   0x08
+#define DATA_SELECTOR                   0x10
+#define USER_CODE_32_SELECTOR           0x18
+#define USER_DATA_32_SELECTOR           0x20
 
 /******* x86-64 selectors ********/
-#define CODE_64_SELECTOR    0x28
-#define STACK_64_SELECTOR   0x30
-#define USER_CODE_64_SELECTOR   0x38
-#define USER_DATA_64_SELECTOR   0x40
+#define CODE_64_SELECTOR                0x28
+#define STACK_64_SELECTOR               0x30
+#define USER_CODE_COMPAT_SELECTOR       0x38
+#define USER_DATA_COMPAT_SELECTOR       0x40
+#define USER_CODE_64_SELECTOR           0x48
+#define USER_DATA_64_SELECTOR           0x50
 
-#define TSS_SELECTOR        0x48
+#define TSS_SELECTOR                    0x58
 
 /*
  * Descriptor Types
@@ -52,13 +54,22 @@
 #define SEG_TYPE_DATA_RW    0x2
 #define SEG_TYPE_CODE_RW    0xa
 
+#define USER_RPL            0x03
+
 #ifndef ASSEMBLY
 
 #include <sys/types.h>
 
 typedef uint16_t seg_sel_t;
 
-void set_global_desc(seg_sel_t sel, void *base, uint32_t limit,
-                     uint8_t present, uint8_t ring, uint8_t sys, uint8_t type, uint8_t gran, uint8_t bits);
+void set_global_desc(seg_sel_t sel,
+                        void *base,
+                        uint32_t limit,
+                        uint8_t present,
+                        uint8_t ring,
+                        uint8_t sys,
+                        uint8_t type,
+                        uint8_t gran,
+                        uint8_t bits);
 
 #endif
diff --git a/arch/x86/include/arch/x86/mmu.h b/arch/x86/include/arch/x86/mmu.h
index b71cdc62..9722a59a 100644
--- a/arch/x86/include/arch/x86/mmu.h
+++ b/arch/x86/include/arch/x86/mmu.h
@@ -1,6 +1,6 @@
 /*
  * Copyright (c) 2008 Travis Geiselbrecht
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -32,6 +32,8 @@
 #define X86_MMU_PG_P        0x001           /* P    Valid                   */
 #define X86_MMU_PG_RW       0x002           /* R/W  Read/Write              */
 #define X86_MMU_PG_U        0x004           /* U/S  User/Supervisor         */
+#define X86_MMU_PG_PWT      0x008           /* PWT  cache bit               */
+#define X86_MMU_PG_PCD      0x010           /* PCD  cacke bit               */
 #define X86_MMU_PG_PS       0x080           /* PS   Page size (0=4k,1=4M)   */
 #define X86_MMU_PG_PTE_PAT  0x080           /* PAT  PAT index               */
 #define X86_MMU_PG_G        0x100           /* G    Global                  */
@@ -52,7 +54,7 @@
 /* PAE mode */
 #define X86_PDPT_ADDR_MASK  (0x00000000ffffffe0ul)
 #define X86_PG_FRAME        (0xfffffffffffff000ul)
-#define X86_PHY_ADDR_MASK   (0x000ffffffffffffful)
+#define X86_PHY_ADDR_MASK   (0x000ffffffffffffful)  /* ISDM:4.1.4 MAXPHYADDR is at most 52 */
 #define X86_FLAGS_MASK      (0x0000000000000ffful)  /* NX Bit is ignored in the PAE mode */
 #define X86_PTE_NOT_PRESENT (0xFFFFFFFFFFFFFFFEul)
 #define X86_2MB_PAGE_FRAME  (0x000fffffffe00000ul)
diff --git a/arch/x86/include/arch/x86/mp.h b/arch/x86/include/arch/x86/mp.h
new file mode 100644
index 00000000..0e1cfbe4
--- /dev/null
+++ b/arch/x86/include/arch/x86/mp.h
@@ -0,0 +1,59 @@
+/*
+ * Copyright (c) 2018 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files
+ * (the "Software"), to deal in the Software without restriction,
+ * including without limitation the rights to use, copy, modify, merge,
+ * publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so,
+ * subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+#pragma once
+
+#include <arch/x86.h>
+
+typedef struct x86_global_state {
+#ifdef STACK_PROTECTOR
+    uint8_t padding[40];
+    uint64_t stack_guard;
+#endif
+    void *cur_thread;
+    uint64_t syscall_stack;
+} x86_global_states_t;
+
+#define CUR_THREAD_OFFSET       __offsetof(struct x86_global_state, cur_thread)
+#define SYSACLL_STACK_OFFSET    __offsetof(struct x86_global_state, syscall_stack)
+
+extern x86_global_states_t global_state[SMP_MAX_CPUS];
+
+static inline void * x86_get_current_thread(void)
+{
+    return (void *)x86_read_gs_with_offset(CUR_THREAD_OFFSET);
+}
+
+static inline void x86_set_current_thread(void *cur_thread)
+{
+    x86_write_gs_with_offset(CUR_THREAD_OFFSET, (uint64_t)cur_thread);
+}
+
+static inline uint64_t x86_get_syscall_stack(void)
+{
+    return x86_read_gs_with_offset(SYSACLL_STACK_OFFSET);
+}
+
+static inline void x86_set_syscall_stack(uint64_t stack)
+{
+    x86_write_gs_with_offset(SYSACLL_STACK_OFFSET, stack);
+}
diff --git a/arch/x86/local_apic.c b/arch/x86/local_apic.c
new file mode 100644
index 00000000..434cc0a4
--- /dev/null
+++ b/arch/x86/local_apic.c
@@ -0,0 +1,190 @@
+/*
+ * Copyright (c) 2018 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files
+ * (the "Software"), to deal in the Software without restriction,
+ * including without limitation the rights to use, copy, modify, merge,
+ * publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so,
+ * subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include <stdint.h>
+#include <assert.h>
+#include <arch/x86.h>
+#include <arch/x86/mmu.h>
+#include <arch/arch_ops.h>
+#include <arch/local_apic.h>
+#include <kernel/vm.h>
+#include <bits.h>
+
+#ifdef EPT_DEBUG
+#include <platform/vmcall.h>
+#endif
+
+typedef enum {
+    LAPIC_ID_REG            = 0x2,
+    LAPIC_EOI               = 0xB,
+    LAPIC_SIVR              = 0xF,
+    LAPIC_ISR0              = 0x10,
+    LAPIC_INTR_CMD_REG      = 0x30, /* 64-bits in x2APIC */
+    LAPIC_INTR_CMD_HI_REG   = 0x31, /* not available in x2APIC */
+    LAPIC_SELF_IPI_REG      = 0x3F  /* not available in xAPIC */
+} lapic_reg_id_t;
+
+#define PAGE_4K_MASK 0xfffULL
+
+#define MSR_APIC_BASE       0x1B
+#define LAPIC_ENABLED       (1ULL << 11)
+#define LAPIC_X2_ENABLED    (1ULL << 10)
+#define LAPIC_BASE_ADDR(base_msr) ((base_msr) & (~PAGE_4K_MASK))
+
+/* deliver status bit 12. 0 idle, 1 send pending. */
+#define APIC_DS_BIT         (1ULL << 12)
+#define MSR_X2APIC_BASE     0x800
+
+#define APIC_DM_FIXED       0x000
+#define APIC_LEVEL_ASSERT   0x4000
+#define APIC_DEST_SELF      0x40000
+
+static volatile uint64_t lapic_base_virtual_addr = 0;
+
+static uint32_t lapic_x1_read_reg(lapic_reg_id_t reg_id)
+{
+    DEBUG_ASSERT(lapic_base_virtual_addr);
+    uint64_t addr = lapic_base_virtual_addr + (uint64_t)(reg_id << 4);
+
+    return *(volatile uint32_t*)(addr);
+}
+
+static void lapic_x1_write_reg(lapic_reg_id_t reg_id, uint32_t data)
+{
+    DEBUG_ASSERT(lapic_base_virtual_addr);
+    uint64_t addr = lapic_base_virtual_addr + (uint64_t)(reg_id << 4);
+
+    *(volatile uint32_t*)addr = data;
+}
+
+/* caller must make sure xAPIC mode. */
+static void lapic_x1_wait_for_ipi(void)
+{
+    uint32_t icr_low;
+
+    while (1) {
+        icr_low = lapic_x1_read_reg(LAPIC_INTR_CMD_REG);
+        if ((icr_low & APIC_DS_BIT) == 0)
+            break;
+    }
+}
+
+static uint64_t lapic_x2_read_reg(lapic_reg_id_t reg_id)
+{
+    return read_msr(MSR_X2APIC_BASE + reg_id);
+}
+
+static void lapic_x2_write_reg(lapic_reg_id_t reg_id, uint64_t data)
+{
+    write_msr(MSR_X2APIC_BASE + reg_id, data);
+}
+
+void local_apic_init(void)
+{
+    uint64_t lapic_base_phy_addr = read_msr(MSR_APIC_BASE);
+
+/*
+ * Hard code mapping here, at this point, memory manager has not
+ * been initialized yet, cannot use vmm_alloc_physical().
+ * lapic_base_virtual_addr has been covered by 64 GB mapping here.
+ */
+    lapic_base_phy_addr = LAPIC_BASE_ADDR(lapic_base_phy_addr);
+    lapic_base_virtual_addr =  KERNEL_ASPACE_BASE + lapic_base_phy_addr;
+
+#ifdef EPT_DEBUG
+	make_ept_update_vmcall(ADD, lapic_base_phy_addr, PAGE_SIZE);
+#endif
+}
+
+void lapic_eoi(void)
+{
+    lapic_x1_write_reg(LAPIC_EOI, 1);
+}
+
+void lapic_software_disable(void)
+{
+    lapic_x1_write_reg(LAPIC_SIVR, 0xFF);
+}
+
+bool send_self_ipi(uint32_t vector)
+{
+    uint32_t icr_low;
+    uint64_t apic_base_msr = read_msr(MSR_APIC_BASE);
+
+    if (!(apic_base_msr & LAPIC_ENABLED)) {
+        return false;
+    }
+
+    icr_low = APIC_DEST_SELF | APIC_LEVEL_ASSERT | APIC_DM_FIXED | vector;
+
+    if (apic_base_msr & LAPIC_X2_ENABLED) {
+        lapic_x2_write_reg(LAPIC_SELF_IPI_REG, (uint64_t)vector);
+    } else {
+        lapic_x1_wait_for_ipi();
+        lapic_x1_write_reg(LAPIC_INTR_CMD_REG, icr_low);
+    }
+
+    return true;
+}
+
+
+/* Remap Local API instead of hard code mapping */
+void local_apic_reinit(void)
+{
+    status_t ret;
+    uint64_t lapic_base_phy_addr = read_msr(MSR_APIC_BASE);
+
+    lapic_base_phy_addr = LAPIC_BASE_ADDR(lapic_base_phy_addr);
+    ret = vmm_alloc_physical(vmm_get_kernel_aspace(),
+            "lapic",
+            4096,
+            (void **)&lapic_base_virtual_addr,
+            PAGE_SIZE_SHIFT,
+            lapic_base_phy_addr,
+            0,
+            ARCH_MMU_FLAG_UNCACHED_DEVICE);
+
+    if (ret) {
+        dprintf(CRITICAL, "Failed to allocate memory for Local APIC!\n");
+        return;
+    }
+}
+
+bool local_apic_vector_in_service(uint32_t vector)
+{
+    uint32_t idx = vector / 32;
+    uint32_t bit = vector % 32;
+    uint64_t apic_base_msr = read_msr(MSR_APIC_BASE);
+    uint64_t val;
+
+    if (!(apic_base_msr & LAPIC_ENABLED)) {
+        return false;
+    }
+
+    if (apic_base_msr & LAPIC_X2_ENABLED) {
+        val = lapic_x2_read_reg(LAPIC_ISR0 + idx);
+    } else {
+        val = (uint64_t)lapic_x1_read_reg(LAPIC_ISR0 + idx);
+    }
+
+    return !!BIT(val, bit);
+}
diff --git a/arch/x86/rules.mk b/arch/x86/rules.mk
index e26dfb9a..cec15326 100644
--- a/arch/x86/rules.mk
+++ b/arch/x86/rules.mk
@@ -24,8 +24,8 @@ KERNEL_BASE ?= 0xffffffff80000000
 KERNEL_LOAD_OFFSET ?= 0x00200000
 KERNEL_ASPACE_BASE ?= 0xffffff8000000000UL # -512GB
 KERNEL_ASPACE_SIZE ?= 0x0000008000000000UL
-USER_ASPACE_BASE   ?= 0x0000000000000000UL
-USER_ASPACE_SIZE   ?= 0x0000800000000000UL
+USER_ASPACE_BASE   ?= 0x0000000000001000UL
+USER_ASPACE_SIZE   ?= 0x00007FFFFFFFF000UL
 SUBARCH_DIR := $(LOCAL_DIR)/64
 endif
 
@@ -38,6 +38,8 @@ GLOBAL_DEFINES += \
 	KERNEL_LOAD_OFFSET=$(KERNEL_LOAD_OFFSET) \
 	KERNEL_ASPACE_BASE=$(KERNEL_ASPACE_BASE) \
 	KERNEL_ASPACE_SIZE=$(KERNEL_ASPACE_SIZE) \
+	USER_ASPACE_BASE=$(USER_ASPACE_BASE) \
+	USER_ASPACE_SIZE=$(USER_ASPACE_SIZE) \
 	SMP_MAX_CPUS=1 \
 	X86_WITH_FPU=1
 
@@ -47,14 +49,24 @@ MODULE_SRCS += \
 	$(SUBARCH_DIR)/exceptions.S \
 	$(SUBARCH_DIR)/mmu.c \
 	$(SUBARCH_DIR)/ops.S \
-\
+	$(SUBARCH_DIR)/usercopy.c \
 	$(LOCAL_DIR)/arch.c \
 	$(LOCAL_DIR)/cache.c \
 	$(LOCAL_DIR)/gdt.S \
 	$(LOCAL_DIR)/thread.c \
 	$(LOCAL_DIR)/faults.c \
 	$(LOCAL_DIR)/descriptor.c \
-	$(LOCAL_DIR)/fpu.c
+	$(LOCAL_DIR)/fpu.c \
+	$(LOCAL_DIR)/local_apic.c
+
+ifeq (true,$(call TOBOOL,$(STACK_PROTECTOR)))
+MODULE_SRCS += \
+	$(LOCAL_DIR)/stack_chk.c
+endif
+
+ifeq (true, $(WITH_CUSTOMIZED_BOOTSTRAP))
+	MODULE_SRCS := $(filter-out $(SUBARCH_DIR)/start.S, $(MODULE_SRCS))
+endif
 
 include $(LOCAL_DIR)/toolchain.mk
 
@@ -74,6 +86,9 @@ $(warning ARCH_x86_TOOLCHAIN_PREFIX = $(ARCH_x86_TOOLCHAIN_PREFIX))
 $(warning ARCH_x86_64_TOOLCHAIN_PREFIX = $(ARCH_x86_64_TOOLCHAIN_PREFIX))
 $(warning TOOLCHAIN_PREFIX = $(TOOLCHAIN_PREFIX))
 
+ifeq ($(call TOBOOL,$(CLANGBUILD)), true)
+ARCH_COMPILEFLAGS += $(ARCH_$(ARCH)_COMPILEFLAGS)
+endif
 
 cc-option = $(shell if test -z "`$(1) $(2) -S -o /dev/null -xc /dev/null 2>&1`"; \
 	then echo "$(2)"; else echo "$(3)"; fi ;)
@@ -83,17 +98,26 @@ GLOBAL_CFLAGS += $(call cc-option,$(CC),-fno-stack-protector,)
 
 GLOBAL_COMPILEFLAGS += -fasynchronous-unwind-tables
 GLOBAL_COMPILEFLAGS += -gdwarf-2
-GLOBAL_COMPILEFLAGS += -fno-pic
+#GLOBAL_COMPILEFLAGS += -fno-pic
+GLOBAL_COMPILEFLAGS += -msoft-float
 GLOBAL_LDFLAGS += -z max-page-size=4096
+GLOBAL_LDFLAGS += -z noexecstack
 
 ifeq ($(SUBARCH),x86-64)
+ifeq (true,$(call TOBOOL,$(STACK_PROTECTOR)))
+GLOBAL_DEFINES += \
+  STACK_PROTECTOR=1
+GLOBAL_COMPILEFLAGS += -fstack-protector-strong
+else
 GLOBAL_COMPILEFLAGS += -fno-stack-protector
-GLOBAL_COMPILEFLAGS += -mcmodel=kernel
-GLOBAL_COMPILEFLAGS += -mno-red-zone
+endif
+KERNEL_COMPILEFLAGS += -mcmodel=kernel
+KERNEL_COMPILEFLAGS += -mno-red-zone
+KERNEL_COMPILEFLAGS += -fPIE -include arch/hidden.h
 endif # SUBARCH x86-64
 
 
-ARCH_OPTFLAGS := -O2
+ARCH_OPTFLAGS := -O2 -D_FORTIFY_SOURCE=2
 
 LINKER_SCRIPT += $(SUBARCH_BUILDDIR)/kernel.ld
 
diff --git a/lib/libc/string/arch/x86/memset.S b/arch/x86/stack_chk.c
similarity index 83%
rename from lib/libc/string/arch/x86/memset.S
rename to arch/x86/stack_chk.c
index 43367f6d..e2cfa896 100644
--- a/lib/libc/string/arch/x86/memset.S
+++ b/arch/x86/stack_chk.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2009 Corey Tabaka
+ * Copyright (c) 2019 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -20,18 +20,11 @@
  * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
  * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
-#include <asm.h>
 
-/* TODO: */
-
-.text
-.align 2
-
-/* void bzero(void *s, size_t n); */
-FUNCTION(bzero)
-    ret
-
-/* void *memset(void *s, int c, size_t n); */
-FUNCTION(memset)
-    ret
+#include <debug.h>
+#include <trace.h>
 
+void __stack_chk_fail(void)
+{
+    panic("kernel stack is corrupted\n");
+}
diff --git a/arch/x86/thread.c b/arch/x86/thread.c
index e75f1997..df9a34ee 100644
--- a/arch/x86/thread.c
+++ b/arch/x86/thread.c
@@ -1,7 +1,7 @@
 /*
  * Copyright (c) 2009 Corey Tabaka
  * Copyright (c) 2014 Travis Geiselbrecht
- * Copyright (c) 2015 Intel Corporation
+ * Copyright (c) 2015-2018 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining
  * a copy of this software and associated documentation files
@@ -32,19 +32,17 @@
 #include <arch/x86/descriptor.h>
 #include <arch/fpu.h>
 
-/* we're uniprocessor at this point for x86, so store a global pointer to the current thread */
-struct thread *_current_thread;
-
 static void initial_thread_func(void) __NO_RETURN;
 static void initial_thread_func(void)
 {
     int ret;
+    thread_t *cur_thread = get_current_thread();
 
     /* release the thread lock that was implicitly held across the reschedule */
     spin_unlock(&thread_lock);
     arch_enable_ints();
 
-    ret = _current_thread->entry(_current_thread->arg);
+    ret = cur_thread->entry(cur_thread->arg);
 
     thread_exit(ret);
 }
@@ -123,38 +121,29 @@ void arch_context_switch(thread_t *oldthread, thread_t *newthread)
         :
         : "d" (&oldthread->arch.sp), "a" (newthread->arch.sp)
     );
-
-    /*__asm__ __volatile__ (
-        "pushf              \n\t"
-        "pushl %%cs         \n\t"
-        "pushl $1f          \n\t"
-        "pushl %%gs         \n\t"
-        "pushl %%fs         \n\t"
-        "pushl %%es         \n\t"
-        "pushl %%ds         \n\t"
-        "pusha              \n\t"
-        "movl %%esp,(%%edx) \n\t"
-        "movl %%eax,%%esp   \n\t"
-        "popa               \n\t"
-        "popl %%ds          \n\t"
-        "popl %%es          \n\t"
-        "popl %%fs          \n\t"
-        "popl %%gs          \n\t"
-        "iret               \n\t"
-        "1: "
-        :
-        : "d" (&oldthread->arch.sp), "a" (newthread->arch.sp)
-    );*/
 }
-#endif
 
-#if ARCH_X86_64
+#elif ARCH_X86_64
 
 void arch_context_switch(thread_t *oldthread, thread_t *newthread)
 {
+    uint64_t stack_top = (uint64_t)newthread->stack + newthread->stack_size;
+    tss_t *tss_base= get_tss_base();
 #if X86_WITH_FPU
     fpu_context_switch(oldthread, newthread);
 #endif
+    tss_base->rsp0 = stack_top;
+    write_msr(SYSENTER_ESP_MSR, stack_top);
+
+#ifdef STACK_PROTECTOR
+    if(oldthread->tls[TLS_ENTRY_TRUSTY]) {
+        oldthread->arch.fs_base = read_msr(X86_MSR_FS_BASE);
+    }
+
+    if(newthread->tls[TLS_ENTRY_TRUSTY]) {
+        write_msr(X86_MSR_FS_BASE, newthread->arch.fs_base);
+    }
+#endif
 
     x86_64_context_switch(&oldthread->arch.sp, newthread->arch.sp);
 }
diff --git a/arch/x86/toolchain.mk b/arch/x86/toolchain.mk
index dba751e8..aa041fdc 100644
--- a/arch/x86/toolchain.mk
+++ b/arch/x86/toolchain.mk
@@ -22,13 +22,30 @@ ARCH_x86_64_TOOLCHAIN_INCLUDED := 1
 
 ifndef ARCH_x86_64_TOOLCHAIN_PREFIX
 ARCH_x86_64_TOOLCHAIN_PREFIX := x86_64-elf-
-FOUNDTOOL=$(shell which $(ARCH_x86_64_TOOLCHAIN_PREFIX)gcc)
 endif
 
+FOUNDTOOL=$(shell which $(ARCH_x86_64_TOOLCHAIN_PREFIX)gcc)
 ifeq ($(FOUNDTOOL),)
 $(error cannot find toolchain, please set ARCH_x86_64_TOOLCHAIN_PREFIX or add it to your path)
 endif
 
+ifeq ($(call TOBOOL,$(CLANGBUILD)),true)
+
+CLANG_X86_64_TARGET_SYS ?= linux
+CLANG_X86_64_TARGET_ABI ?= gnu
+
+CLANG_X86_64_AS_DIR := $(shell dirname $(shell dirname $(ARCH_x86_64_TOOLCHAIN_PREFIX)))
+
+AS_PATH := $(wildcard $(CLANG_X86_64_AS_DIR)/*/bin/as)
+ifeq ($(AS_PATH),)
+$(error Could not find $(CLANG_X86_64_AS_DIR)/*/bin/as, did the directory structure change?)
+endif
+
+ARCH_x86_COMPILEFLAGS += -target x86_64-$(CLANG_X86_64_TARGET_SYS)-$(CLANG_X86_64_TARGET_ABI) \
+                         --gcc-toolchain=$(CLANG_X86_64_AS_DIR)/
+
+endif
+
 endif
 endif
 
diff --git a/engine.mk b/engine.mk
index c0a45fa3..3522bfae 100644
--- a/engine.mk
+++ b/engine.mk
@@ -63,8 +63,9 @@ GLOBAL_INCLUDES := $(BUILDDIR) $(GLOBAL_UAPI_INCLUDES) $(GLOBAL_SHARED_INCLUDES)
 GLOBAL_OPTFLAGS ?= $(ARCH_OPTFLAGS)
 GLOBAL_COMPILEFLAGS := -g -finline -include $(CONFIGHEADER)
 GLOBAL_COMPILEFLAGS += -Werror -Wall -Wsign-compare -Wno-multichar -Wno-unused-function -Wno-unused-label -Wno-tautological-compare
-GLOBAL_COMPILEFLAGS += -fno-short-enums -fno-common
+GLOBAL_COMPILEFLAGS += -fno-short-enums -fno-common -fno-builtin
 GLOBAL_CFLAGS := --std=c11 -Wstrict-prototypes -Wwrite-strings
+GLOBAL_CFLAGS += -Wformat -Wformat-security
 GLOBAL_CPPFLAGS := --std=c++11 -fno-exceptions -fno-rtti -fno-threadsafe-statics
 #GLOBAL_CPPFLAGS += -Weffc++
 GLOBAL_ASMFLAGS := -DASSEMBLY
@@ -133,6 +134,9 @@ BUILDID ?=
 # comment out or override if you want to see the full output of each command
 NOECHO ?= @
 
+# try to include the project file
+-include project/$(PROJECT).mk
+
 # default to building with clang
 CLANGBUILD ?= true
 override CLANGBUILD := $(call TOBOOL,$(CLANGBUILD))
@@ -141,8 +145,6 @@ ifeq ($(call TOBOOL,$(CLANGBUILD)), true)
 GLOBAL_COMPILEFLAGS += -Wimplicit-fallthrough
 endif
 
-# try to include the project file
--include project/$(PROJECT).mk
 ifndef TARGET
 $(error couldn't find project or project doesn't define target)
 endif
@@ -184,6 +186,10 @@ GLOBAL_DEFINES += \
 	LK_DEBUGLEVEL=$(DEBUG)
 endif
 
+ifeq ($(call TOBOOL,$(ENABLE_STATIC_LIB)),true)
+STATIC_LIBS :=
+endif
+
 # allow additional defines from outside the build system
 ifneq ($(EXTERNAL_DEFINES),)
 GLOBAL_DEFINES += $(EXTERNAL_DEFINES)
diff --git a/include/arch/hidden.h b/include/arch/hidden.h
new file mode 100644
index 00000000..e36e6da8
--- /dev/null
+++ b/include/arch/hidden.h
@@ -0,0 +1,5 @@
+/*
+ * Use a pragma to set all objects to hidden, the command-line option does not
+ * apply to externs.
+ */
+#pragma GCC visibility push(hidden)
diff --git a/include/arch/ops.h b/include/arch/ops.h
index f0977519..906757e2 100644
--- a/include/arch/ops.h
+++ b/include/arch/ops.h
@@ -40,8 +40,14 @@ static bool arch_in_int_handler(void);
 
 static int atomic_swap(volatile int *ptr, int val);
 static int atomic_add(volatile int *ptr, int val);
+
+#if ARCH_X86_64
+static int atomic_and(volatile int *ptr, long val);
+static int atomic_or(volatile int *ptr, long val);
+#else
 static int atomic_and(volatile int *ptr, int val);
 static int atomic_or(volatile int *ptr, int val);
+#endif
 
 static uint32_t arch_cycle_count(void);
 
diff --git a/include/arch/usercopy.h b/include/arch/usercopy.h
index 028d3996..cdc46623 100644
--- a/include/arch/usercopy.h
+++ b/include/arch/usercopy.h
@@ -26,8 +26,7 @@
 
 #include <sys/types.h>
 
-//TODO: Add support for 64-bit user_addr_t
-typedef uint32_t user_addr_t;
+typedef uintptr_t user_addr_t;
 
 status_t arch_copy_from_user(void *kdest, user_addr_t usrc, size_t len);
 status_t arch_copy_to_user(user_addr_t udest, const void *ksrc, size_t len);
diff --git a/lib/libc/include/endian.h b/lib/libc/include/endian.h
index 90da5eab..6a00bd2c 100644
--- a/lib/libc/include/endian.h
+++ b/lib/libc/include/endian.h
@@ -69,6 +69,10 @@
 #define ntohl(n) BE32(n)
 #define htonl(h) BE32(h)
 
+/* 32-bit network byte swap stuff */
+#define htobe32(h) BE32(h)
+#define be32toh(b) BE32(b)
+
 /* 64-bit network byte swap stuff */
 #define htobe64(h) BE64(h)
 #define be64toh(b) BE64(b)
diff --git a/lib/libc/include/string.h b/lib/libc/include/string.h
index c6aa757a..32b7656b 100644
--- a/lib/libc/include/string.h
+++ b/lib/libc/include/string.h
@@ -25,6 +25,7 @@
 
 #include <stddef.h>
 #include <lk/compiler.h>
+#include <sys/types.h>
 
 __BEGIN_CDECLS
 
@@ -33,6 +34,7 @@ int   memcmp (void const *, const void *, size_t) __PURE;
 void *memcpy (void *, void const *, size_t);
 void *memmove(void *, void const *, size_t);
 void *memset (void *, int, size_t);
+status_t memcpy_s(void *, size_t, const void *, size_t);
 
 char       *strcat(char *, char const *);
 char       *strchr(char const *, int) __PURE;
diff --git a/lib/libc/string/arch/x86-64/memcpy.S b/lib/libc/string/arch/x86/64/memcpy.S
similarity index 100%
rename from lib/libc/string/arch/x86-64/memcpy.S
rename to lib/libc/string/arch/x86/64/memcpy.S
diff --git a/lib/libc/string/arch/x86/64/memcpy_s.c b/lib/libc/string/arch/x86/64/memcpy_s.c
new file mode 100644
index 00000000..06255fdb
--- /dev/null
+++ b/lib/libc/string/arch/x86/64/memcpy_s.c
@@ -0,0 +1,41 @@
+/*
+ * Copyright (c) 2018 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining
+ * a copy of this software and associated documentation files
+ * (the "Software"), to deal in the Software without restriction,
+ * including without limitation the rights to use, copy, modify, merge,
+ * publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so,
+ * subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+#include <string.h>
+#include <sys/types.h>
+#include <uapi/err.h>
+
+status_t memcpy_s(void *dest, size_t dest_size, const void *src, size_t count)
+{
+    if ((NULL == dest) || (NULL == src))
+        return ERR_NOT_VALID;
+
+    if (0 == count)
+        return NO_ERROR;
+
+    if (dest_size < count)
+        return ERR_OUT_OF_RANGE;
+
+    memcpy(dest, src, count);
+
+    return NO_ERROR;
+}
diff --git a/lib/libc/string/arch/x86-64/memset.S b/lib/libc/string/arch/x86/64/memset.S
similarity index 100%
rename from lib/libc/string/arch/x86-64/memset.S
rename to lib/libc/string/arch/x86/64/memset.S
diff --git a/lib/libc/string/arch/x86-64/rules.mk b/lib/libc/string/arch/x86/64/rules.mk
similarity index 59%
rename from lib/libc/string/arch/x86-64/rules.mk
rename to lib/libc/string/arch/x86/64/rules.mk
index 3dc080bb..9f76699b 100644
--- a/lib/libc/string/arch/x86-64/rules.mk
+++ b/lib/libc/string/arch/x86/64/rules.mk
@@ -1,10 +1,9 @@
 LOCAL_DIR := $(GET_LOCAL_DIR)
 
-ASM_STRING_OPS := #bcopy bzero memcpy memmove memset
+ASM_STRING_OPS :=
 
 MODULE_SRCS += \
-	#$(LOCAL_DIR)/memcpy.S \
-	#$(LOCAL_DIR)/memset.S
+	$(LOCAL_DIR)/memcpy_s.c
 
 # filter out the C implementation
 C_STRING_OPS := $(filter-out $(ASM_STRING_OPS),$(C_STRING_OPS))
diff --git a/lib/libc/string/arch/x86/rules.mk b/lib/libc/string/arch/x86/rules.mk
index 3dc080bb..1babff96 100644
--- a/lib/libc/string/arch/x86/rules.mk
+++ b/lib/libc/string/arch/x86/rules.mk
@@ -1,11 +1,5 @@
 LOCAL_DIR := $(GET_LOCAL_DIR)
 
-ASM_STRING_OPS := #bcopy bzero memcpy memmove memset
-
-MODULE_SRCS += \
-	#$(LOCAL_DIR)/memcpy.S \
-	#$(LOCAL_DIR)/memset.S
-
-# filter out the C implementation
-C_STRING_OPS := $(filter-out $(ASM_STRING_OPS),$(C_STRING_OPS))
-
+ifeq ($(SUBARCH),x86-64)
+include $(LOCAL_DIR)/64/rules.mk
+endif
diff --git a/make/compile.mk b/make/compile.mk
index bae61d57..65004f91 100644
--- a/make/compile.mk
+++ b/make/compile.mk
@@ -41,46 +41,48 @@ $(MODULE_OBJS): MODULE_ASMFLAGS:=$(MODULE_ASMFLAGS)
 $(MODULE_OBJS): MODULE_SRCDEPS:=$(MODULE_SRCDEPS)
 $(MODULE_OBJS): MODULE_INCLUDES:=$(MODULE_INCLUDES)
 
+COMPILEFLAGS = $(GLOBAL_COMPILEFLAGS) $(KERNEL_COMPILEFLAGS)
+
 $(MODULE_COBJS): $(BUILDDIR)/%.o: %.c $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CFLAGS) $(ARCH_CFLAGS) $(MODULE_CFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CFLAGS) $(ARCH_CFLAGS) $(MODULE_CFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 $(MODULE_CPPOBJS): $(BUILDDIR)/%.o: %.cpp $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 $(MODULE_CCOBJS): $(BUILDDIR)/%.o: %.cc $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 $(MODULE_ASMOBJS): $(BUILDDIR)/%.o: %.S $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_ASMFLAGS) $(ARCH_ASMFLAGS) $(MODULE_ASMFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_ASMFLAGS) $(ARCH_ASMFLAGS) $(MODULE_ASMFLAGS) $(THUMBCFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 # overridden arm versions
 $(MODULE_ARM_COBJS): $(BUILDDIR)/%.o: %.c $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CFLAGS) $(ARCH_CFLAGS) $(MODULE_CFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CFLAGS) $(ARCH_CFLAGS) $(MODULE_CFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 $(MODULE_ARM_CPPOBJS): $(BUILDDIR)/%.o: %.cpp $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 $(MODULE_ARM_CCOBJS): $(BUILDDIR)/%.o: %.cc $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_CPPFLAGS) $(ARCH_CPPFLAGS) $(MODULE_CPPFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 $(MODULE_ARM_ASMOBJS): $(BUILDDIR)/%.o: %.S $(MODULE_SRCDEPS)
 	@$(MKDIR)
 	@echo compiling $<
-	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(GLOBAL_COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_ASMFLAGS) $(ARCH_ASMFLAGS) $(MODULE_ASMFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
+	$(NOECHO)$(CC) $(GLOBAL_OPTFLAGS) $(MODULE_OPTFLAGS) $(COMPILEFLAGS) $(ARCH_COMPILEFLAGS) $(MODULE_COMPILEFLAGS) $(GLOBAL_ASMFLAGS) $(ARCH_ASMFLAGS) $(MODULE_ASMFLAGS) $(GLOBAL_INCLUDES) $(MODULE_INCLUDES) -c $< -MD -MP -MT $@ -MF $(@:%o=%d) -o $@
 
 # clear some variables we set here
 MODULE_CSRCS :=
diff --git a/make/module.mk b/make/module.mk
index b8aafa91..860cb09a 100644
--- a/make/module.mk
+++ b/make/module.mk
@@ -92,11 +92,31 @@ include make/compile.mk
 ifeq (true,$(call TOBOOL,$(MODULE_STATIC_LIB)))
 
 MODULE_OBJECT := $(call TOBUILDDIR,$(MODULE_SRCDIR).mod.a)
+
+ifeq (true,$(call TOBOOL,$(ENABLE_STATIC_LIB)))
+MODULE_NAME := $(notdir $(MODULE_OBJECT))
+FILT_RESULT := $(strip $(foreach t, $(STATIC_LIBS), $(if $(filter $(notdir $(t)), $(MODULE_NAME)), $(t),)))
+
+ifneq ($(FILT_RESULT),)
+MODULE_OBJECT := $(FILT_RESULT)
+$(MODULE_OBJECT) :
+else
+STATIC_LIBS += $(MODULE_OBJECT)
+
 $(MODULE_OBJECT): $(MODULE_OBJS) $(MODULE_EXTRA_OBJS)
 	@$(MKDIR)
 	@echo creating $@
 	$(NOECHO)rm -f $@
 	$(NOECHO)$(AR) rcs $@ $^
+endif
+
+else
+$(MODULE_OBJECT): $(MODULE_OBJS) $(MODULE_EXTRA_OBJS)
+	@$(MKDIR)
+	@echo creating $@
+	$(NOECHO)rm -f $@
+	$(NOECHO)$(AR) rcs $@ $^
+endif
 
 else
 
-- 
2.21.0

