From 0132013c98a1b70f9a0e94bd90f891d0e987a08d Mon Sep 17 00:00:00 2001
From: Vinay Prasad Kompella <vinay.kompella@intel.com>
Date: Mon, 4 Nov 2024 10:12:09 +0530
Subject: [PATCH 4/6] Implement AVX2 instructions in x64 assembler

This patch implements all the avx2 instructions required
for supporting all the existing HVec instructions.
Its a building block to enable AVX2 based 256-bit vectorization.

x86 has overlapping FP and vector registers, so xmm/ymm is the same register
Xmm registers now have vector length, to determine if it is Xmm/Ymm/Zmm.

Test: art/test.py -b --host
          Run the above test with and without "avx,avx2" features

Tracked-On: OAM-126116
Signed-off-by: Vinay Prasad Kompella <vinay.kompella@intel.com>
---
 compiler/utils/assembler_test.h               |   69 +-
 compiler/utils/x86_64/assembler_x86_64.cc     | 2368 ++++++++++-------
 compiler/utils/x86_64/assembler_x86_64.h      |  122 +-
 .../utils/x86_64/assembler_x86_64_test.cc     |  983 ++++++-
 compiler/utils/x86_64/constants_x86_64.h      |   12 +-
 disassembler/disassembler_x86.cc              |  308 ++-
 disassembler/disassembler_x86.h               |   26 +-
 .../arch/x86/instruction_set_features_x86.h   |    2 +-
 8 files changed, 2803 insertions(+), 1087 deletions(-)

diff --git a/compiler/utils/assembler_test.h b/compiler/utils/assembler_test.h
index 05c79f3b21..846ed57e88 100644
--- a/compiler/utils/assembler_test.h
+++ b/compiler/utils/assembler_test.h
@@ -34,6 +34,10 @@
 
 namespace art HIDDEN {
 
+typedef void (*DriverFnPtr)(const std::vector<uint8_t>& art_code,
+                            const std::string& assembly_text,
+                            const std::string& test_name);
+
 // Helper for a constexpr string length.
 constexpr size_t ConstexprStrLen(char const* str, size_t count = 0) {
   return ('\0' == str[0]) ? count : ConstexprStrLen(str+1, count+1);
@@ -72,6 +76,13 @@ class AssemblerTest : public AssemblerTestBase {
     DriverWrapper(assembly_string, test_name);
   }
 
+  // Since it's a string arg, the driver assumes assembler is already called.
+  void CustomDriverStr(DriverFnPtr custom_driver,
+                       const std::string& assembly_string,
+                       const std::string& test_name) {
+    DriverWrapper(assembly_string, test_name, custom_driver);
+  }
+
   //
   // Register repeats.
   //
@@ -760,6 +771,54 @@ class AssemblerTest : public AssemblerTestBase {
         fmt);
   }
 
+  // Repeats over addresses and vec-registers provided by fixture.
+  std::string RepeatAV(void (Ass::*f)(const Addr&, VecReg), const std::string& fmt) {
+    return RepeatAV(f, GetAddresses(), fmt);
+  }
+
+  // Variant that takes explicit vector of address
+  // (to test restricted addressing modes set).
+  std::string RepeatAV(void (Ass::*f)(const Addr&, VecReg),
+                       const std::vector<Addr>& a,
+                       const std::string& fmt) {
+    return RepeatTemplatedMemReg<Addr, VecReg>(f,
+                                               a,
+                                               GetVectorRegisters(),
+                                               &AssemblerTest::GetAddrName,
+                                               &AssemblerTest::GetVecRegName,
+                                               fmt);
+  }
+
+  // Repeats over vec-registers and addresses provided by fixture.
+  std::string RepeatVA(void (Ass::*f)(VecReg, const Addr&), const std::string& fmt) {
+    return RepeatVA(f, GetAddresses(), fmt);
+  }
+
+  // Variant that takes explicit vector of address
+  // (to test restricted addressing modes set).
+  std::string RepeatVA(void (Ass::*f)(VecReg, const Addr&),
+                       const std::vector<Addr>& a,
+                       const std::string& fmt) {
+    return RepeatTemplatedRegMem<VecReg, Addr>(f,
+                                               GetVectorRegisters(),
+                                               a,
+                                               &AssemblerTest::GetVecRegName,
+                                               &AssemblerTest::GetAddrName,
+                                               fmt);
+  }
+
+  std::string RepeatVVI(void (Ass::*f)(VecReg, VecReg, const Imm&),
+                        size_t imm_bytes,
+                        const std::string& fmt) {
+    return RepeatTemplatedRegistersImm<VecReg, VecReg>(f,
+                                                       GetVectorRegisters(),
+                                                       GetVectorRegisters(),
+                                                       &AssemblerTest::GetVecRegName,
+                                                       &AssemblerTest::GetVecRegName,
+                                                       imm_bytes,
+                                                       fmt);
+  }
+
   template <typename ImmType>
   std::string RepeatVIb(void (Ass::*f)(VecReg, ImmType),
                         int imm_bits,
@@ -1624,14 +1683,20 @@ class AssemblerTest : public AssemblerTestBase {
   // Override this to pad the code with NOPs to a certain size if needed.
   virtual void Pad([[maybe_unused]] std::vector<uint8_t>& data) {}
 
-  void DriverWrapper(const std::string& assembly_text, const std::string& test_name) {
+  void DriverWrapper(const std::string& assembly_text,
+                     const std::string& test_name,
+                     DriverFnPtr custom_driver = nullptr) {
     assembler_->FinalizeCode();
     size_t cs = assembler_->CodeSize();
     std::unique_ptr<std::vector<uint8_t>> data(new std::vector<uint8_t>(cs));
     MemoryRegion code(&(*data)[0], data->size());
     assembler_->CopyInstructions(code);
     Pad(*data);
-    Driver(*data, assembly_text, test_name);
+    if (custom_driver != nullptr) {
+      (*custom_driver)(*data, assembly_text, test_name);
+    } else {
+      Driver(*data, assembly_text, test_name);
+    }
   }
 
   static constexpr size_t kWarnManyCombinationsThreshold = 500;
diff --git a/compiler/utils/x86_64/assembler_x86_64.cc b/compiler/utils/x86_64/assembler_x86_64.cc
index 89a1d0923a..7b3f7caa62 100644
--- a/compiler/utils/x86_64/assembler_x86_64.cc
+++ b/compiler/utils/x86_64/assembler_x86_64.cc
@@ -24,12 +24,20 @@
 namespace art HIDDEN {
 namespace x86_64 {
 
+inline uint8_t GetEncodedVexLen(XmmRegister xmmReg) {
+  return (xmmReg.IsYMM() ? SET_VEX_L_256 : SET_VEX_L_128);
+}
+
 std::ostream& operator<<(std::ostream& os, const CpuRegister& reg) {
   return os << reg.AsRegister();
 }
 
 std::ostream& operator<<(std::ostream& os, const XmmRegister& reg) {
-  return os << reg.AsFloatRegister();
+  if (reg.IsYMM()) {
+    return os << "ymm" << static_cast<int>(reg.AsFloatRegister());
+  } else {
+    return os << reg.AsFloatRegister();
+  }
 }
 
 std::ostream& operator<<(std::ostream& os, const X87Register& reg) {
@@ -64,13 +72,8 @@ std::ostream& operator<<(std::ostream& os, const Address& addr) {
   }
 }
 
-bool X86_64Assembler::CpuHasAVXorAVX2FeatureFlag() {
-  if (has_AVX_ || has_AVX2_) {
-    return true;
-  }
-  return false;
-}
-
+bool X86_64Assembler::CpuHasAVXFeatureFlag() { return has_AVX_; }
+bool X86_64Assembler::CpuHasAVX2FeatureFlag() { return has_AVX2_; }
 
 void X86_64Assembler::call(CpuRegister reg) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -414,7 +417,7 @@ void X86_64Assembler::leal(CpuRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovaps(dst, src);
     return;
   }
@@ -425,14 +428,26 @@ void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::movups(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmovups(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitOptionalRex32(dst, src);
+  EmitUint8(0x0F);
+  EmitUint8(0x10);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
 
 /**VEX.128.0F.WIG 28 /r VMOVAPS xmm1, xmm2 */
 void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   uint8_t byte_zero, byte_one, byte_two;
   bool is_twobyte_form = true;
   bool load = dst.NeedsRex();
   bool store = !load;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   if (src.NeedsRex()&& dst.NeedsRex()) {
     is_twobyte_form = false;
@@ -443,18 +458,13 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
   X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
   if (is_twobyte_form) {
     bool rex_bit = (load) ? dst.NeedsRex() : src.NeedsRex();
-    byte_one = EmitVexPrefixByteOne(rex_bit,
-                                    vvvv_reg,
-                                    SET_VEX_L_128,
-                                    SET_VEX_PP_NONE);
+    byte_one = EmitVexPrefixByteOne(rex_bit, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
                                     /*X=*/ false,
                                     src.NeedsRex(),
                                     SET_VEX_M_0F);
-    byte_two = EmitVexPrefixByteTwo(/*W=*/ false,
-                                    SET_VEX_L_128,
-                                    SET_VEX_PP_NONE);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(byte_zero);
   EmitUint8(byte_one);
@@ -476,7 +486,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovaps(dst, src);
     return;
   }
@@ -489,11 +499,12 @@ void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
 
 /**VEX.128.0F.WIG 28 /r VMOVAPS xmm1, m128 */
 void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
   // Instruction VEX Prefix
+  uint8_t VEX_L = GetEncodedVexLen(dst);
   uint8_t rex = src.rex();
   bool Rex_x = rex & GET_REX_X;
   bool Rex_b = rex & GET_REX_B;
@@ -503,18 +514,13 @@ void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -528,7 +534,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovups(dst, src);
     return;
   }
@@ -541,11 +547,12 @@ void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
 
 /** VEX.128.0F.WIG 10 /r VMOVUPS xmm1, m128 */
 void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
   // Instruction VEX Prefix
+  uint8_t VEX_L = GetEncodedVexLen(dst);
   uint8_t rex = src.rex();
   bool Rex_x = rex & GET_REX_X;
   bool Rex_b = rex & GET_REX_B;
@@ -555,18 +562,13 @@ void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -581,7 +583,7 @@ void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovaps(dst, src);
     return;
   }
@@ -594,12 +596,13 @@ void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
 
 /** VEX.128.0F.WIG 29 /r VMOVAPS m128, xmm1 */
 void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
 
   // Instruction VEX Prefix
+  uint8_t VEX_L = GetEncodedVexLen(src);
   uint8_t rex = dst.rex();
   bool Rex_x = rex & GET_REX_X;
   bool Rex_b = rex & GET_REX_B;
@@ -609,18 +612,13 @@ void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -634,7 +632,7 @@ void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovups(dst, src);
     return;
   }
@@ -647,12 +645,13 @@ void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
 
 /** VEX.128.0F.WIG 11 /r VMOVUPS m128, xmm1 */
 void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
 
   // Instruction VEX Prefix
+  uint8_t VEX_L = GetEncodedVexLen(src);
   uint8_t rex = dst.rex();
   bool Rex_x = rex & GET_REX_X;
   bool Rex_b = rex & GET_REX_B;
@@ -662,18 +661,13 @@ void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -686,8 +680,52 @@ void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
+/** VEX.128.0F.WIG 10/11 /r VMOVUPS xmm1, xmm2
+    VEX.256.0F.WIG 10/11 /r VMOVUPS ymm1, ymm2 */
+void X86_64Assembler::vmovups(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteZero, ByteOne, ByteTwo;
+  bool is_twobyte_form = true;
+  bool is_load = src.NeedsRex() && !dst.NeedsRex();
+  // Instruction VEX Prefix
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  if (dst.NeedsRex() && src.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+    ByteOne = EmitVexPrefixByteOne(
+        is_load ? src.NeedsRex() : dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  // Instruction Opcode
+  if (is_load) {
+    EmitUint8(0x11);
+    EmitXmmRegisterOperand(src.LowBits(), dst);
+  } else {
+    EmitUint8(0x10);
+    // Instruction Operands
+    EmitXmmRegisterOperand(dst.LowBits(), src);
+  }
+}
 
 void X86_64Assembler::movss(XmmRegister dst, const Address& src) {
+  if (dst.IsYMM()) {
+    vmovss(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
   EmitOptionalRex32(dst, src);
@@ -698,6 +736,10 @@ void X86_64Assembler::movss(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movss(const Address& dst, XmmRegister src) {
+  if (src.IsYMM()) {
+    vmovss(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
   EmitOptionalRex32(src, dst);
@@ -708,6 +750,10 @@ void X86_64Assembler::movss(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movss(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmovss(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
   EmitOptionalRex32(src, dst);  // Movss is MR encoding instead of the usual RM.
@@ -716,6 +762,116 @@ void X86_64Assembler::movss(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(src.LowBits(), dst);
 }
 
+/* VEX.LIG.F3.0F.WIG 10 /r VMOVSS xmm1, m32
+   Since LIG VEX.L = 0 and WIG => VEX.W = 0
+*/
+void X86_64Assembler::vmovss(XmmRegister dst, const Address& src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = false;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  uint8_t rex = src.rex();
+  bool Rex_x = rex & GET_REX_X;
+  bool Rex_b = rex & GET_REX_B;
+  if (!Rex_b && !Rex_x) {
+    is_twobyte_form = true;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F3);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), Rex_x, Rex_b, SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, SET_VEX_L_128, SET_VEX_PP_F3);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(0x10);
+
+  // Instruction Operands
+  EmitOperand(dst.LowBits(), src);
+}
+
+/* VEX.LIG.F3.0F.WIG 11 /r VMOVSS m32, xmm1 */
+void X86_64Assembler::vmovss(const Address& dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = false;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  uint8_t rex = dst.rex();
+  bool Rex_x = rex & GET_REX_X;
+  bool Rex_b = rex & GET_REX_B;
+  if (!Rex_b && !Rex_x) {
+    is_twobyte_form = true;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F3);
+  } else {
+    byte_one = EmitVexPrefixByteOne(src.NeedsRex(), Rex_x, Rex_b, SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, SET_VEX_L_128, SET_VEX_PP_F3);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(0x11);
+
+  // Instruction Operands
+  EmitOperand(src.LowBits(), dst);
+}
+
+/* VEX.LIG.F3.0F.WIG 10/11 /r VMOVSS xmm1, xmm2, xmm3 */
+void X86_64Assembler::vmovss(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = true;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  if (dst.NeedsRex() && src2.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  bool is_store = src2.NeedsRex() && !dst.NeedsRex();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(
+        is_store ? src2.NeedsRex() : dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F3);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F3);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+
+  // Instruction Opcode
+  if (is_store) {
+    // Special opcode only when src2 needs rex
+    EmitUint8(0x11);
+    EmitXmmRegisterOperand(src2.LowBits(), dst);
+  } else {
+    EmitUint8(0x10);
+    EmitXmmRegisterOperand(dst.LowBits(), src2);
+  }
+}
 
 void X86_64Assembler::movsxd(CpuRegister dst, CpuRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -742,6 +898,10 @@ void X86_64Assembler::movd(CpuRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movd(XmmRegister dst, CpuRegister src, bool is64bit) {
+  if (dst.IsYMM()) {
+    vmovd(dst, src, is64bit);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, is64bit, dst.NeedsRex(), false, src.NeedsRex());
@@ -751,6 +911,10 @@ void X86_64Assembler::movd(XmmRegister dst, CpuRegister src, bool is64bit) {
 }
 
 void X86_64Assembler::movd(CpuRegister dst, XmmRegister src, bool is64bit) {
+  if (src.IsYMM()) {
+    vmovd(dst, src, is64bit);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, is64bit, src.NeedsRex(), false, dst.NeedsRex());
@@ -759,6 +923,70 @@ void X86_64Assembler::movd(CpuRegister dst, XmmRegister src, bool is64bit) {
   EmitOperand(src.LowBits(), Operand(dst));
 }
 
+void X86_64Assembler::vmovq(XmmRegister dst, CpuRegister src) { vmovd(dst, src, true); }
+
+void X86_64Assembler::vmovq(CpuRegister dst, XmmRegister src) { vmovd(dst, src, true); }
+
+void X86_64Assembler::vmovd(XmmRegister dst, CpuRegister src, bool is64bit) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !(is64bit || src.NeedsRex());
+
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/is64bit, SET_VEX_L_128, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(0x6E);
+
+  // Instruction Operands
+  EmitOperand(dst.LowBits(), Operand(src));
+}
+
+void X86_64Assembler::vmovd(CpuRegister dst, XmmRegister src, bool is64bit) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !(is64bit || dst.NeedsRex());
+
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(src.NeedsRex(),
+                                    /*X=*/false,
+                                    dst.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/is64bit, SET_VEX_L_128, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(0x7E);
+
+  // Instruction Operands
+  EmitOperand(src.LowBits(), Operand(dst));
+}
+
 void X86_64Assembler::addss(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF3);
@@ -839,6 +1067,10 @@ void X86_64Assembler::divss(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::addps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vaddps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -848,6 +1080,10 @@ void X86_64Assembler::addps(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::subps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vsubps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -856,63 +1092,20 @@ void X86_64Assembler::subps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vaddps(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x58);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, 0x58, SET_VEX_PP_NONE, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::vsubps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t byte_zero = 0x00, byte_one = 0x00, byte_two = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), /*X=*/ false, src2.NeedsRex(), SET_VEX_M_0F);
-    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(byte_zero);
-  EmitUint8(byte_one);
-  if (!is_twobyte_form) {
-    EmitUint8(byte_two);
-  }
-  EmitUint8(0x5C);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x5C, SET_VEX_PP_NONE);
 }
 
 
 void X86_64Assembler::mulps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmulps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -921,37 +1114,14 @@ void X86_64Assembler::mulps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vmulps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x59);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x59, SET_VEX_PP_NONE, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::divps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vdivps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -960,39 +1130,14 @@ void X86_64Assembler::divps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x5E, SET_VEX_PP_NONE);
+}
+
+void X86_64Assembler::vfmadd213ss(XmmRegister acc, XmmRegister left, XmmRegister right) {
+  DCHECK(CpuHasAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x5E);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
-}
-
-void X86_64Assembler::vfmadd213ss(XmmRegister acc, XmmRegister left, XmmRegister right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form=*/ false);
+  ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form=*/false);
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(left.AsFloatRegister());
   ByteOne = EmitVexPrefixByteOne(acc.NeedsRex(),
@@ -1008,7 +1153,7 @@ void X86_64Assembler::vfmadd213ss(XmmRegister acc, XmmRegister left, XmmRegister
 }
 
 void X86_64Assembler::vfmadd213sd(XmmRegister acc, XmmRegister left, XmmRegister right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
   ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form=*/ false);
@@ -1047,7 +1192,7 @@ void X86_64Assembler::fstps(const Address& dst) {
 
 
 void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovapd(dst, src);
     return;
   }
@@ -1061,10 +1206,11 @@ void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src) {
 
 /** VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, xmm2 */
 void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = true;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   if (src.NeedsRex() && dst.NeedsRex()) {
     is_twobyte_form = false;
@@ -1075,18 +1221,13 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
-    ByteOne = EmitVexPrefixByteOne(rex_bit,
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(rex_bit, vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1108,7 +1249,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovapd(dst, src);
     return;
   }
@@ -1122,10 +1263,11 @@ void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
 
 /** VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, m128 */
 void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -1137,18 +1279,13 @@ void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1162,7 +1299,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovupd(dst, src);
     return;
   }
@@ -1176,10 +1313,11 @@ void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
 
 /** VEX.128.66.0F.WIG 10 /r VMOVUPD xmm1, m128 */
 void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -1191,18 +1329,13 @@ void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1215,7 +1348,7 @@ void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovapd(dst, src);
     return;
   }
@@ -1229,10 +1362,12 @@ void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
 
 /** VEX.128.66.0F.WIG 29 /r VMOVAPD m128, xmm1 */
 void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = GetEncodedVexLen(src);
+
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
   bool Rex_x = rex & GET_REX_X;
@@ -1243,18 +1378,13 @@ void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1268,7 +1398,7 @@ void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovupd(dst, src);
     return;
   }
@@ -1282,10 +1412,11 @@ void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
 
 /** VEX.128.66.0F.WIG 11 /r VMOVUPD m128, xmm1 */
 void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = GetEncodedVexLen(src);
 
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
@@ -1297,18 +1428,13 @@ void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1323,6 +1449,10 @@ void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movsd(XmmRegister dst, const Address& src) {
+  if (dst.IsYMM()) {
+    vmovsd(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
   EmitOptionalRex32(dst, src);
@@ -1333,6 +1463,10 @@ void X86_64Assembler::movsd(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::movsd(const Address& dst, XmmRegister src) {
+  if (src.IsYMM()) {
+    vmovsd(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
   EmitOptionalRex32(src, dst);
@@ -1343,6 +1477,10 @@ void X86_64Assembler::movsd(const Address& dst, XmmRegister src) {
 
 
 void X86_64Assembler::movsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmovsd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
   EmitOptionalRex32(src, dst);  // Movsd is MR encoding instead of the usual RM.
@@ -1351,6 +1489,114 @@ void X86_64Assembler::movsd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(src.LowBits(), dst);
 }
 
+/* VEX.LIG.F2.0F.WIG 10 /r VMOVSD xmm1, m32 */
+void X86_64Assembler::vmovsd(XmmRegister dst, const Address& src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = false;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  uint8_t rex = src.rex();
+  bool Rex_x = rex & GET_REX_X;
+  bool Rex_b = rex & GET_REX_B;
+  if (!Rex_b && !Rex_x) {
+    is_twobyte_form = true;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F2);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), Rex_x, Rex_b, SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, SET_VEX_L_128, SET_VEX_PP_F2);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(0x10);
+
+  // Instruction Operands
+  EmitOperand(dst.LowBits(), src);
+}
+
+/* VEX.LIG.F2.0F.WIG 11 /r VMOVSD m32, xmm1 */
+void X86_64Assembler::vmovsd(const Address& dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = false;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  uint8_t rex = dst.rex();
+  bool Rex_x = rex & GET_REX_X;
+  bool Rex_b = rex & GET_REX_B;
+  if (!Rex_b && !Rex_x) {
+    is_twobyte_form = true;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F2);
+  } else {
+    byte_one = EmitVexPrefixByteOne(src.NeedsRex(), Rex_x, Rex_b, SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, SET_VEX_L_128, SET_VEX_PP_F2);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(0x11);
+
+  // Instruction Operands
+  EmitOperand(src.LowBits(), dst);
+}
+
+/* VEX.LIG.F2.0F.WIG 10/11 /r VMOVSD xmm1, xmm2, xmm3 */
+void X86_64Assembler::vmovsd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = true;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  if (dst.NeedsRex() && src2.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  bool is_store = src2.NeedsRex() && !dst.NeedsRex();
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(
+        is_store ? src2.NeedsRex() : dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F2);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_F2);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+
+  // Instruction Opcode && Instruction Operands
+  if (is_store) {
+    // Opcode only when src2 needs rex
+    EmitUint8(0x11);
+    EmitXmmRegisterOperand(src2.LowBits(), dst);
+  } else {
+    EmitUint8(0x10);
+    EmitXmmRegisterOperand(dst.LowBits(), src2);
+  }
+}
 
 void X86_64Assembler::addsd(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1433,6 +1679,10 @@ void X86_64Assembler::divsd(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::addpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vaddpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1443,37 +1693,16 @@ void X86_64Assembler::addpd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vaddpd(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x58);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, 0x58, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 
 void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vsubpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1484,35 +1713,15 @@ void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x5C);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x5C, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::mulpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmulpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1522,37 +1731,14 @@ void X86_64Assembler::mulpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vmulpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x59);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x59, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::divpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vdivpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1563,37 +1749,12 @@ void X86_64Assembler::divpd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x5E);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x5E, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovdqa(dst, src);
     return;
   }
@@ -1607,10 +1768,11 @@ void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src) {
 
 /** VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, xmm2 */
 void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = true;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   // Instruction VEX Prefix
   if (src.NeedsRex() && dst.NeedsRex()) {
@@ -1621,18 +1783,13 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
-    ByteOne = EmitVexPrefixByteOne(rex_bit,
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(rex_bit, vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1654,7 +1811,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovdqa(dst, src);
     return;
   }
@@ -1668,10 +1825,11 @@ void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
 
 /** VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, m128 */
 void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t  ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -1683,18 +1841,13 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1708,7 +1861,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movdqu(XmmRegister dst, const Address& src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (dst.IsYMM()) {
     vmovdqu(dst, src);
     return;
   }
@@ -1723,10 +1876,11 @@ void X86_64Assembler::movdqu(XmmRegister dst, const Address& src) {
 /** VEX.128.F3.0F.WIG 6F /r VMOVDQU xmm1, m128
 Load Unaligned */
 void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
 
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -1738,18 +1892,13 @@ void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_F3);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_F3);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1763,7 +1912,7 @@ void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovdqa(dst, src);
     return;
   }
@@ -1777,10 +1926,12 @@ void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
 
 /** VEX.128.66.0F.WIG 7F /r VMOVDQA m128, xmm1 */
 void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = GetEncodedVexLen(src);
+
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
   bool Rex_x = rex & GET_REX_X;
@@ -1791,18 +1942,13 @@ void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1816,7 +1962,7 @@ void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
-  if (CpuHasAVXorAVX2FeatureFlag()) {
+  if (src.IsYMM()) {
     vmovdqu(dst, src);
     return;
   }
@@ -1830,10 +1976,11 @@ void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
 
 /** VEX.128.F3.0F.WIG 7F /r VMOVDQU m128, xmm1 */
 void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVXFeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = GetEncodedVexLen(src);
 
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
@@ -1845,18 +1992,13 @@ void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
-    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
-                                   vvvv_reg,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_F3);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    Rex_x,
                                    Rex_b,
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
-                                   SET_VEX_PP_F3);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_F3);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1870,6 +2012,10 @@ void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
 }
 
 void X86_64Assembler::paddb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1880,38 +2026,17 @@ void X86_64Assembler::paddb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
-  bool is_twobyte_form = false;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddb(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFC);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, 0xFC, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 
 void X86_64Assembler::psubb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1922,36 +2047,16 @@ void X86_64Assembler::psubb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpsubb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, 0xF8, SET_VEX_PP_66);
+}
+
+
+void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddw(dst, dst, src);
+    return;
   }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xF8);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
-}
-
-
-void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -1961,38 +2066,16 @@ void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddw(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFD);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, 0xFD, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
-
 void X86_64Assembler::psubw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2002,36 +2085,16 @@ void X86_64Assembler::psubw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpsubw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xF9);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, 0xF9, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::pmullw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmullw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2041,37 +2104,15 @@ void X86_64Assembler::pmullw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpmullw(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xD5);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xD5, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2081,37 +2122,16 @@ void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpaddd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddd(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFE);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, 0xFE, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::psubd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2122,6 +2142,10 @@ void X86_64Assembler::psubd(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::pmulld(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmulld(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2132,9 +2156,10 @@ void X86_64Assembler::pmulld(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
   ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form*/ false);
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
@@ -2142,7 +2167,7 @@ void X86_64Assembler::vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F_38);
-  ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
   EmitUint8(ByteTwo);
@@ -2151,6 +2176,10 @@ void X86_64Assembler::vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src
 }
 
 void X86_64Assembler::paddq(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddq(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2161,38 +2190,17 @@ void X86_64Assembler::paddq(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!add_left.NeedsRex()) {
-    return vpaddq(dst, add_right, add_left);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xD4);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(
+      dst, add_left, add_right, 0xD4, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 
 void X86_64Assembler::psubq(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubq(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2202,36 +2210,16 @@ void X86_64Assembler::psubq(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpsubq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFB);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, 0xFB, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::paddusb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddusb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2242,6 +2230,10 @@ void X86_64Assembler::paddusb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::paddsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2252,6 +2244,10 @@ void X86_64Assembler::paddsb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::paddusw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddusw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2262,6 +2258,10 @@ void X86_64Assembler::paddusw(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::paddsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpaddsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2272,6 +2272,10 @@ void X86_64Assembler::paddsw(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::psubusb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubusb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2282,6 +2286,10 @@ void X86_64Assembler::psubusb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::psubsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2292,36 +2300,16 @@ void X86_64Assembler::psubsb(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::vpsubd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   add_right.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xFA);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, add_left, add_right, 0xFA, SET_VEX_PP_66);
 }
 
 
 void X86_64Assembler::psubusw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubusw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2332,6 +2320,10 @@ void X86_64Assembler::psubusw(XmmRegister dst, XmmRegister src) {
 
 
 void X86_64Assembler::psubsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpsubsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2340,6 +2332,61 @@ void X86_64Assembler::psubsw(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/*VEX.128.66.0F.WIG DC /r VPADDUSB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG DC /r VPADDUSB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpaddusb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xDC, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F.WIG EC /r VPADDSB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG EC /r VPADDSB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpaddsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xEC, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F.WIG DD /r VPADDUSW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG DD /r VPADDUSW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpaddusw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xDD, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F.WIG ED /r VPADDSW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG ED /r VPADDSW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpaddsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xED, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F.WIG D8 /r VPSUBUSB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG D8 /r VPSUBUSB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpsubusb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xD8, SET_VEX_PP_66);
+}
+
+/*VEX.128.66.0F.WIG E8 /r VPSUBSB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG E8 /r VPSUBSB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpsubsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xE8, SET_VEX_PP_66);
+}
+
+/*VEX.128.66.0F.WIG D9 /r VPSUBUSW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG D9 /r VPSUBUSW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpsubusw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xD9, SET_VEX_PP_66);
+}
+
+/*VEX.128.66.0F.WIG E9 /r VPSUBSW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG E9 /r VPSUBSW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpsubsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xE9, SET_VEX_PP_66);
+}
 
 void X86_64Assembler::cvtsi2ss(XmmRegister dst, CpuRegister src) {
   cvtsi2ss(dst, src, false);
@@ -2512,6 +2559,10 @@ void X86_64Assembler::cvtsd2ss(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::cvtdq2ps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vcvtdq2ps(dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2519,6 +2570,34 @@ void X86_64Assembler::cvtdq2ps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::vcvtdq2ps(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!src.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_NONE);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x5B);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
 
 void X86_64Assembler::cvtdq2pd(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -2661,6 +2740,10 @@ void X86_64Assembler::xorpd(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::xorpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vxorpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2680,6 +2763,10 @@ void X86_64Assembler::xorps(XmmRegister dst, const Address& src) {
 
 
 void X86_64Assembler::xorps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vxorps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2688,6 +2775,10 @@ void X86_64Assembler::xorps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pxor(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpxor(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2698,98 +2789,18 @@ void X86_64Assembler::pxor(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG EF /r VPXOR xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpxor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpxor(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xEF);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xEF, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 /* VEX.128.0F.WIG 57 /r VXORPS xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vxorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vxorps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x57);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x57, SET_VEX_PP_NONE, /*is_commutative*/ true);
 }
 
 /* VEX.128.66.0F.WIG 57 /r VXORPD xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vxorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vxorpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x57);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x57, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::andpd(XmmRegister dst, const Address& src) {
@@ -2802,6 +2813,10 @@ void X86_64Assembler::andpd(XmmRegister dst, const Address& src) {
 }
 
 void X86_64Assembler::andpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2811,6 +2826,10 @@ void X86_64Assembler::andpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::andps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2819,6 +2838,10 @@ void X86_64Assembler::andps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pand(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpand(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2829,98 +2852,18 @@ void X86_64Assembler::pand(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG DB /r VPAND xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpand(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpand(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xDB);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xDB, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 /* VEX.128.0F 54 /r VANDPS xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vandps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vandps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x54);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x54, SET_VEX_PP_NONE, /*is_commutative*/ true);
 }
 
 /* VEX.128.66.0F 54 /r VANDPD xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vandpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vandpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x54);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x54, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::andn(CpuRegister dst, CpuRegister src1, CpuRegister src2) {
@@ -2943,6 +2886,10 @@ void X86_64Assembler::andn(CpuRegister dst, CpuRegister src1, CpuRegister src2)
 }
 
 void X86_64Assembler::andnpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandnpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2952,6 +2899,10 @@ void X86_64Assembler::andnpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::andnps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vandnps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -2960,6 +2911,10 @@ void X86_64Assembler::andnps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pandn(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpandn(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -2970,95 +2925,25 @@ void X86_64Assembler::pandn(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG DF /r VPANDN xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpandn(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xDF);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xDF, SET_VEX_PP_66);
 }
 
 /* VEX.128.0F 55 /r VANDNPS xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vandnps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x55);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x55, SET_VEX_PP_NONE);
 }
 
 /* VEX.128.66.0F 55 /r VANDNPD xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vandnpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  }
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x55);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x55, SET_VEX_PP_66);
 }
 
 void X86_64Assembler::orpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vorpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3068,6 +2953,10 @@ void X86_64Assembler::orpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::orps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vorps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -3076,6 +2965,10 @@ void X86_64Assembler::orps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::por(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpor(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3086,101 +2979,25 @@ void X86_64Assembler::por(XmmRegister dst, XmmRegister src) {
 
 /* VEX.128.66.0F.WIG EB /r VPOR xmm1, xmm2, xmm3/m128 */
 void X86_64Assembler::vpor(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vpor(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0xEB);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xEB, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 /* VEX.128.0F 56 /r VORPS xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vorps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vorps(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x56);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x56, SET_VEX_PP_NONE, /*is_commutative*/ true);
 }
 
 /* VEX.128.66.0F 56 /r VORPD xmm1,xmm2, xmm3/m128 */
 void X86_64Assembler::vorpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
-  bool is_twobyte_form = false;
-  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!src2.NeedsRex()) {
-    is_twobyte_form = true;
-  } else if (!src1.NeedsRex()) {
-    return vorpd(dst, src2, src1);
-  }
-  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
-  X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
-  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
-  if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  } else {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
-                                   /*X=*/ false,
-                                   src2.NeedsRex(),
-                                   SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
-  }
-  EmitUint8(ByteZero);
-  EmitUint8(ByteOne);
-  if (!is_twobyte_form) {
-    EmitUint8(ByteTwo);
-  }
-  EmitUint8(0x56);
-  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x56, SET_VEX_PP_66, /*is_commutative*/ true);
 }
 
 void X86_64Assembler::pavgb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpavgb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3190,6 +3007,10 @@ void X86_64Assembler::pavgb(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pavgw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpavgw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3198,6 +3019,20 @@ void X86_64Assembler::pavgw(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/*VEX.128.66.0F.WIG E0 /r VPAVGB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG E0 /r VPAVGB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpavgb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xE0, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F.WIG E3 /r VPAVGW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG E3 /r VPAVGW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpavgw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0xE3, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
 void X86_64Assembler::psadbw(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -3217,9 +3052,10 @@ void X86_64Assembler::pmaddwd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::vpmaddwd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
-  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   } else if (!src1.NeedsRex()) {
@@ -3230,13 +3066,13 @@ void X86_64Assembler::vpmaddwd(XmmRegister dst, XmmRegister src1, XmmRegister sr
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -3267,6 +3103,31 @@ void X86_64Assembler::phaddd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+/*VEX.128.66.0F38.WIG 02 /r VPHADDD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38.WIG 02 /r VPHADDD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vphaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  uint8_t byte_zero, byte_one, byte_two;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(/*is_twobyte_form*/ false);
+  byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                  /*X=*/false,
+                                  src2.NeedsRex(),
+                                  SET_VEX_M_0F_38);
+  byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  EmitUint8(byte_two);
+  // Instruction Opcode
+  EmitUint8(0x02);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
 void X86_64Assembler::haddps(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0xF2);
@@ -3324,6 +3185,10 @@ void X86_64Assembler::hsubpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3334,6 +3199,10 @@ void X86_64Assembler::pminsb(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxsb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3344,6 +3213,10 @@ void X86_64Assembler::pmaxsb(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3353,6 +3226,10 @@ void X86_64Assembler::pminsw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxsw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3362,6 +3239,10 @@ void X86_64Assembler::pmaxsw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminsd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3372,6 +3253,10 @@ void X86_64Assembler::pminsd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxsd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3382,6 +3267,10 @@ void X86_64Assembler::pmaxsd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminub(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminub(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3391,6 +3280,10 @@ void X86_64Assembler::pminub(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxub(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxub(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3400,6 +3293,10 @@ void X86_64Assembler::pmaxub(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminuw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminuw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3410,6 +3307,10 @@ void X86_64Assembler::pminuw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxuw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxuw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3420,6 +3321,10 @@ void X86_64Assembler::pmaxuw(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pminud(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpminud(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3430,6 +3335,10 @@ void X86_64Assembler::pminud(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pmaxud(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpmaxud(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3440,6 +3349,10 @@ void X86_64Assembler::pmaxud(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::minps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vminps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -3448,6 +3361,10 @@ void X86_64Assembler::minps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::maxps(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmaxps(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
   EmitUint8(0x0F);
@@ -3456,6 +3373,10 @@ void X86_64Assembler::maxps(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::minpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vminpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3465,6 +3386,10 @@ void X86_64Assembler::minpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::maxpd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vmaxpd(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3474,6 +3399,10 @@ void X86_64Assembler::maxpd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pcmpeqb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpcmpeqb(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3510,6 +3439,11 @@ void X86_64Assembler::pcmpeqq(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::vpcmpeqb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecArithAndLogicalOperation(dst, src1, src2, 0x74, SET_VEX_PP_66, /*is_commutative*/ true);
+}
+
 void X86_64Assembler::pcmpgtb(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -3538,6 +3472,10 @@ void X86_64Assembler::pcmpgtd(XmmRegister dst, XmmRegister src) {
 }
 
 void X86_64Assembler::pcmpgtq(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpcmpgtq(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3547,6 +3485,11 @@ void X86_64Assembler::pcmpgtq(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::vpcmpgtq(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x37);
+}
+
 void X86_64Assembler::shufpd(XmmRegister dst, XmmRegister src, const Immediate& imm) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -3580,6 +3523,10 @@ void X86_64Assembler::pshufd(XmmRegister dst, XmmRegister src, const Immediate&
 
 
 void X86_64Assembler::punpcklbw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpunpcklbw(dst, dst, src);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex32(dst, src);
@@ -3661,6 +3608,10 @@ void X86_64Assembler::punpckhqdq(XmmRegister dst, XmmRegister src) {
 
 void X86_64Assembler::psllw(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsllw(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3673,6 +3624,10 @@ void X86_64Assembler::psllw(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::pslld(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpslld(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3685,6 +3640,10 @@ void X86_64Assembler::pslld(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psllq(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsllq(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3694,9 +3653,33 @@ void X86_64Assembler::psllq(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+/*VEX.128.66.0F.WIG 71 /6 ib VPSLLW xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 71 /6 ib VPSLLW ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsllw(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x71, 6);
+}
+
+/*VEX.128.66.0F.WIG 72 /6 ib VPSLLD xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 72 /6 ib VPSLLD ymm1, ymm2, imm8*/
+void X86_64Assembler::vpslld(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x72, 6);
+}
+
+/*VEX.128.66.0F.WIG 73 /6 ib VPSLLQ xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 73 /6 ib VPSLLQ ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsllq(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x73, 6);
+}
 
 void X86_64Assembler::psraw(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsraw(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3709,6 +3692,10 @@ void X86_64Assembler::psraw(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psrad(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrad(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3718,9 +3705,26 @@ void X86_64Assembler::psrad(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+/*VEX.128.66.0F.WIG 71 /4 ib VPSRAW xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 71 /4 ib VPSRAW ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsraw(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x71, 4);
+}
+
+/*VEX.128.66.0F.WIG 72 /4 ib VPSRAD xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 72 /4 ib VPSRAD ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsrad(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x72, 4);
+}
 
 void X86_64Assembler::psrlw(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrlw(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3733,6 +3737,10 @@ void X86_64Assembler::psrlw(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psrld(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrld(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3745,6 +3753,10 @@ void X86_64Assembler::psrld(XmmRegister reg, const Immediate& shift_count) {
 
 void X86_64Assembler::psrlq(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
+  if (reg.IsYMM()) {
+    vpsrlq(reg, reg, shift_count);
+    return;
+  }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
   EmitOptionalRex(false, false, false, false, reg.NeedsRex());
@@ -3766,6 +3778,302 @@ void X86_64Assembler::psrldq(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+/*VEX.128.66.0F.WIG 71 /2 ib VPSRLW xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 71 /2 ib VPSRLW ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsrlw(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x71, 2);
+}
+
+/*VEX.128.66.0F.WIG 72 /2 ib VPSRLD xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 72 /2 ib VPSRLD ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsrld(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x72, 2);
+}
+
+/*VEX.128.66.0F.WIG 73 /2 ib VPSRLQ xmm1, xmm2, imm8
+  VEX.256.66.0F.WIG 73 /2 ib VPSRLQ ymm1, ymm2, imm8*/
+void X86_64Assembler::vpsrlq(XmmRegister dst, XmmRegister src, const Immediate& shift_count) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecShiftOperation(dst, src, shift_count, 0x73, 2);
+}
+
+/*VEX.256.66.0F3A.W1 01 /r ib VPERMPD ymm1, ymm2/m256, imm8*/
+void X86_64Assembler::vpermpd(XmmRegister dst, XmmRegister src, const Immediate& indices) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  DCHECK(dst.IsYMM());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  ByteZero = EmitVexPrefixByteZero(/*is_twobyte_form*/ false);
+  ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                 /*X=*/false,
+                                 src.NeedsRex(),
+                                 SET_VEX_M_0F_3A);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/true, VEX_L, SET_VEX_PP_66);
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  EmitUint8(ByteTwo);
+  EmitUint8(0x01);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+  EmitUint8(indices.value());
+}
+
+/*VEX.128.66.0F38 38 /r VPMINSB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38 38 /r VPMINSB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpminsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x38);
+}
+
+/*VEX.128.66.0F38.WIG 3C /r VPMAXSB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38.WIG 3C /r VPMAXSB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpmaxsb(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x3C);
+}
+
+/*VEX.128.66.0F EA /r VPMINSW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F EA /r VPMINSW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpminsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte*/ false,
+                         /*opcode*/ 0xEA,
+                         /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F.WIG EE /r VPMAXSW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG EE /r VPMAXSW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpmaxsw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte*/ false,
+                         /*opcode*/ 0xEE,
+                         /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F38.WIG 39 /r VPMINSD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38.WIG 39 /r VPMINSD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpminsd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x39);
+}
+
+/*VEX.128.66.0F38.WIG 3D /r VPMAXSD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38.WIG 3D /r VPMAXSD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpmaxsd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x3D);
+}
+
+/*VEX.128.66.0F DA /r VPMINUB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F DA /r VPMINUB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpminub(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte*/ false,
+                         /*opcode*/ 0xDA,
+                         /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F DE /r VPMAXUB xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F DE /r VPMAXUB ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpmaxub(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst,
+                         src1,
+                         src2,
+                         SET_VEX_PP_66,
+                         /*is_vex_3byte*/ false,
+                         /*opcode*/ 0xDE,
+                         /*is_commutative*/ true);
+}
+
+/*VEX.128.66.0F38 3A/r VPMINUW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38 3A/r VPMINUW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpminuw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x3A);
+}
+
+/*VEX.128.66.0F38 3E/r VPMAXUW xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38 3E/r VPMAXUW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpmaxuw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x3E);
+}
+
+/*VEX.128.66.0F38.WIG 3B /r VPMINUD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38.WIG 3B /r VPMINUD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpminud(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x3B);
+}
+
+/*VEX.128.66.0F38.WIG 3F /r VPMAXUD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F38.WIG 3F /r VPMAXUD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpmaxud(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ true, /*opcode*/ 0x3F);
+}
+
+/*VEX.128.0F.WIG 5D /r VMINPS xmm1, xmm2, xmm3/m128
+  VEX.256.0F.WIG 5D /r VMINPS ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vminps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_NONE, /*is_vex_3byte*/ false, /*opcode*/ 0x5D);
+}
+
+/*VEX.128.0F.WIG 5F /r VMAXPS xmm1, xmm2, xmm3/m128
+  VEX.256.0F.WIG 5F /r VMAXPS ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vmaxps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_NONE, /*is_vex_3byte*/ false, /*opcode*/ 0x5F);
+}
+
+/*VEX.128.66.0F.WIG 5D /r VMINPD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG 5D /r VMINPD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vminpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ false, /*opcode*/ 0x5D);
+}
+
+/*VEX.128.66.0F.WIG 5F /r VMAXPD xmm1, xmm2, xmm3/m128
+  VEX.256.66.0F.WIG 5F /r VMAXPD ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vmaxpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  EmitVecMinMaxOperation(dst, src1, src2, SET_VEX_PP_66, /*is_vex_3byte*/ false, /*opcode*/ 0x5F);
+}
+
+void X86_64Assembler::vbroadcastss(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x18);
+}
+
+void X86_64Assembler::vbroadcastsd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x19);
+}
+
+void X86_64Assembler::vpbroadcastb(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x78);
+}
+
+void X86_64Assembler::vpbroadcastw(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x79);
+}
+
+void X86_64Assembler::vpbroadcastd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x58);
+}
+
+void X86_64Assembler::vpbroadcastq(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag());
+  EmitVecBroadcastInstruction(dst, src, 0x59);
+}
+
+void X86_64Assembler::pabsb(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpabsb(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, false, dst.NeedsRex(), false, src.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(0x38);
+  EmitUint8(0x1C);
+  EmitRegisterOperand(dst.LowBits(), src.LowBits());
+}
+
+void X86_64Assembler::pabsw(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpabsw(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, false, dst.NeedsRex(), false, src.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(0x38);
+  EmitUint8(0x1D);
+  EmitRegisterOperand(dst.LowBits(), src.LowBits());
+}
+
+void X86_64Assembler::pabsd(XmmRegister dst, XmmRegister src) {
+  if (dst.IsYMM()) {
+    vpabsd(dst, src);
+    return;
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex(false, false, dst.NeedsRex(), false, src.NeedsRex());
+  EmitUint8(0x0F);
+  EmitUint8(0x38);
+  EmitUint8(0x1E);
+  EmitRegisterOperand(dst.LowBits(), src.LowBits());
+}
+
+void X86_64Assembler::vpabsb(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecBroadcastInstruction(dst, src, 0x1C);
+}
+
+void X86_64Assembler::vpabsw(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecBroadcastInstruction(dst, src, 0x1D);
+}
+
+void X86_64Assembler::vpabsd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  EmitVecBroadcastInstruction(dst, src, 0x1E);
+}
+
+/*VEX.128.66.0F.WIG 60/r VPUNPCKLBW xmm1,xmm2, xmm3/m128
+  VEX.256.66.0F.WIG 60 /r VPUNPCKLBW ymm1, ymm2, ymm3/m256*/
+void X86_64Assembler::vpunpcklbw(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+  DCHECK(CpuHasAVX2FeatureFlag() || (!dst.IsYMM() && CpuHasAVXFeatureFlag()));
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = true;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  if (src2.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+
+  // Instruction Opcode
+  EmitUint8(0x60);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
 
 void X86_64Assembler::fldl(const Address& src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -5347,6 +5655,14 @@ void X86_64Assembler::ud2() {
   EmitUint8(0x0B);
 }
 
+void X86_64Assembler::vzeroupper() {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0xC5);
+  EmitUint8(0xF8);
+  EmitUint8(0x77);
+}
+
 void X86_64Assembler::LoadDoubleConstant(XmmRegister dst, double value) {
   // TODO: Need to have a code constants table.
   int64_t constant = bit_cast<int64_t, double>(value);
@@ -5616,6 +5932,10 @@ void X86_64Assembler::EmitRex64(CpuRegister dst, XmmRegister src) {
   EmitOptionalRex(false, true, dst.NeedsRex(), false, src.NeedsRex());
 }
 
+void X86_64Assembler::EmitRex64(XmmRegister dst, XmmRegister src) {
+  EmitOptionalRex(false, true, dst.NeedsRex(), false, src.NeedsRex());
+}
+
 void X86_64Assembler::EmitRex64(CpuRegister dst, const Operand& operand) {
   uint8_t rex = 0x48 | operand.rex();  // REX.W000
   if (dst.NeedsRex()) {
@@ -5756,6 +6076,145 @@ uint8_t X86_64Assembler::EmitVexPrefixByteOne(bool R, bool X, bool B, int SET_VE
   return vex_prefix;
 }
 
+void X86_64Assembler::EmitVecArithAndLogicalOperation(XmmRegister dst,
+                                                      XmmRegister src1,
+                                                      XmmRegister src2,
+                                                      uint8_t opcode,
+                                                      int vex_pp,
+                                                      bool is_commutative) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  if (!src2.NeedsRex()) {
+    is_twobyte_form = true;
+  } else if (is_commutative && !src1.NeedsRex()) {
+    return EmitVecArithAndLogicalOperation(dst, src2, src1, opcode, vex_pp, is_commutative);
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, vex_pp);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/false,
+                                   src2.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, vex_pp);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(opcode);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
+void X86_64Assembler::EmitVecMinMaxOperation(XmmRegister dst,
+                                             XmmRegister src1,
+                                             XmmRegister src2,
+                                             uint8_t vex_pp,
+                                             bool is_vex_3byte,
+                                             uint8_t opcode,
+                                             bool is_commutative) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = !is_vex_3byte;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+
+  if (is_twobyte_form) {
+    if (is_commutative && src2.NeedsRex() && !src1.NeedsRex()) {
+      return EmitVecMinMaxOperation(dst, src2, src1, vex_pp, is_vex_3byte, opcode, is_commutative);
+    }
+    is_twobyte_form = !src2.NeedsRex();
+  }
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, vex_pp);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src2.NeedsRex(),
+                                    is_vex_3byte ? SET_VEX_M_0F_38 : SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, vex_pp);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(opcode);
+
+  // Instruction Operands
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
+void X86_64Assembler::EmitVecBroadcastInstruction(XmmRegister dst,
+                                                  XmmRegister src,
+                                                  uint8_t opcode) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  ByteZero = EmitVexPrefixByteZero(false /*is_twobyte_form */);
+  ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                 /*X=*/false,
+                                 src.NeedsRex(),
+                                 SET_VEX_M_0F_38);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/false, VEX_L, SET_VEX_PP_66);
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  EmitUint8(ByteTwo);
+  EmitUint8(opcode);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
+
+void X86_64Assembler::EmitVecShiftOperation(XmmRegister dst,
+                                            XmmRegister src,
+                                            const Immediate& shift_count,
+                                            uint8_t opcode,
+                                            uint8_t operand_byte) {
+  DCHECK(CpuHasAVXFeatureFlag());
+  uint8_t byte_zero, byte_one, byte_two;
+  bool is_twobyte_form = true;
+  uint8_t VEX_L = GetEncodedVexLen(dst);
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+
+  if (src.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  // Instruction VEX Prefix
+  byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(dst.AsFloatRegister());
+  if (is_twobyte_form) {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                    /*X=*/false,
+                                    src.NeedsRex(),
+                                    SET_VEX_M_0F);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(byte_zero);
+  EmitUint8(byte_one);
+  if (!is_twobyte_form) {
+    EmitUint8(byte_two);
+  }
+  // Instruction Opcode
+  EmitUint8(opcode);
+
+  // Instruction Operands
+  EmitXmmRegisterOperand(operand_byte, src);
+  EmitUint8(shift_count.value());
+}
+
 uint8_t X86_64Assembler::EmitVexPrefixByteOne(bool R,
                                               X86_64ManagedRegister operand,
                                               int SET_VEX_L,
@@ -5803,7 +6262,9 @@ uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
     vex_prefix |= SET_VEX_W;
   }
   // Bits[6:3] - 'vvvv' the source or dest register specifier
-  if (operand.IsXmmRegister()) {
+  if (operand.IsNoRegister()) {
+    vex_prefix |= 0x78;
+  } else if (operand.IsXmmRegister()) {
     XmmRegister vvvv = operand.AsXmmRegister();
     int inverted_reg = 15 - static_cast<int>(vvvv.AsFloatRegister());
     uint8_t reg = static_cast<uint8_t>(inverted_reg);
@@ -5825,25 +6286,8 @@ uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
 uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
                                               int SET_VEX_L,
                                               int SET_VEX_PP) {
-  // Vex Byte 2,
-  uint8_t vex_prefix = VEX_INIT;
-
-  /** Bit[7] This bits needs to be set to '1' with default value.
-  When using C4H form of VEX prefix, REX.W value is ignored */
-  if (W) {
-    vex_prefix |= SET_VEX_W;
-  }
-  /** Bits[6:3] - 'vvvv' the source or dest register specifier */
-  vex_prefix |= (0x0F << 3);
-  /** Bit[2] - "L" If VEX.L = 1 indicates 256-bit vector operation,
-  VEX.L = 0 indicates 128 bit vector operation */
-  vex_prefix |= SET_VEX_L;
-
-  // Bits[1:0] -  "pp"
-  if (SET_VEX_PP != SET_VEX_PP_NONE) {
-    vex_prefix |= SET_VEX_PP;
-  }
-  return vex_prefix;
+  X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+  return EmitVexPrefixByteTwo(W, vvvv_reg, SET_VEX_L, SET_VEX_PP);
 }
 
 }  // namespace x86_64
diff --git a/compiler/utils/x86_64/assembler_x86_64.h b/compiler/utils/x86_64/assembler_x86_64.h
index b7475cd367..909a3f305c 100644
--- a/compiler/utils/x86_64/assembler_x86_64.h
+++ b/compiler/utils/x86_64/assembler_x86_64.h
@@ -484,16 +484,21 @@ class X86_64Assembler final : public Assembler {
   void movups(XmmRegister dst, const Address& src);  // load unaligned
   void movaps(const Address& dst, XmmRegister src);  // store aligned
   void movups(const Address& dst, XmmRegister src);  // store unaligned
+  void movups(XmmRegister dst, XmmRegister src);
 
   void vmovaps(XmmRegister dst, XmmRegister src);     // move
   void vmovaps(XmmRegister dst, const Address& src);  // load aligned
   void vmovaps(const Address& dst, XmmRegister src);  // store aligned
   void vmovups(XmmRegister dst, const Address& src);  // load unaligned
   void vmovups(const Address& dst, XmmRegister src);  // store unaligned
+  void vmovups(XmmRegister dst, XmmRegister src);
 
   void movss(XmmRegister dst, const Address& src);
   void movss(const Address& dst, XmmRegister src);
   void movss(XmmRegister dst, XmmRegister src);
+  void vmovss(XmmRegister dst, const Address& src);
+  void vmovss(const Address& dst, XmmRegister src);
+  void vmovss(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void movsxd(CpuRegister dst, CpuRegister src);
   void movsxd(CpuRegister dst, const Address& src);
@@ -502,6 +507,10 @@ class X86_64Assembler final : public Assembler {
   void movd(CpuRegister dst, XmmRegister src);  // Note: this is the r64 version, formally movq.
   void movd(XmmRegister dst, CpuRegister src, bool is64bit);
   void movd(CpuRegister dst, XmmRegister src, bool is64bit);
+  void vmovq(XmmRegister dst, CpuRegister src);
+  void vmovq(CpuRegister dst, XmmRegister src);
+  void vmovd(XmmRegister dst, CpuRegister src, bool is64bit = false);
+  void vmovd(CpuRegister dst, XmmRegister src, bool is64bit = false);
 
   void addss(XmmRegister dst, XmmRegister src);
   void addss(XmmRegister dst, const Address& src);
@@ -517,15 +526,10 @@ class X86_64Assembler final : public Assembler {
   void mulps(XmmRegister dst, XmmRegister src);
   void divps(XmmRegister dst, XmmRegister src);
 
-  void vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-
   void vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
   void vsubps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vsubpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void vfmadd213ss(XmmRegister accumulator, XmmRegister left, XmmRegister right);
   void vfmadd213sd(XmmRegister accumulator, XmmRegister left, XmmRegister right);
@@ -545,6 +549,9 @@ class X86_64Assembler final : public Assembler {
   void movsd(XmmRegister dst, const Address& src);
   void movsd(const Address& dst, XmmRegister src);
   void movsd(XmmRegister dst, XmmRegister src);
+  void vmovsd(XmmRegister dst, const Address& src);
+  void vmovsd(const Address& dst, XmmRegister src);
+  void vmovsd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void addsd(XmmRegister dst, XmmRegister src);
   void addsd(XmmRegister dst, const Address& src);
@@ -560,6 +567,11 @@ class X86_64Assembler final : public Assembler {
   void mulpd(XmmRegister dst, XmmRegister src);
   void divpd(XmmRegister dst, XmmRegister src);
 
+  void vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vsubpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
   void movdqa(XmmRegister dst, XmmRegister src);     // move
   void movdqa(XmmRegister dst, const Address& src);  // load aligned
   void movdqu(XmmRegister dst, const Address& src);  // load unaligned
@@ -576,23 +588,23 @@ class X86_64Assembler final : public Assembler {
   void psubb(XmmRegister dst, XmmRegister src);
 
   void vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vpsubb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void paddw(XmmRegister dst, XmmRegister src);
   void psubw(XmmRegister dst, XmmRegister src);
   void pmullw(XmmRegister dst, XmmRegister src);
-  void vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
-  void vpsubb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
   void vpsubw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vpsubd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmullw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void paddd(XmmRegister dst, XmmRegister src);
   void psubd(XmmRegister dst, XmmRegister src);
   void pmulld(XmmRegister dst, XmmRegister src);
-  void vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void vpaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void paddq(XmmRegister dst, XmmRegister src);
   void psubq(XmmRegister dst, XmmRegister src);
@@ -609,6 +621,15 @@ class X86_64Assembler final : public Assembler {
   void psubusw(XmmRegister dst, XmmRegister src);
   void psubsw(XmmRegister dst, XmmRegister src);
 
+  void vpaddusb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddusw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubusb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubusw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpsubsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
   void cvtsi2ss(XmmRegister dst, CpuRegister src);  // Note: this is the r/m32 version.
   void cvtsi2ss(XmmRegister dst, CpuRegister src, bool is64bit);
   void cvtsi2ss(XmmRegister dst, const Address& src, bool is64bit);
@@ -630,6 +651,7 @@ class X86_64Assembler final : public Assembler {
   void cvttsd2si(CpuRegister dst, XmmRegister src, bool is64bit);
 
   void cvtdq2ps(XmmRegister dst, XmmRegister src);
+  void vcvtdq2ps(XmmRegister dst, XmmRegister src);
   void cvtdq2pd(XmmRegister dst, XmmRegister src);
 
   void comiss(XmmRegister a, XmmRegister b);
@@ -681,11 +703,15 @@ class X86_64Assembler final : public Assembler {
 
   void pavgb(XmmRegister dst, XmmRegister src);  // no addr variant (for now)
   void pavgw(XmmRegister dst, XmmRegister src);
+  void vpavgb(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vpavgw(XmmRegister dst, XmmRegister src, XmmRegister src2);
+
   void psadbw(XmmRegister dst, XmmRegister src);
   void pmaddwd(XmmRegister dst, XmmRegister src);
   void vpmaddwd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
   void phaddw(XmmRegister dst, XmmRegister src);
   void phaddd(XmmRegister dst, XmmRegister src);
+  void vphaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
   void haddps(XmmRegister dst, XmmRegister src);
   void haddpd(XmmRegister dst, XmmRegister src);
   void phsubw(XmmRegister dst, XmmRegister src);
@@ -712,24 +738,59 @@ class X86_64Assembler final : public Assembler {
   void minpd(XmmRegister dst, XmmRegister src);
   void maxpd(XmmRegister dst, XmmRegister src);
 
+  void vpminsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vpmaxsb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxsw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminsd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxsd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
+  void vpminub(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vpmaxub(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminuw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxuw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpminud(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpmaxud(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
+  void vminps(XmmRegister dst, XmmRegister src1, XmmRegister src2);  // no addr variant (for now)
+  void vmaxps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vminpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vmaxpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+
   void pcmpeqb(XmmRegister dst, XmmRegister src);
   void pcmpeqw(XmmRegister dst, XmmRegister src);
   void pcmpeqd(XmmRegister dst, XmmRegister src);
   void pcmpeqq(XmmRegister dst, XmmRegister src);
+  void vpcmpeqb(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void pcmpgtb(XmmRegister dst, XmmRegister src);
   void pcmpgtw(XmmRegister dst, XmmRegister src);
   void pcmpgtd(XmmRegister dst, XmmRegister src);
   void pcmpgtq(XmmRegister dst, XmmRegister src);  // SSE4.2
+  void vpcmpgtq(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void shufpd(XmmRegister dst, XmmRegister src, const Immediate& imm);
   void shufps(XmmRegister dst, XmmRegister src, const Immediate& imm);
   void pshufd(XmmRegister dst, XmmRegister src, const Immediate& imm);
+  void vbroadcastss(XmmRegister dst, XmmRegister src);
+  void vbroadcastsd(XmmRegister dst, XmmRegister src);
+  void vpbroadcastb(XmmRegister dst, XmmRegister src);
+  void vpbroadcastw(XmmRegister dst, XmmRegister src);
+  void vpbroadcastd(XmmRegister dst, XmmRegister src);
+  void vpbroadcastq(XmmRegister dst, XmmRegister src);
+
+  void pabsb(XmmRegister dst, XmmRegister src);
+  void pabsw(XmmRegister dst, XmmRegister src);
+  void pabsd(XmmRegister dst, XmmRegister src);
+  void vpabsb(XmmRegister dst, XmmRegister src);
+  void vpabsw(XmmRegister dst, XmmRegister src);
+  void vpabsd(XmmRegister dst, XmmRegister src);
 
   void punpcklbw(XmmRegister dst, XmmRegister src);
   void punpcklwd(XmmRegister dst, XmmRegister src);
   void punpckldq(XmmRegister dst, XmmRegister src);
   void punpcklqdq(XmmRegister dst, XmmRegister src);
+  void vpunpcklbw(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
   void punpckhbw(XmmRegister dst, XmmRegister src);
   void punpckhwd(XmmRegister dst, XmmRegister src);
@@ -739,15 +800,26 @@ class X86_64Assembler final : public Assembler {
   void psllw(XmmRegister reg, const Immediate& shift_count);
   void pslld(XmmRegister reg, const Immediate& shift_count);
   void psllq(XmmRegister reg, const Immediate& shift_count);
+  void vpsllw(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpslld(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsllq(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
 
   void psraw(XmmRegister reg, const Immediate& shift_count);
   void psrad(XmmRegister reg, const Immediate& shift_count);
   // no psraq
+  void vpsraw(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsrad(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  // vpsraq defined only with EVEX
 
   void psrlw(XmmRegister reg, const Immediate& shift_count);
   void psrld(XmmRegister reg, const Immediate& shift_count);
   void psrlq(XmmRegister reg, const Immediate& shift_count);
   void psrldq(XmmRegister reg, const Immediate& shift_count);
+  void vpsrlw(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsrld(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+  void vpsrlq(XmmRegister dst, XmmRegister src, const Immediate& shift_count);
+
+  void vpermpd(XmmRegister dst, XmmRegister src, const Immediate& indices);
 
   void flds(const Address& src);
   void fstps(const Address& dst);
@@ -989,6 +1061,8 @@ class X86_64Assembler final : public Assembler {
 
   void ud2();
 
+  void vzeroupper();
+
   //
   // Macros for High-level operations.
   //
@@ -1110,7 +1184,8 @@ class X86_64Assembler final : public Assembler {
     }
   }
 
-  bool CpuHasAVXorAVX2FeatureFlag();
+  bool CpuHasAVXFeatureFlag();
+  bool CpuHasAVX2FeatureFlag();
 
  private:
   void EmitUint8(uint8_t value);
@@ -1154,6 +1229,7 @@ class X86_64Assembler final : public Assembler {
   void EmitRex64(XmmRegister dst, const Operand& operand);
   void EmitRex64(XmmRegister dst, CpuRegister src);
   void EmitRex64(CpuRegister dst, XmmRegister src);
+  void EmitRex64(XmmRegister dst, XmmRegister src);
 
   // Emit a REX prefix to normalize byte registers plus necessary register bit encodings.
   // `normalize_both` parameter controls if the REX prefix is checked only for the `src` register
@@ -1179,6 +1255,26 @@ class X86_64Assembler final : public Assembler {
                                int SET_VEX_L,
                                int SET_VEX_PP);
 
+  void EmitVecMinMaxOperation(XmmRegister dst,
+                              XmmRegister src1,
+                              XmmRegister src2,
+                              uint8_t vex_pp,
+                              bool is_vex_3byte,
+                              uint8_t opcode,
+                              bool is_commutative = false);
+  void EmitVecArithAndLogicalOperation(XmmRegister dst,
+                                       XmmRegister src1,
+                                       XmmRegister src2,
+                                       uint8_t opcode,
+                                       int vex_pp,
+                                       bool is_commutative = false);
+  void EmitVecShiftOperation(XmmRegister dst,
+                             XmmRegister src,
+                             const Immediate& shift_count,
+                             uint8_t opcode,
+                             uint8_t operand_byte);
+  void EmitVecBroadcastInstruction(XmmRegister dst, XmmRegister src, uint8_t opcode);
+
   // Helper function to emit a shorter variant of XCHG if at least one operand is RAX/EAX/AX.
   bool try_xchg_rax(CpuRegister dst,
                     CpuRegister src,
diff --git a/compiler/utils/x86_64/assembler_x86_64_test.cc b/compiler/utils/x86_64/assembler_x86_64_test.cc
index e351baafee..c5e68f1913 100644
--- a/compiler/utils/x86_64/assembler_x86_64_test.cc
+++ b/compiler/utils/x86_64/assembler_x86_64_test.cc
@@ -24,6 +24,7 @@
 #include "base/macros.h"
 #include "base/malloc_arena_pool.h"
 #include "base/stl_util.h"
+#include "disassembler_x86.h"
 #include "jni_macro_assembler_x86_64.h"
 #include "utils/assembler_test.h"
 #include "utils/jni_macro_assembler_test.h"
@@ -136,13 +137,15 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
                                                  x86_64::Address,
                                                  x86_64::CpuRegister,
                                                  x86_64::XmmRegister,
-                                                 x86_64::Immediate> {
+                                                 x86_64::Immediate,
+						 x86_64::XmmRegister> {
  public:
   using Base = AssemblerTest<x86_64::X86_64Assembler,
                              x86_64::Address,
                              x86_64::CpuRegister,
                              x86_64::XmmRegister,
-                             x86_64::Immediate>;
+                             x86_64::Immediate,
+			     x86_64::XmmRegister>;
 
  protected:
   AssemblerX86_64Test() : Base() {
@@ -153,6 +156,35 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
     return InstructionSet::kX86_64;
   }
 
+  static void VerifyDisassemblerDriver(const std::vector<uint8_t>& art_code,
+                                       const std::string& ref_assembly_text,
+                                       [[maybe_unused]] const std::string& test_name) {
+    ASSERT_NE(ref_assembly_text.length(), 0U) << "Empty assembly";
+    std::string art_disassembly;
+    MemoryRegion codeMem(const_cast<uint8_t*>(&art_code[0]), art_code.size());
+    x86::DisassemblerX86* disasm = static_cast<art::x86::DisassemblerX86*>(
+        Disassembler::Create(InstructionSet::kX86_64,
+                             new DisassemblerOptions(false,
+                                                     codeMem.begin(),
+                                                     codeMem.end(),
+                                                     /* can_read_literals_= */ true,
+                                                     &Thread::DumpThreadOffset<PointerSize::k64>)));
+    size_t length = 0;
+    std::stringstream sstream;
+    for (const uint8_t* cur = codeMem.begin(); cur < codeMem.end(); cur += length) {
+      length = (disasm)->Dump(sstream, cur);
+      // ART dumps disassembly in this format
+      // Address: Hexbytes \t%-7s<prefix> opcode ....
+      // Extract just the disassembly and compress spaces
+      std::string disassembly = sstream.str();
+      disassembly = disassembly.substr(disassembly.find('\t') + 1);
+      disassembly = disassembly.substr(disassembly.find_first_not_of(" "));
+      art_disassembly += disassembly;
+      sstream.str("");
+    }
+    ASSERT_EQ(art_disassembly, ref_assembly_text) << "Disassembler check failed.";
+  }
+
   void SetUpHelpers() override {
     if (addresses_singleton_.size() == 0) {
       // One addressing mode to test the repeat drivers.
@@ -309,6 +341,28 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
     return ArrayRef<const x86_64::XmmRegister>(kFPRegisters);
   }
 
+  ArrayRef<const x86_64::XmmRegister> GetVectorRegisters() override {
+    static constexpr x86_64::XmmRegister kVectorRegisters[] = {
+        x86_64::XmmRegister(x86_64::XMM0, 32),
+        x86_64::XmmRegister(x86_64::XMM1, 32),
+        x86_64::XmmRegister(x86_64::XMM2, 32),
+        x86_64::XmmRegister(x86_64::XMM3, 32),
+        x86_64::XmmRegister(x86_64::XMM4, 32),
+        x86_64::XmmRegister(x86_64::XMM5, 32),
+        x86_64::XmmRegister(x86_64::XMM6, 32),
+        x86_64::XmmRegister(x86_64::XMM7, 32),
+        x86_64::XmmRegister(x86_64::XMM8, 32),
+        x86_64::XmmRegister(x86_64::XMM9, 32),
+        x86_64::XmmRegister(x86_64::XMM10, 32),
+        x86_64::XmmRegister(x86_64::XMM11, 32),
+        x86_64::XmmRegister(x86_64::XMM12, 32),
+        x86_64::XmmRegister(x86_64::XMM13, 32),
+        x86_64::XmmRegister(x86_64::XMM14, 32),
+        x86_64::XmmRegister(x86_64::XMM15, 32),
+    };
+    return ArrayRef<const x86_64::XmmRegister>(kVectorRegisters);
+  }
+
   x86_64::Immediate CreateImmediate(int64_t imm_value) override {
     return x86_64::Immediate(imm_value);
   }
@@ -1266,11 +1320,11 @@ TEST_F(AssemblerX86_64Test, Movaps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg2}, %{reg1}"), "vmovaps");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg2}, %{reg1}"), "vmovaps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg2}, %{reg1}"), "avx_movaps");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg2}, %{reg1}"), "avx_movaps");
 }
 
 TEST_F(AssemblerX86_64Test, MovapsStore) {
@@ -1278,11 +1332,11 @@ TEST_F(AssemblerX86_64Test, MovapsStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg}, {mem}"), "vmovaps_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg}, {mem}"), "vmovaps_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg}, {mem}"), "avx_movaps_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg}, {mem}"), "avx_movaps_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovapsLoad) {
@@ -1290,11 +1344,11 @@ TEST_F(AssemblerX86_64Test, MovapsLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovaps, "vmovaps {mem}, %{reg}"), "vmovaps_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovaps, "vmovaps {mem}, %{reg}"), "vmovaps_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movaps, "vmovaps {mem}, %{reg}"), "avx_movaps_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movaps, "vmovaps {mem}, %{reg}"), "avx_movaps_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovupsStore) {
@@ -1302,11 +1356,11 @@ TEST_F(AssemblerX86_64Test, MovupsStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg}, {mem}"), "vmovups_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg}, {mem}"), "vmovups_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movups, "vmovups %{reg}, {mem}"), "avx_movups_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movups, "vmovups %{reg}, {mem}"), "avx_movups_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovupsLoad) {
@@ -1314,11 +1368,19 @@ TEST_F(AssemblerX86_64Test, MovupsLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovups, "vmovups {mem}, %{reg}"), "vmovups_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovups, "vmovups {mem}, %{reg}"), "vmovups_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movups, "vmovups {mem}, %{reg}"), "avx_movups_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movups, "vmovups {mem}, %{reg}"), "avx_movups_l");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovupsR2R) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg2}, %{reg1}"), "vmovups_r2r");
+}
+
+TEST_F(AssemblerX86_64AVXTest, MovupsR2R) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movups, "vmovups %{reg2}, %{reg1}"), "avx_movups_r2r");
 }
 
 TEST_F(AssemblerX86_64Test, Movss) {
@@ -1330,11 +1392,11 @@ TEST_F(AssemblerX86_64Test, Movapd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg2}, %{reg1}"), "vmovapd");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg2}, %{reg1}"), "vmovapd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg2}, %{reg1}"), "avx_movapd");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg2}, %{reg1}"), "avx_movapd");
 }
 
 TEST_F(AssemblerX86_64Test, MovapdStore) {
@@ -1342,11 +1404,11 @@ TEST_F(AssemblerX86_64Test, MovapdStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg}, {mem}"), "vmovapd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg}, {mem}"), "vmovapd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg}, {mem}"), "avx_movapd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg}, {mem}"), "avx_movapd_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovapdLoad) {
@@ -1354,11 +1416,11 @@ TEST_F(AssemblerX86_64Test, MovapdLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovapd, "vmovapd {mem}, %{reg}"), "vmovapd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovapd, "vmovapd {mem}, %{reg}"), "vmovapd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movapd, "vmovapd {mem}, %{reg}"), "avx_movapd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movapd, "vmovapd {mem}, %{reg}"), "avx_movapd_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovupdStore) {
@@ -1366,11 +1428,11 @@ TEST_F(AssemblerX86_64Test, MovupdStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovupd, "vmovupd %{reg}, {mem}"), "vmovupd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovupd, "vmovupd %{reg}, {mem}"), "vmovupd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movupd, "vmovupd %{reg}, {mem}"), "avx_movupd_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movupd, "vmovupd %{reg}, {mem}"), "avx_movupd_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovupdLoad) {
@@ -1378,11 +1440,11 @@ TEST_F(AssemblerX86_64Test, MovupdLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovupd, "vmovupd {mem}, %{reg}"), "vmovupd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovupd, "vmovupd {mem}, %{reg}"), "vmovupd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movupd, "vmovupd {mem}, %{reg}"), "avx_movupd_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movupd, "vmovupd {mem}, %{reg}"), "avx_movupd_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movsd) {
@@ -1394,11 +1456,11 @@ TEST_F(AssemblerX86_64Test, Movdqa) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdqa) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg2}, %{reg1}"), "vmovdqa");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg2}, %{reg1}"), "vmovdqa");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movdqa) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg2}, %{reg1}"), "avx_movdqa");
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg2}, %{reg1}"), "avx_movdqa");
 }
 
 TEST_F(AssemblerX86_64Test, MovdqaStore) {
@@ -1406,11 +1468,11 @@ TEST_F(AssemblerX86_64Test, MovdqaStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdqaStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg}, {mem}"), "vmovdqa_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa %{reg}, {mem}"), "vmovdqa_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdqaStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg}, {mem}"), "avx_movdqa_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movdqa, "vmovdqa %{reg}, {mem}"), "avx_movdqa_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovdqaLoad) {
@@ -1418,11 +1480,11 @@ TEST_F(AssemblerX86_64Test, MovdqaLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdqaLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa {mem}, %{reg}"), "vmovdqa_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa {mem}, %{reg}"), "vmovdqa_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdqaLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movdqa, "vmovdqa {mem}, %{reg}"), "avx_movdqa_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movdqa, "vmovdqa {mem}, %{reg}"), "avx_movdqa_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovdquStore) {
@@ -1430,11 +1492,11 @@ TEST_F(AssemblerX86_64Test, MovdquStore) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdquStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu %{reg}, {mem}"), "vmovdqu_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu %{reg}, {mem}"), "vmovdqu_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdquStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movdqu, "vmovdqu %{reg}, {mem}"), "avx_movdqu_s");
+  DriverStr(RepeatAV(&x86_64::X86_64Assembler::movdqu, "vmovdqu %{reg}, {mem}"), "avx_movdqu_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovdquLoad) {
@@ -1442,11 +1504,11 @@ TEST_F(AssemblerX86_64Test, MovdquLoad) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovdquLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu {mem}, %{reg}"), "vmovdqu_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::vmovdqu, "vmovdqu {mem}, %{reg}"), "vmovdqu_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovdquLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movdqu, "vmovdqu {mem}, %{reg}"), "avx_movdqu_l");
+  DriverStr(RepeatVA(&x86_64::X86_64Assembler::movdqu, "vmovdqu {mem}, %{reg}"), "avx_movdqu_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movd1) {
@@ -1470,8 +1532,8 @@ TEST_F(AssemblerX86_64Test, Addps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAddps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vaddps, "vaddps %{reg3}, %{reg2}, %{reg1}"), "vaddps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vaddps, "vaddps %{reg3}, %{reg2}, %{reg1}"),
+            "vaddps");
 }
 
 TEST_F(AssemblerX86_64Test, Addpd) {
@@ -1479,8 +1541,8 @@ TEST_F(AssemblerX86_64Test, Addpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAddpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vaddpd, "vaddpd %{reg3}, %{reg2}, %{reg1}"), "vaddpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vaddpd, "vaddpd %{reg3}, %{reg2}, %{reg1}"),
+            "vaddpd");
 }
 
 TEST_F(AssemblerX86_64Test, Subss) {
@@ -1496,8 +1558,8 @@ TEST_F(AssemblerX86_64Test, Subps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VSubps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vsubps, "vsubps %{reg3},%{reg2}, %{reg1}"), "vsubps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vsubps, "vsubps %{reg3},%{reg2}, %{reg1}"),
+            "vsubps");
 }
 
 TEST_F(AssemblerX86_64Test, Subpd) {
@@ -1505,8 +1567,8 @@ TEST_F(AssemblerX86_64Test, Subpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VSubpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vsubpd, "vsubpd %{reg3}, %{reg2}, %{reg1}"), "vsubpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vsubpd, "vsubpd %{reg3}, %{reg2}, %{reg1}"),
+            "vsubpd");
 }
 
 TEST_F(AssemblerX86_64Test, Mulss) {
@@ -1522,8 +1584,8 @@ TEST_F(AssemblerX86_64Test, Mulps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMulps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vmulps, "vmulps %{reg3}, %{reg2}, %{reg1}"), "vmulps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmulps, "vmulps %{reg3}, %{reg2}, %{reg1}"),
+            "vmulps");
 }
 
 TEST_F(AssemblerX86_64Test, Mulpd) {
@@ -1531,8 +1593,8 @@ TEST_F(AssemblerX86_64Test, Mulpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMulpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vmulpd, "vmulpd %{reg3}, %{reg2}, %{reg1}"), "vmulpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmulpd, "vmulpd %{reg3}, %{reg2}, %{reg1}"),
+            "vmulpd");
 }
 
 TEST_F(AssemblerX86_64Test, Divss) {
@@ -1548,8 +1610,8 @@ TEST_F(AssemblerX86_64Test, Divps) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VDivps) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vdivps, "vdivps %{reg3}, %{reg2}, %{reg1}"), "vdivps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vdivps, "vdivps %{reg3}, %{reg2}, %{reg1}"),
+            "vdivps");
 }
 
 TEST_F(AssemblerX86_64Test, Divpd) {
@@ -1557,8 +1619,8 @@ TEST_F(AssemblerX86_64Test, Divpd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VDivpd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vdivpd, "vdivpd %{reg3}, %{reg2}, %{reg1}"), "vdivpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vdivpd, "vdivpd %{reg3}, %{reg2}, %{reg1}"),
+            "vdivpd");
 }
 
 TEST_F(AssemblerX86_64Test, Paddb) {
@@ -1566,8 +1628,8 @@ TEST_F(AssemblerX86_64Test, Paddb) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddb) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddb, "vpaddb %{reg3}, %{reg2}, %{reg1}"), "vpaddb");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddb, "vpaddb %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddb");
 }
 
 TEST_F(AssemblerX86_64Test, Psubb) {
@@ -1575,8 +1637,8 @@ TEST_F(AssemblerX86_64Test, Psubb) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubb) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubb, "vpsubb %{reg3},%{reg2}, %{reg1}"), "vpsubb");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubb, "vpsubb %{reg3},%{reg2}, %{reg1}"),
+            "vpsubb");
 }
 
 TEST_F(AssemblerX86_64Test, Paddw) {
@@ -1584,13 +1646,13 @@ TEST_F(AssemblerX86_64Test, Paddw) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubw) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubw, "vpsubw %{reg3}, %{reg2}, %{reg1}"), "vpsubw");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubw, "vpsubw %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubw");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddw) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddw, "vpaddw %{reg3}, %{reg2}, %{reg1}"), "vpaddw");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddw, "vpaddw %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddw");
 }
 
 TEST_F(AssemblerX86_64Test, Psubw) {
@@ -1602,8 +1664,8 @@ TEST_F(AssemblerX86_64Test, Pmullw) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPmullw) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpmullw, "vpmullw %{reg3}, %{reg2}, %{reg1}"), "vpmullw");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmullw, "vpmullw %{reg3}, %{reg2}, %{reg1}"),
+            "vpmullw");
 }
 
 TEST_F(AssemblerX86_64Test, Paddd) {
@@ -1611,8 +1673,8 @@ TEST_F(AssemblerX86_64Test, Paddd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddd, "vpaddd %{reg3}, %{reg2}, %{reg1}"), "vpaddd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddd, "vpaddd %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddd");
 }
 
 TEST_F(AssemblerX86_64Test, Psubd) {
@@ -1620,8 +1682,8 @@ TEST_F(AssemblerX86_64Test, Psubd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubd) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubd, "vpsubd %{reg3}, %{reg2}, %{reg1}"), "vpsubd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubd, "vpsubd %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubd");
 }
 
 TEST_F(AssemblerX86_64Test, Pmulld) {
@@ -1629,8 +1691,8 @@ TEST_F(AssemblerX86_64Test, Pmulld) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPmulld) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpmulld, "vpmulld %{reg3}, %{reg2}, %{reg1}"), "vpmulld");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmulld, "vpmulld %{reg3}, %{reg2}, %{reg1}"),
+            "vpmulld");
 }
 
 TEST_F(AssemblerX86_64Test, Paddq) {
@@ -1638,8 +1700,8 @@ TEST_F(AssemblerX86_64Test, Paddq) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPaddq) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpaddq, "vpaddq %{reg3}, %{reg2}, %{reg1}"), "vpaddq");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddq, "vpaddq %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddq");
 }
 
 TEST_F(AssemblerX86_64Test, Psubq) {
@@ -1647,8 +1709,8 @@ TEST_F(AssemblerX86_64Test, Psubq) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPsubq) {
-  DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubq, "vpsubq %{reg3}, %{reg2}, %{reg1}"), "vpsubq");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubq, "vpsubq %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubq");
 }
 
 TEST_F(AssemblerX86_64Test, Paddusb) {
@@ -1772,18 +1834,17 @@ TEST_F(AssemblerX86_64Test, Pxor) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPXor) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpxor,
-                      "vpxor %{reg3}, %{reg2}, %{reg1}"), "vpxor");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpxor, "vpxor %{reg3}, %{reg2}, %{reg1}"), "vpxor");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VXorps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vxorps,
-                      "vxorps %{reg3}, %{reg2}, %{reg1}"), "vxorps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vxorps, "vxorps %{reg3}, %{reg2}, %{reg1}"),
+            "vxorps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VXorpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vxorpd,
-                      "vxorpd %{reg3}, %{reg2}, %{reg1}"), "vxorpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vxorpd, "vxorpd %{reg3}, %{reg2}, %{reg1}"),
+            "vxorpd");
 }
 
 TEST_F(AssemblerX86_64Test, Andps) {
@@ -1799,18 +1860,17 @@ TEST_F(AssemblerX86_64Test, Pand) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPAnd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpand,
-                      "vpand %{reg3}, %{reg2}, %{reg1}"), "vpand");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpand, "vpand %{reg3}, %{reg2}, %{reg1}"), "vpand");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAndps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandps,
-                      "vandps %{reg3}, %{reg2}, %{reg1}"), "vandps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandps, "vandps %{reg3}, %{reg2}, %{reg1}"),
+            "vandps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAndpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandpd,
-                      "vandpd %{reg3}, %{reg2}, %{reg1}"), "vandpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandpd, "vandpd %{reg3}, %{reg2}, %{reg1}"),
+            "vandpd");
 }
 
 TEST_F(AssemblerX86_64Test, Andn) {
@@ -1829,18 +1889,18 @@ TEST_F(AssemblerX86_64Test, Pandn) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPAndn) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpandn,
-                      "vpandn %{reg3}, %{reg2}, %{reg1}"), "vpandn");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpandn, "vpandn %{reg3}, %{reg2}, %{reg1}"),
+            "vpandn");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAndnps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandnps,
-                      "vandnps %{reg3}, %{reg2}, %{reg1}"), "vandnps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandnps, "vandnps %{reg3}, %{reg2}, %{reg1}"),
+            "vandnps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAndnpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vandnpd,
-                      "vandnpd %{reg3}, %{reg2}, %{reg1}"), "vandnpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vandnpd, "vandnpd %{reg3}, %{reg2}, %{reg1}"),
+            "vandnpd");
 }
 
 TEST_F(AssemblerX86_64Test, Orps) {
@@ -1856,18 +1916,15 @@ TEST_F(AssemblerX86_64Test, Por) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPor) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpor,
-                      "vpor %{reg3}, %{reg2}, %{reg1}"), "vpor");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpor, "vpor %{reg3}, %{reg2}, %{reg1}"), "vpor");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Vorps) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vorps,
-                      "vorps %{reg3}, %{reg2}, %{reg1}"), "vorps");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vorps, "vorps %{reg3}, %{reg2}, %{reg1}"), "vorps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Vorpd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vorpd,
-                      "vorpd %{reg3}, %{reg2}, %{reg1}"), "vorpd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vorpd, "vorpd %{reg3}, %{reg2}, %{reg1}"), "vorpd");
 }
 
 TEST_F(AssemblerX86_64Test, Pavgb) {
@@ -1887,8 +1944,8 @@ TEST_F(AssemblerX86_64Test, Pmaddwd) {
 }
 
 TEST_F(AssemblerX86_64AVXTest, VPmaddwd) {
-  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vpmaddwd,
-                      "vpmaddwd %{reg3}, %{reg2}, %{reg1}"), "vpmaddwd");
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaddwd, "vpmaddwd %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaddwd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VFmadd213ss) {
@@ -2147,6 +2204,744 @@ TEST_F(AssemblerX86_64Test, Psrldq) {
             "psrldq $2, %xmm15\n", "psrldqi");
 }
 
+// Assembler tests for AVX
+TEST_F(AssemblerX86_64AVXTest, VMovssLoad) {
+  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovss, "vmovss {mem}, %{reg}"), "vmovss_l");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovssStore) {
+  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovss, "vmovss %{reg}, {mem}"), "vmovss_s");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovss) {
+  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vmovss, "vmovss %{reg3}, %{reg2}, %{reg1}"),
+            "vmovss");
+}
+
+// TEST_F(AssemblerX86_64AVXTest, VMovsdLoad) {
+//   DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovsd, "vmovsd {mem}, %{reg}"), "vmovsd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, VMovsdStore) {
+//   DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovsd, "vmovsd %{reg}, {mem}"), "vmovsd_s");
+// }
+
+TEST_F(AssemblerX86_64AVXTest, VMovsd) {
+  DriverStr(RepeatFFF(&x86_64::X86_64Assembler::vmovsd, "vmovsd %{reg3}, %{reg2}, %{reg1}"),
+            "vmovsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovd1) {
+  DriverStr(RepeatFR(&x86_64::X86_64Assembler::vmovq, "vmovq %{reg2}, %{reg1}"), "vmovd.1");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMovd2) {
+  DriverStr(RepeatRF(&x86_64::X86_64Assembler::vmovq, "vmovq %{reg2}, %{reg1}"), "vmovd.2");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPaddusb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddusb, "vpaddusb %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddusb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPaddsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddsb, "vpaddsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPaddusw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddusw, "vpaddusw %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddusw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPaddsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpaddsw, "vpaddsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpaddsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsubusb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubusb, "vpsubusb %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubusb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsubsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubsb, "vpsubsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsubusw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubusw, "vpsubusw %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubusw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsubsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpsubsw, "vpsubsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpsubsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VCvtdq2ps) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vcvtdq2ps, "vcvtdq2ps %{reg2}, %{reg1}"),
+            "vcvtdq2ps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPavgb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpavgb, "vpavgb %{reg3}, %{reg2}, %{reg1}"),
+            "vpavgb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPavgw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpavgw, "vpavgw %{reg3}, %{reg2}, %{reg1}"),
+            "vpavgw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPhaddd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vphaddd, "vphaddd %{reg3}, %{reg2}, %{reg1}"),
+            "vphaddd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPminsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminsb, "vpminsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpminsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPmaxsb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxsb, "vpmaxsb %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPminsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminsw, "vpminsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpminsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPmaxsw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxsw, "vpmaxsw %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPminsd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminsd, "vpminsd %{reg3}, %{reg2}, %{reg1}"),
+            "vpminsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPmaxsd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxsd, "vpmaxsd %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPminub) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminub, "vpminub %{reg3}, %{reg2}, %{reg1}"),
+            "vpminub");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPmaxub) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxub, "vpmaxub %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxub");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPminuw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminuw, "vpminuw %{reg3}, %{reg2}, %{reg1}"),
+            "vpminuw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPmaxuw) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxuw, "vpmaxuw %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxuw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPminud) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpminud, "vpminud %{reg3}, %{reg2}, %{reg1}"),
+            "vpminud");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPmaxud) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpmaxud, "vpmaxud %{reg3}, %{reg2}, %{reg1}"),
+            "vpmaxud");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMinps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vminps, "vminps %{reg3}, %{reg2}, %{reg1}"),
+            "vminps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMaxps) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmaxps, "vmaxps %{reg3}, %{reg2}, %{reg1}"),
+            "vmaxps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMinpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vminpd, "vminpd %{reg3}, %{reg2}, %{reg1}"),
+            "vminpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VMaxpd) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vmaxpd, "vmaxpd %{reg3}, %{reg2}, %{reg1}"),
+            "vmaxpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPcmpeqb) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpcmpeqb, "vpcmpeqb %{reg3}, %{reg2}, %{reg1}"),
+            "vpcmpeqb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPcmpgtq) {
+  DriverStr(RepeatVVV(&x86_64::X86_64Assembler::vpcmpgtq, "vpcmpgtq %{reg3}, %{reg2}, %{reg1}"),
+            "vpcmpgtq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VBroadcastss) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vbroadcastss, "vbroadcastss %{reg2}, %{reg1}"),
+            "vbroadcastss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VBroadcastsd) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vbroadcastsd, "vbroadcastsd %{reg2}, %{reg1}"),
+            "vbroadcastsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastb) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastb, "vpbroadcastb %{reg2}, %{reg1}"),
+            "vpbroadcastb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastw) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastw, "vpbroadcastw %{reg2}, %{reg1}"),
+            "vpbroadcastw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastd) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastd, "vpbroadcastd %{reg2}, %{reg1}"),
+            "vpbroadcastd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPbroadcastq) {
+  DriverStr(RepeatVF(&x86_64::X86_64Assembler::vpbroadcastq, "vpbroadcastq %{reg2}, %{reg1}"),
+            "vpbroadcastq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pabsb) {
+  DriverStr(RepeatFF(&x86_64::X86_64Assembler::pabsb, "pabsb %{reg2}, %{reg1}"), "pabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pabsw) {
+  DriverStr(RepeatFF(&x86_64::X86_64Assembler::pabsw, "pabsw %{reg2}, %{reg1}"), "pabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, Pabsd) {
+  DriverStr(RepeatFF(&x86_64::X86_64Assembler::pabsd, "pabsd %{reg2}, %{reg1}"), "pabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPabsb) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vpabsb, "vpabsb %{reg2}, %{reg1}"), "vpabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPabsw) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vpabsw, "vpabsw %{reg2}, %{reg1}"), "vpabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPabsd) {
+  DriverStr(RepeatVV(&x86_64::X86_64Assembler::vpabsd, "vpabsd %{reg2}, %{reg1}"), "vpabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsllw) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsllw, 1U, "vpsllw ${imm}, %{reg2}, %{reg1}"),
+            "vpsllw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPslld) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpslld, 1U, "vpslld ${imm}, %{reg2}, %{reg1}"),
+            "vpslld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsllq) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsllq, 1U, "vpsllq ${imm}, %{reg2}, %{reg1}"),
+            "vpsllq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsraw) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsraw, 1U, "vpsraw ${imm}, %{reg2}, %{reg1}"),
+            "vpsraw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsrad) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsrad, 1U, "vpsrad ${imm}, %{reg2}, %{reg1}"),
+            "vpsrad");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsrlw) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsrlw, 1U, "vpsrlw ${imm}, %{reg2}, %{reg1}"),
+            "vpsrlw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsrld) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsrld, 1U, "vpsrld ${imm}, %{reg2}, %{reg1}"),
+            "vpsrld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPsrlq) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpsrlq, 1U, "vpsrlq ${imm}, %{reg2}, %{reg1}"),
+            "vpsrlq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, VPermpd) {
+  DriverStr(RepeatVVI(&x86_64::X86_64Assembler::vpermpd, 1U, "vpermpd ${imm}, %{reg2}, %{reg1}"),
+            "vpermpd");
+}
+
+// Disassembler tests
+TEST_F(AssemblerX86_64AVXTest, DisassVMovaps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vmovaps, "vmovaps {reg1}, {reg2}"),
+                  "disass-vmovaps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassMovaps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::movaps, "vmovaps {reg1}, {reg2}"),
+                  "disass-avx_movaps");
+}
+
+// TODO: Disassembler tests cannot handle memory right now because of difference in
+//       the format between ART disassembly and the combination generator
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovaps,
+//                        "vmovaps {mem}, {reg}"), "disass-vmovaps_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movaps,
+//                        "vmovaps {mem}, {reg}"), "disass-avx_movaps_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovaps,
+//                        "vmovaps {reg}, {mem}"), "disass-vmovaps_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movaps,
+//                        "vmovaps {reg}, {mem}"), "disass-avx_movaps_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovups,
+//                        "vmovups {mem}, {reg}"), "disass-vmovups_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupsStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movups,
+//                        "vmovups {mem}, {reg}"), "disass-avx_movups_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovups,
+//                        "vmovups {reg}, {mem}"), "disass-vmovups_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupsLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movups,
+//                        "vmovups {reg}, {mem}"), "disass-avx_movups_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovssLoad){
+//    CustomDriverStr(VerifyDisassemblerDriver, RepeatFA(&x86_64::X86_64Assembler::vmovss,
+//                         "vmovss {reg}, {mem}"), "disass-vmovss_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovssStore){
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAF(&x86_64::X86_64Assembler::vmovss,
+//                        "vmovss {mem}, {reg}"), "disass-vmovss_s");
+// }
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovss) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFFF(&x86_64::X86_64Assembler::vmovss, "vmovss {reg1}, {reg2}, {reg3}"),
+                  "disass-vmovss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovapd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vmovapd, "vmovapd {reg1}, {reg2}"),
+                  "disass-vmovapd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassMovapd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::movapd, "vmovapd {reg1}, {reg2}"),
+                  "disass-avx_movapd");
+}
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovapd,
+//                        "vmovapd {mem}, {reg}"), "disass-vmovapd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movapd,
+//                        "vmovapd {mem}, {reg}"), "disass-avx_movapd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovapdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovapd,
+//                        "vmovapd {reg}, {mem}"), "disass-vmovapd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovapdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movapd,
+//                        "vmovapd {reg}, {mem}"), "disass-avx_movapd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovupd,
+//                        "vmovupd {mem}, {reg}"), "disass-vmovupd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movupd,
+//                        "vmovupd {mem}, {reg}"), "disass-avx_movupd_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovupdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovupd,
+//                        "vmovupd {reg}, {mem}"), "disass-vmovupd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovupdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movupd,
+//                        "vmovupd {reg}, {mem}"), "disass-avx_movupd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovsdLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatFA(&x86_64::X86_64Assembler::vmovsd,
+//                        "vmovsd {reg}, {mem}"), "disass-vmovsd_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovsdStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAF(&x86_64::X86_64Assembler::vmovsd,
+//                        "vmovsd {mem}, {reg}"), "disass-vmovsd_s");
+// }
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFFF(&x86_64::X86_64Assembler::vmovsd, "vmovsd {reg1}, {reg2}, {reg3}"),
+                  "disass-vmovsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovdqa) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vmovdqa, "vmovdqa {reg1}, {reg2}"),
+                  "disass-vmovdqa");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassMovdqa) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::movdqa, "vmovdqa {reg1}, {reg2}"),
+                  "disass-avx_movdqa");
+}
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdqaStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovdqa,
+//                        "vmovdqa {mem}, {reg}"), "disass-vmovdqa_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdqaStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movdqa,
+//                        "vmovdqa {mem}, {reg}"), "disass-avx_movdqa_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdqaLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovdqa,
+//                        "vmovdqa {reg}, {mem}"), "disass-vmovdqa_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdqaLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movdqa,
+//                        "vmovdqa {reg}, {mem}"), "disass-avx_movdqa_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdquStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::vmovdqu,
+//                        "vmovdqu {mem}, {reg}"), "disass-vmovdqu_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdquStore) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatAV(&x86_64::X86_64Assembler::movdqu,
+//                        "vmovdqu {mem}, {reg}"), "disass-avx_movdqu_s");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassVMovdquLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::vmovdqu,
+//                        "vmovdqu {reg}, {mem}"), "disass-vmovdqu_l");
+// }
+
+// TEST_F(AssemblerX86_64AVXTest, DisassMovdquLoad) {
+//   CustomDriverStr(VerifyDisassemblerDriver, RepeatVA(&x86_64::X86_64Assembler::movdqu,
+//                        "vmovdqu {reg}, {mem}"), "disass-avx_movdqu_l");
+// }
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovd1) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFR(&x86_64::X86_64Assembler::vmovq, "vmovq {reg1}, {reg2}"),
+                  "disass-vmovd.1");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVMovd2) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatRF(&x86_64::X86_64Assembler::vmovq, "vmovq {reg1}, {reg2}"),
+                  "disass-vmovd.2");
+}
+
+// TODO: Disassembler tests cannot handle commutative instructions like vaddps
+//       Hence no disassembler tests for add, mul, avg, xor, or, and etc.
+
+TEST_F(AssemblerX86_64AVXTest, DisassVSubps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vsubps, "vsubps {reg1}, {reg2}, {reg3}"),
+                  "disass-vsubps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVSubpd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vsubpd, "vsubpd {reg1}, {reg2}, {reg3}"),
+                  "disass-vsubpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVDivps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vdivps, "vdivps {reg1}, {reg2}, {reg3}"),
+                  "disassvdivps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVDivpd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vdivpd, "vdivpd {reg1}, {reg2}, {reg3}"),
+                  "disass-vdivpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubb, "vpsubb {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubw, "vpsubw {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubw");
+}
+
+TEST_F(AssemblerX86_64Test, DisassCvtdq2ps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::cvtdq2ps, "cvtdq2ps {reg1}, {reg2}"),
+                  "disass-cvtdq2ps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubd, "vpsubd {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPmulld) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpmulld, "vpmulld {reg1}, {reg2}, {reg3}"),
+                  "disass-vpmulld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubq) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubq, "vpsubq {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubusb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubusb, "vpsubusb {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubusb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubsb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubsb, "vpsubsb {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubusw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubusw, "vpsubusw {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubusw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsubsw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpsubsw, "vpsubsw {reg1}, {reg2}, {reg3}"),
+                  "disass-vpsubsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVCvtdq2ps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vcvtdq2ps, "vcvtdq2ps {reg1}, {reg2}"),
+                  "disass-vcvtdq2ps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPAndn) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vpandn, "vpandn {reg1}, {reg2}, {reg3}"),
+                  "disass-vpandn");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVAndnps) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vandnps, "vandnps {reg1}, {reg2}, {reg3}"),
+                  "disass-vandnps");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVAndnpd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vandnpd, "vandnpd {reg1}, {reg2}, {reg3}"),
+                  "disass-vandnpd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPhaddd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVV(&x86_64::X86_64Assembler::vphaddd, "vphaddd {reg1}, {reg2}, {reg3}"),
+                  "disass-vphaddd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVFmadd213ss) {
+  CustomDriverStr(
+      VerifyDisassemblerDriver,
+      RepeatFFF(&x86_64::X86_64Assembler::vfmadd213ss, "vfmadd213ss {reg1}, {reg2}, {reg3}"),
+      "disass-vfmadd213ss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVFmadd213sd) {
+  CustomDriverStr(
+      VerifyDisassemblerDriver,
+      RepeatFFF(&x86_64::X86_64Assembler::vfmadd213sd, "vfmadd213sd {reg1}, {reg2}, {reg3}"),
+      "disass-vfmadd213sd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVBroadcastss) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vbroadcastss, "vbroadcastss {reg1}, {reg2}"),
+                  "disass-vbroadcastss");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVBroadcastsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vbroadcastsd, "vbroadcastsd {reg1}, {reg2}"),
+                  "disass-vbroadcastsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastb, "vpbroadcastb {reg1}, {reg2}"),
+                  "disass-vpbroadcastb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastw, "vpbroadcastw {reg1}, {reg2}"),
+                  "disass-vpbroadcastw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastd, "vpbroadcastd {reg1}, {reg2}"),
+                  "disass-vpbroadcastd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPbroadcastq) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVF(&x86_64::X86_64Assembler::vpbroadcastq, "vpbroadcastq {reg1}, {reg2}"),
+                  "disass-vpbroadcastq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassPabsb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::pabsb, "pabsb {reg1}, {reg2}"),
+                  "disass-pabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassPabsw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::pabsw, "pabsw {reg1}, {reg2}"),
+                  "disass-pabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassPabsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatFF(&x86_64::X86_64Assembler::pabsd, "pabsd {reg1}, {reg2}"),
+                  "disass-pabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPabsb) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vpabsb, "vpabsb {reg1}, {reg2}"),
+                  "disass-vpabsb");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPabsw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vpabsw, "vpabsw {reg1}, {reg2}"),
+                  "disass-vpabsw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPabsd) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVV(&x86_64::X86_64Assembler::vpabsd, "vpabsd {reg1}, {reg2}"),
+                  "disass-vpabsd");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsllw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsllw, 1U, "vpsllw {reg2}, {reg1}, {imm}"),
+                  "disass-vpsllw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPslld) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpslld, 1U, "vpslld {reg2}, {reg1}, {imm}"),
+                  "disass-vpslld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsllq) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsllq, 1U, "vpsllq {reg2}, {reg1}, {imm}"),
+                  "disass-vpsllq");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsraw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsraw, 1U, "vpsraw {reg2}, {reg1}, {imm}"),
+                  "disass-vpsraw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsrad) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsrad, 1U, "vpsrad {reg2}, {reg1}, {imm}"),
+                  "disass-vpsrad");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsrlw) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsrlw, 1U, "vpsrlw {reg2}, {reg1}, {imm}"),
+                  "disass-vpsrlw");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsrld) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsrld, 1U, "vpsrld {reg2}, {reg1}, {imm}"),
+                  "disass-vpsrld");
+}
+
+TEST_F(AssemblerX86_64AVXTest, DisassVPsrlq) {
+  CustomDriverStr(VerifyDisassemblerDriver,
+                  RepeatVVI(&x86_64::X86_64Assembler::vpsrlq, 1U, "vpsrlq {reg2}, {reg1}, {imm}"),
+                  "disass-vpsrlq");
+}
+
 std::string x87_fn([[maybe_unused]] AssemblerX86_64Test::Base* assembler_test,
                    x86_64::X86_64Assembler* assembler) {
   std::ostringstream str;
diff --git a/compiler/utils/x86_64/constants_x86_64.h b/compiler/utils/x86_64/constants_x86_64.h
index 52ac987766..22f72169cd 100644
--- a/compiler/utils/x86_64/constants_x86_64.h
+++ b/compiler/utils/x86_64/constants_x86_64.h
@@ -51,8 +51,10 @@ std::ostream& operator<<(std::ostream& os, const CpuRegister& reg);
 
 class XmmRegister {
  public:
-  explicit constexpr XmmRegister(FloatRegister r) : reg_(r) {}
-  explicit constexpr XmmRegister(int r) : reg_(FloatRegister(r)) {}
+  explicit constexpr XmmRegister(FloatRegister r) : reg_(r), vector_length_(0) {}
+  explicit constexpr XmmRegister(int r) : reg_(FloatRegister(r)), vector_length_(0) {}
+  constexpr XmmRegister(FloatRegister r, size_t vector_length) : reg_(r), vector_length_(vector_length) {}
+  constexpr XmmRegister(int r, size_t vector_length) : reg_(FloatRegister(r)), vector_length_(vector_length) {}
   constexpr FloatRegister AsFloatRegister() const {
     return reg_;
   }
@@ -65,8 +67,14 @@ class XmmRegister {
   bool operator==(const XmmRegister& other) const {
     return reg_ == other.reg_;
   }
+  size_t GetVecLen() const { return vector_length_; }
+  bool IsYMM() const {
+    return vector_length_ == 32;  // 256-bit
+  }
+
  private:
   const FloatRegister reg_;
+  size_t vector_length_;
 };
 std::ostream& operator<<(std::ostream& os, const XmmRegister& reg);
 
diff --git a/disassembler/disassembler_x86.cc b/disassembler/disassembler_x86.cc
index bf8af72c3e..b4765bc091 100644
--- a/disassembler/disassembler_x86.cc
+++ b/disassembler/disassembler_x86.cc
@@ -96,6 +96,8 @@ static void DumpAnyReg(std::ostream& os, uint8_t rex, size_t reg,
     DumpReg0(os, rex, reg, byte_operand, size_override);
   } else if (reg_file == SSE) {
     os << "xmm" << reg;
+  } else if (reg_file == AVX) {
+    os << "ymm" << reg;
   } else {
     os << "mm" << reg;
   }
@@ -288,6 +290,8 @@ void DisassemblerX86::GetInstructionDetails(const uint8_t* instr, x86_instr *ins
 }
 
 size_t DisassemblerX86::DumpInstruction(std::ostream& os, const uint8_t* instr,x86_instr *insn_x86) {
+  InstructionContext ctxt(this, instr);
+  instr = ctxt.shadowInstr;
   if (os) {
      size_t nop_size = DumpNops(os, instr);
      if (nop_size != 0u) {
@@ -360,7 +364,9 @@ size_t DisassemblerX86::DumpInstruction(std::ostream& os, const uint8_t* instr,x
   bool no_ops = false;
   RegFile src_reg_file = GPR;
   RegFile dst_reg_file = GPR;
-
+  bool needs_vex = false;  // To detect VEX only instructions
+  bool has_3_operands = false;
+  bool width_qualified = false;
 
   switch (*instr) {
 #define DISASSEMBLER_ENTRY(opname, \
@@ -463,9 +469,11 @@ DISASSEMBLER_ENTRY(cmp,
       case 0x10: case 0x11:
         if (prefix[0] == 0xF2) {
           opcode1 = "movsd";
+          has_3_operands = ctxt.hasVex && ((*(instr + 1) & 0xC0) == 0xC0);
           prefix[0] = 0;  // clear prefix now it's served its purpose as part of the opcode
         } else if (prefix[0] == 0xF3) {
           opcode1 = "movss";
+          has_3_operands = ctxt.hasVex && ((*(instr + 1) & 0xC0) == 0xC0);
           prefix[0] = 0;  // clear prefix now it's served its purpose as part of the opcode
         } else if (prefix[2] == 0x66) {
           opcode1 = "movupd";
@@ -593,10 +601,38 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x02:
               opcode1 = "phaddd";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
               break;
+            case 0x18:
+              opcode1 = "broadcastss";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x19:
+              opcode1 = "broadcastsd";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x1C:
+              opcode1 = "pabsb";
+              width_qualified = has_modrm = load = true;
+              src_reg_file = dst_reg_file = SSE;
+              break;
+            case 0x1D:
+              opcode1 = "pabsw";
+              width_qualified = has_modrm = load = true;
+              src_reg_file = dst_reg_file = SSE;
+              break;
+            case 0x1E:
+              opcode1 = "pabsd";
+              width_qualified = has_modrm = load = true;
+              src_reg_file = dst_reg_file = SSE;
+              break;
             case 0x29:
               opcode1 = "pcmpeqq";
               prefix[2] = 0;
@@ -614,6 +650,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x38:
               opcode1 = "pminsb";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -621,6 +658,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x39:
               opcode1 = "pminsd";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -628,6 +666,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3A:
               opcode1 = "pminuw";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -635,6 +674,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3B:
               opcode1 = "pminud";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -642,6 +682,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3C:
               opcode1 = "pmaxsb";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -649,6 +690,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3D:
               opcode1 = "pmaxsd";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -656,6 +698,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3E:
               opcode1 = "pmaxuw";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -663,6 +706,7 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x3F:
               opcode1 = "pmaxud";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
@@ -670,10 +714,43 @@ DISASSEMBLER_ENTRY(cmp,
             case 0x40:
               opcode1 = "pmulld";
               prefix[2] = 0;
+              has_3_operands = ctxt.hasVex;
               has_modrm = true;
               load = true;
               src_reg_file = dst_reg_file = SSE;
               break;
+            case 0x58:
+              opcode1 = "pbroadcastd";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x59:
+              opcode1 = "pbroadcastq";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x78:
+              opcode1 = "pbroadcastb";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0x79:
+              opcode1 = "pbroadcastw";
+              needs_vex = has_modrm = load = true;
+              dst_reg_file = AVX;
+              src_reg_file = SSE;
+              break;
+            case 0xA9:
+              opcode1 = "fmadd213ss";
+              if ((rex & REX_W) != 0) {
+                opcode1 = "fmadd213sd";
+              }
+              width_qualified = has_3_operands = needs_vex = has_modrm = load = true;
+              dst_reg_file = src_reg_file = SSE;
+              break;
             default:
               opcode_tmp = StringPrintf("unknown opcode '0F 38 %02X'", *instr);
               opcode1 = opcode_tmp.c_str();
@@ -687,6 +764,12 @@ DISASSEMBLER_ENTRY(cmp,
         instr++;
         if (prefix[2] == 0x66) {
           switch (*instr) {
+            case 0x01:
+              opcode1 = "permpd";
+              width_qualified = needs_vex = has_modrm = load = true;
+              dst_reg_file = src_reg_file = AVX;
+              immediate_bytes = 1;
+              break;
             case 0x0A:
               opcode1 = "roundss";
               prefix[2] = 0;
@@ -764,6 +847,7 @@ DISASSEMBLER_ENTRY(cmp,
           case 0x5F: opcode1 = "max"; break;
           default: LOG(FATAL) << "Unreachable"; UNREACHABLE();
         }
+        has_3_operands = ctxt.hasVex;
         if (prefix[2] == 0x66) {
           opcode2 = "pd";
           prefix[2] = 0;  // clear prefix now it's served its purpose as part of the opcode
@@ -861,6 +945,10 @@ DISASSEMBLER_ENTRY(cmp,
           dst_reg_file = MMX;
         }
         opcode1 = "movd";
+        if ((rex & REX_W) != 0) {
+          opcode1 = "movq";
+        }
+        width_qualified = true;
         load = true;
         has_modrm = true;
         break;
@@ -911,6 +999,7 @@ DISASSEMBLER_ENTRY(cmp,
             "unknown-71", "unknown-71", "psrlw", "unknown-71",
             "psraw",      "unknown-71", "psllw", "unknown-71"};
         modrm_opcodes = x71_opcodes;
+        has_3_operands = ctxt.hasVex;
         reg_is_opcode = true;
         has_modrm = true;
         store = true;
@@ -927,6 +1016,7 @@ DISASSEMBLER_ENTRY(cmp,
             "unknown-72", "unknown-72", "psrld", "unknown-72",
             "psrad",      "unknown-72", "pslld", "unknown-72"};
         modrm_opcodes = x72_opcodes;
+        has_3_operands = ctxt.hasVex;
         reg_is_opcode = true;
         has_modrm = true;
         store = true;
@@ -943,6 +1033,7 @@ DISASSEMBLER_ENTRY(cmp,
             "unknown-73", "unknown-73", "psrlq", "psrldq",
             "unknown-73", "unknown-73", "psllq", "unknown-73"};
         modrm_opcodes = x73_opcodes;
+        has_3_operands = ctxt.hasVex;
         reg_is_opcode = true;
         has_modrm = true;
         store = true;
@@ -962,10 +1053,15 @@ DISASSEMBLER_ENTRY(cmp,
           case 0x75: opcode1 = "pcmpeqw"; break;
           case 0x76: opcode1 = "pcmpeqd"; break;
         }
+        has_3_operands = ctxt.hasVex;
         prefix[2] = 0;
         has_modrm = true;
         load = true;
         break;
+      case 0x77:
+        needs_vex = true;
+        opcode1 = "zeroupper";
+        break;
       case 0x7C:
         if (prefix[0] == 0xF2) {
           opcode1 = "haddps";
@@ -990,6 +1086,10 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = MMX;
         }
         opcode1 = "movd";
+        if ((rex & REX_W) != 0) {
+          opcode1 = "movq";
+        }
+        width_qualified = true;
         has_modrm = true;
         store = true;
         break;
@@ -1189,6 +1289,7 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = dst_reg_file = MMX;
         }
         opcode1 = "paddq";
+        has_3_operands = ctxt.hasVex;
         prefix[2] = 0;
         has_modrm = true;
         load = true;
@@ -1201,6 +1302,7 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = dst_reg_file = MMX;
         }
         opcode1 = "pand";
+        has_3_operands = ctxt.hasVex;
         prefix[2] = 0;
         has_modrm = true;
         load = true;
@@ -1209,6 +1311,7 @@ DISASSEMBLER_ENTRY(cmp,
         if (prefix[2] == 0x66) {
           opcode1 = "pmullw";
           prefix[2] = 0;
+          has_3_operands = ctxt.hasVex;
           has_modrm = true;
           load = true;
           src_reg_file = dst_reg_file = SSE;
@@ -1223,6 +1326,7 @@ DISASSEMBLER_ENTRY(cmp,
       case 0xDC:
       case 0xDD:
       case 0xDE:
+      case 0xDF:
       case 0xE0:
       case 0xE3:
       case 0xE8:
@@ -1237,6 +1341,7 @@ DISASSEMBLER_ENTRY(cmp,
         } else {
           src_reg_file = dst_reg_file = MMX;
         }
+        has_3_operands = ctxt.hasVex;
         switch (*instr) {
           case 0xD8: opcode1 = "psubusb"; break;
           case 0xD9: opcode1 = "psubusw"; break;
@@ -1244,6 +1349,9 @@ DISASSEMBLER_ENTRY(cmp,
           case 0xDC: opcode1 = "paddusb"; break;
           case 0xDD: opcode1 = "paddusw"; break;
           case 0xDE: opcode1 = "pmaxub"; break;
+          case 0xDF:
+            opcode1 = "pandn";
+            break;
           case 0xE0: opcode1 = "pavgb"; break;
           case 0xE3: opcode1 = "pavgw"; break;
           case 0xE8: opcode1 = "psubsb"; break;
@@ -1266,6 +1374,7 @@ DISASSEMBLER_ENTRY(cmp,
         }
         opcode1 = "por";
         prefix[2] = 0;
+        has_3_operands = ctxt.hasVex;
         has_modrm = true;
         load = true;
         break;
@@ -1277,11 +1386,13 @@ DISASSEMBLER_ENTRY(cmp,
           src_reg_file = dst_reg_file = MMX;
         }
         opcode1 = "pxor";
+        has_3_operands = ctxt.hasVex;
         prefix[2] = 0;
         has_modrm = true;
         load = true;
         break;
       case 0xF4:
+      case 0xF5:
       case 0xF6:
       case 0xF8:
       case 0xF9:
@@ -1298,6 +1409,9 @@ DISASSEMBLER_ENTRY(cmp,
         }
         switch (*instr) {
           case 0xF4: opcode1 = "pmuludq"; break;
+          case 0xF5:
+            opcode1 = "pmaddwd";
+            break;
           case 0xF6: opcode1 = "psadbw"; break;
           case 0xF8: opcode1 = "psubb"; break;
           case 0xF9: opcode1 = "psubw"; break;
@@ -1307,6 +1421,7 @@ DISASSEMBLER_ENTRY(cmp,
           case 0xFD: opcode1 = "paddw"; break;
           case 0xFE: opcode1 = "paddd"; break;
         }
+        has_3_operands = ctxt.hasVex;
         prefix[2] = 0;
         has_modrm = true;
         load = true;
@@ -1504,6 +1619,25 @@ DISASSEMBLER_ENTRY(cmp,
     opcode1 = opcode_tmp.c_str();
     break;
   }
+
+  if (needs_vex && !ctxt.hasVex) {
+    opcode_tmp = StringPrintf("unknown opcode '%02X', may be missing VEX prefix", *instr);
+    opcode1 = opcode_tmp.c_str();
+  }
+
+  // If it needs vex, then we know the exact register types before hand
+  if (!needs_vex && ctxt.hasVex && ctxt.Vex.vector_length == 1) {
+    if (src_reg_file == MMX) {
+      src_reg_file = SSE;
+    } else if (src_reg_file == SSE) {
+      src_reg_file = AVX;
+    }
+    if (dst_reg_file == MMX) {
+      dst_reg_file = SSE;
+    } else if (dst_reg_file == SSE) {
+      dst_reg_file = AVX;
+    }
+  }
   std::ostringstream args;
   // We force the REX prefix to be available for 64-bit target
   // in order to dump addr (base/index) registers correctly.
@@ -1530,13 +1664,17 @@ DISASSEMBLER_ENTRY(cmp,
       opcode3 = modrm_opcodes[reg_or_opcode];
     }
 
-    // Add opcode suffixes to indicate size.
-    if (byte_operand) {
-      opcode4 = "b";
-    } else if ((rex & REX_W) != 0) {
-      opcode4 = "q";
-    } else if (prefix[2] == 0x66) {
-      opcode4 = "w";
+    // Not applicable for vex only opcodes and opcodes with
+    // fully qualified widths, ex: vpbroadcastb, vpabsb
+    if (!needs_vex && !width_qualified) {
+      // Add opcode suffixes to indicate size.
+      if (byte_operand) {
+        opcode4 = "b";
+      } else if ((rex & REX_W) != 0) {
+        opcode4 = "q";
+      } else if (prefix[2] == 0x66) {
+        opcode4 = "w";
+      }
     }
 
     if (load) {
@@ -1544,6 +1682,10 @@ DISASSEMBLER_ENTRY(cmp,
         DumpReg(args, rex, reg_or_opcode, byte_operand, prefix[2], dst_reg_file);
         args << ", ";
       }
+      if (has_3_operands) {
+        DumpAnyReg(args, 0, ctxt.Vex.operand, false, 0, dst_reg_file);
+        args << ", ";
+      }
       DumpSegmentOverride(args, prefix[1]);
 
       args << address;
@@ -1551,6 +1693,10 @@ DISASSEMBLER_ENTRY(cmp,
       DCHECK(store);
       DumpSegmentOverride(args, prefix[1]);
       args << address;
+      if (has_3_operands) {
+        args << ", ";
+        DumpAnyReg(args, 0, ctxt.Vex.operand, false, 0, dst_reg_file);
+      }
       if (!reg_is_opcode) {
         args << ", ";
         DumpReg(args, rex, reg_or_opcode, byte_operand, prefix[2], src_reg_file);
@@ -1616,10 +1762,21 @@ DISASSEMBLER_ENTRY(cmp,
     case 0: prefix_str = ""; break;
     default: LOG(FATAL) << "Unreachable"; UNREACHABLE();
   }
+  size_t actual_bytes_read =
+      (instr - begin_instr - (ctxt.Vex.shadow_prefix_length - ctxt.Vex.prefix_length));
   if (os) {
-     os << FormatInstructionPointer(begin_instr)
-        << StringPrintf(": %22s    \t%-7s%s%s%s%s%s ", DumpCodeHex(begin_instr, instr).c_str(),
-                     prefix_str, opcode0, opcode1, opcode2, opcode3, opcode4)
+  opcode0 = (ctxt.hasVex) ? "v" : opcode0;
+  // We may be decoding from a shadow buffer, make adjustments to read from orig buffer
+  const uint8_t* adjustedInstr = ctxt.origInstr + actual_bytes_read;
+  os << FormatInstructionPointer(ctxt.origInstr)
+     << StringPrintf(": %22s    \t%-7s%s%s%s%s%s ",
+                     DumpCodeHex(ctxt.origInstr, adjustedInstr).c_str(),
+                     prefix_str,
+                     opcode0,
+                     opcode1,
+                     opcode2,
+                     opcode3,
+                     opcode4)
         << args.str() << '\n';
   }
   if (insn_x86 != nullptr) {
@@ -1635,9 +1792,136 @@ DISASSEMBLER_ENTRY(cmp,
      insn_x86->prefix[3] = prefix[3];
      insn_x86->opcode = instr;
   }
-  return instr - begin_instr;
+  return actual_bytes_read;
 }  // NOLINT(readability/fn_size)
 
+DisassemblerX86::InstructionContext::InstructionContext(DisassemblerX86* disass,
+                                                        const uint8_t* instr) {
+  disassembler = disass;
+  origInstr = instr;
+  shadowInstr = instr;
+  // The existing disassembler understands REX prefix
+  // Inorder to reuse all existing code, interpret the VEX Prefix here,
+  // and generate byte sequence as if it were emitted with a REX prefix.
+  // Later when we enable EVEX ISA we should use similar strategy.
+  if (Vex.ConvertToRex(
+          instr, shadowInstrBuffer, disassembler->GetDisassemblerOptions()->end_address_)) {
+    shadowInstr = shadowInstrBuffer;
+    hasVex = true;
+  }
+}
+
+bool DisassemblerX86::InstructionContext::VexPrefix::ConvertToRex(const uint8_t* instr,
+                                                                  uint8_t* const decodeBuffer,
+                                                                  const uint8_t* instr_end) {
+  if (*instr != TWO_BYTE_VEX && *instr != THREE_BYTE_VEX) {
+    prefix_length = 0;
+    shadow_prefix_length = 0;
+    vector_length = 0;
+    operand = 0;
+    return false;
+  }
+
+  memset(decodeBuffer, 0, kMaxInstructionLength + 1);
+  uint8_t byteZero = 0, byteOne = 0, byteTwo = 0;
+  uint8_t pp = 0, rex = 0, mm1 = 0, mm2 = 0;
+  byteZero = *instr;
+  byteOne = *(instr + 1);
+  instr += 2;
+
+  if (byteZero == TWO_BYTE_VEX) {
+    prefix_length = 2;
+    // Emit REX prefix if required
+    // Extract REX.R [7] bit
+    // Note that in VEX prefix R is stored as 1's complement
+    if (((byteOne >> 7) & 1) == 0) {
+      rex = 0x44;  // REX.0R00
+    }
+    // VEX_M is assumed as SET_VEX_M_0F for 2-byte VEX
+    mm1 = 0x0F;
+    // Extract the VEX_PP [1:0] bits as prefix
+    pp = (byteOne & 0x03);
+    // Extract VEX_L [2] bit
+    vector_length = (byteOne >> 2) & 0x01;
+    // Extract operand [6:3] bits is operand stored as 1's complement
+    operand = (~(byteOne >> 3) & 0x0F);
+  } else {
+    prefix_length = 3;
+    byteTwo = *instr;
+    instr++;
+    // Emit REX prefix if required
+    // Extract REX.RXB [7:5] bits
+    // Note that in VEX prefix R is stored as 1's complement
+    if ((byteOne & 0xE0) != 0xE0) {
+      rex = 0x40;
+      rex |= (~(byteOne >> 5) & 0x07);
+    }
+    // Extract the VEX_M [4:0] bits
+    switch ((byteOne & 0x0F)) {
+      case 0x01:  // SET_VEX_M_0F
+        mm1 = 0x0F;
+        break;
+      case 0x02:  // SET_VEX_M_0F_38
+        mm1 = 0x0F;
+        mm2 = 0x38;
+        break;
+      case 0x03:  // SET_VEX_M_0F_3A
+        mm1 = 0x0F;
+        mm2 = 0x3A;
+        break;
+      default:
+        break;
+    }
+    // Extract the REX.W [7] bit
+    if ((byteTwo & 0x80) != 0) {
+      rex |= 0x48;
+    }
+    // Extract the VEX_PP [1:0] bits as prefix
+    pp = (byteTwo & 0x03);
+    // Extract VEX_L [2] bit
+    vector_length = (byteTwo >> 2) & 0x01;
+    // Extract operand [6:3] bits is operand stored as 1's complement
+    operand = (~(byteTwo >> 3) & 0x0F);
+  }
+
+  // Decode the pp prefix
+  switch (pp) {
+    case 0x01:  // SET_VEX_PP_66
+      pp = 0x66;
+      break;
+    case 0x02:  // SET_VEX_PP_F3
+      pp = 0xF3;
+      break;
+    case 0x03:  // SET_VEX_PP_F2
+      pp = 0xF2;
+      break;
+    default:  // SET_VEX_PP_NONE
+      pp = 0;
+      break;
+  }
+
+  // Resultant byte sequence to pass on
+  // pp rex mm1 mm2 instr
+  int idx = 0;
+  if (pp != 0) {
+    decodeBuffer[idx++] = pp;
+  }
+  if (rex != 0) {
+    decodeBuffer[idx++] = rex;
+  }
+  if (mm1 != 0) {
+    decodeBuffer[idx++] = mm1;
+  }
+  if (mm2 != 0) {
+    decodeBuffer[idx++] = mm2;
+  }
+  shadow_prefix_length = idx;
+  // Fill the remaining buffer
+  for (; idx < kMaxInstructionLength && instr < instr_end; ++instr) {
+    decodeBuffer[idx++] = *instr;
+  }
+  return true;
+}
 
 }  // namespace x86
 }  // namespace art
diff --git a/disassembler/disassembler_x86.h b/disassembler/disassembler_x86.h
index 009643e052..47d62b0345 100644
--- a/disassembler/disassembler_x86.h
+++ b/disassembler/disassembler_x86.h
@@ -31,7 +31,9 @@ struct x86_instr {
         const uint8_t* opcode;            /* Opcode byte */
 };
 
-enum RegFile { GPR, MMX, SSE };
+static constexpr uint8_t kMaxInstructionLength = 15;
+
+enum RegFile { GPR, MMX, SSE, AVX };
 
 class DisassemblerX86 final : public Disassembler {
  public:
@@ -43,6 +45,28 @@ class DisassemblerX86 final : public Disassembler {
   void Dump(std::ostream& os, const uint8_t* begin, const uint8_t* end) override;
 
  private:
+  struct InstructionContext {
+    DisassemblerX86* disassembler;
+    bool hasVex;
+    const uint8_t* origInstr;
+    const uint8_t* shadowInstr;
+    uint8_t shadowInstrBuffer[kMaxInstructionLength + 1];
+
+    struct VexPrefix {
+      uint8_t prefix_length;
+      uint8_t shadow_prefix_length;
+      uint8_t vector_length;
+      uint8_t operand;
+
+      bool ConvertToRex(const uint8_t* instr,
+                        uint8_t* const decodeBuffer,
+                        const uint8_t* instr_end);
+    };
+    struct VexPrefix Vex;
+
+    InstructionContext(DisassemblerX86* disass, const uint8_t* instr);
+  };
+
   size_t DumpNops(std::ostream& os, const uint8_t* instr);
   size_t DumpInstruction(std::ostream& os, const uint8_t* instr, x86_instr *ins = nullptr);
 
diff --git a/runtime/arch/x86/instruction_set_features_x86.h b/runtime/arch/x86/instruction_set_features_x86.h
index e6fbc33fdb..668c4ed741 100644
--- a/runtime/arch/x86/instruction_set_features_x86.h
+++ b/runtime/arch/x86/instruction_set_features_x86.h
@@ -31,7 +31,7 @@
 #define SET_VEX_M_0F_3A 0x03
 #define SET_VEX_W       0x80
 #define SET_VEX_L_128   0x00
-#define SET_VEL_L_256   0x04
+#define SET_VEX_L_256   0x04
 #define SET_VEX_PP_NONE 0x00
 #define SET_VEX_PP_66   0x01
 #define SET_VEX_PP_F3   0x02
-- 
2.25.1

