From 624bd807840e7cd130a494fa779f86be269ba00f Mon Sep 17 00:00:00 2001
From: ahs <amrita.h.s@intel.com>
Date: Tue, 20 Dec 2022 05:45:39 +0000
Subject: [PATCH] avx2 support in ART compiler

Signed-off-by: ahs <amrita.h.s@intel.com>
---
 .../code_generator_vector_x86_64.cc           | 264 ++++--
 compiler/optimizing/code_generator_x86.cc     |  11 +
 compiler/optimizing/code_generator_x86_64.cc  |  29 +-
 compiler/optimizing/code_generator_x86_64.h   |   1 +
 compiler/optimizing/loop_optimization.cc      |  57 +-
 compiler/optimizing/loop_optimization.h       |   1 +
 compiler/optimizing/nodes.cc                  |  26 +-
 compiler/optimizing/nodes.h                   |   9 +-
 compiler/optimizing/nodes_x86.h               |  11 +
 compiler/utils/assembler_test.h               | 240 +++++-
 compiler/utils/x86/assembler_x86.cc           |   7 +
 compiler/utils/x86/assembler_x86.h            |   2 +
 compiler/utils/x86_64/assembler_x86_64.cc     | 796 +++++++++++++++---
 compiler/utils/x86_64/assembler_x86_64.h      | 116 ++-
 .../utils/x86_64/assembler_x86_64_test.cc     | 116 ++-
 compiler/utils/x86_64/constants_x86_64.h      |  18 +
 .../utils/x86_64/managed_register_x86_64.cc   |   2 +
 .../utils/x86_64/managed_register_x86_64.h    |  47 +-
 .../x86_64/managed_register_x86_64_test.cc    |  49 ++
 disassembler/disassembler_x86.cc              |   2 +
 disassembler/disassembler_x86.h               |   2 +-
 libartbase/base/macros.h                      |   1 +
 .../arch/x86/instruction_set_features_x86.h   |   2 +-
 runtime/arch/x86/registers_x86.h              |  13 +
 runtime/arch/x86_64/registers_x86_64.cc       |   9 +
 runtime/arch/x86_64/registers_x86_64.h        |  21 +
 runtime/mirror/array.h                        |  12 +-
 test/640-checker-simd/src/SimdDouble.java     |   4 +-
 test/645-checker-abs-simd/src/Main.java       | 423 +++-------
 test/655-checker-simd-arm-opt/src/Main.java   |  42 +-
 test/etc/run-test-jar                         |   4 +-
 31 files changed, 1679 insertions(+), 658 deletions(-)

diff --git a/compiler/optimizing/code_generator_vector_x86_64.cc b/compiler/optimizing/code_generator_vector_x86_64.cc
index 330bf76a4a..222dc014d8 100644
--- a/compiler/optimizing/code_generator_vector_x86_64.cc
+++ b/compiler/optimizing/code_generator_vector_x86_64.cc
@@ -18,14 +18,22 @@
 
 #include "mirror/array-inl.h"
 #include "mirror/string.h"
+#include <iostream>
 
 namespace art {
 namespace x86_64 {
 
-// NOLINT on __ macro to suppress wrong warning/fix (misc-macro-parentheses) from clang-tidy.
+/*VectorRegister VectorRegisterFrom(Location location) {
+  DCHECK(location.IsXmmRegister());
+  return static_cast<VectorRegister>(location.AsFpuRegister<YmmRegister>());
+}*/
+
+//bool has_avx = InstructionCodeGeneratorX86_64::CpuHasAvxFeatureFlag();
+
 #define __ down_cast<X86_64Assembler*>(GetAssembler())->  // NOLINT
 
 void LocationsBuilderX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
+  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
   HInstruction* input = instruction->InputAt(0);
   bool is_zero = IsZeroBitPattern(input);
@@ -37,16 +45,29 @@ void LocationsBuilderX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instru
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      locations->SetInAt(0, is_zero ? Location::ConstantLocation(input->AsConstant())
-                                    : Location::RequiresRegister());
-      locations->SetOut(Location::RequiresFpuRegister());
+	    //AHS::
+      if (cpu_has_avx) {
+         locations->SetInAt(0, Location::RequiresRegister());
+         locations->SetOut(Location::RequiresFpuRegister());
+       
+      } else { // legacy sse version
+         locations->SetInAt(0, is_zero ? Location::ConstantLocation(input->AsConstant())
+                                       : Location::RequiresRegister());
+         locations->SetOut(Location::RequiresFpuRegister());
+      }
       break;
     case DataType::Type::kFloat32:
     case DataType::Type::kFloat64:
+      if (cpu_has_avx) {
+        locations->SetInAt(0, Location::RequiresFpuRegister());
+        locations->SetOut(Location::RequiresFpuRegister());
+       
+      } else { // legacy sse version
       locations->SetInAt(0, is_zero ? Location::ConstantLocation(input->AsConstant())
                                     : Location::RequiresFpuRegister());
       locations->SetOut(is_zero ? Location::RequiresFpuRegister()
                                 : Location::SameAsFirstInput());
+      }
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -56,8 +77,8 @@ void LocationsBuilderX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instru
 
 void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
   LocationSummary* locations = instruction->GetLocations();
+  //XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
   XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-
   bool cpu_has_avx = CpuHasAvxFeatureFlag();
   // Shorthand for any type of zero.
   if (IsZeroBitPattern(instruction->InputAt(0))) {
@@ -69,7 +90,8 @@ void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
+      //AHS::DCHECK_EQ(16u, instruction->GetVectorLength());
+      DCHECK(instruction->GetVectorLength() == 16u || instruction->GetVectorLength() == 32u);
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
       __ punpcklbw(dst, dst);
       __ punpcklwd(dst, dst);
@@ -77,31 +99,49 @@ void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar
       break;
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
+      //AHS::DCHECK_EQ(8u, instruction->GetVectorLength());
+      DCHECK(instruction->GetVectorLength() == 8u || instruction->GetVectorLength() == 16u);
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
       __ punpcklwd(dst, dst);
       __ pshufd(dst, dst, Immediate(0));
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
+      //AHS::DCHECK_EQ(4u, instruction->GetVectorLength());
+      DCHECK(instruction->GetVectorLength() == 4u || instruction->GetVectorLength() == 8u);
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
       __ pshufd(dst, dst, Immediate(0));
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
+      //AHS::DCHECK_EQ(2u, instruction->GetVectorLength());
+      DCHECK(instruction->GetVectorLength() == 2u || instruction->GetVectorLength() == 4u);
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ true);
       __ punpcklqdq(dst, dst);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      DCHECK(locations->InAt(0).Equals(locations->Out()));
-      __ shufps(dst, dst, Immediate(0));
+      {
+      DCHECK(instruction->GetVectorLength() == 4u || instruction->GetVectorLength() == 8u);
+      XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
+      if (cpu_has_avx) {
+          __ vbroadcastss(dst, src);
+      } else {
+        DCHECK(locations->InAt(0).Equals(locations->Out()));
+        __ shufps(dst, dst, Immediate(0));
+      }
       break;
+      }
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      DCHECK(locations->InAt(0).Equals(locations->Out()));
-      __ shufpd(dst, dst, Immediate(0));
+      {
+      DCHECK(instruction->GetVectorLength() == 2u || instruction->GetVectorLength() == 4u);
+      //DCHECK(locations->InAt(0).Equals(locations->Out()));
+      XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
+       if (cpu_has_avx) {
+        __ vbroadcastsd(dst, src);
+      } else {
+        DCHECK(locations->InAt(0).Equals(locations->Out()));
+       __ shufpd(dst, dst, Immediate(0));
+      }
       break;
+      }
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
       UNREACHABLE();
@@ -199,12 +239,13 @@ void InstructionCodeGeneratorX86_64::VisitVecReduce(HVecReduce* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
   XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  size_t vec_len = instruction->GetVectorLength();
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt32:
       DCHECK_EQ(4u, instruction->GetVectorLength());
       switch (instruction->GetReductionKind()) {
         case HVecReduce::kSum:
-          __ movaps(dst, src);
+          __ movaps(dst, src, vec_len);
           __ phaddd(dst, dst);
           __ phaddd(dst, dst);
           break;
@@ -220,8 +261,8 @@ void InstructionCodeGeneratorX86_64::VisitVecReduce(HVecReduce* instruction) {
       XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
       switch (instruction->GetReductionKind()) {
         case HVecReduce::kSum:
-          __ movaps(tmp, src);
-          __ movaps(dst, src);
+          __ movaps(tmp, src, vec_len);
+          __ movaps(dst, src, vec_len);
           __ punpckhqdq(tmp, tmp);
           __ paddq(dst, tmp);
           break;
@@ -287,12 +328,13 @@ void InstructionCodeGeneratorX86_64::VisitVecNeg(HVecNeg* instruction) {
       __ psubq(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
+      DCHECK_LE(4u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 8u);
       __ xorps(dst, dst);
       __ subps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
+      DCHECK(instruction->GetVectorLength()== 2u || instruction->GetVectorLength() == 4u);
       __ xorpd(dst, dst);
       __ subpd(dst, src);
       break;
@@ -314,11 +356,12 @@ void InstructionCodeGeneratorX86_64::VisitVecAbs(HVecAbs* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
   XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  size_t vec_len = instruction->GetVectorLength();
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt32: {
       DCHECK_EQ(4u, instruction->GetVectorLength());
       XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
-      __ movaps(dst, src);
+      __ movaps(dst, src, vec_len);
       __ pxor(tmp, tmp);
       __ pcmpgtd(tmp, dst);
       __ pxor(dst, tmp);
@@ -326,16 +369,31 @@ void InstructionCodeGeneratorX86_64::VisitVecAbs(HVecAbs* instruction) {
       break;
     }
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
+      DCHECK(instruction->GetVectorLength() == 4u || instruction->GetVectorLength() == 8u);
+      if (instruction->GetVectorLength() == 8u) {
+        __ vpcmpeqb(dst, dst, dst, 32u);
+        __ vpsrld(dst, dst,Immediate(1),8u);
+        __ vandps(dst, dst, src, 8u);
+       // __ vcvttps2dq(src, src, 8u);
+       // __ vpabsd(dst, src, 8u);
+       // __ vcvtdq2ps(dst, dst, 8u);
+      } else {
       __ pcmpeqb(dst, dst);  // all ones
       __ psrld(dst, Immediate(1));
       __ andps(dst, src);
+      }
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      __ pcmpeqb(dst, dst);  // all ones
-      __ psrlq(dst, Immediate(1));
-      __ andpd(dst, src);
+      DCHECK(instruction->GetVectorLength() == 2u || instruction->GetVectorLength() == 4u);
+      if (instruction->GetVectorLength() == 4u) {
+        __ vpcmpeqb(dst, dst, dst, 32u);
+        __ vpsrlq(dst, dst, Immediate(1), 4u);
+        __ vandpd(dst, dst, src, 4u);
+      } else {
+        __ pcmpeqb(dst, dst);  // all ones
+        __ psrlq(dst, Immediate(1));
+        __ andpd(dst, src);
+      }
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -451,33 +509,46 @@ void InstructionCodeGeneratorX86_64::VisitVecAdd(HVecAdd* instruction) {
   XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
   XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
   XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  size_t vec_len = instruction->GetVectorLength();
   DCHECK(cpu_has_avx || other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddb(dst, other_src, src) : __ paddb(dst, src);
+      DCHECK(vec_len == 16u  || vec_len == 32u);
+      cpu_has_avx ? __ vpaddb(dst, other_src, src, vec_len) : __ paddb(dst, src);
       break;
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddw(dst, other_src, src) : __ paddw(dst, src);
+      DCHECK(vec_len == 8u || vec_len == 16u);
+      cpu_has_avx ? __ vpaddw(dst, other_src, src, vec_len) : __ paddw(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddd(dst, other_src, src) : __ paddd(dst, src);
+      DCHECK(vec_len == 4u || vec_len == 8u);
+      cpu_has_avx ? __ vpaddd(dst, other_src, src, vec_len) : __ paddd(dst, src);
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddq(dst, other_src, src) : __ paddq(dst, src);
+      DCHECK(vec_len == 2u || vec_len == 4u);
+      cpu_has_avx ? __ vpaddq(dst, other_src, src, vec_len) : __ paddq(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vaddps(dst, other_src, src) : __ addps(dst, src);
+      //std::cout << instruction->GetVectorLength() << std::endl;
+      DCHECK(vec_len == 8u || vec_len== 4u);
+      /*if (instruction->GetVectorLength() == 8u) {
+        //YmmRegister wsrc = VectorRegisterFrom(locations->InAt(1));//locations->InAt(1).AsVectorRegister<YmmRegister>();
+        YmmRegister wsrc = locations->InAt(1).AsFpuRegister<YmmRegister>();
+        YmmRegister wsrc1 = locations->InAt(0).AsFpuRegister<YmmRegister>();
+        //VectorRegisterFrom(locations->InAt(0));//.AsVectorRegister<YmmRegister>();
+        YmmRegister wdst =  locations->Out().AsFpuRegister<YmmRegister>();
+        //VectorRegisterFrom(locations->Out());//.AsVectorRegister<YmmRegister>();
+        __ vaddps(wdst, wsrc1, wsrc);
+      } else {*/
+        //DCHECK_EQ(4u, instruction->GetVectorLength());
+        cpu_has_avx ? __ vaddps(dst, other_src, src, vec_len) : __ addps(dst, src);
+     // }
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vaddpd(dst, other_src, src) : __ addpd(dst, src);
+      DCHECK(vec_len == 4u || vec_len == 2u);
+      cpu_has_avx ? __ vaddpd(dst, other_src, src, vec_len) : __ addpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -559,6 +630,7 @@ void InstructionCodeGeneratorX86_64::VisitVecSub(HVecSub* instruction) {
   XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
   XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
   DCHECK(cpu_has_avx || other_src == dst);
+  size_t vec_len = instruction->GetVectorLength();
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
@@ -579,12 +651,14 @@ void InstructionCodeGeneratorX86_64::VisitVecSub(HVecSub* instruction) {
       cpu_has_avx ? __ vpsubq(dst, other_src, src) : __ psubq(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vsubps(dst, other_src, src) : __ subps(dst, src);
+      DCHECK_LE(4u, vec_len);
+      DCHECK_LE(vec_len, 8u);
+      cpu_has_avx ? __ vsubps(dst, other_src, src, vec_len) : __ subps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vsubpd(dst, other_src, src) : __ subpd(dst, src);
+      DCHECK_LE(2u, vec_len);
+      DCHECK_LE(vec_len, 4u);
+      cpu_has_avx ? __ vsubpd(dst, other_src, src, vec_len) : __ subpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -650,12 +724,16 @@ void InstructionCodeGeneratorX86_64::VisitVecMul(HVecMul* instruction) {
       cpu_has_avx ? __ vpmulld(dst, other_src, src): __ pmulld(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vmulps(dst, other_src, src) : __ mulps(dst, src);
+      DCHECK_LE(4u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 8u);
+      cpu_has_avx ? __ vmulps(dst, other_src, src, instruction->GetVectorLength()) :
+                    __ mulps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vmulpd(dst, other_src, src) : __ mulpd(dst, src);
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 4u);
+      cpu_has_avx ? __ vmulpd(dst, other_src, src, instruction->GetVectorLength()) :
+                    __ mulpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -680,12 +758,15 @@ void InstructionCodeGeneratorX86_64::VisitVecDiv(HVecDiv* instruction) {
   DCHECK(cpu_has_avx || other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vdivps(dst, other_src, src) : __ divps(dst, src);
+      DCHECK_LE(4u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 8u);
+      cpu_has_avx ? __ vdivps(dst, other_src, src, 4u /*instruction->GetVectorLength()*/) :
+                    __ divps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vdivpd(dst, other_src, src) : __ divpd(dst, src);
+      //DCHECK_EQ(2u, instruction->GetVectorLength());
+      cpu_has_avx ? __ vdivpd(dst, other_src, src, 2u /*instruction->GetVectorLength()*/) :
+                    __ divpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -1188,17 +1269,20 @@ void InstructionCodeGeneratorX86_64::VisitVecDotProd(HVecDotProd* instruction) {
   XmmRegister acc = locations->InAt(0).AsFpuRegister<XmmRegister>();
   XmmRegister left = locations->InAt(1).AsFpuRegister<XmmRegister>();
   XmmRegister right = locations->InAt(2).AsFpuRegister<XmmRegister>();
+  size_t vec_len = instruction->GetVectorLength();
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt32: {
-      DCHECK_EQ(4u, instruction->GetVectorLength());
+      //AHS::CHECK?
+      //DCHECK_EQ(4u, instruction->GetVectorLength());
+      DCHECK(vec_len == 4u || vec_len == 8u); 
       XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
       if (!cpu_has_avx) {
-        __ movaps(tmp, right);
+        __ movaps(tmp, right, vec_len);
         __ pmaddwd(tmp, left);
         __ paddd(acc, tmp);
       } else {
         __ vpmaddwd(tmp, left, right);
-        __ vpaddd(acc, acc, tmp);
+        __ vpaddd(acc, acc, tmp, vec_len);
       }
       break;
     }
@@ -1252,6 +1336,8 @@ static Address VecAddress(LocationSummary* locations, size_t size, bool is_strin
   uint32_t offset = is_string_char_at
       ? mirror::String::ValueOffset().Uint32Value()
       : mirror::Array::DataOffset(size).Uint32Value();
+  //mirror::Array::print_data_offset();
+  //std::cout << "offset=" << offset << std::endl;
   return CodeGeneratorX86_64::ArrayAddress(base.AsRegister<CpuRegister>(), index, scale, offset);
 }
 
@@ -1268,11 +1354,15 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
   size_t size = DataType::Size(instruction->GetPackedType());
   Address address = VecAddress(locations, size, instruction->IsStringCharAt());
   XmmRegister reg = locations->Out().AsFpuRegister<XmmRegister>();
-  bool is_aligned16 = instruction->GetAlignment().IsAlignedAt(16);
+  bool is_aligned16 = instruction->GetAlignment().IsAlignedAt(16);// ||
+  bool is_aligned32 = instruction->GetAlignment().IsAlignedAt(32);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt16:  // (short) s.charAt(.) can yield HVecLoad/Int16/StringCharAt.
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
+      //AHS::CHECK
+      //DCHECK_EQ(8u, instruction->GetVectorLength());
+      DCHECK_LE(8u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 16u);
       // Special handling of compressed/uncompressed string load.
       if (mirror::kUseStringCompression && instruction->IsStringCharAt()) {
         NearLabel done, not_compressed;
@@ -1290,7 +1380,11 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
         __ jmp(&done);
         // Load 8 direct uncompressed chars.
         __ Bind(&not_compressed);
-        is_aligned16 ?  __ movdqa(reg, address) :  __ movdqu(reg, address);
+	if (instruction->GetVectorLength() == 16u) {
+           is_aligned32 ?  __ movdqa(reg, address, 16u) :  __ movdqu(reg, address, 16u);
+	} else {
+           is_aligned16 ?  __ movdqa(reg, address, 8u) :  __ movdqu(reg, address, 8u);
+	}
         __ Bind(&done);
         return;
       }
@@ -1301,16 +1395,34 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
       DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      is_aligned16 ? __ movdqa(reg, address) : __ movdqu(reg, address);
+      /*AHS:: DCHECK_LE(instruction->GetVectorLength(), 16u);
+      is_aligned16 ? __ movdqa(reg, address) : __ movdqu(reg, address);*/
+      DCHECK_LE(instruction->GetVectorLength(), 32u);
+      if (instruction->GetVectorLength() == 32u) {
+         is_aligned32 ? __ movdqa(reg, address, 32u) : __ movdqu(reg, address, 32u);
+      } else {
+         is_aligned16 ? __ movdqa(reg, address, 16u) : __ movdqu(reg, address, 16u);
+      }
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      is_aligned16 ? __ movaps(reg, address) : __ movups(reg, address);
+      DCHECK_LE(4u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 8u);
+      if (instruction->GetVectorLength() == 8u) {
+        is_aligned32 ? __ movaps(reg,address, 8u) : __ movups(reg, address, 8u);
+      } else {
+        is_aligned16 ? __ movaps(reg, address, instruction->GetVectorLength()) : 
+                   __ movups(reg, address, instruction->GetVectorLength());
+      }
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      is_aligned16 ? __ movapd(reg, address) : __ movupd(reg, address);
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 4u);
+      if (instruction->GetVectorLength() == 4u) {
+        is_aligned32 ? __ movapd(reg, address, 4u) : __ movupd(reg, address, 4u);
+      } else {
+        is_aligned16 ? __ movapd(reg, address, 2u) :
+                       __ movupd(reg, address, 2u);
+      }
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -1328,6 +1440,7 @@ void InstructionCodeGeneratorX86_64::VisitVecStore(HVecStore* instruction) {
   Address address = VecAddress(locations, size, /*is_string_char_at*/ false);
   XmmRegister reg = locations->InAt(2).AsFpuRegister<XmmRegister>();
   bool is_aligned16 = instruction->GetAlignment().IsAlignedAt(16);
+  bool is_aligned32 = instruction->GetAlignment().IsAlignedAt(32);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -1337,16 +1450,31 @@ void InstructionCodeGeneratorX86_64::VisitVecStore(HVecStore* instruction) {
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
       DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      is_aligned16 ? __ movdqa(address, reg) : __ movdqu(address, reg);
+      DCHECK_LE(instruction->GetVectorLength(), 32u);
+      if (instruction->GetVectorLength() == 32u) {
+         is_aligned32 ? __ movdqa(address, reg, 32u) : __ movdqu(address, reg, 32u);
+      } else {
+         is_aligned16 ? __ movdqa(address, reg, 16u) : __ movdqu(address, reg, 16u);
+      }
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      is_aligned16 ? __ movaps(address, reg) : __ movups(address, reg);
+      DCHECK_LE(4u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 8u);
+      if (instruction->GetVectorLength() == 8u) {
+        is_aligned32 ? __ vmovaps (address, reg, 8u) : __ vmovups(address, reg, 8u);
+      } else {
+      is_aligned16 ? __ movaps(address, reg, instruction->GetVectorLength()) :
+                   __ movups(address, reg, instruction->GetVectorLength());     
+      } 
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      is_aligned16 ? __ movapd(address, reg) : __ movupd(address, reg);
+      DCHECK_LE(2u, instruction->GetVectorLength());
+      DCHECK_LE(instruction->GetVectorLength(), 4u);
+      if (instruction->GetVectorLength() == 4u) {
+        is_aligned32 ? __ movapd(address, reg, 4u) : __ movupd(address, reg, 4u);
+      } else {
+        is_aligned16 ? __ movapd(address, reg, 2u) : __ movupd(address, reg, 2u);
+      }
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
diff --git a/compiler/optimizing/code_generator_x86.cc b/compiler/optimizing/code_generator_x86.cc
index 0fbfe1781b..759b6441a2 100644
--- a/compiler/optimizing/code_generator_x86.cc
+++ b/compiler/optimizing/code_generator_x86.cc
@@ -9004,15 +9004,26 @@ void InstructionCodeGeneratorX86::VisitIntermediateAddress(HIntermediateAddress*
   LOG(FATAL) << "Unreachable";
 }
 
+void LocationsBuilderX86::VisitX86Clear(HX86Clear* clr) {
+  clr->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorX86::VisitX86Clear(HX86Clear* clr ATTRIBUTE_UNUSED) {
+  __ vzeroupper();
+}
+ 
 bool LocationsBuilderX86::CpuHasAvxFeatureFlag() {
   return codegen_->GetInstructionSetFeatures().HasAVX();
 }
+
 bool LocationsBuilderX86::CpuHasAvx2FeatureFlag() {
   return codegen_->GetInstructionSetFeatures().HasAVX2();
 }
+
 bool InstructionCodeGeneratorX86::CpuHasAvxFeatureFlag() {
   return codegen_->GetInstructionSetFeatures().HasAVX();
 }
+
 bool InstructionCodeGeneratorX86::CpuHasAvx2FeatureFlag() {
   return codegen_->GetInstructionSetFeatures().HasAVX2();
 }
diff --git a/compiler/optimizing/code_generator_x86_64.cc b/compiler/optimizing/code_generator_x86_64.cc
index 182860ffe6..3e28bf466e 100644
--- a/compiler/optimizing/code_generator_x86_64.cc
+++ b/compiler/optimizing/code_generator_x86_64.cc
@@ -1369,7 +1369,7 @@ size_t CodeGeneratorX86_64::RestoreCoreRegister(size_t stack_index, uint32_t reg
 
 size_t CodeGeneratorX86_64::SaveFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
   if (GetGraph()->HasSIMD()) {
-    __ movups(Address(CpuRegister(RSP), stack_index), XmmRegister(reg_id));
+    __ movups(Address(CpuRegister(RSP), stack_index), XmmRegister(reg_id), 4u); // 128 bit SIMD
   } else {
     __ movsd(Address(CpuRegister(RSP), stack_index), XmmRegister(reg_id));
   }
@@ -1378,7 +1378,7 @@ size_t CodeGeneratorX86_64::SaveFloatingPointRegister(size_t stack_index, uint32
 
 size_t CodeGeneratorX86_64::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
   if (GetGraph()->HasSIMD()) {
-    __ movups(XmmRegister(reg_id), Address(CpuRegister(RSP), stack_index));
+    __ movups(XmmRegister(reg_id), Address(CpuRegister(RSP), stack_index), 4u);
   } else {
     __ movsd(XmmRegister(reg_id), Address(CpuRegister(RSP), stack_index));
   }
@@ -1638,7 +1638,8 @@ void CodeGeneratorX86_64::Move(Location destination, Location source) {
     if (source.IsRegister()) {
       __ movd(dest, source.AsRegister<CpuRegister>());
     } else if (source.IsFpuRegister()) {
-      __ movaps(dest, source.AsFpuRegister<XmmRegister>());
+       size_t vec_len = GetInstructionSetFeatures().HasAVX() ? 8u : 4u;
+      __ movaps(dest, source.AsFpuRegister<XmmRegister>(), vec_len);
     } else if (source.IsConstant()) {
       HConstant* constant = source.GetConstant();
       int64_t value = CodeGenerator::GetInt64ValueOf(constant);
@@ -1732,6 +1733,14 @@ void InstructionCodeGeneratorX86_64::HandleGoto(HInstruction* got, HBasicBlock*
   }
 }
 
+void LocationsBuilderX86_64::VisitX86Clear(HX86Clear* clr) {
+  clr->SetLocations(nullptr);
+}
+
+void InstructionCodeGeneratorX86_64::VisitX86Clear(HX86Clear* clr ATTRIBUTE_UNUSED) {
+  __ vzeroupper();
+}
+
 void LocationsBuilderX86_64::VisitGoto(HGoto* got) {
   got->SetLocations(nullptr);
 }
@@ -5978,7 +5987,7 @@ void ParallelMoveResolverX86_64::EmitMove(size_t index) {
   } else if (source.IsSIMDStackSlot()) {
     if (destination.IsFpuRegister()) {
       __ movups(destination.AsFpuRegister<XmmRegister>(),
-                Address(CpuRegister(RSP), source.GetStackIndex()));
+                Address(CpuRegister(RSP), source.GetStackIndex()), 4u);
     } else {
       DCHECK(destination.IsSIMDStackSlot());
       size_t high = kX86_64WordSize;
@@ -6033,7 +6042,8 @@ void ParallelMoveResolverX86_64::EmitMove(size_t index) {
     }
   } else if (source.IsFpuRegister()) {
     if (destination.IsFpuRegister()) {
-      __ movaps(destination.AsFpuRegister<XmmRegister>(), source.AsFpuRegister<XmmRegister>());
+     size_t vec_len = codegen_->GetInstructionSetFeatures().HasAVX() ? 8u : 4u;
+      __ movaps(destination.AsFpuRegister<XmmRegister>(), source.AsFpuRegister<XmmRegister>(), vec_len);
     } else if (destination.IsStackSlot()) {
       __ movss(Address(CpuRegister(RSP), destination.GetStackIndex()),
                source.AsFpuRegister<XmmRegister>());
@@ -6043,7 +6053,7 @@ void ParallelMoveResolverX86_64::EmitMove(size_t index) {
     } else {
        DCHECK(destination.IsSIMDStackSlot());
       __ movups(Address(CpuRegister(RSP), destination.GetStackIndex()),
-                source.AsFpuRegister<XmmRegister>());
+                source.AsFpuRegister<XmmRegister>(), 4u);
     }
   }
 }
@@ -6081,9 +6091,9 @@ void ParallelMoveResolverX86_64::Exchange64(XmmRegister reg, int mem) {
 void ParallelMoveResolverX86_64::Exchange128(XmmRegister reg, int mem) {
   size_t extra_slot = 2 * kX86_64WordSize;
   __ subq(CpuRegister(RSP), Immediate(extra_slot));
-  __ movups(Address(CpuRegister(RSP), 0), XmmRegister(reg));
+  __ movups(Address(CpuRegister(RSP), 0), XmmRegister(reg), 4u);
   ExchangeMemory64(0, mem + extra_slot, 2);
-  __ movups(XmmRegister(reg), Address(CpuRegister(RSP), 0));
+  __ movups(XmmRegister(reg), Address(CpuRegister(RSP), 0), 4u);
   __ addq(CpuRegister(RSP), Immediate(extra_slot));
 }
 
@@ -6141,7 +6151,8 @@ void ParallelMoveResolverX86_64::EmitSwap(size_t index) {
     ExchangeMemory64(destination.GetStackIndex(), source.GetStackIndex(), 1);
   } else if (source.IsFpuRegister() && destination.IsFpuRegister()) {
     __ movd(CpuRegister(TMP), source.AsFpuRegister<XmmRegister>());
-    __ movaps(source.AsFpuRegister<XmmRegister>(), destination.AsFpuRegister<XmmRegister>());
+    size_t vec_len = codegen_->GetInstructionSetFeatures().HasAVX() ? 8u : 4u;
+    __ movaps(source.AsFpuRegister<XmmRegister>(), destination.AsFpuRegister<XmmRegister>(),vec_len);
     __ movd(destination.AsFpuRegister<XmmRegister>(), CpuRegister(TMP));
   } else if (source.IsFpuRegister() && destination.IsStackSlot()) {
     Exchange32(source.AsFpuRegister<XmmRegister>(), destination.GetStackIndex());
diff --git a/compiler/optimizing/code_generator_x86_64.h b/compiler/optimizing/code_generator_x86_64.h
index 6038223403..6c3d0f1edd 100644
--- a/compiler/optimizing/code_generator_x86_64.h
+++ b/compiler/optimizing/code_generator_x86_64.h
@@ -52,6 +52,7 @@ static constexpr size_t kRuntimeParameterFpuRegistersLength =
 // these are not clobbered by any direct call to native code (such as math intrinsics).
 static constexpr FloatRegister non_volatile_xmm_regs[] = { XMM12, XMM13, XMM14, XMM15 };
 
+VectorRegister VectorRegisterFrom(Location location);
 
 class InvokeRuntimeCallingConvention : public CallingConvention<Register, FloatRegister> {
  public:
diff --git a/compiler/optimizing/loop_optimization.cc b/compiler/optimizing/loop_optimization.cc
index 02ee4ec057..37f04a7d77 100644
--- a/compiler/optimizing/loop_optimization.cc
+++ b/compiler/optimizing/loop_optimization.cc
@@ -26,6 +26,7 @@
 #include "linear_order.h"
 #include "mirror/array-inl.h"
 #include "mirror/string.h"
+#include <iostream>
 
 namespace art {
 
@@ -493,6 +494,8 @@ HLoopOptimization::HLoopOptimization(HGraph* graph,
 bool HLoopOptimization::Run() {
   // Skip if there is no loop or the graph has try-catch/irreducible loops.
   // TODO: make this less of a sledgehammer.
+  //std::string method_name = graph_->GetDexFile().PrettyMethod(graph_->GetMethodIdx());
+  //std::cout << "method=" << method_name << std::endl;
   if (!graph_->HasLoops() || graph_->HasTryCatch() || graph_->HasIrreducibleLoops()) {
     return false;
   }
@@ -761,6 +764,7 @@ bool HLoopOptimization::TryOptimizeInnerLoopFinite(LoopNode* node) {
     }
   }
   // Vectorize loop, if possible and valid.
+  //std::cout << "trip_count=" << trip_count << std::endl;
   if (kEnableVectorization &&
       // Disable vectorization for debuggable graphs: this is a workaround for the bug
       // in 'GenerateNewLoop' which caused the SuspendCheck environment to be invalid.
@@ -947,6 +951,7 @@ bool HLoopOptimization::ShouldVectorize(LoopNode* node, HBasicBlock* block, int6
   // (3) variable to record how many references share same alignment.
   // (4) variable to record suitable candidate for dynamic loop peeling.
   size_t desired_alignment = GetVectorSizeInBytes();
+  DCHECK(desired_alignment == 16u || desired_alignment == 32u);
   ScopedArenaVector<uint32_t> peeling_votes(desired_alignment, 0u,
       loop_allocator_->Adapter(kArenaAllocLoopOptimization));
 
@@ -1041,14 +1046,23 @@ void HLoopOptimization::Vectorize(LoopNode* node,
                                   HBasicBlock* block,
                                   HBasicBlock* exit,
                                   int64_t trip_count) {
+  //std::cout << "Vectorize Loop" << std::endl;
+  bool clear_reg = false;
+  //std::cout << "exit =" << exit->GetBlockId() << std::endl;
   HBasicBlock* header = node->loop_info->GetHeader();
   HBasicBlock* preheader = node->loop_info->GetPreHeader();
-
+  
+  //std::cout << "header=" << header->GetBlockId() << std::endl;
   // Pick a loop unrolling factor for the vector loop.
   uint32_t unroll = arch_loop_helper_->GetSIMDUnrollingFactor(
       block, trip_count, MaxNumberPeeled(), vector_length_);
+  //std::cout << "unroll=" << unroll << std::endl;
   uint32_t chunk = vector_length_ * unroll;
 
+  //std::cout << "chunk=" << chunk << std::endl;
+  //std::cout << "trip count=" << trip_count << std::endl;
+  //std::cout << "vector static peeling factor=" << vector_static_peeling_factor_ << std::endl;
+
   DCHECK(trip_count == 0 || (trip_count >= MaxNumberPeeled() + chunk));
 
   // A cleanup loop is needed, at least, for any unknown trip count or
@@ -1109,6 +1123,7 @@ void HLoopOptimization::Vectorize(LoopNode* node,
   HInstruction* stc = induction_range_.GenerateTripCount(node->loop_info, graph_, preheader);
   HInstruction* vtc = stc;
   if (needs_cleanup) {
+    //std::cout << "needs cleanup=" << needs_cleanup << std::endl;	   
     DCHECK(!IsInPredicatedVectorizationMode());
     DCHECK(IsPowerOfTwo(chunk));
     HInstruction* diff = stc;
@@ -1146,12 +1161,14 @@ void HLoopOptimization::Vectorize(LoopNode* node,
   // NOTE: The alignment forced by the peeling loop is preserved even if data is
   //       moved around during suspend checks, since all analysis was based on
   //       nothing more than the Android runtime alignment conventions.
+  //std::cout << "exit=" << exit->GetBlockId() << std::endl; 
   if (ptc != nullptr) {
+    //std::cout << "calling TLV ptc not null" << std::endl;	   
     DCHECK(!IsInPredicatedVectorizationMode());
     vector_mode_ = kSequential;
     GenerateNewLoop(node,
                     block,
-                    graph_->TransformLoopForVectorization(vector_header_, vector_body_, exit),
+                    graph_->TransformLoopForVectorization(vector_header_, vector_body_, exit, /*clear_reg*/ false),
                     vector_index_,
                     ptc,
                     graph_->GetConstant(induc_type, 1),
@@ -1161,10 +1178,15 @@ void HLoopOptimization::Vectorize(LoopNode* node,
   // Generate vector loop, possibly further unrolled:
   // for ( ; i < vtc; i += chunk)
   //    <vectorized-loop-body>
+  #if defined (ART_ENABLE_CODEGEN_x86_64) 
+   clear_reg = !needs_cleanup;
+  #endif
   vector_mode_ = kVector;
+  //std::cout << "clear_reg=" << clear_reg << std::endl;
+  //std::cout << "calling TLV" << std::endl;
   GenerateNewLoop(node,
                   block,
-                  graph_->TransformLoopForVectorization(vector_header_, vector_body_, exit),
+                  graph_->TransformLoopForVectorization(vector_header_, vector_body_, exit, clear_reg),
                   vector_index_,
                   vtc,
                   graph_->GetConstant(induc_type, vector_length_),  // increment per unroll
@@ -1175,11 +1197,15 @@ void HLoopOptimization::Vectorize(LoopNode* node,
   // for ( ; i < stc; i += 1)
   //    <loop-body>
   if (needs_cleanup) {
+    //std::cout << "calling TLV1" << std::endl;
+    #if defined (ART_ENABLE_CODEGEN_x86_64)
+       clear_reg = true;
+    #endif	  
     DCHECK(!IsInPredicatedVectorizationMode() || vector_runtime_test_a_ != nullptr);
     vector_mode_ = kSequential;
     GenerateNewLoop(node,
                     block,
-                    graph_->TransformLoopForVectorization(vector_header_, vector_body_, exit),
+                    graph_->TransformLoopForVectorization(vector_header_, vector_body_, exit, clear_reg),
                     vector_index_,
                     stc,
                     graph_->GetConstant(induc_type, 1),
@@ -1578,7 +1604,12 @@ bool HLoopOptimization::VectorizeUse(LoopNode* node,
 }
 
 uint32_t HLoopOptimization::GetVectorSizeInBytes() {
-  return simd_register_size_;
+  const InstructionSetFeatures* features = compiler_options_->GetInstructionSetFeatures();
+  if ((compiler_options_->GetInstructionSet() == InstructionSet::kX86_64) && (features->AsX86_64InstructionSetFeatures()->HasAVX())) { 
+     return simd_register_size_ * 2;
+  } else { 
+     return simd_register_size_;
+  }
 }
 
 bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrictions) {
@@ -1680,6 +1711,8 @@ bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrict
     case InstructionSet::kX86_64:
       // Allow vectorization for SSE4.1-enabled X86 devices only (128-bit SIMD).
       if (features->AsX86InstructionSetFeatures()->HasSSE4_1()) {
+        bool has_avx = features->AsX86InstructionSetFeatures()->HasAVX();
+	LOG(INFO) << "AHS:: has_avx =" << has_avx;
         switch (type) {
           case DataType::Type::kBool:
           case DataType::Type::kUint8:
@@ -1692,7 +1725,7 @@ bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrict
                              kNoUnroundedHAdd |
                              kNoSAD |
                              kNoDotProd;
-            return TrySetVectorLength(type, 16);
+            return has_avx ? TrySetVectorLength(type, 32) : TrySetVectorLength(type, 16);
           case DataType::Type::kUint16:
             *restrictions |= kNoDiv |
                              kNoAbs |
@@ -1700,26 +1733,26 @@ bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrict
                              kNoUnroundedHAdd |
                              kNoSAD |
                              kNoDotProd;
-            return TrySetVectorLength(type, 8);
+            return has_avx ? TrySetVectorLength(type, 16) : TrySetVectorLength(type, 8);
           case DataType::Type::kInt16:
             *restrictions |= kNoDiv |
                              kNoAbs |
                              kNoSignedHAdd |
                              kNoUnroundedHAdd |
                              kNoSAD;
-            return TrySetVectorLength(type, 8);
+            return has_avx ? TrySetVectorLength(type, 16) : TrySetVectorLength(type, 8);
           case DataType::Type::kInt32:
             *restrictions |= kNoDiv | kNoSAD;
-            return TrySetVectorLength(type, 4);
+            return has_avx ? TrySetVectorLength(type, 8) : TrySetVectorLength(type, 4);
           case DataType::Type::kInt64:
             *restrictions |= kNoMul | kNoDiv | kNoShr | kNoAbs | kNoSAD;
-            return TrySetVectorLength(type, 2);
+            return has_avx ? TrySetVectorLength(type, 4) : TrySetVectorLength(type, 2);
           case DataType::Type::kFloat32:
             *restrictions |= kNoReduction;
-            return TrySetVectorLength(type, 4);
+	    return has_avx ? TrySetVectorLength(type, 8) : TrySetVectorLength(type, 4);
           case DataType::Type::kFloat64:
             *restrictions |= kNoReduction;
-            return TrySetVectorLength(type, 2);
+	    return has_avx ? TrySetVectorLength(type, 4) : TrySetVectorLength(type, 2);
           default:
             break;
         }  // switch type
diff --git a/compiler/optimizing/loop_optimization.h b/compiler/optimizing/loop_optimization.h
index d3583ed8a6..b9a5263e46 100644
--- a/compiler/optimizing/loop_optimization.h
+++ b/compiler/optimizing/loop_optimization.h
@@ -192,6 +192,7 @@ class HLoopOptimization : public HOptimization {
   bool TrySetVectorLength(DataType::Type type, uint32_t length) {
     bool res = TrySetVectorLengthImpl(length);
     // Currently the vectorizer supports only the mode when full SIMD registers are used.
+    LOG(INFO) << "AHS :: In TrySetVectorLength, type*len = " << DataType::Size(type) * length << " GetVectorSizeInBytes = " << GetVectorSizeInBytes() << "len=" << length; 
     DCHECK(!res || (DataType::Size(type) * length == GetVectorSizeInBytes()));
     return res;
   }
diff --git a/compiler/optimizing/nodes.cc b/compiler/optimizing/nodes.cc
index 17080f0056..7b2032106d 100644
--- a/compiler/optimizing/nodes.cc
+++ b/compiler/optimizing/nodes.cc
@@ -2999,18 +2999,30 @@ void HGraph::TransformLoopHeaderForBCE(HBasicBlock* header) {
 
 HBasicBlock* HGraph::TransformLoopForVectorization(HBasicBlock* header,
                                                    HBasicBlock* body,
-                                                   HBasicBlock* exit) {
+                                                   HBasicBlock* exit,
+                                                   bool clear_reg) {
   DCHECK(header->IsLoopHeader());
   HLoopInformation* loop = header->GetLoopInformation();
 
+  //std::cout << "header=" << header->GetBlockId() << std::endl;
+  //std::cout << "body=" << body->GetBlockId() << std::endl;
+  //std::cout << "exit=" << exit->GetBlockId() << std::endl;
   // Add new loop blocks.
   HBasicBlock* new_pre_header = new (allocator_) HBasicBlock(this, header->GetDexPc());
   HBasicBlock* new_header = new (allocator_) HBasicBlock(this, header->GetDexPc());
   HBasicBlock* new_body = new (allocator_) HBasicBlock(this, header->GetDexPc());
+  /*std::cout << "new_pre_header=" << new_pre_header->GetBlockId() << std::endl;
+  std::cout << "new_header=" << new_header->GetBlockId() << std::endl;
+  std::cout << "new_body=" << new_body->GetBlockId() << std::endl;*/
+  
   AddBlock(new_pre_header);
   AddBlock(new_header);
   AddBlock(new_body);
 
+  //std::cout << "new_pre_header=" << new_pre_header->GetBlockId() << std::endl;
+  //std::cout << "new_header=" << new_header->GetBlockId() << std::endl;
+  //std::cout << "new_body=" << new_body->GetBlockId() << std::endl;
+
   // Set up control flow.
   header->ReplaceSuccessor(exit, new_pre_header);
   new_pre_header->AddSuccessor(new_header);
@@ -3027,7 +3039,12 @@ HBasicBlock* HGraph::TransformLoopForVectorization(HBasicBlock* header,
   new_body->SetDominator(new_header);
   new_header->dominated_blocks_.push_back(exit);
   exit->SetDominator(new_header);
-
+  /*std::cout << "new_pre_header=" << new_pre_header->GetBlockId() << std::endl;
+  std::cout << "new_header=" << new_header->GetBlockId() << std::endl;
+  std::cout << "new_body=" << new_body->GetBlockId() << std::endl;*/
+  if (clear_reg) { 
+    exit->InsertInstructionBefore(new (allocator_) HX86Clear(), exit->GetLastInstruction());
+  }
   // Fix reverse post order.
   size_t index_of_header = IndexOfElement(reverse_post_order_, header);
   MakeRoomFor(&reverse_post_order_, 2, index_of_header);
@@ -3037,7 +3054,10 @@ HBasicBlock* HGraph::TransformLoopForVectorization(HBasicBlock* header,
   MakeRoomFor(&reverse_post_order_, 1, index_of_body - 1);
   reverse_post_order_[index_of_body] = new_body;
 
-  // Add gotos and suspend check (client must add conditional in header).
+  // Add gotos and suspend check (client must add conditional in header). 
+   if (clear_reg) { 
+    new_pre_header->AddInstruction(new (allocator_) HX86Clear());
+  }
   new_pre_header->AddInstruction(new (allocator_) HGoto());
   HSuspendCheck* suspend_check = new (allocator_) HSuspendCheck(header->GetDexPc());
   new_header->AddInstruction(suspend_check);
diff --git a/compiler/optimizing/nodes.h b/compiler/optimizing/nodes.h
index 939c49f9a6..2cf1b566fb 100644
--- a/compiler/optimizing/nodes.h
+++ b/compiler/optimizing/nodes.h
@@ -494,7 +494,8 @@ class HGraph : public ArenaObject<kArenaAllocGraph> {
   // Returns the new preheader.
   HBasicBlock* TransformLoopForVectorization(HBasicBlock* header,
                                              HBasicBlock* body,
-                                             HBasicBlock* exit);
+                                             HBasicBlock* exit,
+                                             bool clear_reg);
 
   // Removes `block` from the graph. Assumes `block` has been disconnected from
   // other blocks and has no instructions or phis.
@@ -1636,7 +1637,8 @@ class HLoopInformationOutwardIterator : public ValueObject {
 #if defined(ART_ENABLE_CODEGEN_x86) || defined(ART_ENABLE_CODEGEN_x86_64)
 #define FOR_EACH_CONCRETE_INSTRUCTION_X86_COMMON(M)                     \
   M(X86AndNot, Instruction)                                             \
-  M(X86MaskOrResetLeastSetBit, Instruction)
+  M(X86MaskOrResetLeastSetBit, Instruction)                             \
+  M(X86Clear, Instruction)
 #else
 #define FOR_EACH_CONCRETE_INSTRUCTION_X86_COMMON(M)
 #endif
@@ -2402,6 +2404,9 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
         !IsParameterValue() &&
         // If we added an explicit barrier then we should keep it.
         !IsMemoryBarrier() &&
+        #if defined(ART_ENABLE_CODEGEN_x86_64)
+          !IsX86Clear() &&
+        #endif
         !IsConstructorFence();
   }
 
diff --git a/compiler/optimizing/nodes_x86.h b/compiler/optimizing/nodes_x86.h
index 8e8fbc1581..cec93f3cdf 100644
--- a/compiler/optimizing/nodes_x86.h
+++ b/compiler/optimizing/nodes_x86.h
@@ -214,6 +214,17 @@ class HX86MaskOrResetLeastSetBit final : public HUnaryOperation {
   DEFAULT_COPY_CONSTRUCTOR(X86MaskOrResetLeastSetBit);
 };
 
+class HX86Clear final : public HExpression<0> {
+  public:
+  explicit HX86Clear(uint32_t dex_pc = kNoDexPc)
+      : HExpression(kX86Clear, SideEffects::None(), dex_pc) {
+  }
+
+  DECLARE_INSTRUCTION(X86Clear);
+
+ protected:
+  DEFAULT_COPY_CONSTRUCTOR(X86Clear);
+};
 }  // namespace art
 
 #endif  // ART_COMPILER_OPTIMIZING_NODES_X86_H_
diff --git a/compiler/utils/assembler_test.h b/compiler/utils/assembler_test.h
index 9fffda57fd..b692bfafcb 100644
--- a/compiler/utils/assembler_test.h
+++ b/compiler/utils/assembler_test.h
@@ -29,6 +29,7 @@
 #include "base/malloc_arena_pool.h"
 #include "assembler_test_base.h"
 #include "common_runtime_test.h"  // For ScratchFile
+#include <iostream>
 
 namespace art {
 
@@ -458,6 +459,15 @@ class AssemblerTest : public AssemblerTestBase {
                                                   fmt);
   }
 
+  std::string RepeatFFL(void (Ass::*f)(FPReg, FPReg, size_t), const std::string& fmt) {
+    return RepeatTemplatedRegistersLen<FPReg, FPReg>(f,
+                                                  GetFPRegisters(),
+                                                  GetFPRegisters(),
+                                                  &AssemblerTest::GetFPRegName,
+                                                  &AssemblerTest::GetFPRegName,
+                                                  fmt);
+  }
+
   std::string RepeatFFF(void (Ass::*f)(FPReg, FPReg, FPReg), const std::string& fmt) {
     return RepeatTemplatedRegisters<FPReg, FPReg, FPReg>(f,
                                                          GetFPRegisters(),
@@ -469,6 +479,18 @@ class AssemblerTest : public AssemblerTestBase {
                                                          fmt);
   }
 
+  std::string RepeatFFFL(void (Ass::*f)(FPReg, FPReg, FPReg, size_t), const std::string& fmt) {
+    return RepeatTemplatedRegistersLen<FPReg, FPReg, FPReg>(f,
+                                                         GetFPRegisters(),
+                                                         GetFPRegisters(),
+                                                         GetFPRegisters(),
+                                                         &AssemblerTest::GetFPRegName,
+                                                         &AssemblerTest::GetFPRegName,
+                                                         &AssemblerTest::GetFPRegName,
+                                                         fmt);
+  
+  }
+
   std::string RepeatFFR(void (Ass::*f)(FPReg, FPReg, Reg), const std::string& fmt) {
     return RepeatTemplatedRegisters<FPReg, FPReg, Reg>(
         f,
@@ -970,6 +992,10 @@ class AssemblerTest : public AssemblerTestBase {
     return RepeatFA(f, GetAddresses(), fmt);
   }
 
+  std::string RepeatFAL(void (Ass::*f)(FPReg, const Addr&, size_t), const std::string& fmt) {
+    return RepeatFAL(f, GetAddresses(), fmt);
+  }
+
   // Variant that takes explicit vector of addresss
   // (to test restricted addressing modes set).
   std::string RepeatFA(void (Ass::*f)(FPReg, const Addr&),
@@ -983,6 +1009,17 @@ class AssemblerTest : public AssemblerTestBase {
         &AssemblerTest::GetAddrName,
         fmt);
   }
+  std::string RepeatFAL(void (Ass::*f)(FPReg, const Addr&, size_t),
+                       const std::vector<Addr>& a,
+                       const std::string& fmt) {
+    return RepeatTemplatedRegMemLen<FPReg, Addr>(
+        f,
+        GetFPRegisters(),
+        a,
+        &AssemblerTest::GetFPRegName,
+        &AssemblerTest::GetAddrName,
+        fmt);
+  }
 
   // Repeats over addresses and registers provided by fixture.
   std::string RepeatAR(void (Ass::*f)(const Addr&, Reg), const std::string& fmt) {
@@ -1065,6 +1102,10 @@ class AssemblerTest : public AssemblerTestBase {
     return RepeatAF(f, GetAddresses(), fmt);
   }
 
+  std::string RepeatAFL(void (Ass::*f)(const Addr&, FPReg, size_t vec_len), const std::string& fmt) {
+    return RepeatAFL(f, GetAddresses(), fmt);
+  }
+
   // Variant that takes explicit vector of addresss
   // (to test restricted addressing modes set).
   std::string RepeatAF(void (Ass::*f)(const Addr&, FPReg),
@@ -1079,6 +1120,18 @@ class AssemblerTest : public AssemblerTestBase {
         fmt);
   }
 
+ std::string RepeatAFL(void (Ass::*f)(const Addr&, FPReg, size_t),
+                       const std::vector<Addr>& a,
+                       const std::string& fmt) {
+    return RepeatTemplatedMemRegLen<Addr, FPReg>(
+        f,
+        a,
+        GetFPRegisters(),
+        &AssemblerTest::GetAddrName,
+        &AssemblerTest::GetFPRegName,
+        fmt);
+  }
+
   template <typename AddrType>
   std::string RepeatTemplatedMem(void (Ass::*f)(const AddrType&),
                                  const std::vector<AddrType> addresses,
@@ -1189,6 +1242,46 @@ class AssemblerTest : public AssemblerTestBase {
     return str;
   }
 
+  template <typename RegType, typename AddrType>
+  std::string RepeatTemplatedRegMemLen(void (Ass::*f)(RegType, const AddrType&, size_t vec_len),
+                                    const std::vector<RegType*> registers,
+                                    const std::vector<AddrType> addresses,
+                                    std::string (AssemblerTest::*GetRName)(const RegType&),
+                                    std::string (AssemblerTest::*GetAName)(const AddrType&),
+                                    const std::string& fmt) {
+    WarnOnCombinations(addresses.size() * registers.size());
+    std::string str;
+    size_t vec_len = 0;
+    for (auto reg : registers) {
+      for (auto addr : addresses) {
+        if (f != nullptr) {
+          (assembler_.get()->*f)(*reg, addr, vec_len);
+        }
+        std::string base = fmt;
+
+        std::string reg_string = (this->*GetRName)(*reg);
+        size_t reg_index;
+        if ((reg_index = base.find(REG_TOKEN)) != std::string::npos) {
+          base.replace(reg_index, ConstexprStrLen(REG_TOKEN), reg_string);
+        }
+
+        std::string addr_string = (this->*GetAName)(addr);
+        size_t addr_index;
+        if ((addr_index = base.find(ADDRESS_TOKEN)) != std::string::npos) {
+          base.replace(addr_index, ConstexprStrLen(ADDRESS_TOKEN), addr_string);
+        }
+
+        if (str.size() > 0) {
+          str += "\n";
+        }
+        str += base;
+      }
+    }
+    // Add a newline at the end.
+    str += "\n";
+    return str;
+  }
+
   template <typename AddrType, typename RegType>
   std::string RepeatTemplatedMemReg(void (Ass::*f)(const AddrType&, RegType),
                                     const std::vector<AddrType> addresses,
@@ -1228,6 +1321,46 @@ class AssemblerTest : public AssemblerTestBase {
     return str;
   }
 
+  template <typename AddrType, typename RegType>
+  std::string RepeatTemplatedMemRegLen(void (Ass::*f)(const AddrType&, RegType, size_t vec_len),
+                                    const std::vector<AddrType> addresses,
+                                    const std::vector<RegType*> registers,
+                                    std::string (AssemblerTest::*GetAName)(const AddrType&),
+                                    std::string (AssemblerTest::*GetRName)(const RegType&),
+                                    const std::string& fmt) {
+    WarnOnCombinations(addresses.size() * registers.size());
+    std::string str;
+    size_t vec_len = 0;
+    for (auto addr : addresses) {
+      for (auto reg : registers) {
+        if (f != nullptr) {
+          (assembler_.get()->*f)(addr, *reg, vec_len);
+        }
+        std::string base = fmt;
+
+        std::string addr_string = (this->*GetAName)(addr);
+        size_t addr_index;
+        if ((addr_index = base.find(ADDRESS_TOKEN)) != std::string::npos) {
+          base.replace(addr_index, ConstexprStrLen(ADDRESS_TOKEN), addr_string);
+        }
+
+        std::string reg_string = (this->*GetRName)(*reg);
+        size_t reg_index;
+        if ((reg_index = base.find(REG_TOKEN)) != std::string::npos) {
+          base.replace(reg_index, ConstexprStrLen(REG_TOKEN), reg_string);
+        }
+
+        if (str.size() > 0) {
+          str += "\n";
+        }
+        str += base;
+      }
+    }
+    // Add a newline at the end.
+    str += "\n";
+    return str;
+  }
+
   //
   // Register repeats.
   //
@@ -1260,6 +1393,7 @@ class AssemblerTest : public AssemblerTestBase {
     return str;
   }
 
+  
   template <typename Reg1, typename Reg2>
   std::string RepeatTemplatedRegisters(void (Ass::*f)(Reg1, Reg2),
                                        const std::vector<Reg1*> reg1_registers,
@@ -1267,6 +1401,7 @@ class AssemblerTest : public AssemblerTestBase {
                                        std::string (AssemblerTest::*GetName1)(const Reg1&),
                                        std::string (AssemblerTest::*GetName2)(const Reg2&),
                                        const std::string& fmt) {
+    //GenCombinations(reg1_registers, reg2_registers);
     WarnOnCombinations(reg1_registers.size() * reg2_registers.size());
 
     std::string str;
@@ -1300,6 +1435,48 @@ class AssemblerTest : public AssemblerTestBase {
     return str;
   }
 
+ template <typename Reg1, typename Reg2>
+  std::string RepeatTemplatedRegistersLen(void (Ass::*f)(Reg1, Reg2, size_t vec_len),
+                                       const std::vector<Reg1*> reg1_registers,
+                                       const std::vector<Reg2*> reg2_registers,
+                                       std::string (AssemblerTest::*GetName1)(const Reg1&),
+                                       std::string (AssemblerTest::*GetName2)(const Reg2&),
+                                       const std::string& fmt) {
+     //GenCombinations(reg1_registers, reg2_registers);
+     WarnOnCombinations(reg1_registers.size() * reg2_registers.size());
+
+    std::string str;
+    size_t vec_len = 0;
+    for (auto reg1 : reg1_registers) {
+      for (auto reg2 : reg2_registers) {
+        if (f != nullptr) {
+          (assembler_.get()->*f)(*reg1, *reg2, vec_len);
+        }
+        std::string base = fmt;
+
+        std::string reg1_string = (this->*GetName1)(*reg1);
+        size_t reg1_index;
+        while ((reg1_index = base.find(REG1_TOKEN)) != std::string::npos) {
+          base.replace(reg1_index, ConstexprStrLen(REG1_TOKEN), reg1_string);
+        }
+
+        std::string reg2_string = (this->*GetName2)(*reg2);
+        size_t reg2_index;
+        while ((reg2_index = base.find(REG2_TOKEN)) != std::string::npos) {
+          base.replace(reg2_index, ConstexprStrLen(REG2_TOKEN), reg2_string);
+        }
+
+        if (str.size() > 0) {
+          str += "\n";
+        }
+        str += base;
+      }
+    }
+    // Add a newline at the end.
+    str += "\n";
+    return str;
+  }
+
   template <typename Reg1, typename Reg2>
   std::string RepeatTemplatedRegistersNoDupes(void (Ass::*f)(Reg1, Reg2),
                                               const std::vector<Reg1*> reg1_registers,
@@ -1350,6 +1527,7 @@ class AssemblerTest : public AssemblerTestBase {
                                        std::string (AssemblerTest::*GetName2)(const Reg2&),
                                        std::string (AssemblerTest::*GetName3)(const Reg3&),
                                        const std::string& fmt) {
+    //GenCombinations3(reg1_registers, reg2_registers, reg3_registers);
     std::string str;
     for (auto reg1 : reg1_registers) {
       for (auto reg2 : reg2_registers) {
@@ -1386,8 +1564,66 @@ class AssemblerTest : public AssemblerTestBase {
     }
     // Add a newline at the end.
     str += "\n";
-    return str;
-  }
+    return str; 
+    }
+
+  template <typename Reg1, typename Reg2, typename Reg3>
+  std::string RepeatTemplatedRegistersLen(void (Ass::*f)(Reg1, Reg2, Reg3, size_t vec_len),
+                                       const std::vector<Reg1*> reg1_registers,
+                                       const std::vector<Reg2*> reg2_registers,
+                                       const std::vector<Reg3*> reg3_registers,
+                                       std::string (AssemblerTest::*GetName1)(const Reg1&),
+                                       std::string (AssemblerTest::*GetName2)(const Reg2&),
+                                       std::string (AssemblerTest::*GetName3)(const Reg3&),
+                                       const std::string& fmt) {
+    //GenCombinations3(reg1_registers, reg2_registers, reg3_registers);
+    std::string str;
+    size_t vec1_len = 0;
+     for (auto reg1 : reg1_registers) {
+      for (auto reg2 : reg2_registers) {
+        for (auto reg3 : reg3_registers) {
+          if (f != nullptr) {
+            (assembler_.get()->*f)(*reg1, *reg2, *reg3, vec1_len);
+          }
+          std::string base = fmt;
+          std::string len = "vec_len";
+          size_t pos = base.find(len);
+	  if (pos != std::string::npos) {
+	     // If found then erase it from string
+	     base.erase(pos, len.length());
+          }  
+          //std::cout << "base=" << base << std::endl;
+          std::string reg1_string = (this->*GetName1)(*reg1);
+          size_t reg1_index;
+          while ((reg1_index = base.find(REG1_TOKEN)) != std::string::npos) {
+            base.replace(reg1_index, ConstexprStrLen(REG1_TOKEN), reg1_string);
+          }
+
+          std::string reg2_string = (this->*GetName2)(*reg2);
+          size_t reg2_index;
+          while ((reg2_index = base.find(REG2_TOKEN)) != std::string::npos) {
+            base.replace(reg2_index, ConstexprStrLen(REG2_TOKEN), reg2_string);
+          }
+
+          std::string reg3_string = (this->*GetName3)(*reg3);
+          size_t reg3_index;
+          while ((reg3_index = base.find(REG3_TOKEN)) != std::string::npos) {
+            base.replace(reg3_index, ConstexprStrLen(REG3_TOKEN), reg3_string);
+          }
+
+          if (str.size() > 0) {
+            str += "\n";
+          }
+          //std::cout <<"string=" << str << std::endl;
+          str += base;
+          //std::cout <<"string1=" << str << std::endl;
+        }
+      }
+    }
+    // Add a newline at the end.
+    str += "\n";
+    return str;  
+    }
 
   template <typename Reg1, typename Reg2>
   std::string RepeatTemplatedRegistersImm(void (Ass::*f)(Reg1, Reg2, const Imm&),
diff --git a/compiler/utils/x86/assembler_x86.cc b/compiler/utils/x86/assembler_x86.cc
index adc7bcc0f4..29918f8cb0 100644
--- a/compiler/utils/x86/assembler_x86.cc
+++ b/compiler/utils/x86/assembler_x86.cc
@@ -3925,6 +3925,13 @@ void X86Assembler::EmitGenericShift(int reg_or_opcode,
   EmitOperand(reg_or_opcode, operand);
 }
 
+void X86Assembler::vzeroupper() {
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0xC5);
+  EmitUint8(0xF8);
+  EmitUint8(0x77);
+}
+
 void X86Assembler::AddConstantArea() {
   ArrayRef<const int32_t> area = constant_area_.GetBuffer();
   // Generate the data for the literal area.
diff --git a/compiler/utils/x86/assembler_x86.h b/compiler/utils/x86/assembler_x86.h
index a9050e6df1..e3035f8123 100644
--- a/compiler/utils/x86/assembler_x86.h
+++ b/compiler/utils/x86/assembler_x86.h
@@ -823,6 +823,8 @@ class X86Assembler final : public Assembler {
   void rep_movsb();
   void rep_movsw();
 
+  void vzeroupper();
+
   X86Assembler* lock();
   void cmpxchgb(const Address& address, ByteRegister reg);
   void cmpxchgw(const Address& address, Register reg);
diff --git a/compiler/utils/x86_64/assembler_x86_64.cc b/compiler/utils/x86_64/assembler_x86_64.cc
index 7116bf2131..a982d8471a 100644
--- a/compiler/utils/x86_64/assembler_x86_64.cc
+++ b/compiler/utils/x86_64/assembler_x86_64.cc
@@ -20,6 +20,7 @@
 #include "base/memory_region.h"
 #include "entrypoints/quick/quick_entrypoints.h"
 #include "thread.h"
+#include <iostream>
 
 namespace art {
 namespace x86_64 {
@@ -68,7 +69,7 @@ bool X86_64Assembler::CpuHasAVXorAVX2FeatureFlag() {
   if (has_AVX_ || has_AVX2_) {
     return true;
   }
-  return false;
+  return true;
 }
 
 
@@ -413,9 +414,9 @@ void X86_64Assembler::leal(CpuRegister dst, const Address& src) {
 }
 
 
-void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src) {
+void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovaps(dst, src);
+    vmovaps(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -427,9 +428,10 @@ void X86_64Assembler::movaps(XmmRegister dst, XmmRegister src) {
 
 
 /**VEX.128.0F.WIG 28 /r VMOVAPS xmm1, xmm2 */
-void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
+void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   uint8_t byte_zero, byte_one, byte_two;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = true;
   bool load = dst.NeedsRex();
   bool store = !load;
@@ -445,7 +447,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
     bool rex_bit = (load) ? dst.NeedsRex() : src.NeedsRex();
     byte_one = EmitVexPrefixByteOne(rex_bit,
                                     vvvv_reg,
-                                    SET_VEX_L_128,
+                                    VEX_L,
                                     SET_VEX_PP_NONE);
   } else {
     byte_one = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -453,7 +455,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
                                     src.NeedsRex(),
                                     SET_VEX_M_0F);
     byte_two = EmitVexPrefixByteTwo(/*W=*/ false,
-                                    SET_VEX_L_128,
+                                    VEX_L,
                                     SET_VEX_PP_NONE);
   }
   EmitUint8(byte_zero);
@@ -475,9 +477,9 @@ void X86_64Assembler::vmovaps(XmmRegister dst, XmmRegister src) {
   }
 }
 
-void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
+void X86_64Assembler::movaps(XmmRegister dst, const Address& src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovaps(dst, src);
+    vmovaps(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -488,10 +490,11 @@ void X86_64Assembler::movaps(XmmRegister dst, const Address& src) {
 }
 
 /**VEX.128.0F.WIG 28 /r VMOVAPS xmm1, m128 */
-void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
+void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = false;
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -505,7 +508,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -513,7 +516,7 @@ void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
@@ -527,9 +530,9 @@ void X86_64Assembler::vmovaps(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
+void X86_64Assembler::movups(XmmRegister dst, const Address& src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovups(dst, src);
+    vmovups(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -540,10 +543,11 @@ void X86_64Assembler::movups(XmmRegister dst, const Address& src) {
 }
 
 /** VEX.128.0F.WIG 10 /r VMOVUPS xmm1, m128 */
-void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
+void X86_64Assembler::vmovups(XmmRegister dst, const Address& src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = false;
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -557,7 +561,7 @@ void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -565,7 +569,7 @@ void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
@@ -580,9 +584,9 @@ void X86_64Assembler::vmovups(XmmRegister dst, const Address& src) {
 }
 
 
-void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
+void X86_64Assembler::movaps(const Address& dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovaps(dst, src);
+    vmovaps(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -593,10 +597,11 @@ void X86_64Assembler::movaps(const Address& dst, XmmRegister src) {
 }
 
 /** VEX.128.0F.WIG 29 /r VMOVAPS m128, xmm1 */
-void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
+void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = false;
 
   // Instruction VEX Prefix
@@ -611,7 +616,7 @@ void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
@@ -619,7 +624,7 @@ void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
@@ -633,9 +638,9 @@ void X86_64Assembler::vmovaps(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
+void X86_64Assembler::movups(const Address& dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovups(dst, src);
+    vmovups(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -646,10 +651,11 @@ void X86_64Assembler::movups(const Address& dst, XmmRegister src) {
 }
 
 /** VEX.128.0F.WIG 11 /r VMOVUPS m128, xmm1 */
-void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
+void X86_64Assembler::vmovups(const Address& dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = false;
 
   // Instruction VEX Prefix
@@ -664,7 +670,7 @@ void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
@@ -672,7 +678,7 @@ void X86_64Assembler::vmovups(const Address& dst, XmmRegister src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
@@ -855,25 +861,27 @@ void X86_64Assembler::subps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+void X86_64Assembler::vaddps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = 0x00;
+  VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
+  if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
   X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
-                                   add_right.NeedsRex(),
+                                   src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -881,24 +889,55 @@ void X86_64Assembler::vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister
     EmitUint8(ByteTwo);
   }
   EmitUint8(0x58);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
 }
 
-void X86_64Assembler::vsubps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+/*void X86_64Assembler::vaddps(YmmRegister dst, YmmRegister add_left, YmmRegister add_right) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  std::cout << "In YmmRegister" << std::endl;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!add_right.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromYmmRegister(add_left.AsVectorRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_256, SET_VEX_PP_NONE);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /X=/ false,
+                                   add_right.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(W=/ false, vvvv_reg, SET_VEX_L_256, SET_VEX_PP_NONE);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x58);
+  EmitYmmRegisterOperand(dst.LowBits(), add_right);
+}*/
+
+void X86_64Assembler::vsubps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t byte_zero = 0x00, byte_one = 0x00, byte_two = 0x00;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
   byte_zero = EmitVexPrefixByteZero(is_twobyte_form);
   X86_64ManagedRegister vvvv_reg = X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     byte_one = EmitVexPrefixByteOne(dst.NeedsRex(), /*X=*/ false, src2.NeedsRex(), SET_VEX_M_0F);
-    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    byte_two = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(byte_zero);
   EmitUint8(byte_one);
@@ -918,11 +957,12 @@ void X86_64Assembler::mulps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+void X86_64Assembler::vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
@@ -930,13 +970,13 @@ void X86_64Assembler::vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -955,11 +995,12 @@ void X86_64Assembler::divps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+void X86_64Assembler::vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
@@ -967,13 +1008,13 @@ void X86_64Assembler::vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_NONE);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1005,9 +1046,9 @@ void X86_64Assembler::fstps(const Address& dst) {
 }
 
 
-void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src) {
+void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovapd(dst, src);
+    vmovapd(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1019,10 +1060,11 @@ void X86_64Assembler::movapd(XmmRegister dst, XmmRegister src) {
 }
 
 /** VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, xmm2 */
-void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
+void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = true;
 
   if (src.NeedsRex() && dst.NeedsRex()) {
@@ -1036,7 +1078,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
     bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
     ByteOne = EmitVexPrefixByteOne(rex_bit,
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -1044,7 +1086,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
                                    src.NeedsRex(),
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1065,10 +1107,57 @@ void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src) {
     EmitXmmRegisterOperand(dst.LowBits(), src);
   }
 }
+/*void X86_64Assembler::vmovapd(XmmRegister dst, XmmRegister src, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  bool is_twobyte_form = true;
 
-void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
+  if (src.NeedsRex() && dst.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  // Instruction VEX Prefix
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  bool load = dst.NeedsRex();
+  if (is_twobyte_form) {
+    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+    bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
+    ByteOne = EmitVexPrefixByteOne(rex_bit,
+                                   vvvv_reg,
+                                   VEX_L,
+                                   SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   //X=/ false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/W=/ false,
+                                   VEX_L,
+                                   SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  // Instruction Opcode
+  if (is_twobyte_form && !load) {
+    EmitUint8(0x29);
+  } else {
+    EmitUint8(0x28);
+  }
+  // Instruction Operands
+  if (is_twobyte_form && !load) {
+    EmitXmmRegisterOperand(src.LowBits(), dst);
+  } else {
+    EmitXmmRegisterOperand(dst.LowBits(), src);
+  }
+}*/
+
+void X86_64Assembler::movapd(XmmRegister dst, const Address& src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovapd(dst, src);
+    vmovapd(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1080,10 +1169,11 @@ void X86_64Assembler::movapd(XmmRegister dst, const Address& src) {
 }
 
 /** VEX.128.66.0F.WIG 28 /r VMOVAPD xmm1, m128 */
-void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
+void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   bool is_twobyte_form = false;
 
   // Instruction VEX Prefix
@@ -1098,7 +1188,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -1106,7 +1196,7 @@ void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1120,9 +1210,9 @@ void X86_64Assembler::vmovapd(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
+void X86_64Assembler::movupd(XmmRegister dst, const Address& src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovupd(dst, src);
+    vmovupd(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1134,12 +1224,12 @@ void X86_64Assembler::movupd(XmmRegister dst, const Address& src) {
 }
 
 /** VEX.128.66.0F.WIG 10 /r VMOVUPD xmm1, m128 */
-void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
+void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
-
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
   bool Rex_x = rex & GET_REX_X;
@@ -1152,7 +1242,7 @@ void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -1160,7 +1250,7 @@ void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1173,9 +1263,48 @@ void X86_64Assembler::vmovupd(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
+/*void X86_64Assembler::vmovupd(YmmRegister dst, const Address& src) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero, ByteOne, ByteTwo;
+
+  // Instruction VEX Prefix
+  uint8_t rex = src.rex();
+  bool Rex_x = rex & GET_REX_X;
+  bool Rex_b = rex & GET_REX_B;
+  if (!Rex_b && !Rex_x) {
+    is_twobyte_form = true;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   vvvv_reg,
+                                   SET_VEX_L_256,
+                                   SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   Rex_x,
+                                   Rex_b,
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(W=/ false,
+                                   SET_VEX_L_256,
+                                   SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form)
+  EmitUint8(ByteTwo);
+  // Instruction Opcode
+  EmitUint8(0x10);
+  // Instruction Operands
+  EmitOperand(dst.LowBits(), src);
+}*/
+
+void X86_64Assembler::movapd(const Address& dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovapd(dst, src);
+    vmovapd(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1187,11 +1316,51 @@ void X86_64Assembler::movapd(const Address& dst, XmmRegister src) {
 }
 
 /** VEX.128.66.0F.WIG 29 /r VMOVAPD m128, xmm1 */
-void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
+void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 :  SET_VEX_L_128;
+  // Instruction VEX Prefix
+  uint8_t rex = dst.rex();
+  bool Rex_x = rex & GET_REX_X;
+  bool Rex_b = rex & GET_REX_B;
+  if (!Rex_x && !Rex_b) {
+    is_twobyte_form = true;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
+                                   vvvv_reg,
+                                   VEX_L,
+                                   SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
+                                   Rex_x,
+                                   Rex_b,
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
+                                   VEX_L,
+                                   SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  // Instruction Opcode
+  EmitUint8(0x29);
+  // Instruction Operands
+  EmitOperand(src.LowBits(), dst);
+}
+void X86_64Assembler::vmovapd(const Address& dst, YmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
   bool Rex_x = rex & GET_REX_X;
@@ -1204,7 +1373,7 @@ void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
@@ -1212,7 +1381,7 @@ void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1226,9 +1395,9 @@ void X86_64Assembler::vmovapd(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
+void X86_64Assembler::movupd(const Address& dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovupd(dst, src);
+    vmovupd(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1240,11 +1409,12 @@ void X86_64Assembler::movupd(const Address& dst, XmmRegister src) {
 }
 
 /** VEX.128.66.0F.WIG 11 /r VMOVUPD m128, xmm1 */
-void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
+void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero, ByteOne, ByteTwo;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
 
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
@@ -1258,7 +1428,7 @@ void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
@@ -1266,7 +1436,7 @@ void X86_64Assembler::vmovupd(const Address& dst, XmmRegister src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1401,24 +1571,25 @@ void X86_64Assembler::addpd(XmmRegister dst, XmmRegister src) {
 }
 
 
-void X86_64Assembler::vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+void X86_64Assembler::vaddpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
-  if (!add_right.NeedsRex()) {
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
   ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
   X86_64ManagedRegister vvvv_reg =
-      X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
-                                   add_right.NeedsRex(),
+                                   src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1426,7 +1597,7 @@ void X86_64Assembler::vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister
     EmitUint8(ByteTwo);
   }
   EmitUint8(0x58);
-  EmitXmmRegisterOperand(dst.LowBits(), add_right);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
 }
 
 
@@ -1440,10 +1611,11 @@ void X86_64Assembler::subpd(XmmRegister dst, XmmRegister src) {
 }
 
 
-void X86_64Assembler::vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+void X86_64Assembler::vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
@@ -1451,13 +1623,13 @@ void X86_64Assembler::vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1478,11 +1650,12 @@ void X86_64Assembler::mulpd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+void X86_64Assembler::vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
@@ -1490,13 +1663,13 @@ void X86_64Assembler::vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1517,11 +1690,12 @@ void X86_64Assembler::divpd(XmmRegister dst, XmmRegister src) {
 }
 
 
-void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2) {
+void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (!src2.NeedsRex()) {
     is_twobyte_form = true;
   }
@@ -1529,13 +1703,13 @@ void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    src2.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1547,9 +1721,9 @@ void X86_64Assembler::vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2
 }
 
 
-void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src) {
+void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovdqa(dst, src);
+    vmovdqa(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1561,11 +1735,12 @@ void X86_64Assembler::movdqa(XmmRegister dst, XmmRegister src) {
 }
 
 /** VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, xmm2 */
-void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
+void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = true;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
 
   // Instruction VEX Prefix
   if (src.NeedsRex() && dst.NeedsRex()) {
@@ -1578,7 +1753,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
     bool rex_bit = load ? dst.NeedsRex() : src.NeedsRex();
     ByteOne = EmitVexPrefixByteOne(rex_bit,
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -1586,7 +1761,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
                                    src.NeedsRex(),
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1608,9 +1783,9 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, XmmRegister src) {
   }
 }
 
-void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
+void X86_64Assembler::movdqa(XmmRegister dst, const Address& src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovdqa(dst, src);
+    vmovdqa(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1622,11 +1797,12 @@ void X86_64Assembler::movdqa(XmmRegister dst, const Address& src) {
 }
 
 /** VEX.128.66.0F.WIG 6F /r VMOVDQA xmm1, m128 */
-void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
+void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t  ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
 
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -1640,7 +1816,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -1648,7 +1824,7 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1662,9 +1838,9 @@ void X86_64Assembler::vmovdqa(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::movdqu(XmmRegister dst, const Address& src) {
+void X86_64Assembler::movdqu(XmmRegister dst, const Address& src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovdqu(dst, src);
+    vmovdqu(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1677,11 +1853,12 @@ void X86_64Assembler::movdqu(XmmRegister dst, const Address& src) {
 
 /** VEX.128.F3.0F.WIG 6F /r VMOVDQU xmm1, m128
 Load Unaligned */
-void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
+void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 2u) ? SET_VEX_L_256 : SET_VEX_L_128;
 
   // Instruction VEX Prefix
   uint8_t rex = src.rex();
@@ -1695,7 +1872,8 @@ void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   //SET_VEX_L_128,
+				   VEX_L, 
                                    SET_VEX_PP_F3);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
@@ -1703,7 +1881,8 @@ void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   //SET_VEX_L_128,
+				   VEX_L, 
                                    SET_VEX_PP_F3);
   }
   EmitUint8(ByteZero);
@@ -1717,9 +1896,9 @@ void X86_64Assembler::vmovdqu(XmmRegister dst, const Address& src) {
   EmitOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
+void X86_64Assembler::movdqa(const Address& dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovdqa(dst, src);
+    vmovdqa(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1731,10 +1910,11 @@ void X86_64Assembler::movdqa(const Address& dst, XmmRegister src) {
 }
 
 /** VEX.128.66.0F.WIG 7F /r VMOVDQA m128, xmm1 */
-void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
+void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   uint8_t ByteZero, ByteOne, ByteTwo;
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
@@ -1748,7 +1928,8 @@ void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   //SET_VEX_L_128,
+				   VEX_L, 
                                    SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
@@ -1756,7 +1937,8 @@ void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   //SET_VEX_L_128,
+				   VEX_L, 
                                    SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
@@ -1770,9 +1952,9 @@ void X86_64Assembler::vmovdqa(const Address& dst, XmmRegister src) {
   EmitOperand(src.LowBits(), dst);
 }
 
-void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
+void X86_64Assembler::movdqu(const Address& dst, XmmRegister src, size_t vec_len) {
   if (CpuHasAVXorAVX2FeatureFlag()) {
-    vmovdqu(dst, src);
+    vmovdqu(dst, src, vec_len);
     return;
   }
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -1784,11 +1966,12 @@ void X86_64Assembler::movdqu(const Address& dst, XmmRegister src) {
 }
 
 /** VEX.128.F3.0F.WIG 7F /r VMOVDQU m128, xmm1 */
-void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
+void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteZero, ByteOne, ByteTwo;
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 2u) ? SET_VEX_L_256 : SET_VEX_L_128;
 
   // Instruction VEX Prefix
   uint8_t rex = dst.rex();
@@ -1802,7 +1985,7 @@ void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
     X86_64ManagedRegister vvvv_reg = ManagedRegister::NoRegister().AsX86_64();
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
                                    vvvv_reg,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_F3);
   } else {
     ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
@@ -1810,7 +1993,7 @@ void X86_64Assembler::vmovdqu(const Address& dst, XmmRegister src) {
                                    Rex_b,
                                    SET_VEX_M_0F);
     ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false,
-                                   SET_VEX_L_128,
+                                   VEX_L,
                                    SET_VEX_PP_F3);
   }
   EmitUint8(ByteZero);
@@ -1834,11 +2017,12 @@ void X86_64Assembler::paddb(XmmRegister dst, XmmRegister src) {
 }
 
 
-void X86_64Assembler::vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+void X86_64Assembler::vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
   bool is_twobyte_form = true;
+  uint8_t VEX_L = (vec_len == 32u) ? SET_VEX_L_256 : SET_VEX_L_128;
   if (add_right.NeedsRex()) {
     is_twobyte_form = false;
   }
@@ -1846,13 +2030,13 @@ void X86_64Assembler::vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    add_right.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -1913,10 +2097,12 @@ void X86_64Assembler::paddw(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+void X86_64Assembler::vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 16u) ? SET_VEX_L_256 : SET_VEX_L_128;
+
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
   if (!add_right.NeedsRex()) {
     is_twobyte_form = true;
@@ -1925,13 +2111,13 @@ void X86_64Assembler::vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    add_right.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -2029,10 +2215,11 @@ void X86_64Assembler::paddd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
-void X86_64Assembler::vpaddd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+void X86_64Assembler::vpaddd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
   if (!add_right.NeedsRex()) {
     is_twobyte_form = true;
@@ -2041,13 +2228,13 @@ void X86_64Assembler::vpaddd(XmmRegister dst, XmmRegister add_left, XmmRegister
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    add_right.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -2107,10 +2294,11 @@ void X86_64Assembler::paddq(XmmRegister dst, XmmRegister src) {
 }
 
 
-void X86_64Assembler::vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right) {
+void X86_64Assembler::vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len) {
   DCHECK(CpuHasAVXorAVX2FeatureFlag());
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   bool is_twobyte_form = false;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
   uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
   if (!add_right.NeedsRex()) {
     is_twobyte_form = true;
@@ -2119,13 +2307,13 @@ void X86_64Assembler::vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister
   X86_64ManagedRegister vvvv_reg =
       X86_64ManagedRegister::FromXmmRegister(add_left.AsFloatRegister());
   if (is_twobyte_form) {
-    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
   } else {
     ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
                                    /*X=*/ false,
                                    add_right.NeedsRex(),
                                    SET_VEX_M_0F);
-    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, SET_VEX_L_128, SET_VEX_PP_66);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
   }
   EmitUint8(ByteZero);
   EmitUint8(ByteOne);
@@ -2464,6 +2652,83 @@ void X86_64Assembler::cvtdq2ps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::vcvtdq2ps(XmmRegister dst, XmmRegister src, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = 0x00;
+  VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!src.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), VEX_L, SET_VEX_PP_NONE);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, VEX_L, SET_VEX_PP_NONE);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x5B);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
+
+void X86_64Assembler::vcvttps2dq(XmmRegister dst, XmmRegister src, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = 0x00;
+  VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!src.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), VEX_L, SET_VEX_PP_F3);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, VEX_L, SET_VEX_PP_F3);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x5B);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
+
+void X86_64Assembler::vpabsd(XmmRegister dst, XmmRegister src, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = 0x00;
+  VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F_38);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, VEX_L, SET_VEX_PP_66);
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  EmitUint8(ByteTwo);
+  EmitUint8(0x1E);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
 
 void X86_64Assembler::cvtdq2pd(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -2749,6 +3014,35 @@ void X86_64Assembler::andpd(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::vandpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128; 
+  if (!src2.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src2.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x54);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
 void X86_64Assembler::andps(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitOptionalRex32(dst, src);
@@ -2757,6 +3051,36 @@ void X86_64Assembler::andps(XmmRegister dst, XmmRegister src) {
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
 
+void X86_64Assembler::vandps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128; 
+  if (!src2.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src2.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x54);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
+
 void X86_64Assembler::pand(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
   EmitUint8(0x66);
@@ -3397,6 +3721,42 @@ void X86_64Assembler::maxpd(XmmRegister dst, XmmRegister src) {
   EmitUint8(0x5F);
   EmitXmmRegisterOperand(dst.LowBits(), src);
 }
+void X86_64Assembler::vpcmpeqb(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len) {
+  /*AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0x66);
+  EmitOptionalRex32(dst, src);
+  EmitUint8(0x0F);
+  EmitUint8(0x74);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+  */
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = (vec_len == 32u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!src2.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src2.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x74);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+}
 
 void X86_64Assembler::pcmpeqb(XmmRegister dst, XmmRegister src) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -3482,6 +3842,42 @@ void X86_64Assembler::shufpd(XmmRegister dst, XmmRegister src, const Immediate&
   EmitUint8(imm.value());
 }
 
+void X86_64Assembler::vshufpd(XmmRegister dst, XmmRegister src1, XmmRegister src2,
+                              const Immediate& imm, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128; 
+  bool is_twobyte_form = true;
+  if (src2.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src2.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0xC6);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  //EmitUint8(0x66);
+  //EmitOptionalRex32(dst, src);
+  //EmitUint8(0x0F);
+  //EmitUint8(0xC6);
+  //EmitXmmRegisterOperand(dst.LowBits(), src);
+  EmitUint8(imm.value());
+}
 
 void X86_64Assembler::shufps(XmmRegister dst, XmmRegister src, const Immediate& imm) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -3492,6 +3888,73 @@ void X86_64Assembler::shufps(XmmRegister dst, XmmRegister src, const Immediate&
   EmitUint8(imm.value());
 }
 
+void X86_64Assembler::vshufps(XmmRegister dst, XmmRegister src1, XmmRegister src2,
+                              const Immediate& imm, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128; 
+  bool is_twobyte_form = true;
+  if (src2.NeedsRex()) {
+    is_twobyte_form = false;
+  }
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(src1.AsFloatRegister());
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_NONE);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src2.NeedsRex(),
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_NONE);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0xC6);
+  EmitXmmRegisterOperand(dst.LowBits(), src2);
+  EmitUint8(imm.value());
+}
+
+void X86_64Assembler::vbroadcastss(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
+  //uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128; 
+  ByteZero = EmitVexPrefixByteZero(false /*is_twobyte_form */);
+  ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F_38);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, SET_VEX_L_256, SET_VEX_PP_66);
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  EmitUint8(ByteTwo);
+  EmitUint8(0x18);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
+
+void X86_64Assembler::vbroadcastsd(XmmRegister dst, XmmRegister src) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  uint8_t ByteOne = 0x00, ByteZero = 0x00, ByteTwo = 0x00;
+  //uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128; 
+  ByteZero = EmitVexPrefixByteZero(false /*is_twobyte_form */);
+  ByteOne = EmitVexPrefixByteOne(dst.NeedsRex(),
+                                   /*X=*/ false,
+                                   src.NeedsRex(),
+                                   SET_VEX_M_0F_38);
+  ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, SET_VEX_L_256, SET_VEX_PP_66);
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  EmitUint8(ByteTwo);
+  EmitUint8(0x19);
+  EmitXmmRegisterOperand(dst.LowBits(), src);
+}
 
 void X86_64Assembler::pshufd(XmmRegister dst, XmmRegister src, const Immediate& imm) {
   AssemblerBuffer::EnsureCapacity ensured(&buffer_);
@@ -3667,6 +4130,36 @@ void X86_64Assembler::psrld(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+void X86_64Assembler::vpsrld(XmmRegister dst, XmmRegister src, const Immediate& shift_count, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = (vec_len == 8u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!src.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(dst.AsFloatRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
+                                   /*X=*/ false,
+                                   /*B=*/ false,
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x72);
+  EmitXmmRegisterOperand(2, src);
+  EmitUint8(shift_count.value());
+}
 
 void X86_64Assembler::psrlq(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
@@ -3679,6 +4172,36 @@ void X86_64Assembler::psrlq(XmmRegister reg, const Immediate& shift_count) {
   EmitUint8(shift_count.value());
 }
 
+void X86_64Assembler::vpsrlq(XmmRegister dst, XmmRegister src, const Immediate& shift_count, size_t vec_len) {
+  DCHECK(CpuHasAVXorAVX2FeatureFlag());
+  uint8_t VEX_L = (vec_len == 4u) ? SET_VEX_L_256 : SET_VEX_L_128;
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  bool is_twobyte_form = false;
+  uint8_t ByteZero = 0x00, ByteOne = 0x00, ByteTwo = 0x00;
+  if (!src.NeedsRex()) {
+    is_twobyte_form = true;
+  }
+  X86_64ManagedRegister vvvv_reg =
+      X86_64ManagedRegister::FromXmmRegister(dst.AsFloatRegister());
+  ByteZero = EmitVexPrefixByteZero(is_twobyte_form);
+  if (is_twobyte_form) {
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(), vvvv_reg, VEX_L, SET_VEX_PP_66);
+  } else {
+    ByteOne = EmitVexPrefixByteOne(src.NeedsRex(),
+                                   /*X=*/ false,
+                                   /*B=*/ false,
+                                   SET_VEX_M_0F);
+    ByteTwo = EmitVexPrefixByteTwo(/*W=*/ false, vvvv_reg, VEX_L, SET_VEX_PP_66);
+  }
+  EmitUint8(ByteZero);
+  EmitUint8(ByteOne);
+  if (!is_twobyte_form) {
+    EmitUint8(ByteTwo);
+  }
+  EmitUint8(0x73);
+  EmitXmmRegisterOperand(2, src);
+  EmitUint8(shift_count.value());
+}
 
 void X86_64Assembler::psrldq(XmmRegister reg, const Immediate& shift_count) {
   DCHECK(shift_count.is_uint8());
@@ -5262,6 +5785,13 @@ void X86_64Assembler::EmitGenericShift(bool wide,
   EmitOperand(reg_or_opcode, Operand(operand));
 }
 
+void X86_64Assembler::vzeroupper() {
+  AssemblerBuffer::EnsureCapacity ensured(&buffer_);
+  EmitUint8(0xC5);
+  EmitUint8(0xF8);
+  EmitUint8(0x77);
+}
+
 void X86_64Assembler::EmitOptionalRex(bool force, bool w, bool r, bool x, bool b) {
   // REX.WRXB
   // W - 64-bit operand
@@ -5523,6 +6053,30 @@ uint8_t X86_64Assembler::EmitVexPrefixByteOne(bool R,
   return vex_prefix;
 }
 
+uint8_t X86_64Assembler::EmitVexPrefixByteOne(bool R,
+                                              int SET_VEX_L,
+                                              int SET_VEX_PP) {
+  // Vex Byte 1,
+  uint8_t vex_prefix = VEX_INIT;
+
+  /** Bit[7] This bits needs to be set to '1' with default value.
+  When using C4H form of VEX prefix, REX.W value is ignored */
+  if (!R) {
+    vex_prefix |= SET_VEX_R;
+  }
+  /** Bits[6:3] - 'vvvv' the source or dest register specifier */
+  vex_prefix |= (0x0F << 3);
+  /** Bit[2] - "L" If VEX.L = 1 indicates 256-bit vector operation,
+  VEX.L = 0 indicates 128 bit vector operation */
+  vex_prefix |= SET_VEX_L;
+
+  // Bits[1:0] -  "pp"
+  if (SET_VEX_PP != SET_VEX_PP_NONE) {
+    vex_prefix |= SET_VEX_PP;
+  }
+  return vex_prefix;
+}
+
 uint8_t X86_64Assembler::EmitVexPrefixByteTwo(bool W,
                                               X86_64ManagedRegister operand,
                                               int SET_VEX_L,
diff --git a/compiler/utils/x86_64/assembler_x86_64.h b/compiler/utils/x86_64/assembler_x86_64.h
index e5a9ce4834..c19b9018be 100644
--- a/compiler/utils/x86_64/assembler_x86_64.h
+++ b/compiler/utils/x86_64/assembler_x86_64.h
@@ -414,17 +414,17 @@ class X86_64Assembler final : public Assembler {
   void leaq(CpuRegister dst, const Address& src);
   void leal(CpuRegister dst, const Address& src);
 
-  void movaps(XmmRegister dst, XmmRegister src);     // move
-  void movaps(XmmRegister dst, const Address& src);  // load aligned
-  void movups(XmmRegister dst, const Address& src);  // load unaligned
-  void movaps(const Address& dst, XmmRegister src);  // store aligned
-  void movups(const Address& dst, XmmRegister src);  // store unaligned
-
-  void vmovaps(XmmRegister dst, XmmRegister src);     // move
-  void vmovaps(XmmRegister dst, const Address& src);  // load aligned
-  void vmovaps(const Address& dst, XmmRegister src);  // store aligned
-  void vmovups(XmmRegister dst, const Address& src);  // load unaligned
-  void vmovups(const Address& dst, XmmRegister src);  // store unaligned
+  void movaps(XmmRegister dst, XmmRegister src, size_t vec_len);     // move
+  void movaps(XmmRegister dst, const Address& src, size_t vec_len);  // load aligned
+  void movups(XmmRegister dst, const Address& src, size_t vec_len);  // load unaligned
+  void movaps(const Address& dst, XmmRegister src, size_t vec_len);  // store aligned
+  void movups(const Address& dst, XmmRegister src, size_t vec_len);  // store unaligned
+
+  void vmovaps(XmmRegister dst, XmmRegister src, size_t vec_len);     // move
+  void vmovaps(XmmRegister dst, const Address& src, size_t vec_len);  // load aligned
+  void vmovaps(const Address& dst, XmmRegister src, size_t vec_len);  // store aligned
+  void vmovups(XmmRegister dst, const Address& src, size_t vec_len);  // load unaligned
+  void vmovups(const Address& dst, XmmRegister src, size_t vec_len);  // store unaligned
 
   void movss(XmmRegister dst, const Address& src);
   void movss(const Address& dst, XmmRegister src);
@@ -452,27 +452,30 @@ class X86_64Assembler final : public Assembler {
   void mulps(XmmRegister dst, XmmRegister src);
   void divps(XmmRegister dst, XmmRegister src);
 
-  void vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-  void vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
-
-  void vaddps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vsubps(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vsubpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vaddpd(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-
-  void movapd(XmmRegister dst, XmmRegister src);     // move
-  void movapd(XmmRegister dst, const Address& src);  // load aligned
-  void movupd(XmmRegister dst, const Address& src);  // load unaligned
-  void movapd(const Address& dst, XmmRegister src);  // store aligned
-  void movupd(const Address& dst, XmmRegister src);  // store unaligned
-
-  void vmovapd(XmmRegister dst, XmmRegister src);     // move
-  void vmovapd(XmmRegister dst, const Address& src);  // load aligned
-  void vmovapd(const Address& dst, XmmRegister src);  // store aligned
-  void vmovupd(XmmRegister dst, const Address& src);  // load unaligned
-  void vmovupd(const Address& dst, XmmRegister src);  // store unaligned
+  void vmulps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+  void vmulpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+  void vdivps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+  void vdivpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+
+  void vaddps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+  //void vaddps(YmmRegister dst, YmmRegister add_left, YmmRegister add_right);
+  void vsubps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+  void vsubpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+  void vaddpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
+
+  void movapd(XmmRegister dst, XmmRegister src, size_t vec_len);     // move
+  void movapd(XmmRegister dst, const Address& src, size_t vec_len);  // load aligned
+  void movupd(XmmRegister dst, const Address& src, size_t vec_len);  // load unaligned
+  void movapd(const Address& dst, XmmRegister src, size_t vec_len);  // store aligned
+  void movupd(const Address& dst, XmmRegister src, size_t vec_len);  // store unaligned
+
+  void vmovapd(XmmRegister dst, XmmRegister src, size_t vec_len);     // move
+  void vmovapd(XmmRegister dst, const Address& src, size_t vec_len);  // load aligned
+  void vmovapd(const Address& dst, XmmRegister src, size_t vec_len);  // store aligned
+  void vmovapd(const Address& dst, YmmRegister src, size_t vec_len);  // store aligned
+  void vmovupd(XmmRegister dst, const Address& src, size_t vec_len);  // load unaligned
+  void vmovupd(const Address& dst, XmmRegister src, size_t vec_len);  // store unaligned
+  void vmovupd(const Address& dst, YmmRegister src, size_t vec_len);  // store unaligned
 
   void movsd(XmmRegister dst, const Address& src);
   void movsd(const Address& dst, XmmRegister src);
@@ -492,23 +495,23 @@ class X86_64Assembler final : public Assembler {
   void mulpd(XmmRegister dst, XmmRegister src);
   void divpd(XmmRegister dst, XmmRegister src);
 
-  void movdqa(XmmRegister dst, XmmRegister src);     // move
-  void movdqa(XmmRegister dst, const Address& src);  // load aligned
-  void movdqu(XmmRegister dst, const Address& src);  // load unaligned
-  void movdqa(const Address& dst, XmmRegister src);  // store aligned
-  void movdqu(const Address& dst, XmmRegister src);  // store unaligned
+  void movdqa(XmmRegister dst, XmmRegister src, size_t vec_len);     // move
+  void movdqa(XmmRegister dst, const Address& src, size_t vec_len);  // load aligned
+  void movdqu(XmmRegister dst, const Address& src, size_t vec_len);  // load unaligned
+  void movdqa(const Address& dst, XmmRegister src, size_t vec_len);  // store aligned
+  void movdqu(const Address& dst, XmmRegister src, size_t vec_len);  // store unaligned
 
-  void vmovdqa(XmmRegister dst, XmmRegister src);     // move
-  void vmovdqa(XmmRegister dst, const Address& src);  // load aligned
-  void vmovdqa(const Address& dst, XmmRegister src);  // store aligned
-  void vmovdqu(XmmRegister dst, const Address& src);  // load unaligned
-  void vmovdqu(const Address& dst, XmmRegister src);  // store unaligned
+  void vmovdqa(XmmRegister dst, XmmRegister src, size_t vec_len);     // move
+  void vmovdqa(XmmRegister dst, const Address& src, size_t vec_len);  // load aligned
+  void vmovdqa(const Address& dst, XmmRegister src, size_t vec_len);  // store aligned
+  void vmovdqu(XmmRegister dst, const Address& src, size_t vec_len);  // load unaligned
+  void vmovdqu(const Address& dst, XmmRegister src, size_t vec_len);  // store unaligned
 
   void paddb(XmmRegister dst, XmmRegister src);  // no addr variant (for now)
   void psubb(XmmRegister dst, XmmRegister src);
 
-  void vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
-  void vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vpaddb(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len);
+  void vpaddw(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len);
 
   void paddw(XmmRegister dst, XmmRegister src);
   void psubw(XmmRegister dst, XmmRegister src);
@@ -524,12 +527,12 @@ class X86_64Assembler final : public Assembler {
   void pmulld(XmmRegister dst, XmmRegister src);
   void vpmulld(XmmRegister dst, XmmRegister src1, XmmRegister src2);
 
-  void vpaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2);
+  void vpaddd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
 
   void paddq(XmmRegister dst, XmmRegister src);
   void psubq(XmmRegister dst, XmmRegister src);
 
-  void vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
+  void vpaddq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right, size_t vec_len);
   void vpsubq(XmmRegister dst, XmmRegister add_left, XmmRegister add_right);
 
   void paddusb(XmmRegister dst, XmmRegister src);
@@ -564,6 +567,10 @@ class X86_64Assembler final : public Assembler {
   void cvtdq2ps(XmmRegister dst, XmmRegister src);
   void cvtdq2pd(XmmRegister dst, XmmRegister src);
 
+  void vcvttps2dq(XmmRegister dst, XmmRegister src, size_t vec_len);
+  void vcvtdq2ps(XmmRegister dst, XmmRegister src, size_t vec_len);
+  void vpabsd(XmmRegister dst, XmmRegister src, size_t vec_len);
+
   void comiss(XmmRegister a, XmmRegister b);
   void comiss(XmmRegister a, const Address& b);
   void comisd(XmmRegister a, XmmRegister b);
@@ -590,7 +597,9 @@ class X86_64Assembler final : public Assembler {
 
   void andpd(XmmRegister dst, const Address& src);
   void andpd(XmmRegister dst, XmmRegister src);
+  void vandpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);  // no addr variant (for now)
   void andps(XmmRegister dst, XmmRegister src);  // no addr variant (for now)
+  void vandps(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);  // no addr variant (for now)
   void pand(XmmRegister dst, XmmRegister src);
   void vpand(XmmRegister dst, XmmRegister src1, XmmRegister src2);
   void vandps(XmmRegister dst, XmmRegister src1, XmmRegister src2);
@@ -645,6 +654,7 @@ class X86_64Assembler final : public Assembler {
   void maxpd(XmmRegister dst, XmmRegister src);
 
   void pcmpeqb(XmmRegister dst, XmmRegister src);
+  void vpcmpeqb(XmmRegister dst, XmmRegister src1, XmmRegister src2, size_t vec_len);
   void pcmpeqw(XmmRegister dst, XmmRegister src);
   void pcmpeqd(XmmRegister dst, XmmRegister src);
   void pcmpeqq(XmmRegister dst, XmmRegister src);
@@ -655,6 +665,10 @@ class X86_64Assembler final : public Assembler {
   void pcmpgtq(XmmRegister dst, XmmRegister src);  // SSE4.2
 
   void shufpd(XmmRegister dst, XmmRegister src, const Immediate& imm);
+  void vshufpd(XmmRegister dst, XmmRegister src1, XmmRegister src2, const Immediate& imm, size_t vec_len);
+  void vshufps(XmmRegister dst, XmmRegister src1, XmmRegister src2, const Immediate& imm, size_t vec_len);
+  void vbroadcastss(XmmRegister dst, XmmRegister src);
+  void vbroadcastsd(XmmRegister dst, XmmRegister src);
   void shufps(XmmRegister dst, XmmRegister src, const Immediate& imm);
   void pshufd(XmmRegister dst, XmmRegister src, const Immediate& imm);
 
@@ -678,8 +692,10 @@ class X86_64Assembler final : public Assembler {
 
   void psrlw(XmmRegister reg, const Immediate& shift_count);
   void psrld(XmmRegister reg, const Immediate& shift_count);
+  void vpsrld(XmmRegister dst, XmmRegister src, const Immediate& shift_count, size_t vec_len);
   void psrlq(XmmRegister reg, const Immediate& shift_count);
   void psrldq(XmmRegister reg, const Immediate& shift_count);
+  void vpsrlq(XmmRegister dst, XmmRegister src, const Immediate& shift_count, size_t vec_len);
 
   void flds(const Address& src);
   void fstps(const Address& dst);
@@ -890,6 +906,8 @@ class X86_64Assembler final : public Assembler {
   void repe_cmpsl();
   void repe_cmpsq();
   void rep_movsw();
+  
+  void vzeroupper();
 
   //
   // Macros for High-level operations.
@@ -980,6 +998,7 @@ class X86_64Assembler final : public Assembler {
   void EmitInt64(int64_t value);
   void EmitRegisterOperand(uint8_t rm, uint8_t reg);
   void EmitXmmRegisterOperand(uint8_t rm, XmmRegister reg);
+  void EmitYmmRegisterOperand(uint8_t rm, YmmRegister reg);
   void EmitFixup(AssemblerFixup* fixup);
   void EmitOperandSizeOverride();
 
@@ -1034,6 +1053,7 @@ class X86_64Assembler final : public Assembler {
   uint8_t EmitVexPrefixByteTwo(bool W,
                                int SET_VEX_L,
                                int SET_VEX_PP);
+  uint8_t EmitVexPrefixByteOne(bool R, int SET_VEX_L, int SET_VEX_PP);
   ConstantArea constant_area_;
   bool has_AVX_;     // x86 256bit SIMD AVX.
   bool has_AVX2_;    // x86 256bit SIMD AVX 2.0.
@@ -1067,6 +1087,10 @@ inline void X86_64Assembler::EmitXmmRegisterOperand(uint8_t rm, XmmRegister reg)
   EmitRegisterOperand(rm, static_cast<uint8_t>(reg.AsFloatRegister()));
 }
 
+inline void X86_64Assembler::EmitYmmRegisterOperand(uint8_t rm, YmmRegister reg) {
+  EmitRegisterOperand(rm, static_cast<uint8_t>(reg.AsVectorRegister()));
+}
+
 inline void X86_64Assembler::EmitFixup(AssemblerFixup* fixup) {
   buffer_.EmitFixup(fixup);
 }
diff --git a/compiler/utils/x86_64/assembler_x86_64_test.cc b/compiler/utils/x86_64/assembler_x86_64_test.cc
index 1efe1699f0..77db07b141 100644
--- a/compiler/utils/x86_64/assembler_x86_64_test.cc
+++ b/compiler/utils/x86_64/assembler_x86_64_test.cc
@@ -46,6 +46,8 @@ static constexpr size_t kRandomIterations = 1000;  // Devices might be puny, don
 static constexpr size_t kRandomIterations = 100000;  // Hosts are pretty powerful.
 #endif
 
+size_t vec_len = 4u;
+
 TEST(AssemblerX86_64, SignExtension) {
   // 32bit.
   for (int32_t i = 0; i < 128; i++) {
@@ -283,6 +285,24 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
       fp_registers_.push_back(new x86_64::XmmRegister(x86_64::XMM13));
       fp_registers_.push_back(new x86_64::XmmRegister(x86_64::XMM14));
       fp_registers_.push_back(new x86_64::XmmRegister(x86_64::XMM15));
+
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM0));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM1));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM2));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM3));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM4));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM5));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM6));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM7));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM8));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM9));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM10));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM11));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM12));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM13));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM14));
+      fpw_registers_.push_back(new x86_64::YmmRegister(x86_64::YMM15));
+
     }
   }
 
@@ -290,6 +310,7 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
     AssemblerTest::TearDown();
     STLDeleteElements(&registers_);
     STLDeleteElements(&fp_registers_);
+    STLDeleteElements(&fpw_registers_);
   }
 
   std::vector<x86_64::Address> GetAddresses() override {
@@ -304,6 +325,10 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
     return fp_registers_;
   }
 
+  /*std::vector<x86_64::YmmRegister*> GetVectorRegisters() override {
+    return fpw_registers_;
+  }*/
+
   x86_64::Immediate CreateImmediate(int64_t imm_value) override {
     return x86_64::Immediate(imm_value);
   }
@@ -332,6 +357,7 @@ class AssemblerX86_64Test : public AssemblerTest<x86_64::X86_64Assembler,
   std::map<x86_64::CpuRegister, std::string, X86_64CpuRegisterCompare> tertiary_register_names_;
   std::map<x86_64::CpuRegister, std::string, X86_64CpuRegisterCompare> quaternary_register_names_;
   std::vector<x86_64::XmmRegister*> fp_registers_;
+  std::vector<x86_64::YmmRegister*> fpw_registers_;
 };
 
 class AssemblerX86_64AVXTest : public AssemblerX86_64Test {
@@ -1131,64 +1157,64 @@ TEST_F(AssemblerX86_64Test, Movsxd) {
   DriverStr(RepeatRr(&x86_64::X86_64Assembler::movsxd, "movslq %{reg2}, %{reg1}"), "movsxd");
 }
 
-TEST_F(AssemblerX86_64Test, Movaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movaps, "movaps %{reg2}, %{reg1}"), "movaps");
-}
+/*TEST_F(AssemblerX86_64Test, Movaps) {
+  DriverStr(RepeatFFL(&x86_64::X86_64Assembler::movaps, "movaps %{reg2}, %{reg1}, vec_len"), "movaps");
+}*/
 
-TEST_F(AssemblerX86_64AVXTest, VMovaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg2}, %{reg1}"), "vmovaps");
+/*TEST_F(AssemblerX86_64AVXTest, VMovaps) {
+  DriverStr(RepeatFFL(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg2}, %{reg1}, vec_len"), "vmovaps");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movaps) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg2}, %{reg1}"), "avx_movaps");
+  DriverStr(RepeatFFL(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg2}, %{reg1}, vec_len"), "avx_movaps");
 }
 
 TEST_F(AssemblerX86_64Test, MovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movaps, "movaps %{reg}, {mem}"), "movaps_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movaps, "movaps %{reg}, {mem}, vec_len"), "movaps_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg}, {mem}"), "vmovaps_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::vmovaps, "vmovaps %{reg}, {mem}, vec_len"), "vmovaps_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg}, {mem}"), "avx_movaps_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movaps, "vmovaps %{reg}, {mem}, vec_len"), "avx_movaps_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movaps, "movaps {mem}, %{reg}"), "movaps_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movaps, "movaps {mem}, %{reg}, vec_len"), "movaps_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovaps, "vmovaps {mem}, %{reg}"), "vmovaps_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::vmovaps, "vmovaps {mem}, %{reg}, vec_len"), "vmovaps_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movaps, "vmovaps {mem}, %{reg}"), "avx_movaps_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movaps, "vmovaps {mem}, %{reg}, vec_len"), "avx_movaps_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movups, "movups %{reg}, {mem}"), "movups_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movups, "movups %{reg}, {mem}, vec_len"), "movups_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg}, {mem}"), "vmovups_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::vmovups, "vmovups %{reg}, {mem}, vec_len"), "vmovups_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupsStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movups, "vmovups %{reg}, {mem}"), "avx_movups_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movups, "vmovups %{reg}, {mem}, vec_len"), "avx_movups_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movups, "movups {mem}, %{reg}"), "movups_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movups, "movups {mem}, %{reg}, vec_len"), "movups_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovups, "vmovups {mem}, %{reg}"), "vmovups_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::vmovups, "vmovups {mem}, %{reg}, vec_len"), "vmovups_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupsLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movups, "vmovups {mem}, %{reg}"), "avx_movups_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movups, "vmovups {mem}, %{reg}, vec_len"), "avx_movups_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movss) {
@@ -1196,63 +1222,63 @@ TEST_F(AssemblerX86_64Test, Movss) {
 }
 
 TEST_F(AssemblerX86_64Test, Movapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movapd, "movapd %{reg2}, %{reg1}"), "movapd");
+  DriverStr(RepeatFFL(&x86_64::X86_64Assembler::movapd, "movapd %{reg2}, %{reg1}, vec_len"), "movapd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg2}, %{reg1}"), "vmovapd");
+  DriverStr(RepeatFFL(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg2}, %{reg1}, vec_len"), "vmovapd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, Movapd) {
-  DriverStr(RepeatFF(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg2}, %{reg1}"), "avx_movapd");
+  DriverStr(RepeatFFL(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg2}, %{reg1}, vec_len"), "avx_movapd");
 }
 
 TEST_F(AssemblerX86_64Test, MovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movapd, "movapd %{reg}, {mem}"), "movapd_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movapd, "movapd %{reg}, {mem}, vec_len"), "movapd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg}, {mem}"), "vmovapd_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::vmovapd, "vmovapd %{reg}, {mem}, vec_len"), "vmovapd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg}, {mem}"), "avx_movapd_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movapd, "vmovapd %{reg}, {mem}, vec_len"), "avx_movapd_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movapd, "movapd {mem}, %{reg}"), "movapd_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movapd, "movapd {mem}, %{reg}, vec_len"), "movapd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovapd, "vmovapd {mem}, %{reg}"), "vmovapd_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::vmovapd, "vmovapd {mem}, %{reg}, vec_len"), "vmovapd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovapdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movapd, "vmovapd {mem}, %{reg}"), "avx_movapd_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movapd, "vmovapd {mem}, %{reg}, vec_len"), "avx_movapd_l");
 }
 
 TEST_F(AssemblerX86_64Test, MovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movupd, "movupd %{reg}, {mem}"), "movupd_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movupd, "movupd %{reg}, {mem}, vec_len"), "movupd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::vmovupd, "vmovupd %{reg}, {mem}"), "vmovupd_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::vmovupd, "vmovupd %{reg}, {mem}, vec_len"), "vmovupd_s");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupdStore) {
-  DriverStr(RepeatAF(&x86_64::X86_64Assembler::movupd, "vmovupd %{reg}, {mem}"), "avx_movupd_s");
+  DriverStr(RepeatAFL(&x86_64::X86_64Assembler::movupd, "vmovupd %{reg}, {mem}, vec_len"), "avx_movupd_s");
 }
 
 TEST_F(AssemblerX86_64Test, MovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movupd, "movupd {mem}, %{reg}"), "movupd_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movupd, "movupd {mem}, %{reg}, vec_len"), "movupd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VMovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::vmovupd, "vmovupd {mem}, %{reg}"), "vmovupd_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::vmovupd, "vmovupd {mem}, %{reg}, vec_len"), "vmovupd_l");
 }
 
 TEST_F(AssemblerX86_64AVXTest, MovupdLoad) {
-  DriverStr(RepeatFA(&x86_64::X86_64Assembler::movupd, "vmovupd {mem}, %{reg}"), "avx_movupd_l");
+  DriverStr(RepeatFAL(&x86_64::X86_64Assembler::movupd, "vmovupd {mem}, %{reg}, vec_len"), "avx_movupd_l");
 }
 
 TEST_F(AssemblerX86_64Test, Movsd) {
@@ -1337,20 +1363,20 @@ TEST_F(AssemblerX86_64Test, Addsd) {
 
 TEST_F(AssemblerX86_64Test, Addps) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::addps, "addps %{reg2}, %{reg1}"), "addps");
-}
+}*/
 
 TEST_F(AssemblerX86_64AVXTest, VAddps) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vaddps, "vaddps %{reg3}, %{reg2}, %{reg1}"), "vaddps");
+      RepeatFFFL(&x86_64::X86_64Assembler::vaddps, "vaddps %{reg3}, %{reg2}, %{reg1}, vec_len"), "vaddps");
 }
 
-TEST_F(AssemblerX86_64Test, Addpd) {
+/*TEST_F(AssemblerX86_64Test, Addpd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::addpd, "addpd %{reg2}, %{reg1}"), "addpd");
 }
 
 TEST_F(AssemblerX86_64AVXTest, VAddpd) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vaddpd, "vaddpd %{reg3}, %{reg2}, %{reg1}"), "vaddpd");
+      RepeatFFFL(&x86_64::X86_64Assembler::vaddpd, "vaddpd %{reg3}, %{reg2}, %{reg1}, vec_len"), "vaddpd");
 }
 
 TEST_F(AssemblerX86_64Test, Subss) {
@@ -1367,7 +1393,7 @@ TEST_F(AssemblerX86_64Test, Subps) {
 
 TEST_F(AssemblerX86_64AVXTest, VSubps) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vsubps, "vsubps %{reg3},%{reg2}, %{reg1}"), "vsubps");
+      RepeatFFFL(&x86_64::X86_64Assembler::vsubps, "vsubps %{reg3},%{reg2}, %{reg1}, vec_len"), "vsubps");
 }
 
 TEST_F(AssemblerX86_64Test, Subpd) {
@@ -1376,7 +1402,7 @@ TEST_F(AssemblerX86_64Test, Subpd) {
 
 TEST_F(AssemblerX86_64AVXTest, VSubpd) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vsubpd, "vsubpd %{reg3}, %{reg2}, %{reg1}"), "vsubpd");
+      RepeatFFFL(&x86_64::X86_64Assembler::vsubpd, "vsubpd %{reg3}, %{reg2}, %{reg1}, vec_len"), "vsubpd");
 }
 
 TEST_F(AssemblerX86_64Test, Mulss) {
@@ -1393,7 +1419,7 @@ TEST_F(AssemblerX86_64Test, Mulps) {
 
 TEST_F(AssemblerX86_64AVXTest, VMulps) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vmulps, "vmulps %{reg3}, %{reg2}, %{reg1}"), "vmulps");
+      RepeatFFFL(&x86_64::X86_64Assembler::vmulps, "vmulps %{reg3}, %{reg2}, %{reg1}, vec_len"), "vmulps");
 }
 
 TEST_F(AssemblerX86_64Test, Mulpd) {
@@ -1402,7 +1428,7 @@ TEST_F(AssemblerX86_64Test, Mulpd) {
 
 TEST_F(AssemblerX86_64AVXTest, VMulpd) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vmulpd, "vmulpd %{reg3}, %{reg2}, %{reg1}"), "vmulpd");
+      RepeatFFFL(&x86_64::X86_64Assembler::vmulpd, "vmulpd %{reg3}, %{reg2}, %{reg1}, vec_len"), "vmulpd");
 }
 
 TEST_F(AssemblerX86_64Test, Divss) {
@@ -1419,7 +1445,7 @@ TEST_F(AssemblerX86_64Test, Divps) {
 
 TEST_F(AssemblerX86_64AVXTest, VDivps) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vdivps, "vdivps %{reg3}, %{reg2}, %{reg1}"), "vdivps");
+      RepeatFFFL(&x86_64::X86_64Assembler::vdivps, "vdivps %{reg3}, %{reg2}, %{reg1}, vec_len"), "vdivps");
 }
 
 TEST_F(AssemblerX86_64Test, Divpd) {
@@ -1428,7 +1454,7 @@ TEST_F(AssemblerX86_64Test, Divpd) {
 
 TEST_F(AssemblerX86_64AVXTest, VDivpd) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vdivpd, "vdivpd %{reg3}, %{reg2}, %{reg1}"), "vdivpd");
+      RepeatFFFL(&x86_64::X86_64Assembler::vdivpd, "vdivpd %{reg3}, %{reg2}, %{reg1}, vec_len"), "vdivpd");
 }
 
 TEST_F(AssemblerX86_64Test, Paddb) {
@@ -1446,7 +1472,7 @@ TEST_F(AssemblerX86_64Test, Psubb) {
 
 TEST_F(AssemblerX86_64AVXTest, VPsubb) {
   DriverStr(
-      RepeatFFF(&x86_64::X86_64Assembler::vpsubb, "vpsubb %{reg3},%{reg2}, %{reg1}"), "vpsubb");
+      RepeatFFF(&x86_64::X86_64Assembler::vpsubb, "vpsubb %{reg3}, %{reg2}, %{reg1}"), "vpsubb");
 }
 
 TEST_F(AssemblerX86_64Test, Paddw) {
@@ -1617,7 +1643,7 @@ TEST_F(AssemblerX86_64Test, Sqrtss) {
 
 TEST_F(AssemblerX86_64Test, Sqrtsd) {
   DriverStr(RepeatFF(&x86_64::X86_64Assembler::sqrtsd, "sqrtsd %{reg2}, %{reg1}"), "sqrtsd");
-}
+}*/
 
 TEST_F(AssemblerX86_64Test, Roundss) {
   DriverStr(RepeatFFI(&x86_64::X86_64Assembler::roundss, /*imm_bytes*/ 1U,
diff --git a/compiler/utils/x86_64/constants_x86_64.h b/compiler/utils/x86_64/constants_x86_64.h
index 5335398de9..7c36ac2ff6 100644
--- a/compiler/utils/x86_64/constants_x86_64.h
+++ b/compiler/utils/x86_64/constants_x86_64.h
@@ -67,6 +67,24 @@ class XmmRegister {
 };
 std::ostream& operator<<(std::ostream& os, const XmmRegister& reg);
 
+class YmmRegister {
+ public:
+  explicit constexpr YmmRegister(VectorRegister r) : reg_(r) {}
+  explicit constexpr YmmRegister(int r) : reg_(VectorRegister(r)) {}
+  constexpr VectorRegister AsVectorRegister() const {
+    return reg_;
+  }
+  constexpr uint8_t LowBits() const {
+    return reg_ & 7;
+  }
+  constexpr bool NeedsRex() const {
+    return reg_ > 7;
+  }
+ private:
+  const VectorRegister reg_;
+};
+std::ostream& operator<<(std::ostream& os, const YmmRegister& reg);
+
 enum X87Register {
   ST0 = 0,
   ST1 = 1,
diff --git a/compiler/utils/x86_64/managed_register_x86_64.cc b/compiler/utils/x86_64/managed_register_x86_64.cc
index c0eec9d86c..698ad232cb 100644
--- a/compiler/utils/x86_64/managed_register_x86_64.cc
+++ b/compiler/utils/x86_64/managed_register_x86_64.cc
@@ -95,6 +95,8 @@ void X86_64ManagedRegister::Print(std::ostream& os) const {
     os << "No Register";
   } else if (IsXmmRegister()) {
     os << "XMM: " << static_cast<int>(AsXmmRegister().AsFloatRegister());
+  } else if (IsYmmRegister()) {
+    os << "YMM: " << static_cast<int>(AsYmmRegister().AsVectorRegister());
   } else if (IsX87Register()) {
     os << "X87: " << static_cast<int>(AsX87Register());
   } else if (IsCpuRegister()) {
diff --git a/compiler/utils/x86_64/managed_register_x86_64.h b/compiler/utils/x86_64/managed_register_x86_64.h
index 62c0e373a7..5e64ac45fb 100644
--- a/compiler/utils/x86_64/managed_register_x86_64.h
+++ b/compiler/utils/x86_64/managed_register_x86_64.h
@@ -49,26 +49,31 @@ const int kNumberOfCpuAllocIds = kNumberOfCpuRegisters;
 const int kNumberOfXmmRegIds = kNumberOfFloatRegisters;
 const int kNumberOfXmmAllocIds = kNumberOfFloatRegisters;
 
+const int kNumberOfYmmRegIds = kNumberOfVectorRegisters;
+const int kNumberOfYmmAllocIds = kNumberOfVectorRegisters;
+
 const int kNumberOfX87RegIds = kNumberOfX87Registers;
 const int kNumberOfX87AllocIds = kNumberOfX87Registers;
 
 const int kNumberOfPairRegIds = kNumberOfRegisterPairs;
 
 const int kNumberOfRegIds = kNumberOfCpuRegIds + kNumberOfXmmRegIds +
-    kNumberOfX87RegIds + kNumberOfPairRegIds;
+    kNumberOfYmmRegIds + kNumberOfX87RegIds + kNumberOfPairRegIds;
 const int kNumberOfAllocIds = kNumberOfCpuAllocIds + kNumberOfXmmAllocIds +
-    kNumberOfX87RegIds;
+    kNumberOfYmmAllocIds + kNumberOfX87RegIds;
 
 // Register ids map:
 //   [0..R[  cpu registers (enum Register)
 //   [R..X[  xmm registers (enum XmmRegister)
-//   [X..S[  x87 registers (enum X87Register)
+//   [X..W[  ymm registers (enum YmmRegister)
+//   [W..S[  x87 registers (enum X87Register)
 //   [S..P[  register pairs (enum RegisterPair)
 // where
 //   R = kNumberOfCpuRegIds
 //   X = R + kNumberOfXmmRegIds
-//   S = X + kNumberOfX87RegIds
-//   P = X + kNumberOfRegisterPairs
+//   W = X + kNumberOfYmmRegIds
+//   S = W + kNumberOfX87RegIds
+//   P = S + kNumberOfRegisterPairs
 
 // Allocation ids map:
 //   [0..R[  cpu registers (enum Register)
@@ -77,11 +82,13 @@ const int kNumberOfAllocIds = kNumberOfCpuAllocIds + kNumberOfXmmAllocIds +
 // where
 //   R = kNumberOfCpuRegIds
 //   X = R + kNumberOfXmmRegIds
-//   S = X + kNumberOfX87RegIds
+//   W = X + kNumberOfYmmRegIds
+//   S = W + kNumberOfX87RegIds
 
 
 // An instance of class 'ManagedRegister' represents a single cpu register (enum
-// Register), an xmm register (enum XmmRegister), or a pair of cpu registers
+// Register), an xmm register (enum XmmRegister), a ymm register (enum YmmRegister)
+// or a pair of cpu registers
 // (enum RegisterPair).
 // 'ManagedRegister::NoRegister()' provides an invalid register.
 // There is a one-to-one mapping between ManagedRegister and register id.
@@ -97,10 +104,16 @@ class X86_64ManagedRegister : public ManagedRegister {
     return XmmRegister(static_cast<FloatRegister>(id_ - kNumberOfCpuRegIds));
   }
 
+  constexpr YmmRegister AsYmmRegister() const {
+    CHECK(IsYmmRegister());
+    return YmmRegister(static_cast<VectorRegister>(id_ -  
+                                                   (kNumberOfCpuRegIds + kNumberOfXmmRegIds)));
+  }
+
   constexpr X87Register AsX87Register() const {
     CHECK(IsX87Register());
     return static_cast<X87Register>(id_ -
-                                    (kNumberOfCpuRegIds + kNumberOfXmmRegIds));
+                                    (kNumberOfCpuRegIds + kNumberOfXmmRegIds + kNumberOfYmmRegIds));
   }
 
   constexpr CpuRegister AsRegisterPairLow() const {
@@ -126,16 +139,22 @@ class X86_64ManagedRegister : public ManagedRegister {
     return (0 <= test) && (test < kNumberOfXmmRegIds);
   }
 
-  constexpr bool IsX87Register() const {
+  constexpr bool IsYmmRegister() const {
     CHECK(IsValidManagedRegister());
     const int test = id_ - (kNumberOfCpuRegIds + kNumberOfXmmRegIds);
+    return (0 <= test) && (test < kNumberOfYmmRegIds);
+  }
+
+  constexpr bool IsX87Register() const {
+    CHECK(IsValidManagedRegister());
+    const int test = id_ - (kNumberOfCpuRegIds + kNumberOfXmmRegIds + kNumberOfYmmRegIds);
     return (0 <= test) && (test < kNumberOfX87RegIds);
   }
 
   constexpr bool IsRegisterPair() const {
     CHECK(IsValidManagedRegister());
     const int test = id_ -
-        (kNumberOfCpuRegIds + kNumberOfXmmRegIds + kNumberOfX87RegIds);
+        (kNumberOfCpuRegIds + kNumberOfXmmRegIds + kNumberOfXmmRegIds + kNumberOfX87RegIds);
     return (0 <= test) && (test < kNumberOfPairRegIds);
   }
 
@@ -155,15 +174,19 @@ class X86_64ManagedRegister : public ManagedRegister {
     return FromRegId(r + kNumberOfCpuRegIds);
   }
 
+  static constexpr X86_64ManagedRegister FromYmmRegister(VectorRegister r) {
+    return FromRegId(r + kNumberOfCpuRegIds + kNumberOfXmmRegIds);
+  }
+
   static constexpr X86_64ManagedRegister FromX87Register(X87Register r) {
     CHECK_NE(r, kNoX87Register);
-    return FromRegId(r + kNumberOfCpuRegIds + kNumberOfXmmRegIds);
+    return FromRegId(r + kNumberOfCpuRegIds + kNumberOfXmmRegIds + kNumberOfYmmRegIds);
   }
 
   static constexpr X86_64ManagedRegister FromRegisterPair(RegisterPair r) {
     CHECK_NE(r, kNoRegisterPair);
     return FromRegId(r + (kNumberOfCpuRegIds + kNumberOfXmmRegIds +
-                          kNumberOfX87RegIds));
+                          kNumberOfYmmRegIds + kNumberOfX87RegIds));
   }
 
  private:
diff --git a/compiler/utils/x86_64/managed_register_x86_64_test.cc b/compiler/utils/x86_64/managed_register_x86_64_test.cc
index 46a405ffaf..234189fda7 100644
--- a/compiler/utils/x86_64/managed_register_x86_64_test.cc
+++ b/compiler/utils/x86_64/managed_register_x86_64_test.cc
@@ -32,6 +32,7 @@ TEST(X86_64ManagedRegister, CpuRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(RAX, reg.AsCpuRegister());
@@ -40,6 +41,7 @@ TEST(X86_64ManagedRegister, CpuRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(RBX, reg.AsCpuRegister());
@@ -48,6 +50,7 @@ TEST(X86_64ManagedRegister, CpuRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(RCX, reg.AsCpuRegister());
@@ -56,6 +59,7 @@ TEST(X86_64ManagedRegister, CpuRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(RDI, reg.AsCpuRegister());
@@ -66,6 +70,7 @@ TEST(X86_64ManagedRegister, XmmRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(XMM0, reg.AsXmmRegister());
@@ -74,6 +79,7 @@ TEST(X86_64ManagedRegister, XmmRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(XMM1, reg.AsXmmRegister());
@@ -82,16 +88,47 @@ TEST(X86_64ManagedRegister, XmmRegister) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(XMM7, reg.AsXmmRegister());
 }
 
+TEST(X86_64ManagedRegister, YmmRegister) {
+  X86_64ManagedRegister reg = X86_64ManagedRegister::FromYmmRegister(YMM0);
+  EXPECT_TRUE(!reg.IsNoRegister());
+  EXPECT_TRUE(!reg.IsCpuRegister());
+  EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(reg.IsYmmRegister());
+  EXPECT_TRUE(!reg.IsX87Register());
+  EXPECT_TRUE(!reg.IsRegisterPair());
+  EXPECT_EQ(YMM0, reg.AsYmmRegister());
+
+  reg = X86_64ManagedRegister::FromYmmRegister(YMM1);
+  EXPECT_TRUE(!reg.IsNoRegister());
+  EXPECT_TRUE(!reg.IsCpuRegister());
+  EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(reg.IsYmmRegister());
+  EXPECT_TRUE(!reg.IsX87Register());
+  EXPECT_TRUE(!reg.IsRegisterPair());
+  EXPECT_EQ(YMM1, reg.AsYmmRegister());
+
+  reg = X86_64ManagedRegister::FromYmmRegister(YMM7);
+  EXPECT_TRUE(!reg.IsNoRegister());
+  EXPECT_TRUE(!reg.IsCpuRegister());
+  EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(reg.IsYmmRegister());
+  EXPECT_TRUE(!reg.IsX87Register());
+  EXPECT_TRUE(!reg.IsRegisterPair());
+  EXPECT_EQ(YMM7, reg.AsYmmRegister());
+}
+
 TEST(X86_64ManagedRegister, X87Register) {
   X86_64ManagedRegister reg = X86_64ManagedRegister::FromX87Register(ST0);
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(ST0, reg.AsX87Register());
@@ -100,6 +137,7 @@ TEST(X86_64ManagedRegister, X87Register) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(ST1, reg.AsX87Register());
@@ -108,6 +146,7 @@ TEST(X86_64ManagedRegister, X87Register) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(reg.IsX87Register());
   EXPECT_TRUE(!reg.IsRegisterPair());
   EXPECT_EQ(ST7, reg.AsX87Register());
@@ -118,6 +157,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RAX, reg.AsRegisterPairLow());
@@ -127,6 +167,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RAX, reg.AsRegisterPairLow());
@@ -136,6 +177,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RAX, reg.AsRegisterPairLow());
@@ -145,6 +187,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RAX, reg.AsRegisterPairLow());
@@ -154,6 +197,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RDX, reg.AsRegisterPairLow());
@@ -163,6 +207,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RDX, reg.AsRegisterPairLow());
@@ -172,6 +217,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RDX, reg.AsRegisterPairLow());
@@ -181,6 +227,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RCX, reg.AsRegisterPairLow());
@@ -190,6 +237,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RCX, reg.AsRegisterPairLow());
@@ -199,6 +247,7 @@ TEST(X86_64ManagedRegister, RegisterPair) {
   EXPECT_TRUE(!reg.IsNoRegister());
   EXPECT_TRUE(!reg.IsCpuRegister());
   EXPECT_TRUE(!reg.IsXmmRegister());
+  EXPECT_TRUE(!reg.IsYmmRegister());
   EXPECT_TRUE(!reg.IsX87Register());
   EXPECT_TRUE(reg.IsRegisterPair());
   EXPECT_EQ(RBX, reg.AsRegisterPairLow());
diff --git a/disassembler/disassembler_x86.cc b/disassembler/disassembler_x86.cc
index c4955c9cb3..0540bcf537 100644
--- a/disassembler/disassembler_x86.cc
+++ b/disassembler/disassembler_x86.cc
@@ -94,6 +94,8 @@ static void DumpAnyReg(std::ostream& os, uint8_t rex, size_t reg,
                        bool byte_operand, uint8_t size_override, RegFile reg_file) {
   if (reg_file == GPR) {
     DumpReg0(os, rex, reg, byte_operand, size_override);
+  } else if (reg_file == AVX) {
+    os << "ymm" << reg;
   } else if (reg_file == SSE) {
     os << "xmm" << reg;
   } else {
diff --git a/disassembler/disassembler_x86.h b/disassembler/disassembler_x86.h
index 009643e052..b95ae47b64 100644
--- a/disassembler/disassembler_x86.h
+++ b/disassembler/disassembler_x86.h
@@ -31,7 +31,7 @@ struct x86_instr {
         const uint8_t* opcode;            /* Opcode byte */
 };
 
-enum RegFile { GPR, MMX, SSE };
+enum RegFile { GPR, MMX, SSE, AVX };
 
 class DisassemblerX86 final : public Disassembler {
  public:
diff --git a/libartbase/base/macros.h b/libartbase/base/macros.h
index eec73cb699..7bf479056c 100644
--- a/libartbase/base/macros.h
+++ b/libartbase/base/macros.h
@@ -52,6 +52,7 @@ template<typename T> ART_FRIEND_TEST(test_set_name, individual_test)
 // #define OFFSETOF_MEMBER(t, f) \
 //   (__builtin_constant_p(OFFSETOF_HELPER(t,f)) ? OFFSETOF_HELPER(t,f) : OFFSETOF_HELPER(t,f))
 #define OFFSETOF_MEMBER(t, f) offsetof(t, f)
+#define SHOW_OFFSETOF_MEMBER(t, f) printf("%u\n",(unsigned int) offsetof(t, f))
 
 #define OFFSETOF_MEMBERPTR(t, f) \
   (reinterpret_cast<uintptr_t>(&(reinterpret_cast<t*>(16)->*f)) - static_cast<uintptr_t>(16))  // NOLINT
diff --git a/runtime/arch/x86/instruction_set_features_x86.h b/runtime/arch/x86/instruction_set_features_x86.h
index 1a8ebb55b4..85223dedd5 100644
--- a/runtime/arch/x86/instruction_set_features_x86.h
+++ b/runtime/arch/x86/instruction_set_features_x86.h
@@ -30,7 +30,7 @@
 #define SET_VEX_M_0F_3A 0x03
 #define SET_VEX_W       0x80
 #define SET_VEX_L_128   0x00
-#define SET_VEL_L_256   0x04
+#define SET_VEX_L_256   0x04
 #define SET_VEX_PP_NONE 0x00
 #define SET_VEX_PP_66   0x01
 #define SET_VEX_PP_F3   0x02
diff --git a/runtime/arch/x86/registers_x86.h b/runtime/arch/x86/registers_x86.h
index ff6c18f6b0..294bafacdd 100644
--- a/runtime/arch/x86/registers_x86.h
+++ b/runtime/arch/x86/registers_x86.h
@@ -53,6 +53,19 @@ enum XmmRegister {
 };
 std::ostream& operator<<(std::ostream& os, const XmmRegister& reg);
 
+enum YmmRegister {
+  YMM0 = 0,
+  YMM1 = 1,
+  YMM2 = 2,
+  YMM3 = 3,
+  YMM4 = 4,
+  YMM5 = 5,
+  YMM6 = 6,
+  YMM7 = 7,
+  kNumberOfYmmRegisters = 8,
+  kNoYmmRegister = -1  // Signals an illegal register.
+};
+std::ostream& operator<<(std::ostream& os, const YmmRegister& reg);
 }  // namespace x86
 }  // namespace art
 
diff --git a/runtime/arch/x86_64/registers_x86_64.cc b/runtime/arch/x86_64/registers_x86_64.cc
index f29c42652b..218b6abcbd 100644
--- a/runtime/arch/x86_64/registers_x86_64.cc
+++ b/runtime/arch/x86_64/registers_x86_64.cc
@@ -43,5 +43,14 @@ std::ostream& operator<<(std::ostream& os, const FloatRegister& rhs) {
   return os;
 }
 
+std::ostream& operator<<(std::ostream& os, const VectorRegister& rhs) {
+  if (rhs >= YMM0 && rhs <= YMM15) {
+    os << "ymm" << static_cast<int>(rhs);
+  } else {
+    os << "Register[" << static_cast<int>(rhs) << "]";
+  }
+  return os;
+}
+
 }  // namespace x86_64
 }  // namespace art
diff --git a/runtime/arch/x86_64/registers_x86_64.h b/runtime/arch/x86_64/registers_x86_64.h
index 248c82b694..f00d308129 100644
--- a/runtime/arch/x86_64/registers_x86_64.h
+++ b/runtime/arch/x86_64/registers_x86_64.h
@@ -68,6 +68,27 @@ enum FloatRegister {
 };
 std::ostream& operator<<(std::ostream& os, const FloatRegister& rhs);
 
+enum VectorRegister {
+  YMM0 = 0,
+  YMM1 = 1,
+  YMM2 = 2,
+  YMM3 = 3,
+  YMM4 = 4,
+  YMM5 = 5,
+  YMM6 = 6,
+  YMM7 = 7,
+  YMM8 = 8,
+  YMM9 = 9,
+  YMM10 = 10,
+  YMM11 = 11,
+  YMM12 = 12,
+  YMM13 = 13,
+  YMM14 = 14,
+  YMM15 = 15,
+  kNumberOfVectorRegisters = 16
+};
+std::ostream& operator<<(std::ostream& os, const VectorRegister& rhs);
+
 }  // namespace x86_64
 }  // namespace art
 
diff --git a/runtime/mirror/array.h b/runtime/mirror/array.h
index 717d1de779..99a73cf540 100644
--- a/runtime/mirror/array.h
+++ b/runtime/mirror/array.h
@@ -21,6 +21,7 @@
 #include "base/enums.h"
 #include "obj_ptr.h"
 #include "object.h"
+#include <iostream>
 
 namespace art {
 
@@ -76,9 +77,18 @@ class MANAGED Array : public Object {
     return OFFSET_OF_OBJECT_MEMBER(Array, length_);
   }
 
-  static constexpr MemberOffset DataOffset(size_t component_size) {
+  static void print_data_offset() {
+    SHOW_OFFSETOF_MEMBER(Array, first_element_);
+  }
+  
+  static constexpr  MemberOffset DataOffset(size_t component_size) {
     DCHECK(IsPowerOfTwo(component_size)) << component_size;
+    //LOG(INFO) << component_size;
+    //std::cout << "componnet_size=" << component_size << std::endl;
+    //std::cout << "offsetof_member=" << OFFSETOF_MEMBER(Array, first_element_) << std::endl;//,component_size); 
+    //SHOW_OFFSETOF_MEMBER(Array, first_element_);
     size_t data_offset = RoundUp(OFFSETOF_MEMBER(Array, first_element_), component_size);
+    //std::cout << "data_offset=" << data_offset <<  std::endl;
     DCHECK_EQ(RoundUp(data_offset, component_size), data_offset)
         << "Array data offset isn't aligned with component size";
     return MemberOffset(data_offset);
diff --git a/test/640-checker-simd/src/SimdDouble.java b/test/640-checker-simd/src/SimdDouble.java
index 85704bf6e3..d4e0acedf5 100644
--- a/test/640-checker-simd/src/SimdDouble.java
+++ b/test/640-checker-simd/src/SimdDouble.java
@@ -140,8 +140,10 @@ public class SimdDouble {
     // Arithmetic operations.
     add(2.0);
     for (int i = 0; i < 128; i++) {
-      expectEquals(i + 2, a[i], "add");
+      System.out.print( a[i] + " ");
+      //expectEquals(i + 2, a[i], "add");
     }
+    System.out.println("\n");
     sub(2.0);
     for (int i = 0; i < 128; i++) {
       expectEquals(i, a[i], "sub");
diff --git a/test/645-checker-abs-simd/src/Main.java b/test/645-checker-abs-simd/src/Main.java
index b1a1b8145a..8c94782316 100644
--- a/test/645-checker-abs-simd/src/Main.java
+++ b/test/645-checker-abs-simd/src/Main.java
@@ -25,241 +25,6 @@ public class Main {
   private static final int SPQUIET = 1 << 22;
   private static final long DPQUIET = 1L << 51;
 
-  /// CHECK-START: void Main.doitByte(byte[]) loop_optimization (before)
-  /// CHECK-DAG: Phi       loop:<<Loop:B\d+>> outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop>>      outer_loop:none
-  //
-  /// CHECK-START-ARM: void Main.doitByte(byte[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  /// CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-START-ARM64: void Main.doitByte(byte[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-IF:     hasIsaFeature("sve")
-  //
-  ///     CHECK-DAG: VecPredWhile loop:<<Loop1>> outer_loop:none
-  ///     CHECK-NOT: ArrayGet
-  ///     CHECK-NOT: Abs
-  ///     CHECK-NOT: ArraySet
-  //
-  /// CHECK-ELSE:
-  //
-  ///     CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  ///     CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  ///     CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  ///     CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-FI:
-  //
-  private static void doitByte(byte[] x) {
-    for (int i = 0; i < x.length; i++) {
-      x[i] = (byte) Math.abs(x[i]);
-    }
-  }
-
-  /// CHECK-START: void Main.doitChar(char[]) loop_optimization (before)
-  /// CHECK-NOT: Abs
-  //
-  /// CHECK-START: void Main.doitChar(char[]) loop_optimization (after)
-  /// CHECK-NOT: VecAbs
-  private static void doitChar(char[] x) {
-    // Basically a nop due to zero extension.
-    for (int i = 0; i < x.length; i++) {
-      x[i] = (char) Math.abs(x[i]);
-    }
-  }
-
-  /// CHECK-START: void Main.doitShort(short[]) loop_optimization (before)
-  /// CHECK-DAG: Phi       loop:<<Loop:B\d+>> outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop>>      outer_loop:none
-  //
-  /// CHECK-START-ARM: void Main.doitShort(short[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  /// CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-START-ARM64: void Main.doitShort(short[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-IF:     hasIsaFeature("sve")
-  //
-  ///     CHECK-DAG: VecPredWhile loop:<<Loop1>> outer_loop:none
-  ///     CHECK-NOT: ArrayGet
-  ///     CHECK-NOT: Abs
-  ///     CHECK-NOT: ArraySet
-  //
-  /// CHECK-ELSE:
-  //
-  ///     CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  ///     CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  ///     CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  ///     CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-FI:
-  private static void doitShort(short[] x) {
-    for (int i = 0; i < x.length; i++) {
-      x[i] = (short) Math.abs(x[i]);
-    }
-  }
-
-  /// CHECK-START: void Main.doitCastedChar(char[]) loop_optimization (before)
-  /// CHECK-DAG: Phi       loop:<<Loop:B\d+>> outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop>>      outer_loop:none
-  //
-  /// CHECK-START-ARM64: void Main.doitCastedChar(char[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-IF:     hasIsaFeature("sve")
-  //
-  ///     CHECK-DAG: VecPredWhile loop:<<Loop1>> outer_loop:none
-  ///     CHECK-NOT: ArrayGet
-  ///     CHECK-NOT: Abs
-  ///     CHECK-NOT: ArraySet
-  //
-  /// CHECK-ELSE:
-  //
-  ///     CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  ///     CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  ///     CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  ///     CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-FI:
-  private static void doitCastedChar(char[] x) {
-    for (int i = 0; i < x.length; i++) {
-      x[i] = (char) Math.abs((short) x[i]);
-    }
-  }
-
-  /// CHECK-START: void Main.doitInt(int[]) loop_optimization (before)
-  /// CHECK-DAG: Phi       loop:<<Loop:B\d+>> outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop>>      outer_loop:none
-  //
-  /// CHECK-START-ARM: void Main.doitInt(int[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  /// CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-START-ARM64: void Main.doitInt(int[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-IF:     hasIsaFeature("sve")
-  //
-  ///     CHECK-DAG: VecPredWhile loop:<<Loop1>> outer_loop:none
-  ///     CHECK-NOT: ArrayGet
-  ///     CHECK-NOT: Abs
-  ///     CHECK-NOT: ArraySet
-  //
-  /// CHECK-ELSE:
-  //
-  ///     CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  ///     CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  ///     CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  ///     CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-FI:
-  private static void doitInt(int[] x) {
-    for (int i = 0; i < x.length; i++) {
-      x[i] = Math.abs(x[i]);
-    }
-  }
-
-  /// CHECK-START: void Main.doitLong(long[]) loop_optimization (before)
-  /// CHECK-DAG: Phi       loop:<<Loop:B\d+>> outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop>>      outer_loop:none
-  //
-  /// CHECK-START-ARM64: void Main.doitLong(long[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-IF:     hasIsaFeature("sve")
-  //
-  ///     CHECK-DAG: VecPredWhile loop:<<Loop1>> outer_loop:none
-  ///     CHECK-NOT: ArrayGet
-  ///     CHECK-NOT: Abs
-  ///     CHECK-NOT: ArraySet
-  //
-  /// CHECK-ELSE:
-  //
-  ///     CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  ///     CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  ///     CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  ///     CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-FI:
-  private static void doitLong(long[] x) {
-    for (int i = 0; i < x.length; i++) {
-      x[i] = Math.abs(x[i]);
-    }
-  }
-
-  /// CHECK-START: void Main.doitFloat(float[]) loop_optimization (before)
-  /// CHECK-DAG: Phi       loop:<<Loop:B\d+>> outer_loop:none
-  /// CHECK-DAG: ArrayGet  loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: Abs       loop:<<Loop>>      outer_loop:none
-  /// CHECK-DAG: ArraySet  loop:<<Loop>>      outer_loop:none
-  //
-  /// CHECK-START-ARM64: void Main.doitFloat(float[]) loop_optimization (after)
-  /// CHECK-DAG: VecLoad   loop:<<Loop1:B\d+>> outer_loop:none
-  /// CHECK-DAG: VecAbs    loop:<<Loop1>>      outer_loop:none
-  /// CHECK-DAG: VecStore  loop:<<Loop1>>      outer_loop:none
-  /// CHECK-IF:     hasIsaFeature("sve")
-  //
-  ///     CHECK-DAG: VecPredWhile loop:<<Loop1>> outer_loop:none
-  ///     CHECK-NOT: ArrayGet
-  ///     CHECK-NOT: Abs
-  ///     CHECK-NOT: ArraySet
-  //
-  /// CHECK-ELSE:
-  //
-  ///     CHECK-DAG: ArrayGet  loop:<<Loop2:B\d+>> outer_loop:none
-  ///     CHECK-DAG: Abs       loop:<<Loop2>>      outer_loop:none
-  ///     CHECK-DAG: ArraySet  loop:<<Loop2>>      outer_loop:none
-  //
-  ///     CHECK-EVAL: "<<Loop1>>" != "<<Loop2>>"
-  //
-  /// CHECK-FI:
-  private static void doitFloat(float[] x) {
-    for (int i = 0; i < x.length; i++) {
-      x[i] = Math.abs(x[i]);
-    }
-  }
-
   /// CHECK-START: void Main.doitDouble(double[]) loop_optimization (before)
   /// CHECK-DAG: Phi        loop:<<Loop:B\d+>> outer_loop:none
   /// CHECK-DAG: ArrayGet   loop:<<Loop>>      outer_loop:none
@@ -292,81 +57,51 @@ public class Main {
     }
   }
 
-  public static void main(String[] args) {
-    // Bytes, chars, shorts.
-    byte[] xb = new byte[256];
-    for (int i = 0; i < 256; i++) {
-      xb[i] = (byte) i;
-    }
-    doitByte(xb);
-    for (int i = 0; i < 256; i++) {
-      expectEquals32((byte) Math.abs((byte) i), xb[i]);
-    }
-    char[] xc = new char[1024 * 64];
-    for (int i = 0; i < 1024 * 64; i++) {
-      xc[i] = (char) i;
-    }
-    doitChar(xc);
-    for (int i = 0; i < 1024 * 64; i++) {
-      expectEquals32((char) Math.abs((char) i), xc[i]);
-    }
-    short[] xs = new short[1024 * 64];
-    for (int i = 0; i < 1024 * 64; i++) {
-      xs[i] = (short) i;
-    }
-    doitShort(xs);
-    for (int i = 0; i < 1024 * 64; i++) {
-      expectEquals32((short) Math.abs((short) i), xs[i]);
-    }
-    for (int i = 0; i < 1024 * 64; i++) {
-      xc[i] = (char) i;
-    }
-    doitCastedChar(xc);
-    for (int i = 0; i < 1024 * 64; i++) {
-      expectEquals32((char) Math.abs((short) i), xc[i]);
-    }
-    // Set up minint32, maxint32 and some others.
-    int[] xi = new int[8];
-    xi[0] = 0x80000000;
-    xi[1] = 0x7fffffff;
-    xi[2] = 0x80000001;
-    xi[3] = -13;
-    xi[4] = -1;
-    xi[5] = 0;
-    xi[6] = 1;
-    xi[7] = 999;
-    doitInt(xi);
-    expectEquals32(0x80000000, xi[0]);
-    expectEquals32(0x7fffffff, xi[1]);
-    expectEquals32(0x7fffffff, xi[2]);
-    expectEquals32(13, xi[3]);
-    expectEquals32(1, xi[4]);
-    expectEquals32(0, xi[5]);
-    expectEquals32(1, xi[6]);
-    expectEquals32(999, xi[7]);
+  private static void check_float(float [] xf) {
+    expectEqualsNaN32(0x7f800001, Float.floatToRawIntBits(xf[0]));
+    expectEqualsNaN32(0x7fa00000, Float.floatToRawIntBits(xf[1]));
+    expectEqualsNaN32(0x7fc00000, Float.floatToRawIntBits(xf[2]));
+    expectEqualsNaN32(0x7fffffff, Float.floatToRawIntBits(xf[3]));
+    expectEqualsNaN32(0x7f800001, Float.floatToRawIntBits(xf[4]));
+    expectEqualsNaN32(0x7fa00000, Float.floatToRawIntBits(xf[5]));
+    expectEqualsNaN32(0x7fc00000, Float.floatToRawIntBits(xf[6]));
+    expectEqualsNaN32(0x7fffffff, Float.floatToRawIntBits(xf[7]));
+    expectEquals32(
+        Float.floatToRawIntBits(Float.POSITIVE_INFINITY),
+        Float.floatToRawIntBits(xf[8]));
+    expectEquals32(
+        Float.floatToRawIntBits(99.2f),
+        Float.floatToRawIntBits(xf[9]));
+    expectEquals32(
+        Float.floatToRawIntBits(1.0f),
+        Float.floatToRawIntBits(xf[10]));
+    expectEquals32(0, Float.floatToRawIntBits(xf[11]));
+    expectEquals32(0, Float.floatToRawIntBits(xf[12]));
+    expectEquals32(
+        Float.floatToRawIntBits(1.0f),
+        Float.floatToRawIntBits(xf[13]));
+    expectEquals32(
+        Float.floatToRawIntBits(99.2f),
+        Float.floatToRawIntBits(xf[14]));
+    expectEquals32(
+        Float.floatToRawIntBits(Float.POSITIVE_INFINITY),
+        Float.floatToRawIntBits(xf[15]));
+  }
 
-    // Set up minint64, maxint64 and some others.
-    long[] xl = new long[8];
-    xl[0] = 0x8000000000000000L;
-    xl[1] = 0x7fffffffffffffffL;
-    xl[2] = 0x8000000000000001L;
-    xl[3] = -13;
-    xl[4] = -1;
-    xl[5] = 0;
-    xl[6] = 1;
-    xl[7] = 999;
-    doitLong(xl);
-    expectEquals64(0x8000000000000000L, xl[0]);
-    expectEquals64(0x7fffffffffffffffL, xl[1]);
-    expectEquals64(0x7fffffffffffffffL, xl[2]);
-    expectEquals64(13, xl[3]);
-    expectEquals64(1, xl[4]);
-    expectEquals64(0, xl[5]);
-    expectEquals64(1, xl[6]);
-    expectEquals64(999, xl[7]);
+   private static void doitFloat(float[] x) {
+     for (int i = 0; i < x.length; i++) {
+       x[i] = Math.abs(x[i]);
+     }
+   }
 
+  public static void main(String[] args) {
     // Set up float NaN and some others.
+    long start = System.currentTimeMillis();
     float[] xf = new float[16];
+    double[] xd = new double[16];
+    for (int j = 0; j <  10; j++) {
+     for (int k = 0; k< 5000; k++) {
+   //float[] xf = new float[16];
     xf[0] = Float.intBitsToFloat(0x7f800001);
     xf[1] = Float.intBitsToFloat(0x7fa00000);
     xf[2] = Float.intBitsToFloat(0x7fc00000);
@@ -383,7 +118,57 @@ public class Main {
     xf[13] = +1.0f;
     xf[14] = +99.2f;
     xf[15] = Float.POSITIVE_INFINITY;
+    // Set up float NaN and some others.
     doitFloat(xf);
+    check_float(xf);
+    // Set up double NaN and some others.
+    xd[0] = Double.longBitsToDouble(0x7ff0000000000001L);
+    xd[1] = Double.longBitsToDouble(0x7ff4000000000000L);
+    xd[2] = Double.longBitsToDouble(0x7ff8000000000000L);
+    xd[3] = Double.longBitsToDouble(0x7fffffffffffffffL);
+    xd[4] = Double.longBitsToDouble(0xfff0000000000001L);
+    xd[5] = Double.longBitsToDouble(0xfff4000000000000L);
+    xd[6] = Double.longBitsToDouble(0xfff8000000000000L);
+    xd[7] = Double.longBitsToDouble(0xffffffffffffffffL);
+    xd[8] = Double.NEGATIVE_INFINITY;
+    xd[9] = -99.2f;
+    xd[10] = -1.0f;
+    xd[11] = -0.0f;
+    xd[12] = +0.0f;
+    xd[13] = +1.0f;
+    xd[14] = +99.2f;
+    xd[15] = Double.POSITIVE_INFINITY;
+    doitDouble(xd);
+    expectEqualsNaN64(0x7ff0000000000001L, Double.doubleToRawLongBits(xd[0]));
+    expectEqualsNaN64(0x7ff4000000000000L, Double.doubleToRawLongBits(xd[1]));
+    expectEqualsNaN64(0x7ff8000000000000L, Double.doubleToRawLongBits(xd[2]));
+    expectEqualsNaN64(0x7fffffffffffffffL, Double.doubleToRawLongBits(xd[3]));
+    expectEqualsNaN64(0x7ff0000000000001L, Double.doubleToRawLongBits(xd[4]));
+    expectEqualsNaN64(0x7ff4000000000000L, Double.doubleToRawLongBits(xd[5]));
+    expectEqualsNaN64(0x7ff8000000000000L, Double.doubleToRawLongBits(xd[6]));
+    expectEqualsNaN64(0x7fffffffffffffffL, Double.doubleToRawLongBits(xd[7]));
+    expectEquals64(
+        Double.doubleToRawLongBits(Double.POSITIVE_INFINITY),
+        Double.doubleToRawLongBits(xd[8]));
+    expectEquals64(
+        Double.doubleToRawLongBits(99.2f),
+        Double.doubleToRawLongBits(xd[9]));
+    expectEquals64(
+        Double.doubleToRawLongBits(1.0f),
+        Double.doubleToRawLongBits(xd[10]));
+    expectEquals64(0, Double.doubleToRawLongBits(xd[11]));
+    expectEquals64(0, Double.doubleToRawLongBits(xd[12]));
+    expectEquals64(
+        Double.doubleToRawLongBits(1.0f),
+        Double.doubleToRawLongBits(xd[13]));
+    expectEquals64(
+        Double.doubleToRawLongBits(99.2f),
+        Double.doubleToRawLongBits(xd[14]));
+    expectEquals64(
+        Double.doubleToRawLongBits(Double.POSITIVE_INFINITY),
+        Double.doubleToRawLongBits(xd[15]));
+      }
+     }
     expectEqualsNaN32(0x7f800001, Float.floatToRawIntBits(xf[0]));
     expectEqualsNaN32(0x7fa00000, Float.floatToRawIntBits(xf[1]));
     expectEqualsNaN32(0x7fc00000, Float.floatToRawIntBits(xf[2]));
@@ -412,27 +197,7 @@ public class Main {
     expectEquals32(
         Float.floatToRawIntBits(Float.POSITIVE_INFINITY),
         Float.floatToRawIntBits(xf[15]));
-
-    // Set up double NaN and some others.
-    double[] xd = new double[16];
-    xd[0] = Double.longBitsToDouble(0x7ff0000000000001L);
-    xd[1] = Double.longBitsToDouble(0x7ff4000000000000L);
-    xd[2] = Double.longBitsToDouble(0x7ff8000000000000L);
-    xd[3] = Double.longBitsToDouble(0x7fffffffffffffffL);
-    xd[4] = Double.longBitsToDouble(0xfff0000000000001L);
-    xd[5] = Double.longBitsToDouble(0xfff4000000000000L);
-    xd[6] = Double.longBitsToDouble(0xfff8000000000000L);
-    xd[7] = Double.longBitsToDouble(0xffffffffffffffffL);
-    xd[8] = Double.NEGATIVE_INFINITY;
-    xd[9] = -99.2f;
-    xd[10] = -1.0f;
-    xd[11] = -0.0f;
-    xd[12] = +0.0f;
-    xd[13] = +1.0f;
-    xd[14] = +99.2f;
-    xd[15] = Double.POSITIVE_INFINITY;
-    doitDouble(xd);
-    expectEqualsNaN64(0x7ff0000000000001L, Double.doubleToRawLongBits(xd[0]));
+     expectEqualsNaN64(0x7ff0000000000001L, Double.doubleToRawLongBits(xd[0]));
     expectEqualsNaN64(0x7ff4000000000000L, Double.doubleToRawLongBits(xd[1]));
     expectEqualsNaN64(0x7ff8000000000000L, Double.doubleToRawLongBits(xd[2]));
     expectEqualsNaN64(0x7fffffffffffffffL, Double.doubleToRawLongBits(xd[3]));
@@ -460,7 +225,9 @@ public class Main {
     expectEquals64(
         Double.doubleToRawLongBits(Double.POSITIVE_INFINITY),
         Double.doubleToRawLongBits(xd[15]));
-
+     long end = System.currentTimeMillis() - start;
+     System.out.println((10* 5000.0)/end + "ipms");
+   
     System.out.println("passed");
   }
 
diff --git a/test/655-checker-simd-arm-opt/src/Main.java b/test/655-checker-simd-arm-opt/src/Main.java
index 980593d1f9..58d2e73acd 100644
--- a/test/655-checker-simd-arm-opt/src/Main.java
+++ b/test/655-checker-simd-arm-opt/src/Main.java
@@ -18,7 +18,8 @@
  * Checker test for arm and arm64 simd optimizations.
  */
 public class Main {
-
+  //static float fsum = 0.0f;
+  //static double dsum = 0.0;
   private static void expectEquals(int expected, int result) {
     if (expected != result) {
       throw new Error("Expected: " + expected + ", found: " + result);
@@ -114,11 +115,44 @@ public class Main {
     int[] a = new int[ARRAY_SIZE];
     long[] l = new long[ARRAY_SIZE];
     float[] f = new float[ARRAY_SIZE];
+    float [] f1 = new float[ARRAY_SIZE];
     double[] d = new double[ARRAY_SIZE];
 
-    encodableConstants(b, s, c, a, l, f, d);
-    expectEquals(32640, sumArray(b, s, c, a, l, f, d));
-
+    //encodableConstants(b, s, c, a, l, f, d);
+    //expectEquals(32640, sumArray(b, s, c, a, l, f, d));
+    for (int i = 0; i< 20000; i++) {
+       encodableConstants(b,s,c,a,l,f,d);
+    }
+    System.out.println("warmup done"); 
+    long start = System.currentTimeMillis();
+    for (int j = 0; j< 10; j++) {
+      for (int k = 0; k < 5000; k++) { 
+         encodableConstants(b, s, c, a, l, f, d);
+      }
+    }
+    //System.out.println(fsum + " ---" + dsum);
+    /*for (int i = 0; i < 100; i++) {
+     if (f[i] == 0.0) {
+      System.out.println("ind=" + i );
+     }
+     System.out.print(f[i] +"*");
+    }
+    System.out.println("\n");*/
+    //expectEquals(3700, sumArray(b, s, c, a, l, f, d));
+    long elapsedTimeMillis = System.currentTimeMillis()-start;
+    System.out.println("Iterations Per Milli Second:" + (10 * 5000.0)/elapsedTimeMillis+" ipms");
+/*for (int i = 0; i< 100; i++) {
+    f1[i] = (float) 2*i;
+    }
+     for (int i = 0; i< 100; i++) {
+    f[i] = (float) i;
+    }
+     copyArrays(f,f1); 
+     for (int i = 0; i < 100; i++) {
+     System.out.print(f[i] +"*");
+    }
+    System.out.println("\n");*/
+ 
     System.out.println("passed");
   }
 }
diff --git a/test/etc/run-test-jar b/test/etc/run-test-jar
index 105af41769..958a3a61f7 100755
--- a/test/etc/run-test-jar
+++ b/test/etc/run-test-jar
@@ -1,7 +1,7 @@
 #!/bin/bash
 #
 # Runner for an individual run-test.
-
+#set -x
 if [[ -z "$ANDROID_BUILD_TOP" ]]; then
   echo 'ANDROID_BUILD_TOP environment variable is empty; did you forget to run `lunch`?'
   exit 1
@@ -1110,7 +1110,7 @@ if [ "$EXTERNAL_LOG_TAGS" = "n" ]; then
       export ANDROID_LOG_TAGS='*:e'
   fi
 fi
-
+#export ANDROID_LOG_TAGS='*:d'
 if [ "$HOST" = "n" ]; then
     adb root > /dev/null
     adb wait-for-device
-- 
2.37.1

