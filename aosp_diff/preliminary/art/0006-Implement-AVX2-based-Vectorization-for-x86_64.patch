From c107c563a8f3100a5ace2e7aa3f1b99ee4cd0873 Mon Sep 17 00:00:00 2001
From: Vinay Prasad Kompella <vinay.kompella@intel.com>
Date: Fri, 1 Nov 2024 21:20:10 +0530
Subject: [PATCH 6/6] Implement AVX2 based Vectorization for x86_64

This patch enables 256-bit vectorization when AVX2 is enabled.
Vector codegen does not check AVX, relies only on AVX2 for simplicity.
We use a fixed vector length for now, for simplicity

Observed significant gains in tests:
JBench - 19%
SpecJVM (scimark.lu.small) - 35%
        (scimark.lu.large) - 19%

Test: art/test.py -b --host
      Run the above test with and without "avx,avx2" features

Tracked-On: OAM-126116
Change-Id: I7f457c230273fb63b917165270bc5496071f0a54
Signed-off-by: Vinay Prasad Kompella <vinay.kompella@intel.com>
---
 .../code_generator_vector_x86_64.cc           | 632 +++++++++---------
 compiler/optimizing/code_generator_x86_64.cc  |  62 +-
 compiler/optimizing/code_generator_x86_64.h   |   3 +-
 compiler/optimizing/loop_analysis.cc          |   4 +
 compiler/optimizing/loop_analysis.h           |   2 +
 compiler/optimizing/loop_optimization.cc      |  38 +-
 compiler/optimizing/nodes.h                   |  26 +-
 compiler/optimizing/nodes_x86.h               |  11 +
 test/530-checker-lse-simd/src/Main.java       |  12 +
 test/656-checker-simd-opt/src/Main.java       |  44 +-
 .../file_format/c1visualizer/parser.py        |   8 +-
 .../file_format/c1visualizer/struct.py        |   4 +
 tools/checker/match/file.py                   |   3 +
 tools/checker/match/line.py                   |   1 +
 14 files changed, 487 insertions(+), 363 deletions(-)

diff --git a/compiler/optimizing/code_generator_vector_x86_64.cc b/compiler/optimizing/code_generator_vector_x86_64.cc
index 31626f9e44..f83ed0451b 100644
--- a/compiler/optimizing/code_generator_vector_x86_64.cc
+++ b/compiler/optimizing/code_generator_vector_x86_64.cc
@@ -25,6 +25,17 @@ namespace x86_64 {
 // NOLINT on __ macro to suppress wrong warning/fix (misc-macro-parentheses) from clang-tidy.
 #define __ down_cast<X86_64Assembler*>(GetAssembler())->  // NOLINT
 
+static void CheckVectorization(CodeGeneratorX86_64* codegen,
+                               HVecOperation* instruction,
+                               XmmRegister& reg,
+                               bool* uses_avx2 = nullptr) {
+  DCHECK_EQ(instruction->GetVectorLength() * DataType::Size(instruction->GetPackedType()), codegen->GetSIMDRegisterWidth());
+  DCHECK_EQ(codegen->GetInstructionSetFeatures().HasAVX2(), reg.IsYMM());
+  if (uses_avx2 != nullptr) {
+    *uses_avx2 = codegen->GetInstructionSetFeatures().HasAVX2();
+  }
+}
+
 void LocationsBuilderX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
   LocationSummary* locations = new (GetGraph()->GetAllocator()) LocationSummary(instruction);
   HInstruction* input = instruction->InputAt(0);
@@ -60,12 +71,13 @@ void LocationsBuilderX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instru
 
 void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+  // Generic vectorization size check
+  bool uses_avx2 = false;
+  CheckVectorization(codegen_, instruction, dst, &uses_avx2);
   // Shorthand for any type of zero.
   if (IsZeroBitPattern(instruction->InputAt(0))) {
-    cpu_has_avx ? __ vxorps(dst, dst, dst) : __ xorps(dst, dst);
+    uses_avx2 ? __ vxorps(dst, dst, dst) : __ xorps(dst, dst);
     return;
   }
 
@@ -73,44 +85,60 @@ void InstructionCodeGeneratorX86_64::VisitVecReplicateScalar(HVecReplicateScalar
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
-      __ punpcklbw(dst, dst);
-      __ punpcklwd(dst, dst);
-      __ pshufd(dst, dst, Immediate(0));
+      if (!uses_avx2) {
+        __ punpcklbw(dst, dst);
+        __ punpcklwd(dst, dst);
+        __ pshufd(dst, dst, Immediate(0));
+      } else {
+        __ vpbroadcastb(dst, dst);
+      }
       break;
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
-      __ punpcklwd(dst, dst);
-      __ pshufd(dst, dst, Immediate(0));
+      if (!uses_avx2) {  
+        __ punpcklwd(dst, dst);
+        __ pshufd(dst, dst, Immediate(0));
+      } else {
+        __ vpbroadcastw(dst, dst);
+      }
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ false);
-      __ pshufd(dst, dst, Immediate(0));
+      if (!uses_avx2) {
+        __ pshufd(dst, dst, Immediate(0));
+      } else {
+        __ vpbroadcastd(dst, dst);
+      }
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>(), /*64-bit*/ true);
-      __ punpcklqdq(dst, dst);
+      if (!uses_avx2) {
+        __ punpcklqdq(dst, dst);
+      } else {
+        __ vpbroadcastq(dst, dst);
+      }
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      {
+      if (!uses_avx2) {
         XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
         __ movups(dst, src);
+        __ shufps(dst, dst, Immediate(0));
+      } else {
+        XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+        __ vbroadcastss(dst, src);
       }
-      __ shufps(dst, dst, Immediate(0));
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      {
+      if (!uses_avx2) {
         XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
         __ movups(dst, src);
+        __ shufpd(dst, dst, Immediate(0));
+      } else {
+        XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+        __ vbroadcastsd(dst, src);
       }
-      __ shufpd(dst, dst, Immediate(0));
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -150,7 +178,10 @@ void LocationsBuilderX86_64::VisitVecExtractScalar(HVecExtractScalar* instructio
 
 void InstructionCodeGeneratorX86_64::VisitVecExtractScalar(HVecExtractScalar* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, src);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -160,17 +191,13 @@ void InstructionCodeGeneratorX86_64::VisitVecExtractScalar(HVecExtractScalar* in
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
       UNREACHABLE();
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ movd(locations->Out().AsRegister<CpuRegister>(), src, /*64-bit*/ false);
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ movd(locations->Out().AsRegister<CpuRegister>(), src, /*64-bit*/ true);
       break;
     case DataType::Type::kFloat32:
     case DataType::Type::kFloat64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 4u);
       {
         XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
         __ movups(dst, src);
@@ -217,16 +244,27 @@ void LocationsBuilderX86_64::VisitVecReduce(HVecReduce* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecReduce(HVecReduce* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  bool uses_avx2 = false;
+  CheckVectorization(codegen_, instruction, dst, &uses_avx2);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       switch (instruction->GetReductionKind()) {
         case HVecReduce::kSum:
-          __ movaps(dst, src);
-          __ phaddd(dst, dst);
-          __ phaddd(dst, dst);
+          if (!uses_avx2) {
+            __ movaps(dst, src);
+            __ phaddd(dst, dst);
+            __ phaddd(dst, dst);
+          } else {
+            __ vmovaps(dst, src);
+            __ vphaddd(dst, dst, dst);
+            __ vpermpd(dst, dst, Immediate(0xd8));
+            __ vphaddd(dst, dst, dst);
+            __ vphaddd(dst, dst, dst);
+          }
           break;
         case HVecReduce::kMin:
         case HVecReduce::kMax:
@@ -236,14 +274,23 @@ void InstructionCodeGeneratorX86_64::VisitVecReduce(HVecReduce* instruction) {
       }
       break;
     case DataType::Type::kInt64: {
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
+      XmmRegister tmp = locations->GetTemp(0).AsFPVectorRegister<XmmRegister>();
       switch (instruction->GetReductionKind()) {
         case HVecReduce::kSum:
-          __ movaps(tmp, src);
-          __ movaps(dst, src);
-          __ punpckhqdq(tmp, tmp);
-          __ paddq(dst, tmp);
+          if (!uses_avx2) {
+            __ movaps(tmp, src);
+            __ movaps(dst, src);
+            __ punpckhqdq(tmp, tmp);
+            __ paddq(dst, tmp);
+          } else {
+            __ vmovaps(tmp, src);
+            __ vmovaps(dst, src);
+            __ vpermpd(tmp, tmp, Immediate(0x4E));
+            __ vpaddq(dst, dst, tmp);
+            __ vmovaps(tmp, dst);
+            __ vpermpd(tmp, tmp, Immediate(0xB1));
+            __ vpaddq(dst, dst, tmp);
+          }
           break;
         case HVecReduce::kMin:
         case HVecReduce::kMax:
@@ -263,12 +310,14 @@ void LocationsBuilderX86_64::VisitVecCnv(HVecCnv* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecCnv(HVecCnv* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
   DataType::Type from = instruction->GetInputType();
   DataType::Type to = instruction->GetResultType();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   if (from == DataType::Type::kInt32 && to == DataType::Type::kFloat32) {
-    DCHECK_EQ(4u, instruction->GetVectorLength());
     __ cvtdq2ps(dst, src);
   } else {
     LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -281,38 +330,35 @@ void LocationsBuilderX86_64::VisitVecNeg(HVecNeg* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecNeg(HVecNeg* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ pxor(dst, dst);
       __ psubb(dst, src);
       break;
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ pxor(dst, dst);
       __ psubw(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pxor(dst, dst);
       __ psubd(dst, src);
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ pxor(dst, dst);
       __ psubq(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ xorps(dst, dst);
       __ subps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ xorpd(dst, dst);
       __ subpd(dst, src);
       break;
@@ -325,34 +371,45 @@ void InstructionCodeGeneratorX86_64::VisitVecNeg(HVecNeg* instruction) {
 void LocationsBuilderX86_64::VisitVecAbs(HVecAbs* instruction) {
   CreateVecUnOpLocations(GetGraph()->GetAllocator(), instruction);
   // Integral-abs requires a temporary for the comparison.
-  if (instruction->GetPackedType() == DataType::Type::kInt32) {
+  if (instruction->GetPackedType() == DataType::Type::kInt64) {
     instruction->GetLocations()->AddTemp(Location::RequiresFpuRegister());
   }
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecAbs(HVecAbs* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   switch (instruction->GetPackedType()) {
+    case DataType::Type::kBool:
+    case DataType::Type::kInt8:
+      __ pabsb(dst, src);
+      break;
+    case DataType::Type::kInt16:
+      __ pabsw(dst, src);
+      break;
     case DataType::Type::kInt32: {
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
-      __ movaps(dst, src);
-      __ pxor(tmp, tmp);
-      __ pcmpgtd(tmp, dst);
-      __ pxor(dst, tmp);
-      __ psubd(dst, tmp);
+      __ pabsd(dst, src);
       break;
     }
+    case DataType::Type::kInt64: {
+        XmmRegister tmp = locations->GetTemp(0).AsFPVectorRegister<XmmRegister>();
+        __ movaps(dst, src);
+        __ pxor(tmp, tmp);
+        __ pcmpgtq(tmp, dst);
+        __ pxor(dst, tmp);
+        __ psubq(dst, tmp);
+      }
+      break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pcmpeqb(dst, dst);  // all ones
       __ psrld(dst, Immediate(1));
       __ andps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ pcmpeqb(dst, dst);  // all ones
       __ psrlq(dst, Immediate(1));
       __ andpd(dst, src);
@@ -373,16 +430,18 @@ void LocationsBuilderX86_64::VisitVecNot(HVecNot* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecNot(HVecNot* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool: {  // special case boolean-not
-      DCHECK_EQ(16u, instruction->GetVectorLength());
-      XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
-      __ pxor(dst, dst);
-      __ pcmpeqb(tmp, tmp);  // all ones
-      __ psubb(dst, tmp);  // 16 x one
-      __ pxor(dst, src);
+      XmmRegister tmp = locations->GetTemp(0).AsFPVectorRegister<XmmRegister>();
+        __ pxor(dst, dst);
+        __ pcmpeqb(tmp, tmp);  // all ones
+        __ psubb(dst, tmp);  // 16 x one
+        __ pxor(dst, src);
       break;
     }
     case DataType::Type::kUint8:
@@ -391,18 +450,14 @@ void InstructionCodeGeneratorX86_64::VisitVecNot(HVecNot* instruction) {
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
       __ pcmpeqb(dst, dst);  // all ones
       __ pxor(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pcmpeqb(dst, dst);  // all ones
       __ xorps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ pcmpeqb(dst, dst);  // all ones
       __ xorpd(dst, src);
       break;
@@ -435,69 +490,38 @@ static void CreateVecBinOpLocations(ArenaAllocator* allocator, HVecBinaryOperati
   }
 }
 
-static void CreateVecTerOpLocations(ArenaAllocator* allocator, HVecOperation* instruction) {
-  LocationSummary* locations = new (allocator) LocationSummary(instruction);
-  switch (instruction->GetPackedType()) {
-    case DataType::Type::kBool:
-    case DataType::Type::kUint8:
-    case DataType::Type::kInt8:
-    case DataType::Type::kUint16:
-    case DataType::Type::kInt16:
-    case DataType::Type::kInt32:
-    case DataType::Type::kInt64:
-    case DataType::Type::kFloat32:
-    case DataType::Type::kFloat64:
-      locations->SetInAt(0, Location::RequiresFpuRegister());
-      locations->SetInAt(1, Location::RequiresFpuRegister());
-      locations->SetOut(Location::RequiresFpuRegister());
-      break;
-    default:
-      LOG(FATAL) << "Unsupported SIMD type";
-      UNREACHABLE();
-  }
-}
-
 void LocationsBuilderX86_64::VisitVecAdd(HVecAdd* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecAdd(HVecAdd* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddb(dst, other_src, src) : __ paddb(dst, src);
+      __ paddb(dst, src);
       break;
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddw(dst, other_src, src) : __ paddw(dst, src);
+      __ paddw(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddd(dst, other_src, src) : __ paddd(dst, src);
+      __ paddd(dst, src);
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpaddq(dst, other_src, src) : __ paddq(dst, src);
+      __ paddq(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vaddps(dst, other_src, src) : __ addps(dst, src);
+      __ addps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vaddpd(dst, other_src, src) : __ addpd(dst, src);
+      __ addpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -511,24 +535,23 @@ void LocationsBuilderX86_64::VisitVecSaturationAdd(HVecSaturationAdd* instructio
 
 void InstructionCodeGeneratorX86_64::VisitVecSaturationAdd(HVecSaturationAdd* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  DCHECK(locations->InAt(0).Equals(locations->Out()));
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ paddusb(dst, src);
       break;
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ paddsb(dst, src);
       break;
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ paddusw(dst, src);
       break;
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ paddsw(dst, src);
       break;
     default:
@@ -543,19 +566,19 @@ void LocationsBuilderX86_64::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecHalvingAdd(HVecHalvingAdd* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  DCHECK(locations->InAt(0).Equals(locations->Out()));
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
 
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   DCHECK(instruction->IsRounded());
 
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ pavgb(dst, src);
       break;
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ pavgw(dst, src);
       break;
     default:
@@ -565,46 +588,37 @@ void InstructionCodeGeneratorX86_64::VisitVecHalvingAdd(HVecHalvingAdd* instruct
 }
 
 void LocationsBuilderX86_64::VisitVecSub(HVecSub* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecSub(HVecSub* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpsubb(dst, other_src, src) : __ psubb(dst, src);
+      __ psubb(dst, src);
       break;
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpsubw(dst, other_src, src) : __ psubw(dst, src);
+      __ psubw(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpsubd(dst, other_src, src) : __ psubd(dst, src);
+      __ psubd(dst, src);
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpsubq(dst, other_src, src) : __ psubq(dst, src);
+      __ psubq(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vsubps(dst, other_src, src) : __ subps(dst, src);
+      __ subps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vsubpd(dst, other_src, src) : __ subpd(dst, src);
+      __ subpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -618,24 +632,23 @@ void LocationsBuilderX86_64::VisitVecSaturationSub(HVecSaturationSub* instructio
 
 void InstructionCodeGeneratorX86_64::VisitVecSaturationSub(HVecSaturationSub* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  DCHECK(locations->InAt(0).Equals(locations->Out()));
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ psubusb(dst, src);
       break;
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ psubsb(dst, src);
       break;
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ psubusw(dst, src);
       break;
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ psubsw(dst, src);
       break;
     default:
@@ -645,37 +658,30 @@ void InstructionCodeGeneratorX86_64::VisitVecSaturationSub(HVecSaturationSub* in
 }
 
 void LocationsBuilderX86_64::VisitVecMul(HVecMul* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecMul(HVecMul* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpmullw(dst, other_src, src) : __ pmullw(dst, src);
+      __ pmullw(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vpmulld(dst, other_src, src): __ pmulld(dst, src);
+      __ pmulld(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vmulps(dst, other_src, src) : __ mulps(dst, src);
+      __ mulps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vmulpd(dst, other_src, src) : __ mulpd(dst, src);
+      __ mulpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -684,28 +690,23 @@ void InstructionCodeGeneratorX86_64::VisitVecMul(HVecMul* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecDiv(HVecDiv* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecDiv(HVecDiv* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vdivps(dst, other_src, src) : __ divps(dst, src);
+      __ divps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vdivpd(dst, other_src, src) : __ divpd(dst, src);
+      __ divpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -719,41 +720,37 @@ void LocationsBuilderX86_64::VisitVecMin(HVecMin* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecMin(HVecMin* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  DCHECK(locations->InAt(0).Equals(locations->Out()));
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  //   DCHECK(locations->InAt(0).Equals(locations->Out()));
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ pminub(dst, src);
       break;
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ pminsb(dst, src);
       break;
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ pminuw(dst, src);
       break;
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ pminsw(dst, src);
       break;
     case DataType::Type::kUint32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pminud(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pminsd(dst, src);
       break;
     // Next cases are sloppy wrt 0.0 vs -0.0.
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ minps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ minpd(dst, src);
       break;
     default:
@@ -768,41 +765,37 @@ void LocationsBuilderX86_64::VisitVecMax(HVecMax* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecMax(HVecMax* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  DCHECK(locations->InAt(0).Equals(locations->Out()));
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  //   DCHECK(locations->InAt(0).Equals(locations->Out()));
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ pmaxub(dst, src);
       break;
     case DataType::Type::kInt8:
-      DCHECK_EQ(16u, instruction->GetVectorLength());
       __ pmaxsb(dst, src);
       break;
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ pmaxuw(dst, src);
       break;
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ pmaxsw(dst, src);
       break;
     case DataType::Type::kUint32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pmaxud(dst, src);
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pmaxsd(dst, src);
       break;
     // Next cases are sloppy wrt 0.0 vs -0.0.
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ maxps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ maxpd(dst, src);
       break;
     default:
@@ -812,20 +805,17 @@ void InstructionCodeGeneratorX86_64::VisitVecMax(HVecMax* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecAnd(HVecAnd* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecAnd(HVecAnd* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -834,17 +824,13 @@ void InstructionCodeGeneratorX86_64::VisitVecAnd(HVecAnd* instruction) {
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      cpu_has_avx ? __ vpand(dst, other_src, src) : __ pand(dst, src);
+      __ pand(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vandps(dst, other_src, src) : __ andps(dst, src);
+      __ andps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vandpd(dst, other_src, src) : __ andpd(dst, src);
+      __ andpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -853,20 +839,17 @@ void InstructionCodeGeneratorX86_64::VisitVecAnd(HVecAnd* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecAndNot(HVecAndNot* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecAndNot(HVecAndNot* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -875,17 +858,13 @@ void InstructionCodeGeneratorX86_64::VisitVecAndNot(HVecAndNot* instruction) {
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      cpu_has_avx ? __ vpandn(dst, other_src, src) : __ pandn(dst, src);
+      __ pandn(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vandnps(dst, other_src, src) : __ andnps(dst, src);
+      __ andnps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vandnpd(dst, other_src, src) : __ andnpd(dst, src);
+      __ andnpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -894,20 +873,17 @@ void InstructionCodeGeneratorX86_64::VisitVecAndNot(HVecAndNot* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecOr(HVecOr* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecOr(HVecOr* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -916,17 +892,13 @@ void InstructionCodeGeneratorX86_64::VisitVecOr(HVecOr* instruction) {
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      cpu_has_avx ? __ vpor(dst, other_src, src) : __ por(dst, src);
+      __ por(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vorps(dst, other_src, src) : __ orps(dst, src);
+      __ orps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vorpd(dst, other_src, src) : __ orpd(dst, src);
+      __ orpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -935,20 +907,17 @@ void InstructionCodeGeneratorX86_64::VisitVecOr(HVecOr* instruction) {
 }
 
 void LocationsBuilderX86_64::VisitVecXor(HVecXor* instruction) {
-  if (CpuHasAvxFeatureFlag()) {
-    CreateVecTerOpLocations(GetGraph()->GetAllocator(), instruction);
-  } else {
-    CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
-  }
+  CreateVecBinOpLocations(GetGraph()->GetAllocator(), instruction);
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecXor(HVecXor* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister other_src = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister src = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
-  DCHECK(cpu_has_avx || other_src == dst);
+  XmmRegister other_src = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister src = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+  DCHECK(other_src == dst);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -957,17 +926,13 @@ void InstructionCodeGeneratorX86_64::VisitVecXor(HVecXor* instruction) {
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      cpu_has_avx ? __ vpxor(dst, other_src, src) : __ pxor(dst, src);
+      __ pxor(dst, src);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vxorps(dst, other_src, src) : __ xorps(dst, src);
+      __ xorps(dst, src);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      cpu_has_avx ? __ vxorpd(dst, other_src, src) : __ xorpd(dst, src);
+      __ xorpd(dst, src);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -1001,19 +966,19 @@ void InstructionCodeGeneratorX86_64::VisitVecShl(HVecShl* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   DCHECK(locations->InAt(0).Equals(locations->Out()));
   int32_t value = locations->InAt(1).GetConstant()->AsIntConstant()->GetValue();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ psllw(dst, Immediate(static_cast<int8_t>(value)));
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ pslld(dst, Immediate(static_cast<int8_t>(value)));
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ psllq(dst, Immediate(static_cast<int8_t>(value)));
       break;
     default:
@@ -1030,15 +995,16 @@ void InstructionCodeGeneratorX86_64::VisitVecShr(HVecShr* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   DCHECK(locations->InAt(0).Equals(locations->Out()));
   int32_t value = locations->InAt(1).GetConstant()->AsIntConstant()->GetValue();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ psraw(dst, Immediate(static_cast<int8_t>(value)));
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ psrad(dst, Immediate(static_cast<int8_t>(value)));
       break;
     default:
@@ -1055,19 +1021,19 @@ void InstructionCodeGeneratorX86_64::VisitVecUShr(HVecUShr* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   DCHECK(locations->InAt(0).Equals(locations->Out()));
   int32_t value = locations->InAt(1).GetConstant()->AsIntConstant()->GetValue();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, dst);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kUint16:
     case DataType::Type::kInt16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       __ psrlw(dst, Immediate(static_cast<int8_t>(value)));
       break;
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ psrld(dst, Immediate(static_cast<int8_t>(value)));
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ psrlq(dst, Immediate(static_cast<int8_t>(value)));
       break;
     default:
@@ -1110,13 +1076,13 @@ void LocationsBuilderX86_64::VisitVecSetScalars(HVecSetScalars* instruction) {
 
 void InstructionCodeGeneratorX86_64::VisitVecSetScalars(HVecSetScalars* instruction) {
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister dst = locations->Out().AsFpuRegister<XmmRegister>();
+  XmmRegister dst = locations->Out().AsFPVectorRegister<XmmRegister>();
 
   DCHECK_EQ(1u, instruction->InputCount());  // only one input currently implemented
 
+  CheckVectorization(codegen_, instruction, dst);
   // Zero out all other elements first.
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
-  cpu_has_avx ? __ vxorps(dst, dst, dst) : __ xorps(dst, dst);
+  __ xorps(dst, dst);
 
   // Shorthand for any type of zero.
   if (IsZeroBitPattern(instruction->InputAt(0))) {
@@ -1133,20 +1099,16 @@ void InstructionCodeGeneratorX86_64::VisitVecSetScalars(HVecSetScalars* instruct
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
       UNREACHABLE();
     case DataType::Type::kInt32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>());
       break;
     case DataType::Type::kInt64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
       __ movd(dst, locations->InAt(0).AsRegister<CpuRegister>());  // is 64-bit
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      __ movss(dst, locations->InAt(0).AsFpuRegister<XmmRegister>());
+      __ movss(dst, locations->InAt(0).AsFPVectorRegister<XmmRegister>());
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      __ movsd(dst, locations->InAt(0).AsFpuRegister<XmmRegister>());
+      __ movsd(dst, locations->InAt(0).AsFPVectorRegister<XmmRegister>());
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -1203,16 +1165,18 @@ void LocationsBuilderX86_64::VisitVecDotProd(HVecDotProd* instruction) {
 }
 
 void InstructionCodeGeneratorX86_64::VisitVecDotProd(HVecDotProd* instruction) {
-  bool cpu_has_avx = CpuHasAvxFeatureFlag();
   LocationSummary* locations = instruction->GetLocations();
-  XmmRegister acc = locations->InAt(0).AsFpuRegister<XmmRegister>();
-  XmmRegister left = locations->InAt(1).AsFpuRegister<XmmRegister>();
-  XmmRegister right = locations->InAt(2).AsFpuRegister<XmmRegister>();
+  XmmRegister acc = locations->InAt(0).AsFPVectorRegister<XmmRegister>();
+  XmmRegister left = locations->InAt(1).AsFPVectorRegister<XmmRegister>();
+  XmmRegister right = locations->InAt(2).AsFPVectorRegister<XmmRegister>();
+
+  bool uses_avx2 = false;
+  CheckVectorization(codegen_, instruction, acc, &uses_avx2);
+
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt32: {
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
-      if (!cpu_has_avx) {
+      XmmRegister tmp = locations->GetTemp(0).AsFPVectorRegister<XmmRegister>();
+      if (!uses_avx2) {
         __ movaps(tmp, right);
         __ pmaddwd(tmp, left);
         __ paddd(acc, tmp);
@@ -1287,16 +1251,19 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   size_t size = DataType::Size(instruction->GetPackedType());
   Address address = VecAddress(locations, size, instruction->IsStringCharAt());
-  XmmRegister reg = locations->Out().AsFpuRegister<XmmRegister>();
-  bool is_aligned16 = instruction->GetAlignment().IsAlignedAt(16);
+  XmmRegister reg = locations->Out().AsFPVectorRegister<XmmRegister>();
+  bool uses_avx2 = false;
+
+  CheckVectorization(codegen_, instruction, reg, &uses_avx2);
+
+  bool is_aligned = instruction->GetAlignment().IsAlignedAt(reg.IsYMM() ? 32 : 16);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kInt16:  // (short) s.charAt(.) can yield HVecLoad/Int16/StringCharAt.
     case DataType::Type::kUint16:
-      DCHECK_EQ(8u, instruction->GetVectorLength());
       // Special handling of compressed/uncompressed string load.
       if (mirror::kUseStringCompression && instruction->IsStringCharAt()) {
         NearLabel done, not_compressed;
-        XmmRegister tmp = locations->GetTemp(0).AsFpuRegister<XmmRegister>();
+        XmmRegister tmp = locations->GetTemp(0).AsFPVectorRegister<XmmRegister>();
         // Test compression bit.
         static_assert(static_cast<uint32_t>(mirror::StringCompressionFlag::kCompressed) == 0u,
                       "Expecting 0=compressed, 1=uncompressed");
@@ -1304,13 +1271,19 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
         __ testb(Address(locations->InAt(0).AsRegister<CpuRegister>(), count_offset), Immediate(1));
         __ j(kNotZero, &not_compressed);
         // Zero extend 8 compressed bytes into 8 chars.
-        __ movsd(reg, VecAddress(locations, 1, instruction->IsStringCharAt()));
+        if (!uses_avx2) {
+          __ movsd(reg, VecAddress(locations, 1, instruction->IsStringCharAt()));
+        } else {
+          __ movdqu(reg, VecAddress(locations, 1, instruction->IsStringCharAt()));
+          // Permute to 0213, so that we can operate on the low quad words
+          __ vpermpd(reg, reg, Immediate(0xd8));
+        }
         __ pxor(tmp, tmp);
         __ punpcklbw(reg, tmp);
         __ jmp(&done);
         // Load 8 direct uncompressed chars.
         __ Bind(&not_compressed);
-        is_aligned16 ?  __ movdqa(reg, address) :  __ movdqu(reg, address);
+        is_aligned ? __ movdqa(reg, address) : __ movdqu(reg, address);
         __ Bind(&done);
         return;
       }
@@ -1320,17 +1293,13 @@ void InstructionCodeGeneratorX86_64::VisitVecLoad(HVecLoad* instruction) {
     case DataType::Type::kInt8:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      is_aligned16 ? __ movdqa(reg, address) : __ movdqu(reg, address);
+      is_aligned ? __ movdqa(reg, address) : __ movdqu(reg, address);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      is_aligned16 ? __ movaps(reg, address) : __ movups(reg, address);
+      is_aligned ? __ movaps(reg, address) : __ movups(reg, address);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      is_aligned16 ? __ movapd(reg, address) : __ movupd(reg, address);
+      is_aligned ? __ movapd(reg, address) : __ movupd(reg, address);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -1346,8 +1315,11 @@ void InstructionCodeGeneratorX86_64::VisitVecStore(HVecStore* instruction) {
   LocationSummary* locations = instruction->GetLocations();
   size_t size = DataType::Size(instruction->GetPackedType());
   Address address = VecAddress(locations, size, /*is_string_char_at*/ false);
-  XmmRegister reg = locations->InAt(2).AsFpuRegister<XmmRegister>();
-  bool is_aligned16 = instruction->GetAlignment().IsAlignedAt(16);
+  XmmRegister reg = locations->InAt(2).AsFPVectorRegister<XmmRegister>();
+
+  CheckVectorization(codegen_, instruction, reg);
+
+  bool is_aligned = instruction->GetAlignment().IsAlignedAt(reg.IsYMM() ? 32 : 16);
   switch (instruction->GetPackedType()) {
     case DataType::Type::kBool:
     case DataType::Type::kUint8:
@@ -1356,17 +1328,13 @@ void InstructionCodeGeneratorX86_64::VisitVecStore(HVecStore* instruction) {
     case DataType::Type::kInt16:
     case DataType::Type::kInt32:
     case DataType::Type::kInt64:
-      DCHECK_LE(2u, instruction->GetVectorLength());
-      DCHECK_LE(instruction->GetVectorLength(), 16u);
-      is_aligned16 ? __ movdqa(address, reg) : __ movdqu(address, reg);
+      is_aligned ? __ movdqa(address, reg) : __ movdqu(address, reg);
       break;
     case DataType::Type::kFloat32:
-      DCHECK_EQ(4u, instruction->GetVectorLength());
-      is_aligned16 ? __ movaps(address, reg) : __ movups(address, reg);
+      is_aligned ? __ movaps(address, reg) : __ movups(address, reg);
       break;
     case DataType::Type::kFloat64:
-      DCHECK_EQ(2u, instruction->GetVectorLength());
-      is_aligned16 ? __ movapd(address, reg) : __ movupd(address, reg);
+      is_aligned ? __ movapd(address, reg) : __ movupd(address, reg);
       break;
     default:
       LOG(FATAL) << "Unsupported SIMD type: " << instruction->GetPackedType();
@@ -1424,6 +1392,12 @@ void InstructionCodeGeneratorX86_64::VisitVecPredNot(HVecPredNot* instruction) {
   UNREACHABLE();
 }
 
+void LocationsBuilderX86_64::VisitX86Clear(HX86Clear* clear) { clear->SetLocations(nullptr); }
+
+void InstructionCodeGeneratorX86_64::VisitX86Clear(HX86Clear* clear ATTRIBUTE_UNUSED) {
+  __ vzeroupper();
+}
+
 #undef __
 
 }  // namespace x86_64
diff --git a/compiler/optimizing/code_generator_x86_64.cc b/compiler/optimizing/code_generator_x86_64.cc
index 893a485616..acf2d590f6 100644
--- a/compiler/optimizing/code_generator_x86_64.cc
+++ b/compiler/optimizing/code_generator_x86_64.cc
@@ -167,6 +167,27 @@ class SuspendCheckSlowPathX86_64 : public SlowPathCode {
     CodeGeneratorX86_64* x86_64_codegen = down_cast<CodeGeneratorX86_64*>(codegen);
     __ Bind(GetEntryLabel());
     SaveLiveRegisters(codegen, locations);  // Only saves full width XMM for SIMD.
+    // If this is related to an AVX2 loop,
+    //  we must emit vzeroupper here before invoking runtime methods
+    if (codegen->GetGraph()->HasSIMD() && x86_64_codegen->GetInstructionSetFeatures().HasAVX2()) {
+      // Live SIMD registers => is a SIMD loop
+      if (locations->GetNumLiveVectorRegisters() > 0) {
+        __ vzeroupper();
+      } else if (instruction_->GetBlock()->IsInLoop()) {
+        // Slow path: Look at the loop_body to determine its a SIMD loop
+        HBasicBlock* loop_header = instruction_->GetBlock()->GetLoopInformation()->GetHeader();
+        // SIMD loop headers are guarenteed to have loop_body as second successor
+        if (loop_header->GetSuccessors().size() == 2u) {
+          HBasicBlock* loop_body = loop_header->GetSuccessors()[1];
+          for (HInstructionIterator it(loop_body->GetInstructions()); !it.Done(); it.Advance()) {
+            if (it.Current()->IsVecOperation()) {
+              __ vzeroupper();
+              break;
+            }
+          }
+        }
+      }
+    }
     x86_64_codegen->InvokeRuntime(kQuickTestSuspend, instruction_, instruction_->GetDexPc(), this);
     CheckEntrypointTypes<kQuickTestSuspend, void, void>();
     RestoreLiveRegisters(codegen, locations);  // Only restores full width XMM for SIMD.
@@ -6439,10 +6460,21 @@ void ParallelMoveResolverX86_64::EmitMove(size_t index) {
     } else {
       DCHECK(destination.IsSIMDStackSlot());
       size_t high = kX86_64WordSize;
-      __ movq(CpuRegister(TMP), Address(CpuRegister(RSP), source.GetStackIndex()));
-      __ movq(Address(CpuRegister(RSP), destination.GetStackIndex()), CpuRegister(TMP));
-      __ movq(CpuRegister(TMP), Address(CpuRegister(RSP), source.GetStackIndex() + high));
-      __ movq(Address(CpuRegister(RSP), destination.GetStackIndex() + high), CpuRegister(TMP));
+      if (codegen_->GetSIMDRegisterWidth() == (2 * kX86_64WordSize)) {
+        __ movq(CpuRegister(TMP), Address(CpuRegister(RSP), source.GetStackIndex()));
+        __ movq(Address(CpuRegister(RSP), destination.GetStackIndex()), CpuRegister(TMP));
+        __ movq(CpuRegister(TMP), Address(CpuRegister(RSP), source.GetStackIndex() + high));
+        __ movq(Address(CpuRegister(RSP), destination.GetStackIndex() + high), CpuRegister(TMP));
+      } else {
+        //Easier to spill a vector register
+        size_t extra_slot = codegen_->GetSIMDRegisterWidth();
+        __ subq(CpuRegister(RSP), Immediate(extra_slot));
+        __ movups(Address(CpuRegister(RSP), 0), XmmRegister(XMM0, extra_slot));
+        __ movups(XmmRegister(XMM0, extra_slot), Address(CpuRegister(RSP), source.GetStackIndex() + extra_slot));
+        __ movups(Address(CpuRegister(RSP), destination.GetStackIndex() + extra_slot), XmmRegister(XMM0, extra_slot));
+        __ movups(XmmRegister(XMM0, extra_slot), Address(CpuRegister(RSP), 0));
+        __ addq(CpuRegister(RSP), Immediate(extra_slot));
+      }
     }
   } else if (source.IsConstant()) {
     HConstant* constant = source.GetConstant();
@@ -6591,6 +6623,26 @@ void ParallelMoveResolverX86_64::ExchangeMemory64(int mem1, int mem2, int num_of
   }
 }
 
+void ParallelMoveResolverX86_64::ExchangeMemorySIMD(int mem1, int mem2) {
+  // It is better to spill a ymm onto stack than looping with 64-bit regs
+  size_t extra_slot = codegen_->GetSIMDRegisterWidth();
+  if (extra_slot > 2 * kX86_64WordSize) {
+    __ subq(CpuRegister(RSP), Immediate(2 * extra_slot));
+    __ movups(Address(CpuRegister(RSP), 0), XmmRegister(XMM0, extra_slot));
+    __ movups(Address(CpuRegister(RSP), extra_slot), XmmRegister(XMM1, extra_slot));
+    __ movups(XmmRegister(XMM0, extra_slot), Address(CpuRegister(RSP), mem2 + 2 * extra_slot));
+    __ movups(XmmRegister(XMM1, extra_slot), Address(CpuRegister(RSP), mem1 + 2 * extra_slot));
+    __ movups(Address(CpuRegister(RSP), mem2 + 2 * extra_slot), XmmRegister(XMM1, extra_slot));
+    __ movups(Address(CpuRegister(RSP), mem1 + 2 * extra_slot), XmmRegister(XMM0, extra_slot));
+    __ movups(XmmRegister(XMM1, extra_slot), Address(CpuRegister(RSP), extra_slot));
+    __ movups(XmmRegister(XMM0, extra_slot), Address(CpuRegister(RSP), 0));
+    __ addq(CpuRegister(RSP), Immediate(2 * extra_slot));
+  } else {
+    ExchangeMemory64(mem1, mem2, extra_slot >> 3);
+  }
+}
+
+
 void ParallelMoveResolverX86_64::EmitSwap(size_t index) {
   MoveOperands* move = moves_[index];
   Location source = move->GetSource();
@@ -6630,7 +6682,7 @@ void ParallelMoveResolverX86_64::EmitSwap(size_t index) {
     Exchange64(destination.AsFPVectorRegister<XmmRegister>(), source.GetStackIndex());
   } else if (source.IsSIMDStackSlot() && destination.IsSIMDStackSlot()) {
     DCHECK_EQ(source.GetVecLen(), destination.GetVecLen());
-    ExchangeMemory64(destination.GetStackIndex(), source.GetStackIndex(), source.GetVecLen() >> 3);
+    ExchangeMemorySIMD(destination.GetStackIndex(), source.GetStackIndex());
   } else if (source.IsFpuRegister() && destination.IsSIMDStackSlot()) {
     DCHECK_EQ(source.GetVecLen(), destination.GetVecLen());
     ExchangeSIMD(source.AsFPVectorRegister<XmmRegister>(), destination.GetStackIndex());
diff --git a/compiler/optimizing/code_generator_x86_64.h b/compiler/optimizing/code_generator_x86_64.h
index 3f366d01ad..70801910bd 100644
--- a/compiler/optimizing/code_generator_x86_64.h
+++ b/compiler/optimizing/code_generator_x86_64.h
@@ -210,6 +210,7 @@ class ParallelMoveResolverX86_64 : public ParallelMoveResolverWithSwap {
   void ExchangeFPReg(XmmRegister reg1, XmmRegister reg2);
   void ExchangeMemory32(int mem1, int mem2);
   void ExchangeMemory64(int mem1, int mem2, int num_of_qwords);
+  void ExchangeMemorySIMD(int mem1, int mem2);
 
   CodeGeneratorX86_64* const codegen_;
 
@@ -434,7 +435,7 @@ class CodeGeneratorX86_64 : public CodeGenerator {
   }
 
   size_t GetSIMDRegisterWidth() const override {
-    return 2 * kX86_64WordSize;
+    return GetInstructionSetFeatures().HasAVX2() ? 4 * kX86_64WordSize : 2 * kX86_64WordSize;
   }
 
   bool HasOverlappingFPVecRegisters() const override { return true; }
diff --git a/compiler/optimizing/loop_analysis.cc b/compiler/optimizing/loop_analysis.cc
index b3f9e835de..d6bea11047 100644
--- a/compiler/optimizing/loop_analysis.cc
+++ b/compiler/optimizing/loop_analysis.cc
@@ -354,6 +354,10 @@ class X86_64LoopHelper : public ArchDefaultLoopHelper {
 
     return unroll_factor;
   }
+
+  bool NeedsVectorRegisterClear() const override {
+    return codegen_.GetSIMDRegisterWidth() > (2 * codegen_.GetWordSize());
+  }
 };
 
 uint32_t X86_64LoopHelper::GetUnrollingFactor(HLoopInformation* loop_info,
diff --git a/compiler/optimizing/loop_analysis.h b/compiler/optimizing/loop_analysis.h
index cd8f00588d..957b1ee2a1 100644
--- a/compiler/optimizing/loop_analysis.h
+++ b/compiler/optimizing/loop_analysis.h
@@ -182,6 +182,8 @@ class ArchNoOptsLoopHelper : public ArenaObject<kArenaAllocOptimization> {
     return LoopAnalysisInfo::kNoUnrollingFactor;
   }
 
+  virtual bool NeedsVectorRegisterClear() const { return false; }
+
  protected:
   const CodeGenerator& codegen_;
 };
diff --git a/compiler/optimizing/loop_optimization.cc b/compiler/optimizing/loop_optimization.cc
index 8d698d6c82..b0b6536ef8 100644
--- a/compiler/optimizing/loop_optimization.cc
+++ b/compiler/optimizing/loop_optimization.cc
@@ -1515,6 +1515,19 @@ void HLoopOptimization::VectorizeTraditional(LoopNode* node,
                                        LoopAnalysisInfo::kNoUnrollingFactor);
   }
 
+#if defined(ART_ENABLE_CODEGEN_x86_64)
+  if (arch_loop_helper_->NeedsVectorRegisterClear()) {
+    // No more BB are inserted at the exit of vloop
+    // Add the HX86Clear at the exit of the vloop
+    // This is required for X86 when switching out of AVX2 context
+    HBasicBlock* vloop_header = preheader_for_vector_loop->GetSingleSuccessor();
+    DCHECK(vloop_header);
+    HBasicBlock* vloop_exit = vloop_header->GetSuccessors()[0];
+    vloop_exit->InsertInstructionBefore(new (global_allocator_) HX86Clear(),
+                                        vloop_exit->GetLastInstruction());
+  }
+#endif
+
   FinalizeVectorization(node);
 }
 
@@ -2124,6 +2137,9 @@ bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrict
       // Allow vectorization for SSE4.1-enabled X86 devices only (128-bit SIMD).
       *restrictions |= kNoIfCond;
       if (features->AsX86InstructionSetFeatures()->HasSSE4_1()) {
+        bool bSupportedType = true;
+        size_t vector_length = simd_register_size_ / DataType::Size(type);
+        DCHECK_EQ(simd_register_size_ % DataType::Size(type), 0u);
         switch (type) {
           case DataType::Type::kBool:
           case DataType::Type::kUint8:
@@ -2136,7 +2152,7 @@ bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrict
                              kNoUnroundedHAdd |
                              kNoSAD |
                              kNoDotProd;
-            return TrySetVectorLength(type, 16);
+            break;
           case DataType::Type::kUint16:
             *restrictions |= kNoDiv |
                              kNoAbs |
@@ -2144,29 +2160,37 @@ bool HLoopOptimization::TrySetVectorType(DataType::Type type, uint64_t* restrict
                              kNoUnroundedHAdd |
                              kNoSAD |
                              kNoDotProd;
-            return TrySetVectorLength(type, 8);
+            break;
           case DataType::Type::kInt16:
             *restrictions |= kNoDiv |
                              kNoAbs |
                              kNoSignedHAdd |
                              kNoUnroundedHAdd |
                              kNoSAD;
-            return TrySetVectorLength(type, 8);
+            break;
           case DataType::Type::kInt32:
             *restrictions |= kNoDiv | kNoSAD;
-            return TrySetVectorLength(type, 4);
+            break;
           case DataType::Type::kInt64:
             *restrictions |= kNoMul | kNoDiv | kNoShr | kNoAbs | kNoSAD;
-            return TrySetVectorLength(type, 2);
+            break;
           case DataType::Type::kFloat32:
             *restrictions |= kNoReduction;
-            return TrySetVectorLength(type, 4);
+            break;
           case DataType::Type::kFloat64:
             *restrictions |= kNoReduction;
-            return TrySetVectorLength(type, 2);
+            break;
           default:
+            bSupportedType = false;
             break;
         }  // switch type
+        if (bSupportedType) {
+          // Remove ABS restriction for 64-bit
+          if (compiler_options_->GetInstructionSet() == InstructionSet::kX86_64) {
+            *restrictions &= ~kNoAbs;
+          }
+          return TrySetVectorLength(type, vector_length);
+        }
       }
       return false;
     default:
diff --git a/compiler/optimizing/nodes.h b/compiler/optimizing/nodes.h
index 1e3aca64db..3a0032d977 100644
--- a/compiler/optimizing/nodes.h
+++ b/compiler/optimizing/nodes.h
@@ -1691,7 +1691,11 @@ class HLoopInformationOutwardIterator : public ValueObject {
 #define FOR_EACH_CONCRETE_INSTRUCTION_X86_COMMON(M)
 #endif
 
+#if defined(ART_ENABLE_CODEGEN_x86_64)
+#define FOR_EACH_CONCRETE_INSTRUCTION_X86_64(M) M(X86Clear, Instruction)
+#else
 #define FOR_EACH_CONCRETE_INSTRUCTION_X86_64(M)
+#endif
 
 #define FOR_EACH_CONCRETE_INSTRUCTION(M)                                \
   FOR_EACH_CONCRETE_INSTRUCTION_COMMON(M)                               \
@@ -2453,18 +2457,16 @@ class HInstruction : public ArenaObject<kArenaAllocInstruction> {
   }
 
   bool IsRemovable() const {
-    return
-        !DoesAnyWrite() &&
-        // TODO(solanes): Merge calls from IsSuspendCheck to IsControlFlow into one that doesn't
-        // do virtual dispatching.
-        !IsSuspendCheck() &&
-        !IsNop() &&
-        !IsParameterValue() &&
-        // If we added an explicit barrier then we should keep it.
-        !IsMemoryBarrier() &&
-        !IsConstructorFence() &&
-        !IsControlFlow() &&
-        !CanThrow();
+    return !DoesAnyWrite() &&
+           // TODO(solanes): Merge calls from IsSuspendCheck to IsControlFlow into one that doesn't
+           // do virtual dispatching.
+           !IsSuspendCheck() && !IsNop() && !IsParameterValue() &&
+           // If we added an explicit barrier then we should keep it.
+           !IsMemoryBarrier() && !IsConstructorFence() && !IsControlFlow() &&
+#if defined(ART_ENABLE_CODEGEN_x86_64)
+           !IsX86Clear() &&
+#endif
+           !CanThrow();
   }
 
   bool IsDeadAndRemovable() const {
diff --git a/compiler/optimizing/nodes_x86.h b/compiler/optimizing/nodes_x86.h
index 14d9823355..898b6ea6dc 100644
--- a/compiler/optimizing/nodes_x86.h
+++ b/compiler/optimizing/nodes_x86.h
@@ -214,6 +214,17 @@ class HX86MaskOrResetLeastSetBit final : public HUnaryOperation {
   DEFAULT_COPY_CONSTRUCTOR(X86MaskOrResetLeastSetBit);
 };
 
+class HX86Clear final : public HExpression<0> {
+ public:
+  explicit HX86Clear(uint32_t dex_pc = kNoDexPc)
+      : HExpression(kX86Clear, SideEffects::None(), dex_pc) {}
+
+  DECLARE_INSTRUCTION(X86Clear);
+
+ protected:
+  DEFAULT_COPY_CONSTRUCTOR(X86Clear);
+};
+
 }  // namespace art
 
 #endif  // ART_COMPILER_OPTIMIZING_NODES_X86_H_
diff --git a/test/530-checker-lse-simd/src/Main.java b/test/530-checker-lse-simd/src/Main.java
index c9be5a6440..99efbbee79 100644
--- a/test/530-checker-lse-simd/src/Main.java
+++ b/test/530-checker-lse-simd/src/Main.java
@@ -87,6 +87,12 @@ public class Main {
   /// CHECK-NEXT: BelowOrEqual
   //
   /// CHECK:      ArrayGet loop:none
+  /// CHECK-IF:   hasIsaFeature("avx2")
+    /// CHECK-IF: isIsa("x86_64")
+      // Incase of avx2 we must generate a HX86Clear just before return
+      /// CHECK: X86Clear
+    /// CHECK-FI:
+  /// CHECK-FI:
   /// CHECK-NEXT: Return
 
   /// CHECK-START: double Main.$noinline$test02(double[], int) load_store_elimination (after)
@@ -131,6 +137,12 @@ public class Main {
   //
   /// CHECK-START: double Main.$noinline$test03(int) load_store_elimination (before)
   /// CHECK:      ArrayGet loop:none
+  /// CHECK-IF:   hasIsaFeature("avx2")
+    /// CHECK-IF: isIsa("x86_64")
+      // Incase of avx2 we must generate a HX86Clear just before return
+      /// CHECK: X86Clear
+    /// CHECK-FI:
+  /// CHECK-FI:
   /// CHECK-NEXT: Return
 
   /// CHECK-START: double Main.$noinline$test03(int) load_store_elimination (after)
diff --git a/test/656-checker-simd-opt/src/Main.java b/test/656-checker-simd-opt/src/Main.java
index 6b08d6352e..478d4522a8 100644
--- a/test/656-checker-simd-opt/src/Main.java
+++ b/test/656-checker-simd-opt/src/Main.java
@@ -45,8 +45,15 @@ public class Main {
   //
   /// CHECK-ELSE:
   //
-  ///     CHECK-DAG: <<Incr:i\d+>>  IntConstant 4                        loop:none
-  ///     CHECK-DAG: <<Repl:d\d+>>  VecReplicateScalar [<<Cons>>]        loop:none
+  //      Check 256-bit & 128-bit vectorization
+  ///   CHECK-IF: hasIsaFeature("avx2")
+  ///       CHECK-DAG: <<Incr:i\d+>>  IntConstant 8                        loop:none
+  ///       CHECK-DAG: <<Repl:d\d+>>  VecReplicateScalar [<<Cons>>]        loop:none
+  ///   CHECK-ELSE:
+  ///       CHECK-DAG: <<Incr:i\d+>>  IntConstant 4                        loop:none
+  ///       CHECK-DAG: <<Repl:d\d+>>  VecReplicateScalar [<<Cons>>]        loop:none
+  ///   CHECK-FI:
+  //
   ///     CHECK-NOT:                VecReplicateScalar
   ///     CHECK-DAG: <<Phi:i\d+>>   Phi                                  loop:<<Loop:B\d+>> outer_loop:none
   ///     CHECK-DAG: <<Get1:d\d+>>  VecLoad [{{l\d+}},<<Phi>>]           loop:<<Loop>>      outer_loop:none
@@ -290,8 +297,15 @@ public class Main {
   //
   /// CHECK-ELSE:
   //
-  ///     CHECK-DAG: <<L2:j\d+>>    LongConstant 2               loop:none
-  ///     CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Get>>] loop:none
+  //    Check 256-bit & 128-bit vectorization
+  ///   CHECK-IF: hasIsaFeature("avx2")
+  ///       CHECK-DAG: <<L2:j\d+>>    LongConstant 4               loop:none
+  ///       CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Get>>] loop:none
+  ///   CHECK-ELSE:
+  ///       CHECK-DAG: <<L2:j\d+>>    LongConstant 2               loop:none
+  ///       CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Get>>] loop:none
+  ///   CHECK-FI:
+  //
   ///     CHECK-DAG: <<Set:d\d+>>   VecSetScalars [<<L1>>]       loop:none
   ///     CHECK-DAG: <<Phi1:j\d+>>  Phi [<<L0>>,{{j\d+}}]        loop:<<Loop:B\d+>> outer_loop:none
   ///     CHECK-DAG: <<Phi2:d\d+>>  Phi [<<Set>>,{{d\d+}}]       loop:<<Loop>>      outer_loop:none
@@ -331,8 +345,15 @@ public class Main {
   //
   /// CHECK-ELSE:
   //
-  ///     CHECK-DAG: <<I4:i\d+>>    IntConstant 4                       loop:none
-  ///     CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Cnv>>]        loop:none
+  //    Check 256-bit & 128-bit vectorization
+  ///   CHECK-IF: hasIsaFeature("avx2")
+  ///       CHECK-DAG: <<I4:i\d+>>    IntConstant 8                       loop:none
+  ///       CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Cnv>>]        loop:none
+  ///   CHECK-ELSE:
+  ///       CHECK-DAG: <<I4:i\d+>>    IntConstant 4                       loop:none
+  ///       CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Cnv>>]        loop:none
+  ///   CHECK-FI:
+  //
   ///     CHECK-DAG: <<Phi:i\d+>>   Phi [<<I0>>,{{i\d+}}]               loop:<<Loop:B\d+>> outer_loop:none
   ///     CHECK-DAG:                VecStore [{{l\d+}},<<Phi>>,<<Rep>>] loop:<<Loop>>      outer_loop:none
   ///     CHECK-DAG:                Add [<<Phi>>,<<I4>>]                loop:<<Loop>>      outer_loop:none
@@ -372,8 +393,15 @@ public class Main {
   //
   /// CHECK-ELSE:
   //
-  ///     CHECK-DAG: <<I4:i\d+>>    IntConstant 4                       loop:none
-  ///     CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Cnv>>]        loop:none
+  //    Check 256-bit & 128-bit vectorization
+  ///   CHECK-IF: hasIsaFeature("avx2")
+  ///       CHECK-DAG: <<I4:i\d+>>    IntConstant 8                       loop:none
+  ///       CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Cnv>>]        loop:none
+  ///   CHECK-ELSE:
+  ///       CHECK-DAG: <<I4:i\d+>>    IntConstant 4                       loop:none
+  ///       CHECK-DAG: <<Rep:d\d+>>   VecReplicateScalar [<<Cnv>>]        loop:none
+  ///   CHECK-FI:
+  //
   ///     CHECK-DAG: <<Phi:i\d+>>   Phi [<<I0>>,{{i\d+}}]               loop:<<Loop:B\d+>> outer_loop:none
   ///     CHECK-DAG: <<Load:d\d+>>  VecLoad [{{l\d+}},<<Phi>>]          loop:<<Loop>>      outer_loop:none
   ///     CHECK-DAG: <<Add:d\d+>>   VecAdd [<<Load>>,<<Rep>>]           loop:<<Loop>>      outer_loop:none
diff --git a/tools/checker/file_format/c1visualizer/parser.py b/tools/checker/file_format/c1visualizer/parser.py
index 26087cfdf0..408e6c221c 100644
--- a/tools/checker/file_format/c1visualizer/parser.py
+++ b/tools/checker/file_format/c1visualizer/parser.py
@@ -61,7 +61,13 @@ def _parse_c1_line(c1_file, line, line_no, state, filename):
       if not method_name:
         Logger.fail("Empty method name in output", filename, line_no)
 
-      match = re.search(r"isa_features:([\w,-]+)", method_name)
+      # Gather isa info as well
+      match = re.search(r"isa:(\w+)", method_name)
+      if match:
+        c1_file.set_isa(match.group(1))
+
+      # Some features may contain a '.' like sse4.1
+      match = re.search(r"isa_features:([\w\.?\w,-]+)", method_name)
       if match:
         raw_features = match.group(1).split(",")
         # Create a map of features in the form {feature_name: is_enabled}.
diff --git a/tools/checker/file_format/c1visualizer/struct.py b/tools/checker/file_format/c1visualizer/struct.py
index 7f3032016c..1d7c270f36 100644
--- a/tools/checker/file_format/c1visualizer/struct.py
+++ b/tools/checker/file_format/c1visualizer/struct.py
@@ -24,9 +24,13 @@ class C1visualizerFile(PrintableMixin):
     self.base_file_name = os.path.basename(filename)
     self.full_file_name = filename
     self.passes = []
+    self.isa = ""
     self.instruction_set_features = ImmutableDict()
     self.read_barrier_type = "none"
 
+  def set_isa(self, isa):
+    self.isa = isa
+
   def set_isa_features(self, features):
     self.instruction_set_features = ImmutableDict(features)
 
diff --git a/tools/checker/match/file.py b/tools/checker/match/file.py
index 33bb9d1abe..df2984dbe8 100644
--- a/tools/checker/match/file.py
+++ b/tools/checker/match/file.py
@@ -321,6 +321,7 @@ class ExecutionState(object):
 def match_test_case(
     test_case,
     c1_pass,
+    isa,
     instruction_set_features,
     read_barrier_type):
   """ Runs a test case against a C1visualizer graph dump.
@@ -330,6 +331,7 @@ def match_test_case(
   assert test_case.name == c1_pass.name
 
   initial_variables = {
+      "ISA": isa,
       "ISA_FEATURES": instruction_set_features,
       "READ_BARRIER_TYPE": read_barrier_type
   }
@@ -361,6 +363,7 @@ def match_files(checker_file, c1_file, target_arch, debuggable_mode, print_cfg):
       match_test_case(
           test_case,
           c1_pass,
+          c1_file.isa,
           c1_file.instruction_set_features,
           c1_file.read_barrier_type)
       Logger.test_passed()
diff --git a/tools/checker/match/line.py b/tools/checker/match/line.py
index 987ae61186..14b2eafa1d 100644
--- a/tools/checker/match/line.py
+++ b/tools/checker/match/line.py
@@ -128,6 +128,7 @@ def get_eval_text(expression, variables, pos):
 def evaluate_line(checker_line, variables):
   assert checker_line.is_eval_content_statement()
   # Required for eval.
+  isIsa = lambda isa: variables["ISA"] == isa
   hasIsaFeature = lambda feature: variables["ISA_FEATURES"].get(feature, False)
   readBarrierType = lambda barrier_type: variables["READ_BARRIER_TYPE"] == barrier_type
   eval_string = "".join(get_eval_text(expr,
-- 
2.25.1

