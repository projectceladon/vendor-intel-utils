From cfe601bd0333ce209fff5c3b5c53f663af3916cf Mon Sep 17 00:00:00 2001
From: Vinay Kompella <vinay.kompella@intel.com>
Date: Mon, 20 Jul 2020 15:35:16 +0530
Subject: [PATCH] Introduction of 3-Gen concurrent copying collector

This patch seggregates the region space into 3 generations,
and introduces a new GC cycle (mid-term cycle) to improve
the overall throughput of the concurrent copying collector.

Change-Id: I8ee36c95f044b50125706839ac78bfe7b0002af5
Tracked-On:
Signed-off-by: Vinay Kompella <vinay.kompella@intel.com>
---
 cmdline/cmdline_types.h                       |   6 +
 runtime/gc/accounting/remembered_set.cc       |  69 +-
 runtime/gc/accounting/remembered_set.h        |  42 +-
 runtime/gc/collector/concurrent_copying-inl.h |  28 +-
 runtime/gc/collector/concurrent_copying.cc    | 693 +++++++++++++++---
 runtime/gc/collector/concurrent_copying.h     |  62 +-
 runtime/gc/collector/gc_type.h                |   3 +
 runtime/gc/heap-inl.h                         |   1 +
 runtime/gc/heap.cc                            | 104 ++-
 runtime/gc/heap.h                             |  26 +-
 runtime/gc/space/region_space-inl.h           | 187 +++--
 runtime/gc/space/region_space.cc              | 129 +++-
 runtime/gc/space/region_space.h               | 191 ++++-
 runtime/parsed_options.cc                     |   4 +
 runtime/runtime.cc                            |   6 +
 runtime/runtime.h                             |  11 +
 runtime/runtime_options.def                   |   1 +
 17 files changed, 1292 insertions(+), 271 deletions(-)

diff --git a/cmdline/cmdline_types.h b/cmdline/cmdline_types.h
index dd9221d6f8..9fc1c6bfb9 100644
--- a/cmdline/cmdline_types.h
+++ b/cmdline/cmdline_types.h
@@ -428,6 +428,7 @@ struct XGcOption {
   bool verify_pre_gc_heap_ = false;
   bool verify_pre_sweeping_heap_ = kIsDebugBuild;
   bool generational_cc = kEnableGenerationalCCByDefault;
+  bool midterm_cc = generational_cc;
   bool verify_post_gc_heap_ = false;
   bool verify_pre_gc_rosalloc_ = kIsDebugBuild;
   bool verify_pre_sweeping_rosalloc_ = false;
@@ -470,6 +471,10 @@ struct CmdlineType<XGcOption> : CmdlineTypeParser<XGcOption> {
         // for compatibility reasons (this should not prevent the runtime from
         // starting up).
         xgc.generational_cc = false;
+      } else if (gc_option == "midterm_cc") {
+        xgc.midterm_cc = true;
+      } else if (gc_option == "nomidterm_cc") {
+        xgc.midterm_cc = false;
       } else if (gc_option == "postverify") {
         xgc.verify_post_gc_heap_ = true;
       } else if (gc_option == "nopostverify") {
@@ -502,6 +507,7 @@ struct CmdlineType<XGcOption> : CmdlineTypeParser<XGcOption> {
       }
     }
 
+    xgc.midterm_cc &= xgc.generational_cc;
     return Result::Success(std::move(xgc));
   }
 
diff --git a/runtime/gc/accounting/remembered_set.cc b/runtime/gc/accounting/remembered_set.cc
index fba62c3d67..3ec58f0db7 100644
--- a/runtime/gc/accounting/remembered_set.cc
+++ b/runtime/gc/accounting/remembered_set.cc
@@ -110,40 +110,40 @@ class RememberedSetReferenceVisitor {
   bool* const contains_reference_to_target_space_;
 };
 
-class RememberedSetObjectVisitor {
+class RememberedSetDefaultObjectVisitor : public RememberedSetObjectVisitor {
  public:
-  RememberedSetObjectVisitor(space::ContinuousSpace* target_space,
-                             bool* const contains_reference_to_target_space,
-                             collector::GarbageCollector* collector)
-      : collector_(collector), target_space_(target_space),
-        contains_reference_to_target_space_(contains_reference_to_target_space) {}
+  RememberedSetDefaultObjectVisitor() {}
+  virtual ~RememberedSetDefaultObjectVisitor() {}
 
-  void operator()(ObjPtr<mirror::Object> obj) const REQUIRES(Locks::heap_bitmap_lock_)
+  inline void operator()(ObjPtr<mirror::Object> obj) const
+      REQUIRES(Locks::heap_bitmap_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    RememberedSetReferenceVisitor visitor(target_space_, contains_reference_to_target_space_,
+    bool contains_ref_to_target_space = false;
+    RememberedSetReferenceVisitor visitor(target_space_, &contains_ref_to_target_space,
                                           collector_);
     obj->VisitReferences(visitor, visitor);
+    UpdateRefsToTargetSpace(contains_ref_to_target_space);
   }
-
- private:
-  collector::GarbageCollector* const collector_;
-  space::ContinuousSpace* const target_space_;
-  bool* const contains_reference_to_target_space_;
 };
 
 void RememberedSet::UpdateAndMarkReferences(space::ContinuousSpace* target_space,
-                                            collector::GarbageCollector* collector) {
+                                            collector::GarbageCollector* collector,
+                                            RememberedSetObjectVisitor* obj_visitor) {
+  RememberedSetDefaultObjectVisitor rem_set_default_visitor;
   CardTable* card_table = heap_->GetCardTable();
   bool contains_reference_to_target_space = false;
-  RememberedSetObjectVisitor obj_visitor(target_space, &contains_reference_to_target_space,
-                                         collector);
+  if (obj_visitor == nullptr) {
+    obj_visitor = &rem_set_default_visitor;
+  }
+  obj_visitor->Init(target_space, &contains_reference_to_target_space,
+                    collector);
   ContinuousSpaceBitmap* bitmap = space_->GetLiveBitmap();
   CardSet remove_card_set;
   for (uint8_t* const card_addr : dirty_cards_) {
     contains_reference_to_target_space = false;
     uintptr_t start = reinterpret_cast<uintptr_t>(card_table->AddrFromCard(card_addr));
     DCHECK(space_->HasAddress(reinterpret_cast<mirror::Object*>(start)));
-    bitmap->VisitMarkedRange(start, start + CardTable::kCardSize, obj_visitor);
+    bitmap->VisitMarkedRange(start, start + CardTable::kCardSize, *obj_visitor);
     if (!contains_reference_to_target_space) {
       // It was in the dirty card set, but it didn't actually contain
       // a reference to the target space. So, remove it from the dirty
@@ -182,6 +182,41 @@ void RememberedSet::AssertAllDirtyCardsAreWithinSpace() const {
   }
 }
 
+void RememberedSet::DropCardRange(uint8_t* start, uint8_t* end) {
+  CardTable* card_table = heap_->GetCardTable();
+  CardSet remove_card_set;
+  for (uint8_t* const card_addr : dirty_cards_) {
+    auto obj = reinterpret_cast<mirror::Object*>(card_table->AddrFromCard(card_addr));
+    if (obj >= reinterpret_cast<mirror::Object*>(start) &&
+        obj < reinterpret_cast<mirror::Object*>(end)) {
+      remove_card_set.insert(card_addr);
+    }
+  }
+
+  for (uint8_t* const card_addr : remove_card_set) {
+    DCHECK(dirty_cards_.find(card_addr) != dirty_cards_.end());
+    dirty_cards_.erase(card_addr);
+  }
+  remove_card_set.clear();
+}
+
+void RememberedSet::AddDirtyCard(uint8_t* card) {
+  if (dirty_cards_.find(card) == dirty_cards_.end()) {
+    mirror::Object* start = reinterpret_cast<mirror::Object*>(
+        heap_->GetCardTable()->AddrFromCard(card));
+    DCHECK(space_->HasAddress(start));
+    dirty_cards_.insert(card);
+  }
+}
+
+bool RememberedSet::ContainsCard(uint8_t* card) {
+  return (dirty_cards_.find(card) != dirty_cards_.end());
+}
+
+void RememberedSet::Empty() {
+  dirty_cards_.clear();
+}
+
 }  // namespace accounting
 }  // namespace gc
 }  // namespace art
diff --git a/runtime/gc/accounting/remembered_set.h b/runtime/gc/accounting/remembered_set.h
index 3525667534..f2c5e2cd97 100644
--- a/runtime/gc/accounting/remembered_set.h
+++ b/runtime/gc/accounting/remembered_set.h
@@ -21,11 +21,16 @@
 #include "base/locks.h"
 #include "base/safe_map.h"
 #include "runtime_globals.h"
+#include "obj_ptr.h"
 
 #include <set>
 #include <vector>
 
 namespace art {
+namespace mirror {
+class Object;
+}
+
 namespace gc {
 
 namespace collector {
@@ -40,6 +45,36 @@ class Heap;
 
 namespace accounting {
 
+class RememberedSetObjectVisitor {
+ public:
+  RememberedSetObjectVisitor()
+    :collector_(nullptr), target_space_(nullptr),
+     contains_reference_to_target_space_(nullptr) {}
+  virtual ~RememberedSetObjectVisitor() { }
+  void Init(space::ContinuousSpace* target_space,
+            bool* const contains_reference_to_target_space,
+            collector::GarbageCollector* collector) {
+    collector_ = collector;
+    target_space_ = target_space;
+    contains_reference_to_target_space_ = contains_reference_to_target_space;
+  }
+  inline virtual void operator()(ObjPtr<mirror::Object> obj) const
+    REQUIRES(Locks::heap_bitmap_lock_)
+    REQUIRES_SHARED(Locks::mutator_lock_) = 0;
+
+  // This setter ensures, no subclass sets the variable to false directly.
+  inline void UpdateRefsToTargetSpace(bool contains_ref_to_target_space) const {
+    *contains_reference_to_target_space_ |= contains_ref_to_target_space;
+  }
+
+ protected:
+  collector::GarbageCollector* collector_;
+  space::ContinuousSpace* target_space_;
+
+ private:
+  bool* contains_reference_to_target_space_;
+};
+
 // The remembered set keeps track of cards that may contain references
 // from the free list spaces to the bump pointer spaces.
 class RememberedSet {
@@ -52,13 +87,18 @@ class RememberedSet {
 
   // Clear dirty cards and add them to the dirty card set.
   void ClearCards();
+  bool ContainsCard(uint8_t* card);
 
   // Mark through all references to the target space.
   void UpdateAndMarkReferences(space::ContinuousSpace* target_space,
-                               collector::GarbageCollector* collector)
+                               collector::GarbageCollector* collector,
+                               RememberedSetObjectVisitor* visitor = nullptr)
       REQUIRES(Locks::heap_bitmap_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_);
 
+  void DropCardRange(uint8_t* start, uint8_t* end);
+  void AddDirtyCard(uint8_t* card);
+  void Empty();
   void Dump(std::ostream& os);
 
   space::ContinuousSpace* GetSpace() {
diff --git a/runtime/gc/collector/concurrent_copying-inl.h b/runtime/gc/collector/concurrent_copying-inl.h
index 2de79107f4..8fb5fd5216 100644
--- a/runtime/gc/collector/concurrent_copying-inl.h
+++ b/runtime/gc/collector/concurrent_copying-inl.h
@@ -39,7 +39,7 @@ inline mirror::Object* ConcurrentCopying::MarkUnevacFromSpaceRegion(
   if (use_generational_cc_ && !done_scanning_.load(std::memory_order_acquire)) {
     // Everything in the unevac space should be marked for young generation CC,
     // except for large objects.
-    DCHECK(!young_gen_ || region_space_bitmap_->Test(ref) || region_space_->IsLargeObject(ref))
+    DCHECK(!IsYoungGC() || region_space_bitmap_->Test(ref) || region_space_->IsLargeObject(ref))
         << ref << " "
         << ref->GetClass<kVerifyNone, kWithoutReadBarrier>()->PrettyClass();
     // Since the mark bitmap is still filled in from last GC (or from marking phase of 2-phase CC,
@@ -123,13 +123,16 @@ inline mirror::Object* ConcurrentCopying::MarkImmuneSpace(Thread* const self,
   return ref;
 }
 
-template<bool kGrayImmuneObject, bool kNoUnEvac, bool kFromGCThread>
+template<bool kGrayImmuneObject,
+         ConcurrentCopying::RegionTypeFlag kRegionTypeFlag,
+         bool kFromGCThread>
 inline mirror::Object* ConcurrentCopying::Mark(Thread* const self,
                                                mirror::Object* from_ref,
                                                mirror::Object* holder,
                                                MemberOffset offset) {
-  // Cannot have `kNoUnEvac` when Generational CC collection is disabled.
-  DCHECK(!kNoUnEvac || use_generational_cc_);
+  // Cannot have `kIgnoreUnEvac` when Generational CC collection is disabled.
+  DCHECK((kRegionTypeFlag == RegionTypeFlag::kIgnoreNone) || use_generational_cc_);
+  DCHECK((kRegionTypeFlag != RegionTypeFlag::kIgnoreUnevacFromSpaceTenured) || use_midterm_cc_);
   if (from_ref == nullptr) {
     return nullptr;
   }
@@ -170,8 +173,18 @@ inline mirror::Object* ConcurrentCopying::Mark(Thread* const self,
             << "from_ref=" << from_ref << " to_ref=" << to_ref;
         return to_ref;
       }
+      case space::RegionSpace::RegionType::kRegionTypeUnevacFromSpaceTenured:
+        DCHECK(use_midterm_cc_);
+        if (kRegionTypeFlag != RegionTypeFlag::kIgnoreNone) {
+          if (!kFromGCThread) {
+            DCHECK(IsMarkedInUnevacFromSpace(from_ref)) << "Returning unmarked object to mutator";
+          }
+          return from_ref;
+        }
+        return MarkUnevacFromSpaceRegion(self, from_ref, region_space_bitmap_);
       case space::RegionSpace::RegionType::kRegionTypeUnevacFromSpace:
-        if (kNoUnEvac && use_generational_cc_ && !region_space_->IsLargeObject(from_ref)) {
+        if (kRegionTypeFlag == RegionTypeFlag::kIgnoreUnevacFromSpace && use_generational_cc_ &&
+            !region_space_->IsLargeAndYoungObjectUnsafe(from_ref)) {
           if (!kFromGCThread) {
             DCHECK(IsMarkedInUnevacFromSpace(from_ref)) << "Returning unmarked object to mutator";
           }
@@ -207,8 +220,9 @@ inline mirror::Object* ConcurrentCopying::MarkFromReadBarrier(mirror::Object* fr
   if (UNLIKELY(mark_from_read_barrier_measurements_)) {
     ret = MarkFromReadBarrierWithMeasurements(self, from_ref);
   } else {
-    ret = Mark</*kGrayImmuneObject=*/true, /*kNoUnEvac=*/false, /*kFromGCThread=*/false>(self,
-                                                                                         from_ref);
+    ret = Mark</*kGrayImmuneObject=*/true, RegionTypeFlag::kIgnoreNone,
+               /*kFromGCThread=*/false>(self,
+                                        from_ref);
   }
   // Only set the mark bit for baker barrier.
   if (kUseBakerReadBarrier && LIKELY(!rb_mark_bit_stack_full_ && ret->AtomicSetMarkBit(0, 1))) {
diff --git a/runtime/gc/collector/concurrent_copying.cc b/runtime/gc/collector/concurrent_copying.cc
index 9428a0b8cd..3b7e2a6a73 100644
--- a/runtime/gc/collector/concurrent_copying.cc
+++ b/runtime/gc/collector/concurrent_copying.cc
@@ -31,6 +31,7 @@
 #include "gc/accounting/mod_union_table-inl.h"
 #include "gc/accounting/read_barrier_table.h"
 #include "gc/accounting/space_bitmap-inl.h"
+#include "gc/accounting/remembered_set.h"
 #include "gc/gc_pause_listener.h"
 #include "gc/reference_processor.h"
 #include "gc/space/image_space.h"
@@ -67,9 +68,15 @@ static constexpr size_t kSweepArrayChunkFreeSize = 1024;
 // Verify that there are no missing card marks.
 static constexpr bool kVerifyNoMissingCardMarks = kIsDebugBuild;
 
+uint8_t ConcurrentCopying::tenure_threshold_ = space::RegionSpace::kRegionMaxAgeTenureThreshold;
+
+using RegionGen = space::RegionSpace::RegionGen;
+
 ConcurrentCopying::ConcurrentCopying(Heap* heap,
-                                     bool young_gen,
+                                     GcType type,
                                      bool use_generational_cc,
+                                     bool use_midterm_cc,
+                                     bool use_dynamic_threshold,
                                      const std::string& name_prefix,
                                      bool measure_read_barrier_slow_path)
     : GarbageCollector(heap,
@@ -81,7 +88,9 @@ ConcurrentCopying::ConcurrentCopying(Heap* heap,
                                                      kDefaultGcMarkStackSize,
                                                      kDefaultGcMarkStackSize)),
       use_generational_cc_(use_generational_cc),
-      young_gen_(young_gen),
+      use_midterm_cc_(use_midterm_cc),
+      type_(type),
+      use_dynamic_threshold_(use_dynamic_threshold),
       rb_mark_bit_stack_(accounting::ObjectStack::Create("rb copying gc mark stack",
                                                          kReadBarrierMarkStackSize,
                                                          kReadBarrierMarkStackSize)),
@@ -119,10 +128,14 @@ ConcurrentCopying::ConcurrentCopying(Heap* heap,
       gc_grays_immune_objects_(false),
       immune_gray_stack_lock_("concurrent copying immune gray stack lock",
                               kMarkSweepMarkStackLock),
-      num_bytes_allocated_before_gc_(0) {
+      num_bytes_allocated_before_gc_(0),
+      rebuild_rem_sets_during_mark_phase_(false) {
   static_assert(space::RegionSpace::kRegionSize == accounting::ReadBarrierTable::kRegionSize,
                 "The region space size and the read barrier table region size must match");
-  CHECK(use_generational_cc_ || !young_gen_);
+  CHECK(IsYoungGC() || IsMidTermGC() || IsFullGC()) << "Unsupported GcType -"<<type_<<" for CC";
+  CHECK(use_generational_cc_ || !IsYoungGC());
+  CHECK(use_midterm_cc_ || !IsMidTermGC());
+  CHECK(!use_dynamic_threshold_ || use_midterm_cc_);
   Thread* self = Thread::Current();
   {
     ReaderMutexLock mu(self, *Locks::heap_bitmap_lock_);
@@ -197,7 +210,7 @@ void ConcurrentCopying::RunPhases() {
     InitializePhase();
     // In case of forced evacuation, all regions are evacuated and hence no
     // need to compute live_bytes.
-    if (use_generational_cc_ && !young_gen_ && !force_evacuate_all_) {
+    if (use_generational_cc_ && !IsYoungGC() && !force_evacuate_all_) {
       MarkingPhase();
     }
   }
@@ -331,25 +344,48 @@ void ConcurrentCopying::BindBitmaps() {
       if (use_generational_cc_) {
         if (space == region_space_) {
           region_space_bitmap_ = region_space_->GetMarkBitmap();
-        } else if (young_gen_ && space->IsContinuousMemMapAllocSpace()) {
+          if (IsMidTermGC()) {
+            // Clear the bitmaps for Aged regions.
+            region_space_->ClearBitmap(static_cast<uint8_t>(RegionGen::kRegionGenAged) |
+                                       static_cast<uint8_t>(RegionGen::kRegionGenTenuring));
+          } else if (IsFullGC()) {
+            region_space_bitmap_->Clear();
+          }
+        } else if (!IsFullGC() && space->IsContinuousMemMapAllocSpace()) {
           DCHECK_EQ(space->GetGcRetentionPolicy(), space::kGcRetentionPolicyAlwaysCollect);
           space->AsContinuousMemMapAllocSpace()->BindLiveToMarkBitmap();
         }
-        if (young_gen_) {
-          // Age all of the cards for the region space so that we know which evac regions to scan.
-          heap_->GetCardTable()->ModifyCardsAtomic(space->Begin(),
-                                                   space->End(),
-                                                   AgeCardVisitor(),
-                                                   VoidFunctor());
+        if (use_midterm_cc_) {
+          // We have to remember the dirty cards irrespective of young_gen,
+          // to be able to later use them for inter-gen reference scanning
+          ClearDirtyCards(space);
+
+          if (IsFullGC() && rebuild_rem_sets_during_mark_phase_) {
+            DCHECK_NE(tenure_threshold_, region_space_->GetTenureThreshold());
+            // In a full-heap GC cycle, when we are going to re-build our Remsets,
+            // the card-table corresponding to region-space and
+            // non-moving space can be cleared, because this cycle only needs to
+            // capture writes during the marking phase of this cycle to catch
+            // objects that skipped marking due to heap mutation.
+            heap_->GetCardTable()->ClearCardRange(space->Begin(), space->Limit());
+          }
         } else {
-          // In a full-heap GC cycle, the card-table corresponding to region-space and
-          // non-moving space can be cleared, because this cycle only needs to
-          // capture writes during the marking phase of this cycle to catch
-          // objects that skipped marking due to heap mutation. Furthermore,
-          // if the next GC is a young-gen cycle, then it only needs writes to
-          // be captured after the thread-flip of this GC cycle, as that is when
-          // the young-gen for the next GC cycle starts getting populated.
-          heap_->GetCardTable()->ClearCardRange(space->Begin(), space->Limit());
+          if (IsYoungGC()) {
+            // Age all of the cards for the region space so that we know which evac regions to scan.
+            heap_->GetCardTable()->ModifyCardsAtomic(space->Begin(),
+                                                    space->End(),
+                                                    AgeCardVisitor(),
+                                                    VoidFunctor());
+          } else {
+            // In a full-heap GC cycle, the card-table corresponding to region-space and
+            // non-moving space can be cleared, because this cycle only needs to
+            // capture writes during the marking phase of this cycle to catch
+            // objects that skipped marking due to heap mutation. Furthermore,
+            // if the next GC is a young-gen cycle, then it only needs writes to
+            // be captured after the thread-flip of this GC cycle, as that is when
+            // the young-gen for the next GC cycle starts getting populated.
+            heap_->GetCardTable()->ClearCardRange(space->Begin(), space->Limit());
+          }
         }
       } else {
         if (space == region_space_) {
@@ -361,7 +397,7 @@ void ConcurrentCopying::BindBitmaps() {
       }
     }
   }
-  if (use_generational_cc_ && young_gen_) {
+  if (use_generational_cc_ && !IsFullGC()) {
     for (const auto& space : GetHeap()->GetDiscontinuousSpaces()) {
       CHECK(space->IsLargeObjectSpace());
       space->AsLargeObjectSpace()->CopyLiveToMarked();
@@ -386,6 +422,13 @@ void ConcurrentCopying::InitializePhase() {
     rb_slow_path_count_gc_.store(0, std::memory_order_relaxed);
   }
 
+  if (use_midterm_cc_) {
+    region_space_rem_set_ = heap_->FindRememberedSetFromSpace(region_space_);
+    non_moving_space_rem_set_ = heap_->FindRememberedSetFromSpace(heap_->non_moving_space_);
+    DCHECK(region_space_rem_set_ != nullptr);
+    DCHECK(non_moving_space_rem_set_ != nullptr);
+  }
+
   immune_spaces_.Reset();
   bytes_moved_.store(0, std::memory_order_relaxed);
   objects_moved_.store(0, std::memory_order_relaxed);
@@ -394,7 +437,7 @@ void ConcurrentCopying::InitializePhase() {
   GcCause gc_cause = GetCurrentIteration()->GetGcCause();
 
   force_evacuate_all_ = false;
-  if (!use_generational_cc_ || !young_gen_) {
+  if (!use_generational_cc_ || IsFullGC()) {
     if (gc_cause == kGcCauseExplicit ||
         gc_cause == kGcCauseCollectorTransition ||
         GetCurrentIteration()->GetClearSoftReferences()) {
@@ -415,7 +458,7 @@ void ConcurrentCopying::InitializePhase() {
   }
   BindBitmaps();
   if (kVerboseMode) {
-    LOG(INFO) << "young_gen=" << std::boolalpha << young_gen_ << std::noboolalpha;
+    LOG(INFO) << "GcType=" << type_;
     LOG(INFO) << "force_evacuate_all=" << std::boolalpha << force_evacuate_all_ << std::noboolalpha;
     LOG(INFO) << "Largest immune region: " << immune_spaces_.GetLargestImmuneRegion().Begin()
               << "-" << immune_spaces_.GetLargestImmuneRegion().End();
@@ -424,9 +467,6 @@ void ConcurrentCopying::InitializePhase() {
     }
     LOG(INFO) << "GC end of InitializePhase";
   }
-  if (use_generational_cc_ && !young_gen_) {
-    region_space_bitmap_->Clear();
-  }
   mark_stack_mode_.store(ConcurrentCopying::kMarkStackModeThreadLocal, std::memory_order_relaxed);
   // Mark all of the zygote large objects without graying them.
   MarkZygoteLargeObjects();
@@ -518,15 +558,18 @@ class ConcurrentCopying::FlipCallback : public Closure {
     TimingLogger::ScopedTiming split("(Paused)FlipCallback", cc->GetTimings());
     // Note: self is not necessarily equal to thread since thread may be suspended.
     Thread* self = Thread::Current();
-    if (kVerifyNoMissingCardMarks && cc->young_gen_) {
+    if (kVerifyNoMissingCardMarks && (cc->IsYoungGC() || cc->use_midterm_cc_)) {
       cc->VerifyNoMissingCardMarks();
     }
     CHECK_EQ(thread, self);
     Locks::mutator_lock_->AssertExclusiveHeld(self);
     space::RegionSpace::EvacMode evac_mode = space::RegionSpace::kEvacModeLivePercentNewlyAllocated;
-    if (cc->young_gen_) {
+    if (cc->IsYoungGC()) {
       CHECK(!cc->force_evacuate_all_);
       evac_mode = space::RegionSpace::kEvacModeNewlyAllocated;
+    } else if (cc->use_midterm_cc_ && cc->IsMidTermGC()) {
+      CHECK(!cc->force_evacuate_all_);
+      evac_mode = space::RegionSpace::kEvacModeAgedLivePercentNewlyAllocated;
     } else if (cc->force_evacuate_all_) {
       evac_mode = space::RegionSpace::kEvacModeForceAll;
     }
@@ -657,9 +700,12 @@ void ConcurrentCopying::VerifyGrayImmuneObjects() {
 
 class ConcurrentCopying::VerifyNoMissingCardMarkVisitor {
  public:
-  VerifyNoMissingCardMarkVisitor(ConcurrentCopying* cc, ObjPtr<mirror::Object> holder)
+  VerifyNoMissingCardMarkVisitor(ConcurrentCopying* cc, ObjPtr<mirror::Object> holder,
+                                 uint8_t card, bool card_remembered)
     : cc_(cc),
-      holder_(holder) {}
+      holder_(holder),
+      card_(card),
+      card_remembered_(card_remembered) {}
 
   void operator()(ObjPtr<mirror::Object> obj,
                   MemberOffset offset,
@@ -691,37 +737,97 @@ class ConcurrentCopying::VerifyNoMissingCardMarkVisitor {
 
   void CheckReference(mirror::Object* ref, int32_t offset = -1) const
       REQUIRES_SHARED(Locks::mutator_lock_) {
-    if (ref != nullptr && cc_->region_space_->IsInNewlyAllocatedRegion(ref)) {
+    if (ref == nullptr) {
+      return;
+    }
+    bool failed_to_remember = false;
+    if (!card_remembered_) {
+      RegionGen ref_gen = cc_->region_space_->GetGen</*kEvacuating=*/ false>(ref);
+      if (cc_->IsYoungGC()) {
+        failed_to_remember = (ref_gen <= RegionGen::kRegionGenTenuring);
+      } else {
+        // MidTermGC/FullGC would have forgotten tenuring-inter-gen refs
+        failed_to_remember = (ref_gen < RegionGen::kRegionGenTenuring);
+      }
+    }
+    // 1. Clean cards must not have reference to newly allocated region (cannot check for Full-GC)
+    // 2. If using midterm GC all inter-gen references from tenured object must be remembered
+    if ((!cc_->IsFullGC() && card_ == gc::accounting::CardTable::kCardClean &&
+         cc_->region_space_->IsInNewlyAllocatedRegion(ref)) ||
+         failed_to_remember) {
       LOG(FATAL_WITHOUT_ABORT)
         << holder_->PrettyTypeOf() << "(" << holder_.Ptr() << ") references object "
-        << ref->PrettyTypeOf() << "(" << ref << ") in newly allocated region at offset=" << offset;
+        << ref->PrettyTypeOf() << "(" << ref
+        << ") in younger/newly allocated region at offset=" << offset;
+      LOG(FATAL_WITHOUT_ABORT) << "GcType=" << cc_->type_;
       LOG(FATAL_WITHOUT_ABORT) << "time=" << cc_->region_space_->Time();
       constexpr const char* kIndent = "  ";
+      // Remove memory protection from the region space and log debugging information.
+      cc_->region_space_->Unprotect();
       LOG(FATAL_WITHOUT_ABORT) << cc_->DumpReferenceInfo(holder_.Ptr(), "holder_", kIndent);
       LOG(FATAL_WITHOUT_ABORT) << cc_->DumpReferenceInfo(ref, "ref", kIndent);
-      LOG(FATAL) << "Unexpected reference to newly allocated region.";
+      LOG(FATAL) << "Unexpected reference to younger/newly allocated region.";
     }
   }
 
  private:
   ConcurrentCopying* const cc_;
   const ObjPtr<mirror::Object> holder_;
+  const uint8_t card_;
+  const bool card_remembered_;
 };
 
 void ConcurrentCopying::VerifyNoMissingCardMarks() {
   auto visitor = [&](mirror::Object* obj)
       REQUIRES(Locks::mutator_lock_)
       REQUIRES(!mark_stack_lock_) {
-    // Objects on clean cards should never have references to newly allocated regions. Note
-    // that aged cards are also not clean.
-    if (heap_->GetCardTable()->GetCard(obj) == gc::accounting::CardTable::kCardClean) {
-      VerifyNoMissingCardMarkVisitor internal_visitor(this, /*holder=*/ obj);
+    uint8_t* card = heap_->GetCardTable()->CardFromAddr(reinterpret_cast<uint8_t*>(obj));
+    RegionGen obj_gen = RegionGen::kRegionGenTenured;
+    accounting::RememberedSet* rem_set = nullptr;
+    if (region_space_->HasAddress(obj)) {
+      rem_set = region_space_rem_set_;
+      obj_gen = region_space_->GetGen</*kEvacuating=*/ false>(obj);
+    } else if (heap_->non_moving_space_->HasAddress(obj)) {
+      rem_set = non_moving_space_rem_set_;
+    }
+
+    // 1. Objects on clean cards should never have references to newly allocated regions. Note
+    //    that aged cards are also not clean.
+    //    For YoungGC, we scan aged cards as well so it is safe, even if they have references
+    //    to newly allocated regions.
+    //    For MidTermGC/FullGC, all aged cards are already scanned during marking phase
+    // 2. If using midterm CC all inter-gen references from tenured objects must be remembered
+    if ((!IsFullGC() && *card == gc::accounting::CardTable::kCardClean) ||
+        (use_midterm_cc_ && rem_set != nullptr &&
+         obj_gen == RegionGen::kRegionGenTenured &&
+         !rem_set->ContainsCard(card) && *card != gc::accounting::CardTable::kCardDirty)) {
+      // Fake that we remember the card for unwanted cases
+      bool card_remembered = (!use_midterm_cc_ || obj_gen != RegionGen::kRegionGenTenured ||
+                              rem_set == nullptr || rem_set->ContainsCard(card));
+      if (IsFullGC()) {
+        // When we are building rem-sets
+        // We may have missed marking some tenured refs during the mark phase
+        // Dont expect such cards to be remembered, so fake it here
+        // Such objects will be remembered after thread-flip
+        // TODO: This is expected only when rebuilding rem-sets, try to incorporate the exact case
+        card_remembered |= !TestMarkBitmapForRef(obj);
+      }
+      VerifyNoMissingCardMarkVisitor internal_visitor(this, /*holder=*/ obj,
+                                                      *card,
+                                                      card_remembered);
       obj->VisitReferences</*kVisitNativeRoots=*/true, kVerifyNone, kWithoutReadBarrier>(
           internal_visitor, internal_visitor);
     }
   };
   TimingLogger::ScopedTiming split(__FUNCTION__, GetTimings());
-  region_space_->Walk(visitor);
+  if (use_midterm_cc_ && IsFullGC()) {
+    // For Full-GC we cannot validate dirty card sanity as we may clear cards.
+    // So we walk the tenured-gen and validate the rem-sets if using mid-term CC.
+    // TODO: Handle the rebuilding rem-sets case, so we can walk entire region-space.
+    region_space_->WalkTenuredGen(visitor);
+  } else {
+    region_space_->Walk(visitor);
+  }
   {
     ReaderMutexLock rmu(Thread::Current(), *Locks::heap_bitmap_lock_);
     heap_->GetLiveBitmap()->Visit(visitor);
@@ -869,12 +975,14 @@ inline void ConcurrentCopying::ScanImmuneObject(mirror::Object* obj) {
   DCHECK(obj != nullptr);
   DCHECK(immune_spaces_.ContainsObject(obj));
   // Update the fields without graying it or pushing it onto the mark stack.
-  if (use_generational_cc_ && young_gen_) {
+  if (use_generational_cc_ && IsYoungGC()) {
     // Young GC does not care about references to unevac space. It is safe to not gray these as
     // long as scan immune objects happens after scanning the dirty cards.
-    Scan<true>(obj);
+    Scan<RegionTypeFlag::kIgnoreUnevacFromSpace>(obj);
+  } else if (use_midterm_cc_ && IsMidTermGC()) {
+    Scan<RegionTypeFlag::kIgnoreUnevacFromSpaceTenured>(obj);
   } else {
-    Scan<false>(obj);
+    Scan<RegionTypeFlag::kIgnoreNone>(obj);
   }
 }
 
@@ -1031,21 +1139,28 @@ void ConcurrentCopying::CaptureThreadRootsForMarking() {
 }
 
 // Used to scan ref fields of an object.
-template <bool kHandleInterRegionRefs>
+template <bool kHandleInterRegionRefs, bool kHandleInterGenRefs>
 class ConcurrentCopying::ComputeLiveBytesAndMarkRefFieldsVisitor {
  public:
   explicit ComputeLiveBytesAndMarkRefFieldsVisitor(ConcurrentCopying* collector,
                                                    size_t obj_region_idx)
       : collector_(collector),
       obj_region_idx_(obj_region_idx),
-      contains_inter_region_idx_(false) {}
+      contains_inter_region_idx_(false),
+      contains_inter_gen_refs_(false),
+      contains_non_tenuring_inter_gen_refs_(false) {}
 
   void operator()(mirror::Object* obj, MemberOffset offset, bool /* is_static */) const
       ALWAYS_INLINE
       REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES_SHARED(Locks::heap_bitmap_lock_) {
-    DCHECK_EQ(collector_->RegionSpace()->RegionIdxForRef(obj), obj_region_idx_);
-    DCHECK(kHandleInterRegionRefs || collector_->immune_spaces_.ContainsObject(obj));
+    DCHECK(!kHandleInterRegionRefs ||
+           collector_->RegionSpace()->RegionIdxForRef(obj) == obj_region_idx_);
+    DCHECK(kHandleInterRegionRefs || (collector_->IsMidTermGC() && kHandleInterGenRefs) ||
+           collector_->immune_spaces_.ContainsObject(obj));
+    DCHECK(!kHandleInterGenRefs || 
+           collector_->RegionSpace()->GetGen</*kEvacuating=*/ false>(obj) >=
+            RegionGen::kRegionGenTenured);
     CheckReference(obj->GetFieldObject<mirror::Object, kVerifyNone, kWithoutReadBarrier>(offset));
   }
 
@@ -1062,6 +1177,10 @@ class ConcurrentCopying::ComputeLiveBytesAndMarkRefFieldsVisitor {
         && ref->AsReference()->GetReferent<kWithoutReadBarrier>() != nullptr) {
       contains_inter_region_idx_ = true;
     }
+
+    if (kHandleInterGenRefs) {
+      CheckForInterGenReferences(ref->AsReference()->GetReferent<kWithoutReadBarrier>());
+    }
   }
 
   void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
@@ -1082,6 +1201,14 @@ class ConcurrentCopying::ComputeLiveBytesAndMarkRefFieldsVisitor {
     return contains_inter_region_idx_;
   }
 
+  bool ContainsInterGenRefs() const ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_) {
+    return contains_inter_gen_refs_;
+  }
+
+  bool ContainsNonTenuringInterGenRefs() const 
+      ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_) {
+    return contains_non_tenuring_inter_gen_refs_;
+  }
  private:
   void CheckReference(mirror::Object* ref) const
       REQUIRES_SHARED(Locks::mutator_lock_) {
@@ -1090,6 +1217,20 @@ class ConcurrentCopying::ComputeLiveBytesAndMarkRefFieldsVisitor {
       return;
     }
     if (!collector_->TestAndSetMarkBitForRef(ref)) {
+      //Dont expect to see any tenured objects here for mid_term GC.
+      DCHECK(!collector_->IsMidTermGC() ||
+             collector_->region_space_->GetGen</*kEvacuating=*/ false>(ref) !=
+              RegionGen::kRegionGenTenured);
+      // A newly marked non-moving-space object could potentially have young refs
+      // We may have forgotten the card as they were unmarked on alloc stack in the last cycle.
+      // Note: This happens only with MidTermGC as we forget some cards while scanning rem-sets
+      // Case-5: Inter-gen refs from non-moving space objects either new 
+      //         or newly marked as they were on alloc stack the last cycle
+      // TODO: Is it cleaner, if we just consider to remember aged cards as well ?
+      if (collector_->IsMidTermGC() &&
+          collector_->heap_->non_moving_space_->HasAddress(ref)) {
+        collector_->RememberCard(ref);
+      }
       collector_->PushOntoLocalMarkStack(ref);
     }
     if (kHandleInterRegionRefs && !contains_inter_region_idx_) {
@@ -1101,19 +1242,41 @@ class ConcurrentCopying::ComputeLiveBytesAndMarkRefFieldsVisitor {
         contains_inter_region_idx_ = true;
       }
     }
+    CheckForInterGenReferences(ref);
+  }
+
+  void CheckForInterGenReferences(mirror::Object* ref) const ALWAYS_INLINE
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (kHandleInterGenRefs && !contains_non_tenuring_inter_gen_refs_) {
+      RegionGen ref_gen = collector_->RegionSpace()->GetGen</*kEvacuating=*/ false>(ref);
+      contains_non_tenuring_inter_gen_refs_ = (ref_gen < RegionGen::kRegionGenTenuring);
+      if (!contains_inter_gen_refs_) {
+        contains_inter_gen_refs_ = (ref_gen <= RegionGen::kRegionGenTenuring);
+      }
+    }
   }
 
   ConcurrentCopying* const collector_;
   const size_t obj_region_idx_;
   mutable bool contains_inter_region_idx_;
+  mutable bool contains_inter_gen_refs_;
+  mutable bool contains_non_tenuring_inter_gen_refs_;
 };
 
+template <bool kRememberInterGenRefs>
 void ConcurrentCopying::AddLiveBytesAndScanRef(mirror::Object* ref) {
   DCHECK(ref != nullptr);
   DCHECK(!immune_spaces_.ContainsObject(ref));
   DCHECK(TestMarkBitmapForRef(ref));
   size_t obj_region_idx = static_cast<size_t>(-1);
+  bool containsInterRegionRefs = false;
+  // Note that we use TenuredGen for all non-region space objects, so that
+  // we dont consider any references from them into TenuredGen of region space
+  RegionGen obj_gen = RegionGen::kRegionGenTenured;
   if (LIKELY(region_space_->HasAddress(ref))) {
+    if (kRememberInterGenRefs) {
+      obj_gen = region_space_->GetGen</*kEvacuating=*/ false>(ref);
+    }
     obj_region_idx = region_space_->RegionIdxForRefUnchecked(ref);
     // Add live bytes to the corresponding region
     if (!region_space_->IsRegionNewlyAllocated(obj_region_idx)) {
@@ -1124,13 +1287,32 @@ void ConcurrentCopying::AddLiveBytesAndScanRef(mirror::Object* ref) {
       region_space_->AddLiveBytes(ref, alloc_size);
     }
   }
-  ComputeLiveBytesAndMarkRefFieldsVisitor</*kHandleInterRegionRefs*/ true>
-      visitor(this, obj_region_idx);
-  ref->VisitReferences</*kVisitNativeRoots=*/ true, kDefaultVerifyFlags, kWithoutReadBarrier>(
-      visitor, visitor);
+  if (kRememberInterGenRefs && obj_gen == RegionGen::kRegionGenTenured) {
+    ComputeLiveBytesAndMarkRefFieldsVisitor</*kHandleInterRegionRefs*/ true,
+                                            /*kHandleInterGenRefs=*/ true>
+        visitor(this, obj_region_idx);
+    ref->VisitReferences</*kVisitNativeRoots=*/ true, kDefaultVerifyFlags, kWithoutReadBarrier>(
+        visitor, visitor);
+    if (visitor.ContainsNonTenuringInterGenRefs()) {
+      // This must happen only on a region space/non-moving space object
+      // We use the same assumption as used below for obj_region_idx = -1
+      DCHECK(region_space_->HasAddress(ref) || heap_->GetNonMovingSpace()->HasAddress(ref));
+      // Case-4: Inter-gen refs due to tenure threshold updation.
+      //         We are re-building Remsets here as the gen-boundary has changed.
+      RememberCard(ref);
+    }
+    containsInterRegionRefs = visitor.ContainsInterRegionRefs();
+  } else {
+    ComputeLiveBytesAndMarkRefFieldsVisitor</*kHandleInterRegionRefs*/ true,
+                                            /*kHandleInterGenRefs=*/ false>
+        visitor(this, obj_region_idx);
+    ref->VisitReferences</*kVisitNativeRoots=*/ true, kDefaultVerifyFlags, kWithoutReadBarrier>(
+        visitor, visitor);
+    containsInterRegionRefs = visitor.ContainsInterRegionRefs();
+  }
   // Mark the corresponding card dirty if the object contains any
   // inter-region reference.
-  if (visitor.ContainsInterRegionRefs()) {
+  if (containsInterRegionRefs) {
     if (obj_region_idx == static_cast<size_t>(-1)) {
       // If an inter-region ref has been found in a non-region-space, then it
       // must be non-moving-space. This is because this function cannot be
@@ -1216,16 +1398,28 @@ void ConcurrentCopying::PushOntoLocalMarkStack(mirror::Object* ref) {
 
 void ConcurrentCopying::ProcessMarkStackForMarkingAndComputeLiveBytes() {
   // Process thread-local mark stack containing thread roots
-  ProcessThreadLocalMarkStacks(/* disable_weak_ref_access */ false,
-                               /* checkpoint_callback */ nullptr,
-                               [this] (mirror::Object* ref)
-                                   REQUIRES_SHARED(Locks::mutator_lock_) {
-                                 AddLiveBytesAndScanRef(ref);
-                               });
-
-  while (!gc_mark_stack_->IsEmpty()) {
-    mirror::Object* ref = gc_mark_stack_->PopBack();
-    AddLiveBytesAndScanRef(ref);
+  if (LIKELY(!use_midterm_cc_ || !rebuild_rem_sets_during_mark_phase_)) {
+    ProcessThreadLocalMarkStacks(/* disable_weak_ref_access */ false,
+                                 /* checkpoint_callback */ nullptr,
+                                 [this] (mirror::Object* ref)
+                                    REQUIRES_SHARED(Locks::mutator_lock_) {
+                                   AddLiveBytesAndScanRef</*kRememberInterGenRefs*/ false>(ref);
+                                 });
+    while (!gc_mark_stack_->IsEmpty()) {
+      mirror::Object* ref = gc_mark_stack_->PopBack();
+      AddLiveBytesAndScanRef</*kRememberInterGenRefs*/ false>(ref);
+    }
+  } else {
+    ProcessThreadLocalMarkStacks(/* disable_weak_ref_access */ false,
+                                 /* checkpoint_callback */ nullptr,
+                                 [this] (mirror::Object* ref)
+                                    REQUIRES_SHARED(Locks::mutator_lock_) {
+                                   AddLiveBytesAndScanRef</*kRememberInterGenRefs*/ true>(ref);
+                                 });
+    while (!gc_mark_stack_->IsEmpty()) {
+      mirror::Object* ref = gc_mark_stack_->PopBack();
+      AddLiveBytesAndScanRef</*kRememberInterGenRefs*/ true>(ref);
+    }
   }
 }
 
@@ -1234,7 +1428,8 @@ class ConcurrentCopying::ImmuneSpaceCaptureRefsVisitor {
   explicit ImmuneSpaceCaptureRefsVisitor(ConcurrentCopying* cc) : collector_(cc) {}
 
   ALWAYS_INLINE void operator()(mirror::Object* obj) const REQUIRES_SHARED(Locks::mutator_lock_) {
-    ComputeLiveBytesAndMarkRefFieldsVisitor</*kHandleInterRegionRefs*/ false>
+    ComputeLiveBytesAndMarkRefFieldsVisitor</*kHandleInterRegionRefs*/ false,
+                                            /*kHandleInterGenRefs*/ false>
         visitor(collector_, /*obj_region_idx*/ static_cast<size_t>(-1));
     obj->VisitReferences</*kVisitNativeRoots=*/true, kDefaultVerifyFlags, kWithoutReadBarrier>(
         visitor, visitor);
@@ -1248,6 +1443,39 @@ class ConcurrentCopying::ImmuneSpaceCaptureRefsVisitor {
   ConcurrentCopying* const collector_;
 };
 
+class ConcurrentCopying::RememberedSetMarkingVisitor : 
+  public accounting::RememberedSetObjectVisitor {
+ public:
+  explicit RememberedSetMarkingVisitor(ConcurrentCopying* cc,
+                                       accounting::ContinuousSpaceBitmap* inter_region_bitmap)
+      : cc_(cc),
+        inter_region_bitmap_(inter_region_bitmap) { }
+
+  void operator()(ObjPtr<mirror::Object> obj) const REQUIRES(Locks::heap_bitmap_lock_)
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    mirror::Object* ref = obj.Ptr();
+    DCHECK(cc_->region_space_->GetGen</*kEvacuating=*/ false>(ref) >=
+           RegionGen::kRegionGenTenured);
+    ConcurrentCopying::ComputeLiveBytesAndMarkRefFieldsVisitor</*kHandleInterRegionRefs*/ false,
+                                                               /*kHandleInterGenRefs*/ true>
+        visitor(cc_, static_cast<size_t>(-1));
+    ref->VisitReferences</*kVisitNativeRoots=*/ true, kDefaultVerifyFlags, kWithoutReadBarrier>(
+        visitor, visitor);
+    if (visitor.ContainsInterGenRefs()) {
+      // By the end of this cycle, some of the regions will be tenuring
+      // Remembering cards having such refs is not useful beyond this cycle. 
+      UpdateRefsToTargetSpace(visitor.ContainsNonTenuringInterGenRefs());
+      // We dont care about inter-region refs here, only inter-gen refs are interesting to us
+      // Note: We care about tenuring-inter-gen refs as well, we have to scan after thread-flip.
+      inter_region_bitmap_->Set(ref);
+    }
+  }
+
+ private:
+  ConcurrentCopying* const cc_;
+  accounting::ContinuousSpaceBitmap* const inter_region_bitmap_;
+};
+
 /* Invariants for two-phase CC
  * ===========================
  * A) Definitions
@@ -1316,11 +1544,12 @@ void ConcurrentCopying::MarkingPhase() {
   }
   accounting::CardTable* const card_table = heap_->GetCardTable();
   Thread* const self = Thread::Current();
+  DCHECK(!IsYoungGC());
   // Clear live_bytes_ of every non-free region, except the ones that are newly
   // allocated.
-  region_space_->SetAllRegionLiveBytesZero();
+  region_space_->SetAllRegionLiveBytesZero(IsMidTermGC());
   if (kIsDebugBuild) {
-    region_space_->AssertAllRegionLiveBytesZeroOrCleared();
+    region_space_->AssertAllRegionLiveBytesZeroOrCleared(IsMidTermGC());
   }
   // Scan immune spaces
   {
@@ -1343,6 +1572,52 @@ void ConcurrentCopying::MarkingPhase() {
       }
     }
   }
+  if (use_midterm_cc_) {
+    if (IsMidTermGC()) {
+      if (kVerboseMode) {
+        LOG(INFO) << "GC ScanRememberedSets for mid-term GC";
+      }
+      // Scan the tenured/Non-Moving objects which may have references to region space
+      // Remembered Sets are roots for a mid-term GC
+      TimingLogger::ScopedTiming split2("ScanRememberedSets", GetTimings());
+      WriterMutexLock rmu(Thread::Current(), *Locks::heap_bitmap_lock_);
+      for (space::ContinuousSpace* space : GetHeap()->GetContinuousSpaces()) {
+        if (space->IsImageSpace() || space->IsZygoteSpace()) {
+          continue;
+        }
+        accounting::RememberedSet* rem_set = nullptr;
+        if (space == region_space_) {
+          rem_set = region_space_rem_set_;
+        } else {
+          DCHECK_EQ(space, heap_->non_moving_space_);
+          rem_set = non_moving_space_rem_set_;
+        }
+        DCHECK(rem_set != nullptr);
+        RememberedSetMarkingVisitor rem_set_obj_marking_visitor(
+                                                        this,
+                                                        space == region_space_ ?
+                                                          region_space_inter_region_bitmap_.get() :
+                                                          non_moving_space_inter_region_bitmap_.get());
+        rem_set->UpdateAndMarkReferences(region_space_/*target_space*/,
+                                        this/*collector*/,
+                                        &rem_set_obj_marking_visitor);
+      }
+      if (kVerboseMode) {
+        LOG(INFO) << "GC end of ScanRememberedSets for mid-term GC";
+      }
+    } else if (rebuild_rem_sets_during_mark_phase_) {
+      // We are going to rebuild our Remsets for the new tenure threshold,
+      // Let Region Space know about it and invalidate our RemSets
+      DCHECK_NE(tenure_threshold_, region_space_->GetTenureThreshold());
+      region_space_->SetTenureThreshold(tenure_threshold_);
+      if (kVerboseMode) {
+        LOG(INFO)<<"Clear and rebuild rem-sets, new tenure threshold="<<(size_t)tenure_threshold_;
+      }
+      DCHECK(use_midterm_cc_);
+      region_space_rem_set_->Empty();
+      non_moving_space_rem_set_->Empty();
+    }
+  }
   // Scan runtime roots
   {
     TimingLogger::ScopedTiming split2("VisitConcurrentRoots", GetTimings());
@@ -1360,14 +1635,48 @@ void ConcurrentCopying::MarkingPhase() {
   // Process mark stack
   ProcessMarkStackForMarkingAndComputeLiveBytes();
 
+  if (use_midterm_cc_ && !IsMidTermGC() && use_dynamic_threshold_) {
+    if (rebuild_rem_sets_during_mark_phase_) {
+      // We have just finished rebuilding rem-sets
+      // This is not a completion as there may still be some references
+      // that we have missed out. They will be captured after thread-flip anyway.
+      rebuild_rem_sets_during_mark_phase_ = false;
+      // In case mid-term GC is disabled, Lets turn on Mid Term GC again
+      heap_->SetIgnoreMidTerm(false);
+    }
+    int new_threshold = region_space_->ComputeTenureThreshold();
+    if (new_threshold == 0) {
+      // The new threshold suggests mid-term GC may not perform better than a Full GC.
+      // So disable mid-term GC
+      heap_->SetIgnoreMidTerm(true);
+    } else if (new_threshold != tenure_threshold_) {
+      // Threshold has changed from last known value      
+      // Remember the update, so that we rebuild Remsets in the next Full cycle
+      // We cannot update the Region space yet, as it would invalidate our RemSets,
+      // and leave mid-term GC in a limbo at the end of this cycle.
+      // Any scheduling as per gc plan will fail.
+      // Note: We will notify region space in the next Full-GC after re-building Remsets.
+      tenure_threshold_ = new_threshold;
+      rebuild_rem_sets_during_mark_phase_ = true;
+    } else {
+      // We have computed the same threshold as before => our RemSets are populated appropriately
+      // In case mid-term GC is disabled, Lets turn on Mid Term GC again
+      heap_->SetIgnoreMidTerm(false);
+    }
+  }
   if (kVerboseMode) {
     LOG(INFO) << "GC end of MarkingPhase";
   }
 }
 
-template <bool kNoUnEvac>
+template <ConcurrentCopying::RegionTypeFlag kRegionTypeFlag, bool kCheckForInterGenRefs>
 void ConcurrentCopying::ScanDirtyObject(mirror::Object* obj) {
-  Scan<kNoUnEvac>(obj);
+  if (kCheckForInterGenRefs && use_midterm_cc_ &&
+      region_space_->GetGen</*kEvacuating=*/ true>(obj) == RegionGen::kRegionGenTenuring) {
+    Scan<kRegionTypeFlag, /*kRememberInterGenRefs=*/true>(obj);
+  } else {
+    Scan<kRegionTypeFlag>(obj);
+  }
   // Set the read-barrier state of a reference-type object to gray if its
   // referent is not marked yet. This is to ensure that if GetReferent() is
   // called, it triggers the read-barrier to process the referent before use.
@@ -1443,7 +1752,7 @@ void ConcurrentCopying::CopyingPhase() {
             // corresponding mark-bit and, for region space objects,
             // decrementing the object's size from the corresponding region's
             // live_bytes.
-            if (young_gen_) {
+            if (IsYoungGC()) {
               // Don't push or gray unevac refs.
               if (kIsDebugBuild && space == region_space_) {
                 // We may get unevac large objects.
@@ -1453,30 +1762,48 @@ void ConcurrentCopying::CopyingPhase() {
                   LOG(FATAL) << "Scanning " << obj << " not in unevac space";
                 }
               }
-              ScanDirtyObject</*kNoUnEvac*/ true>(obj);
+              ScanDirtyObject<RegionTypeFlag::kIgnoreUnevacFromSpace>(obj);
             } else if (space != region_space_) {
               DCHECK(space == heap_->non_moving_space_);
               // We need to process un-evac references as they may be unprocessed,
               // if they skipped the marking phase due to heap mutation.
-              ScanDirtyObject</*kNoUnEvac*/ false>(obj);
+              // For Mid-term, we cannot have any unprocessed Tenured objects
+              // Dont bother checking intergen refs as the dirty cards are anyway remembered
+              if (IsMidTermGC()) {
+                ScanDirtyObject<RegionTypeFlag::kIgnoreUnevacFromSpaceTenured>(obj);
+              } else {
+                ScanDirtyObject<RegionTypeFlag::kIgnoreNone>(obj);
+              }
               non_moving_space_inter_region_bitmap_->Clear(obj);
             } else if (region_space_->IsInUnevacFromSpace(obj)) {
-              ScanDirtyObject</*kNoUnEvac*/ false>(obj);
+              if (IsMidTermGC()) {
+                ScanDirtyObject<RegionTypeFlag::kIgnoreUnevacFromSpaceTenured>(obj);
+              } else {
+                ScanDirtyObject<RegionTypeFlag::kIgnoreNone>(obj);
+              }
               region_space_inter_region_bitmap_->Clear(obj);
             }
           },
-          accounting::CardTable::kCardAged);
+          // If we are running a 2-phase GC, only dirty cards are interesting.
+          // Aged cards are already scanned in marking phase.
+          IsYoungGC() ? accounting::CardTable::kCardAged : accounting::CardTable::kCardDirty);
 
-      if (!young_gen_) {
-        auto visitor = [this](mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_) {
+      if (!IsYoungGC()) {
+        if (space == region_space_) {
+          auto visitor = [this](mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_) {
                          // We don't need to process un-evac references as any unprocessed
                          // ones will be taken care of in the card-table scan above.
-                         ScanDirtyObject</*kNoUnEvac*/ true>(obj);
+                         // Remember if there are any intergen refs from tenuring objects
+                         ScanDirtyObject<RegionTypeFlag::kIgnoreUnevacFromSpace, true>(obj);
                        };
-        if (space == region_space_) {
           region_space_->ScanUnevacFromSpace(region_space_inter_region_bitmap_.get(), visitor);
         } else {
           DCHECK(space == heap_->non_moving_space_);
+          auto visitor = [this](mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_) {
+                         // We don't need to process un-evac references as any unprocessed
+                         // ones will be taken care of in the card-table scan above.
+                         ScanDirtyObject<RegionTypeFlag::kIgnoreUnevacFromSpace>(obj);
+                       };
           non_moving_space_inter_region_bitmap_->VisitMarkedRange(
               reinterpret_cast<uintptr_t>(space->Begin()),
               reinterpret_cast<uintptr_t>(space->End()),
@@ -2141,7 +2468,7 @@ inline void ConcurrentCopying::ProcessMarkStackRef(mirror::Object* to_ref) {
         << " rb_state=" << to_ref->GetReadBarrierState()
         << " is_marked=" << IsMarked(to_ref)
         << " type=" << to_ref->PrettyTypeOf()
-        << " young_gen=" << std::boolalpha << young_gen_ << std::noboolalpha
+        << " GcType=" << type_
         << " space=" << heap_->DumpSpaceNameFromAddress(to_ref)
         << " region_type=" << rtype
         // TODO: Temporary; remove this when this is no longer needed (b/116087961).
@@ -2153,12 +2480,18 @@ inline void ConcurrentCopying::ProcessMarkStackRef(mirror::Object* to_ref) {
   DCHECK(!region_space_->IsInNewlyAllocatedRegion(to_ref)) << to_ref;
   bool perform_scan = false;
   switch (rtype) {
+    case space::RegionSpace::RegionType::kRegionTypeUnevacFromSpaceTenured:
+      DCHECK(use_midterm_cc_);
+      [[clang::fallthrough]];
     case space::RegionSpace::RegionType::kRegionTypeUnevacFromSpace:
       // Mark the bitmap only in the GC thread here so that we don't need a CAS.
       if (!kUseBakerReadBarrier || !region_space_bitmap_->Set(to_ref)) {
+        // For young gen or mid term we dont expect un-marked UnevacFromSpaceTenured object.
+        DCHECK(rtype == space::RegionSpace::RegionType::kRegionTypeUnevacFromSpace ||
+               (use_midterm_cc_ && IsFullGC()));
         // It may be already marked if we accidentally pushed the same object twice due to the racy
         // bitmap read in MarkUnevacFromSpaceRegion.
-        if (use_generational_cc_ && young_gen_) {
+        if (use_generational_cc_ && IsYoungGC()) {
           CHECK(region_space_->IsLargeObject(to_ref));
           region_space_->ZeroLiveBytesForLargeObject(to_ref);
         }
@@ -2220,10 +2553,29 @@ inline void ConcurrentCopying::ProcessMarkStackRef(mirror::Object* to_ref) {
       }
   }
   if (perform_scan) {
-    if (use_generational_cc_ && young_gen_) {
-      Scan<true>(to_ref);
+    if (use_generational_cc_ && IsYoungGC()) {
+      Scan<RegionTypeFlag::kIgnoreUnevacFromSpace>(to_ref);
+    } else if (use_midterm_cc_) {
+      // A newly marked non-moving space or region_space tenured/tenuring object
+      // Can potentially have references to younger regions
+      // TODO: LOS will not have references to region_space objects, try avoiding
+      if (region_space_->GetGen</*kEvacuating=*/ true>(to_ref) >=
+            RegionGen::kRegionGenTenuring) {
+        if (IsMidTermGC()) {
+          Scan<RegionTypeFlag::kIgnoreUnevacFromSpaceTenured,
+               /*kRememberInterGenRefs=*/true>(to_ref);
+        } else {
+          Scan<RegionTypeFlag::kIgnoreNone, /*kRememberInterGenRefs*/true>(to_ref);
+        }
+      } else {
+        if (IsMidTermGC()) {
+          Scan<RegionTypeFlag::kIgnoreUnevacFromSpaceTenured>(to_ref);
+        } else {
+          Scan<RegionTypeFlag::kIgnoreNone>(to_ref);
+        }
+      }
     } else {
-      Scan<false>(to_ref);
+      Scan<RegionTypeFlag::kIgnoreNone>(to_ref);
     }
   }
   if (kUseBakerReadBarrier) {
@@ -2232,7 +2584,7 @@ inline void ConcurrentCopying::ProcessMarkStackRef(mirror::Object* to_ref) {
         << " rb_state=" << to_ref->GetReadBarrierState()
         << " is_marked=" << IsMarked(to_ref)
         << " type=" << to_ref->PrettyTypeOf()
-        << " young_gen=" << std::boolalpha << young_gen_ << std::noboolalpha
+        << " GcType=" << type_
         << " space=" << heap_->DumpSpaceNameFromAddress(to_ref)
         << " region_type=" << rtype
         // TODO: Temporary; remove this when this is no longer needed (b/116087961).
@@ -2379,7 +2731,7 @@ void ConcurrentCopying::SweepSystemWeaks(Thread* self) {
 }
 
 void ConcurrentCopying::Sweep(bool swap_bitmaps) {
-  if (use_generational_cc_ && young_gen_) {
+  if (use_generational_cc_ && (IsYoungGC() || IsMidTermGC())) {
     // Only sweep objects on the live stack.
     SweepArray(heap_->GetLiveStack(), /* swap_bitmaps= */ false);
   } else {
@@ -2582,7 +2934,7 @@ void ConcurrentCopying::CaptureRssAtPeak() {
     // card table
     add_gc_range(heap_->GetCardTable()->MemMapBegin(), heap_->GetCardTable()->MemMapSize());
     // inter-region refs
-    if (use_generational_cc_ && !young_gen_) {
+    if (use_generational_cc_ && !IsYoungGC()) {
       // region space
       add_gc_range(region_space_inter_region_bitmap_->Begin(),
                    region_space_inter_region_bitmap_->Size());
@@ -2651,12 +3003,23 @@ void ConcurrentCopying::ReclaimPhase() {
       gc_count_++;
     }
 
+    if (use_midterm_cc_ && IsFullGC()) {
+      // Cards in From-Space of TenuredGen are no loner useful.
+      // Trim the rem-set
+      using RegionType = space::RegionSpace::RegionType;
+      region_space_->VisitRegions<RegionType::kRegionTypeFromSpace>(
+                                          static_cast<uint8_t>(RegionGen::kRegionGenTenured),
+                                          [this] (uint8_t* begin, uint8_t* end) {
+                                            region_space_rem_set_->DropCardRange(begin, end);
+                                          }
+                                        );
+    }
     // Cleared bytes and objects, populated by the call to RegionSpace::ClearFromSpace below.
     uint64_t cleared_bytes;
     uint64_t cleared_objects;
     {
       TimingLogger::ScopedTiming split4("ClearFromSpace", GetTimings());
-      region_space_->ClearFromSpace(&cleared_bytes, &cleared_objects, /*clear_bitmap*/ !young_gen_);
+      region_space_->ClearFromSpace(&cleared_bytes, &cleared_objects, /*clear_bitmap*/ !IsYoungGC(), IsMidTermGC());
       // `cleared_bytes` and `cleared_objects` may be greater than the from space equivalents since
       // RegionSpace::ClearFromSpace may clear empty unevac regions.
       CHECK_GE(cleared_bytes, from_bytes);
@@ -2763,7 +3126,8 @@ void ConcurrentCopying::AssertToSpaceInvariant(mirror::Object* obj,
       if (type == RegionType::kRegionTypeToSpace) {
         // OK.
         return;
-      } else if (type == RegionType::kRegionTypeUnevacFromSpace) {
+      } else if (type == RegionType::kRegionTypeUnevacFromSpace ||
+                 type == RegionType::kRegionTypeUnevacFromSpaceTenured) {
         if (!IsMarkedInUnevacFromSpace(ref)) {
           LOG(FATAL_WITHOUT_ABORT) << "Found unmarked reference in unevac from-space:";
           // Remove memory protection from the region space and log debugging information.
@@ -2869,7 +3233,8 @@ void ConcurrentCopying::AssertToSpaceInvariant(GcRootSource* gc_root_source,
       if (type == RegionType::kRegionTypeToSpace) {
         // OK.
         return;
-      } else if (type == RegionType::kRegionTypeUnevacFromSpace) {
+      } else if (type == RegionType::kRegionTypeUnevacFromSpace ||
+                 type == RegionType::kRegionTypeUnevacFromSpaceTenured) {
         if (!IsMarkedInUnevacFromSpace(ref)) {
           LOG(FATAL_WITHOUT_ABORT) << "Found unmarked reference in unevac from-space:";
           // Remove memory protection from the region space and log debugging information.
@@ -3020,7 +3385,7 @@ void ConcurrentCopying::AssertToSpaceInvariantInNonMovingSpace(mirror::Object* o
         << " ref=" << ref
         << " rb_state=" << ref->GetReadBarrierState()
         << " is_marking=" << std::boolalpha << is_marking_ << std::noboolalpha
-        << " young_gen=" << std::boolalpha << young_gen_ << std::noboolalpha
+        << " GcType=" << type_
         << " done_scanning="
         << std::boolalpha << done_scanning_.load(std::memory_order_acquire) << std::noboolalpha
         << " self=" << Thread::Current();
@@ -3028,25 +3393,37 @@ void ConcurrentCopying::AssertToSpaceInvariantInNonMovingSpace(mirror::Object* o
 }
 
 // Used to scan ref fields of an object.
-template <bool kNoUnEvac>
+template <ConcurrentCopying::RegionTypeFlag kRegionTypeFlag , bool kRememberInterGenRefs>
 class ConcurrentCopying::RefFieldsVisitor {
  public:
   explicit RefFieldsVisitor(ConcurrentCopying* collector, Thread* const thread)
-      : collector_(collector), thread_(thread) {
-    // Cannot have `kNoUnEvac` when Generational CC collection is disabled.
-    DCHECK(!kNoUnEvac || collector_->use_generational_cc_);
+      : collector_(collector), thread_(thread),
+        contains_inter_gen_refs_(false) {
+    // Cannot have any other flags except 'kIgnoreNone' when Generational CC collection is disabled.
+    DCHECK((kRegionTypeFlag == RegionTypeFlag::kIgnoreNone) || collector_->use_generational_cc_);
+    DCHECK((kRegionTypeFlag != RegionTypeFlag::kIgnoreUnevacFromSpaceTenured) || collector_->use_midterm_cc_);
+    DCHECK(!kRememberInterGenRefs || collector_->use_midterm_cc_);
   }
 
   void operator()(mirror::Object* obj, MemberOffset offset, bool /* is_static */)
       const ALWAYS_INLINE REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES_SHARED(Locks::heap_bitmap_lock_) {
-    collector_->Process<kNoUnEvac>(obj, offset);
+    collector_->Process<kRegionTypeFlag>(obj, offset);
+    if (kRememberInterGenRefs && !contains_inter_gen_refs_) {
+      CheckForInterGenRefs(obj->GetFieldObject<mirror::Object,
+                                                    kVerifyNone,
+                                                    kWithoutReadBarrier,
+                                                    false>(offset));
+    }
   }
 
   void operator()(ObjPtr<mirror::Class> klass, ObjPtr<mirror::Reference> ref) const
       REQUIRES_SHARED(Locks::mutator_lock_) ALWAYS_INLINE {
     CHECK(klass->IsTypeOfReferenceClass());
     collector_->DelayReferenceReferent(klass, ref);
+    if (kRememberInterGenRefs && !contains_inter_gen_refs_) {
+      CheckForInterGenRefs(ref->GetReferent<kWithoutReadBarrier>());
+    }
   }
 
   void VisitRootIfNonNull(mirror::CompressedReference<mirror::Object>* root) const
@@ -3061,17 +3438,34 @@ class ConcurrentCopying::RefFieldsVisitor {
       ALWAYS_INLINE
       REQUIRES_SHARED(Locks::mutator_lock_) {
     collector_->MarkRoot</*kGrayImmuneObject=*/false>(thread_, root);
+    if (kRememberInterGenRefs && !contains_inter_gen_refs_) {
+      CheckForInterGenRefs(root->AsMirrorPtr());
+    }
   }
 
+  bool ContainsInterGenRefs() { return contains_inter_gen_refs_; }
+
  private:
+  inline void CheckForInterGenRefs(mirror::Object* obj) const
+      REQUIRES_SHARED(Locks::mutator_lock_) {
+    if (collector_->RegionSpace()->GetGen</*kEvacuating=*/ true>(obj) <
+        RegionGen::kRegionGenTenuring) {
+      contains_inter_gen_refs_ = true;
+    }
+  }
+
   ConcurrentCopying* const collector_;
   Thread* const thread_;
+  mutable bool contains_inter_gen_refs_;
 };
 
-template <bool kNoUnEvac>
+template <ConcurrentCopying::RegionTypeFlag kRegionTypeFlag, bool kRememberInterGenRefs>
 inline void ConcurrentCopying::Scan(mirror::Object* to_ref) {
-  // Cannot have `kNoUnEvac` when Generational CC collection is disabled.
-  DCHECK(!kNoUnEvac || use_generational_cc_);
+  // Cannot have any other flags except 'kIgnoreNone' when Generational CC collection is disabled.
+  DCHECK((kRegionTypeFlag == RegionTypeFlag::kIgnoreNone) || use_generational_cc_);
+  DCHECK((kRegionTypeFlag != RegionTypeFlag::kIgnoreUnevacFromSpaceTenured) || use_midterm_cc_);
+  DCHECK(use_midterm_cc_ || !kRememberInterGenRefs);
+
   if (kDisallowReadBarrierDuringScan && !Runtime::Current()->IsActiveTransaction()) {
     // Avoid all read barriers during visit references to help performance.
     // Don't do this in transaction mode because we may read the old value of an field which may
@@ -3080,23 +3474,32 @@ inline void ConcurrentCopying::Scan(mirror::Object* to_ref) {
   }
   DCHECK(!region_space_->IsInFromSpace(to_ref));
   DCHECK_EQ(Thread::Current(), thread_running_gc_);
-  RefFieldsVisitor<kNoUnEvac> visitor(this, thread_running_gc_);
+  DCHECK(!kRememberInterGenRefs || 
+        region_space_->GetGen</*kEvacuating=*/ true>(to_ref) >= RegionGen::kRegionGenTenuring);
+  RefFieldsVisitor<kRegionTypeFlag,
+                   kRememberInterGenRefs> visitor(this, thread_running_gc_);
   // Disable the read barrier for a performance reason.
   to_ref->VisitReferences</*kVisitNativeRoots=*/true, kDefaultVerifyFlags, kWithoutReadBarrier>(
       visitor, visitor);
+  if (kRememberInterGenRefs && visitor.ContainsInterGenRefs()) {
+    // Case 2/3: Inter-gen references due to evacuation/Unevac promotion by GC
+    RememberCard(to_ref);
+  }
   if (kDisallowReadBarrierDuringScan && !Runtime::Current()->IsActiveTransaction()) {
     thread_running_gc_->ModifyDebugDisallowReadBarrier(-1);
   }
 }
 
-template <bool kNoUnEvac>
+template <ConcurrentCopying::RegionTypeFlag kRegionTypeFlag>
 inline void ConcurrentCopying::Process(mirror::Object* obj, MemberOffset offset) {
-  // Cannot have `kNoUnEvac` when Generational CC collection is disabled.
-  DCHECK(!kNoUnEvac || use_generational_cc_);
+  // Cannot have any other flags except 'kIgnoreNone' when Generational CC collection is disabled.
+  DCHECK((kRegionTypeFlag == RegionTypeFlag::kIgnoreNone) || use_generational_cc_);
+  DCHECK((kRegionTypeFlag != RegionTypeFlag::kIgnoreUnevacFromSpaceTenured) || use_midterm_cc_);
   DCHECK_EQ(Thread::Current(), thread_running_gc_);
   mirror::Object* ref = obj->GetFieldObject<
       mirror::Object, kVerifyNone, kWithoutReadBarrier, false>(offset);
-  mirror::Object* to_ref = Mark</*kGrayImmuneObject=*/false, kNoUnEvac, /*kFromGCThread=*/true>(
+  mirror::Object* to_ref = Mark</*kGrayImmuneObject=*/false, kRegionTypeFlag,
+                                /*kFromGCThread=*/true>(
       thread_running_gc_,
       ref,
       /*holder=*/ obj,
@@ -3253,16 +3656,22 @@ void ConcurrentCopying::FillWithDummyObject(Thread* const self,
 }
 
 // Reuse the memory blocks that were copy of objects that were lost in race.
-mirror::Object* ConcurrentCopying::AllocateInSkippedBlock(Thread* const self, size_t alloc_size) {
+mirror::Object* ConcurrentCopying::AllocateInSkippedBlock(Thread* const self, size_t alloc_size,
+                                                          uint8_t age) {
   // Try to reuse the blocks that were unused due to CAS failures.
   CHECK_ALIGNED(alloc_size, space::RegionSpace::kAlignment);
   size_t min_object_size = RoundUp(sizeof(mirror::Object), space::RegionSpace::kAlignment);
   size_t byte_size;
   uint8_t* addr;
+  CHECK(age < kAgeSkipBlockMapSize);
+  std::multimap<size_t, uint8_t*>* skipped_blocks_map = &age_skipped_blocks_map_[age];
+  if (skipped_blocks_map->empty()) {
+    return nullptr;
+  }
   {
     MutexLock mu(self, skipped_blocks_lock_);
-    auto it = skipped_blocks_map_.lower_bound(alloc_size);
-    if (it == skipped_blocks_map_.end()) {
+    auto it = skipped_blocks_map->lower_bound(alloc_size);
+    if (it == skipped_blocks_map->end()) {
       // Not found.
       return nullptr;
     }
@@ -3270,8 +3679,8 @@ mirror::Object* ConcurrentCopying::AllocateInSkippedBlock(Thread* const self, si
     CHECK_GE(byte_size, alloc_size);
     if (byte_size > alloc_size && byte_size - alloc_size < min_object_size) {
       // If remainder would be too small for a dummy object, retry with a larger request size.
-      it = skipped_blocks_map_.lower_bound(alloc_size + min_object_size);
-      if (it == skipped_blocks_map_.end()) {
+      it = skipped_blocks_map->lower_bound(alloc_size + min_object_size);
+      if (it == skipped_blocks_map->end()) {
         // Not found.
         return nullptr;
       }
@@ -3280,7 +3689,7 @@ mirror::Object* ConcurrentCopying::AllocateInSkippedBlock(Thread* const self, si
           << "byte_size=" << byte_size << " it->first=" << it->first << " alloc_size=" << alloc_size;
     }
     // Found a block.
-    CHECK(it != skipped_blocks_map_.end());
+    CHECK(it != skipped_blocks_map->end());
     byte_size = it->first;
     addr = it->second;
     CHECK_GE(byte_size, alloc_size);
@@ -3289,7 +3698,7 @@ mirror::Object* ConcurrentCopying::AllocateInSkippedBlock(Thread* const self, si
     if (kVerboseMode) {
       LOG(INFO) << "Reusing skipped bytes : " << reinterpret_cast<void*>(addr) << ", " << byte_size;
     }
-    skipped_blocks_map_.erase(it);
+    skipped_blocks_map->erase(it);
   }
   memset(addr, 0, byte_size);
   if (byte_size > alloc_size) {
@@ -3305,7 +3714,7 @@ mirror::Object* ConcurrentCopying::AllocateInSkippedBlock(Thread* const self, si
     CHECK(region_space_->IsInToSpace(reinterpret_cast<mirror::Object*>(addr + alloc_size)));
     {
       MutexLock mu(self, skipped_blocks_lock_);
-      skipped_blocks_map_.insert(std::make_pair(byte_size - alloc_size, addr + alloc_size));
+      skipped_blocks_map->insert(std::make_pair(byte_size - alloc_size, addr + alloc_size));
     }
   }
   return reinterpret_cast<mirror::Object*>(addr);
@@ -3331,19 +3740,20 @@ mirror::Object* ConcurrentCopying::Copy(Thread* const self,
   size_t region_space_alloc_size = (obj_size <= space::RegionSpace::kRegionSize)
       ? RoundUp(obj_size, space::RegionSpace::kAlignment)
       : RoundUp(obj_size, space::RegionSpace::kRegionSize);
+  uint8_t age = region_space_->GetAgeForForwarding(from_ref);
   size_t region_space_bytes_allocated = 0U;
   size_t non_moving_space_bytes_allocated = 0U;
   size_t bytes_allocated = 0U;
   size_t dummy;
   bool fall_back_to_non_moving = false;
   mirror::Object* to_ref = region_space_->AllocNonvirtual</*kForEvac=*/ true>(
-      region_space_alloc_size, &region_space_bytes_allocated, nullptr, &dummy);
+      region_space_alloc_size, age, &region_space_bytes_allocated, nullptr, &dummy);
   bytes_allocated = region_space_bytes_allocated;
   if (LIKELY(to_ref != nullptr)) {
     DCHECK_EQ(region_space_alloc_size, region_space_bytes_allocated);
   } else {
     // Failed to allocate in the region space. Try the skipped blocks.
-    to_ref = AllocateInSkippedBlock(self, region_space_alloc_size);
+    to_ref = AllocateInSkippedBlock(self, region_space_alloc_size, age);
     if (to_ref != nullptr) {
       // Succeeded to allocate in a skipped block.
       if (heap_->use_tlab_) {
@@ -3411,8 +3821,8 @@ mirror::Object* ConcurrentCopying::Copy(Thread* const self,
           to_space_bytes_skipped_.fetch_add(bytes_allocated, std::memory_order_relaxed);
           to_space_objects_skipped_.fetch_add(1, std::memory_order_relaxed);
           MutexLock mu(self, skipped_blocks_lock_);
-          skipped_blocks_map_.insert(std::make_pair(bytes_allocated,
-                                                    reinterpret_cast<uint8_t*>(to_ref)));
+          age_skipped_blocks_map_[age].insert(std::make_pair(bytes_allocated,
+                                          reinterpret_cast<uint8_t*>(to_ref)));
         }
       } else {
         DCHECK(heap_->non_moving_space_->HasAddress(to_ref));
@@ -3466,7 +3876,7 @@ mirror::Object* ConcurrentCopying::Copy(Thread* const self,
       } else {
         DCHECK(heap_->non_moving_space_->HasAddress(to_ref));
         DCHECK_EQ(bytes_allocated, non_moving_space_bytes_allocated);
-        if (!use_generational_cc_ || !young_gen_) {
+        if (!use_generational_cc_ || IsFullGC()) {
           // Mark it in the live bitmap.
           CHECK(!heap_->non_moving_space_->GetLiveBitmap()->AtomicTestAndSet(to_ref));
         }
@@ -3502,7 +3912,8 @@ mirror::Object* ConcurrentCopying::IsMarked(mirror::Object* from_ref) {
     DCHECK(to_ref == nullptr || region_space_->IsInToSpace(to_ref) ||
            heap_->non_moving_space_->HasAddress(to_ref))
         << "from_ref=" << from_ref << " to_ref=" << to_ref;
-  } else if (rtype == space::RegionSpace::RegionType::kRegionTypeUnevacFromSpace) {
+  } else if (rtype == space::RegionSpace::RegionType::kRegionTypeUnevacFromSpace ||
+             rtype == space::RegionSpace::RegionType::kRegionTypeUnevacFromSpaceTenured) {
     if (IsMarkedInUnevacFromSpace(from_ref)) {
       to_ref = from_ref;
     } else {
@@ -3635,13 +4046,15 @@ void ConcurrentCopying::FinishPhase() {
     TimingLogger::ScopedTiming split("ClearRegionSpaceCards", GetTimings());
     // We do not currently use the region space cards at all, madvise them away to save ram.
     heap_->GetCardTable()->ClearCardRange(region_space_->Begin(), region_space_->Limit());
-  } else if (use_generational_cc_ && !young_gen_) {
+  } else if (use_generational_cc_ && !IsYoungGC()) {
     region_space_inter_region_bitmap_->Clear();
     non_moving_space_inter_region_bitmap_->Clear();
   }
   {
     MutexLock mu(self, skipped_blocks_lock_);
-    skipped_blocks_map_.clear();
+    for (size_t i = 0; i < kAgeSkipBlockMapSize; ++i) {
+      age_skipped_blocks_map_[i].clear();
+    }
   }
   {
     ReaderMutexLock mu(self, *Locks::mutator_lock_);
@@ -3743,8 +4156,8 @@ mirror::Object* ConcurrentCopying::MarkFromReadBarrierWithMeasurements(Thread* c
   ScopedTrace tr(__FUNCTION__);
   const uint64_t start_time = measure_read_barrier_slow_path_ ? NanoTime() : 0u;
   mirror::Object* ret =
-      Mark</*kGrayImmuneObject=*/true, /*kNoUnEvac=*/false, /*kFromGCThread=*/false>(self,
-                                                                                     from_ref);
+      Mark</*kGrayImmuneObject=*/true, RegionTypeFlag::kIgnoreNone,
+           /*kFromGCThread=*/false>(self, from_ref);
   if (measure_read_barrier_slow_path_) {
     rb_slow_path_ns_.fetch_add(NanoTime() - start_time, std::memory_order_relaxed);
   }
@@ -3767,13 +4180,13 @@ void ConcurrentCopying::DumpPerformanceInfo(std::ostream& os) {
     os << "GC slow path count " << rb_slow_path_count_gc_total_ << "\n";
   }
 
-  os << "Average " << (young_gen_ ? "minor" : "major") << " GC reclaim bytes ratio "
+  os << "Average " << (IsYoungGC() ? "minor" : "major") << " GC reclaim bytes ratio "
      << (reclaimed_bytes_ratio_sum_ / num_gc_cycles) << " over " << num_gc_cycles
      << " GC cycles\n";
 
-  os << "Average " << (young_gen_ ? "minor" : "major") << " GC copied live bytes ratio "
+  os << "Average " << (IsYoungGC() ? "minor" : "major") << " GC copied live bytes ratio "
      << (copied_live_bytes_ratio_sum_ / gc_count_) << " over " << gc_count_
-     << " " << (young_gen_ ? "minor" : "major") << " GCs\n";
+     << " " << (IsYoungGC() ? "minor" : "major") << " GCs\n";
 
   os << "Cumulative bytes moved "
      << cumulative_bytes_moved_.load(std::memory_order_relaxed) << "\n";
@@ -3788,6 +4201,46 @@ void ConcurrentCopying::DumpPerformanceInfo(std::ostream& os) {
      << ")\n";
 }
 
+void ConcurrentCopying::RememberCard(mirror::Object* ref) {
+  DCHECK(use_generational_cc_ && use_midterm_cc_);
+  uint8_t* card = GetHeap()->GetCardTable()->CardFromAddr(ref);
+  if (region_space_->HasAddress(ref)) {
+    region_space_rem_set_->AddDirtyCard(card);
+  } else {
+    DCHECK(heap_->GetNonMovingSpace()->HasAddress(ref));
+    non_moving_space_rem_set_->AddDirtyCard(card);
+  }
+}
+
+void ConcurrentCopying::ClearDirtyCards(space::ContinuousSpace* space) {
+  accounting::CardTable* const card_table = heap_->GetCardTable();
+  DCHECK(use_generational_cc_ && use_midterm_cc_);
+  if (space == region_space_) {
+    DCHECK(region_space_rem_set_ != nullptr);
+    // Clear only the cards belonging to Tenured regions
+    card_table->ModifyCardsAtomic(
+        region_space_->Begin(),
+        region_space_->End(),
+        AgeCardVisitor(),
+        [this, card_table] (uint8_t* card,
+                            uint8_t expected_value,
+                            uint8_t new_value ATTRIBUTE_UNUSED) {
+          mirror::Object* obj = reinterpret_cast<mirror::Object*>(card_table->AddrFromCard(card));
+          if (expected_value == accounting::CardTable::kCardDirty &&
+              (region_space_->GetGen</*kEvacuating=*/ false>(obj) ==
+                RegionGen::kRegionGenTenured)) {
+            // Case-1: Inter-gen references because of Heap mutation
+            region_space_rem_set_->AddDirtyCard(card);
+          }
+        });
+  } else {
+    DCHECK(space == heap_->non_moving_space_);
+    DCHECK(non_moving_space_rem_set_ != nullptr);
+    // Case-1: Inter-gen references because of Heap mutation
+    non_moving_space_rem_set_->ClearCards();
+  }
+}
+
 }  // namespace collector
 }  // namespace gc
 }  // namespace art
diff --git a/runtime/gc/collector/concurrent_copying.h b/runtime/gc/collector/concurrent_copying.h
index 2e5752b91e..19d04b9c45 100644
--- a/runtime/gc/collector/concurrent_copying.h
+++ b/runtime/gc/collector/concurrent_copying.h
@@ -20,6 +20,7 @@
 #include "garbage_collector.h"
 #include "immune_spaces.h"
 #include "offsets.h"
+#include "gc/space/region_space.h"
 
 #include <map>
 #include <memory>
@@ -64,10 +65,23 @@ class ConcurrentCopying : public GarbageCollector {
   // If kGrayDirtyImmuneObjects is true then we gray dirty objects in the GC pause to prevent dirty
   // pages.
   static constexpr bool kGrayDirtyImmuneObjects = true;
+  static constexpr size_t kAgeSkipBlockMapSize = space::RegionSpace::kAgeRegionMapSize;
+  // A temporary store for storing computed threshold.
+  // Necessary to take decisions to rebuild-Remsets.
+  // Note that this has no effect, the real threshold must be set on region-space
+  static uint8_t tenure_threshold_;
+
+  enum class RegionTypeFlag : uint8_t {
+    kIgnoreNone = 0,                  // Ignore nothing
+    kIgnoreUnevacFromSpaceTenured,    // Ignore all UnevacFromSpace objects that are tenured
+    kIgnoreUnevacFromSpace,           // Ignore all UnevacFromSpace objects
+  };
 
   ConcurrentCopying(Heap* heap,
-                    bool young_gen,
+                    GcType type_,
                     bool use_generational_cc,
+                    bool use_midterm_cc,
+                    bool use_dynamic_threshold,
                     const std::string& name_prefix = "",
                     bool measure_read_barrier_slow_path = false);
   ~ConcurrentCopying();
@@ -92,9 +106,7 @@ class ConcurrentCopying : public GarbageCollector {
   void BindBitmaps() REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!Locks::heap_bitmap_lock_);
   GcType GetGcType() const override {
-    return (use_generational_cc_ && young_gen_)
-        ? kGcTypeSticky
-        : kGcTypePartial;
+    return type_;
   }
   CollectorType GetCollectorType() const override {
     return kCollectorTypeCC;
@@ -121,7 +133,9 @@ class ConcurrentCopying : public GarbageCollector {
     return IsMarked(ref) == ref;
   }
   // Mark object `from_ref`, copying it to the to-space if needed.
-  template<bool kGrayImmuneObject = true, bool kNoUnEvac = false, bool kFromGCThread = false>
+  template<bool kGrayImmuneObject = true,
+           RegionTypeFlag kRegionTypeFlag = RegionTypeFlag::kIgnoreNone,
+           bool kFromGCThread = false>
   ALWAYS_INLINE mirror::Object* Mark(Thread* const self,
                                      mirror::Object* from_ref,
                                      mirror::Object* holder = nullptr,
@@ -165,18 +179,18 @@ class ConcurrentCopying : public GarbageCollector {
       REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!mark_stack_lock_, !skipped_blocks_lock_, !immune_gray_stack_lock_);
   // Scan the reference fields of object `to_ref`.
-  template <bool kNoUnEvac>
+  template <RegionTypeFlag kRegionTypeFlag, bool kRememberInterGenRefs = false>
   void Scan(mirror::Object* to_ref) REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!mark_stack_lock_);
   // Scan the reference fields of object 'obj' in the dirty cards during
   // card-table scan. In addition to visiting the references, it also sets the
   // read-barrier state to gray for Reference-type objects to ensure that
   // GetReferent() called on these objects calls the read-barrier on the referent.
-  template <bool kNoUnEvac>
+  template <RegionTypeFlag kRegionTypeFlag, bool kCheckForInterGenRefs = false>
   void ScanDirtyObject(mirror::Object* obj) REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!mark_stack_lock_);
   // Process a field.
-  template <bool kNoUnEvac>
+  template <RegionTypeFlag kRegionTypeFlag>
   void Process(mirror::Object* obj, MemberOffset offset)
       REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!mark_stack_lock_ , !skipped_blocks_lock_, !immune_gray_stack_lock_);
@@ -257,7 +271,7 @@ class ConcurrentCopying : public GarbageCollector {
   void FillWithDummyObject(Thread* const self, mirror::Object* dummy_obj, size_t byte_size)
       REQUIRES(!mark_stack_lock_, !skipped_blocks_lock_, !immune_gray_stack_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_);
-  mirror::Object* AllocateInSkippedBlock(Thread* const self, size_t alloc_size)
+  mirror::Object* AllocateInSkippedBlock(Thread* const self, size_t alloc_size, uint8_t age)
       REQUIRES(!mark_stack_lock_, !skipped_blocks_lock_, !immune_gray_stack_lock_)
       REQUIRES_SHARED(Locks::mutator_lock_);
   void CheckEmptyMarkStack() REQUIRES_SHARED(Locks::mutator_lock_) REQUIRES(!mark_stack_lock_);
@@ -313,6 +327,7 @@ class ConcurrentCopying : public GarbageCollector {
   void ActivateReadBarrierEntrypoints();
 
   void CaptureThreadRootsForMarking() REQUIRES_SHARED(Locks::mutator_lock_);
+  template <bool kRememberInterGenRefs>
   void AddLiveBytesAndScanRef(mirror::Object* ref) REQUIRES_SHARED(Locks::mutator_lock_);
   bool TestMarkBitmapForRef(mirror::Object* ref) REQUIRES_SHARED(Locks::mutator_lock_);
   template <bool kAtomic = false>
@@ -321,6 +336,12 @@ class ConcurrentCopying : public GarbageCollector {
   void ProcessMarkStackForMarkingAndComputeLiveBytes() REQUIRES_SHARED(Locks::mutator_lock_)
       REQUIRES(!mark_stack_lock_);
 
+  ALWAYS_INLINE bool IsYoungGC() { return type_ == kGcTypeSticky; }
+  ALWAYS_INLINE bool IsMidTermGC() { return type_ == kGcTypeMidTerm; }
+  ALWAYS_INLINE bool IsFullGC() { return type_ == kGcTypePartial; }
+  void ClearDirtyCards(space::ContinuousSpace* space) REQUIRES_SHARED(Locks::mutator_lock_);
+  void RememberCard(mirror::Object* ref) REQUIRES_SHARED(Locks::mutator_lock_);
+
   space::RegionSpace* region_space_;      // The underlying region space.
   std::unique_ptr<Barrier> gc_barrier_;
   std::unique_ptr<accounting::ObjectStack> gc_mark_stack_;
@@ -330,10 +351,15 @@ class ConcurrentCopying : public GarbageCollector {
   // for major collections. Generational CC collection is currently only
   // compatible with Baker read barriers. Set in Heap constructor.
   const bool use_generational_cc_;
+  const bool use_midterm_cc_;
 
   // Generational "sticky", only trace through dirty objects in region space.
-  const bool young_gen_;
+  // Mid-term GC, trace through Young/Aged objects in region space.
+  const GcType type_;
 
+  // If set we try to compute tenure threshold dynamically at the end of
+  // marking phase of a Full-GC cycle
+  const bool use_dynamic_threshold_;
   // If true, the GC thread is done scanning marked objects on dirty and aged
   // card (see ConcurrentCopying::CopyingPhase).
   Atomic<bool> done_scanning_;
@@ -412,6 +438,9 @@ class ConcurrentCopying : public GarbageCollector {
   std::unique_ptr<accounting::ContinuousSpaceBitmap> region_space_inter_region_bitmap_;
   std::unique_ptr<accounting::ContinuousSpaceBitmap> non_moving_space_inter_region_bitmap_;
 
+  // Rememembered sets to identify inter-gen references
+  accounting::RememberedSet* region_space_rem_set_;
+  accounting::RememberedSet* non_moving_space_rem_set_;
   // reclaimed_bytes_ratio = reclaimed_bytes/num_allocated_bytes per GC cycle
   float reclaimed_bytes_ratio_sum_;
 
@@ -423,7 +452,10 @@ class ConcurrentCopying : public GarbageCollector {
   // used without going through a GC cycle like other objects. They are reused only
   // if we run out of region space. TODO: Revisit this design.
   Mutex skipped_blocks_lock_ DEFAULT_MUTEX_ACQUIRED_AFTER;
-  std::multimap<size_t, uint8_t*> skipped_blocks_map_ GUARDED_BY(skipped_blocks_lock_);
+  // TODO: Need to have individual locks for each age
+  // The array itself doesnt need any guards, but each of the element
+  // skipped_blocks_maps_[i] should be guarded by skipped_blocks_lock_[i]
+  std::multimap<size_t, uint8_t*> age_skipped_blocks_map_[kAgeSkipBlockMapSize];
   Atomic<size_t> to_space_bytes_skipped_;
   Atomic<size_t> to_space_objects_skipped_;
 
@@ -461,6 +493,8 @@ class ConcurrentCopying : public GarbageCollector {
   // Use signed because after_gc may be larger than before_gc.
   int64_t num_bytes_allocated_before_gc_;
 
+  bool rebuild_rem_sets_during_mark_phase_;
+
   class ActivateReadBarrierEntrypointsCallback;
   class ActivateReadBarrierEntrypointsCheckpoint;
   class AssertToSpaceInvariantFieldVisitor;
@@ -474,7 +508,8 @@ class ConcurrentCopying : public GarbageCollector {
   template <bool kConcurrent> class GrayImmuneObjectVisitor;
   class ImmuneSpaceScanObjVisitor;
   class LostCopyVisitor;
-  template <bool kNoUnEvac> class RefFieldsVisitor;
+  template <RegionTypeFlag kRegionTypeFlag , bool kRememberInterGenRefs>
+  class RefFieldsVisitor;
   class RevokeThreadLocalMarkStackCheckpoint;
   class ScopedGcGraysImmuneObjects;
   class ThreadFlipVisitor;
@@ -485,7 +520,8 @@ class ConcurrentCopying : public GarbageCollector {
   class ImmuneSpaceCaptureRefsVisitor;
   template <bool kAtomicTestAndSet = false> class CaptureRootsForMarkingVisitor;
   class CaptureThreadRootsForMarkingAndCheckpoint;
-  template <bool kHandleInterRegionRefs> class ComputeLiveBytesAndMarkRefFieldsVisitor;
+  template <bool kHandleInterRegionRefs, bool kHandleInterGenRefs> class ComputeLiveBytesAndMarkRefFieldsVisitor;
+  class RememberedSetMarkingVisitor;
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(ConcurrentCopying);
 };
diff --git a/runtime/gc/collector/gc_type.h b/runtime/gc/collector/gc_type.h
index 401444a01d..a2e03aeede 100644
--- a/runtime/gc/collector/gc_type.h
+++ b/runtime/gc/collector/gc_type.h
@@ -30,6 +30,9 @@ enum GcType {
   kGcTypeNone,
   // Sticky mark bits GC that attempts to only free objects allocated since the last GC.
   kGcTypeSticky,
+  // Mid Term GC that marks and frees objects living for a certain collections as
+  // defined by tenure threshold.
+  kGcTypeMidTerm,
   // Partial GC that marks the application heap but not the Zygote.
   kGcTypePartial,
   // Full GC that marks and frees in both the application and Zygote heap.
diff --git a/runtime/gc/heap-inl.h b/runtime/gc/heap-inl.h
index 1c09b5c9bf..824da31e0c 100644
--- a/runtime/gc/heap-inl.h
+++ b/runtime/gc/heap-inl.h
@@ -346,6 +346,7 @@ inline mirror::Object* Heap::TryToAllocate(Thread* self,
       DCHECK(region_space_ != nullptr);
       alloc_size = RoundUp(alloc_size, space::RegionSpace::kAlignment);
       ret = region_space_->AllocNonvirtual<false>(alloc_size,
+                                                  space::RegionSpace::kRegionAgeNewlyAllocated,
                                                   bytes_allocated,
                                                   usable_size,
                                                   bytes_tl_bulk_allocated);
diff --git a/runtime/gc/heap.cc b/runtime/gc/heap.cc
index ff53f7896e..e3ec8d1c3f 100644
--- a/runtime/gc/heap.cc
+++ b/runtime/gc/heap.cc
@@ -111,6 +111,10 @@ static constexpr size_t kMaxConcurrentRemainingBytes = 512 * KB;
 static double GetStickyGcThroughputAdjustment(bool use_generational_cc) {
   return use_generational_cc ? 0.5 : 1.0;
 }
+static constexpr size_t kRemainingFullCollectorBeforeMidTerm = 2;
+static double GetMidTermGcThroughputAdjustment() {
+  return 0.75;
+}
 // Whether or not we compact the zygote in PreZygoteFork.
 static constexpr bool kCompactZygote = kMovingCollector;
 // How many reserve entries are at the end of the allocation stack, these are only needed if the
@@ -204,6 +208,8 @@ Heap::Heap(size_t initial_size,
            bool measure_gc_performance,
            bool use_homogeneous_space_compaction_for_oom,
            bool use_generational_cc,
+           bool use_midterm_cc,
+           size_t tenure_threshold,
            uint64_t min_interval_homogeneous_space_compaction_by_oom,
            bool dump_region_info_before_gc,
            bool dump_region_info_after_gc,
@@ -238,6 +244,9 @@ Heap::Heap(size_t initial_size,
       thread_running_gc_(nullptr),
       last_gc_type_(collector::kGcTypeNone),
       next_gc_type_(collector::kGcTypePartial),
+      preferred_non_sticky_gc_type_(collector::kGcTypeNone),
+      preferred_non_sticky_remaining_count_(kRemainingFullCollectorBeforeMidTerm),
+      ignore_mid_term_(false),
       capacity_(capacity),
       growth_limit_(growth_limit),
       target_footprint_(initial_size),
@@ -282,6 +291,7 @@ Heap::Heap(size_t initial_size,
       semi_space_collector_(nullptr),
       active_concurrent_copying_collector_(nullptr),
       young_concurrent_copying_collector_(nullptr),
+      midterm_concurrent_copying_collector_(nullptr),
       concurrent_copying_collector_(nullptr),
       is_running_on_memory_tool_(Runtime::Current()->IsRunningOnMemoryTool()),
       use_tlab_(use_tlab),
@@ -293,6 +303,7 @@ Heap::Heap(size_t initial_size,
       pending_heap_trim_(nullptr),
       use_homogeneous_space_compaction_for_oom_(use_homogeneous_space_compaction_for_oom),
       use_generational_cc_(use_generational_cc),
+      use_midterm_cc_(use_midterm_cc),
       running_collection_is_blocking_(false),
       blocking_gc_count_(0U),
       blocking_gc_time_(0U),
@@ -505,7 +516,7 @@ Heap::Heap(size_t initial_size,
         space::RegionSpace::CreateMemMap(kRegionSpaceName, capacity_ * 2, request_begin);
     CHECK(region_space_mem_map.IsValid()) << "No region space mem map";
     region_space_ = space::RegionSpace::Create(
-        kRegionSpaceName, std::move(region_space_mem_map), use_generational_cc_);
+        kRegionSpaceName, std::move(region_space_mem_map), use_generational_cc_, use_midterm_cc_);
     AddSpace(region_space_);
   } else if (IsMovingGc(foreground_collector_type_) &&
       foreground_collector_type_ != kCollectorTypeGSS) {
@@ -661,18 +672,36 @@ Heap::Heap(size_t initial_size,
       garbage_collectors_.push_back(semi_space_collector_);
     }
     if (MayUseCollector(kCollectorTypeCC)) {
+      // If tenure threshold is default, we would compute it dynamically,
+      // else, we would use the set value for the run.
+      bool dyn_threshold = (use_midterm_cc_ && 
+                            tenure_threshold == space::RegionSpace::kRegionMaxAgeTenureThreshold);
       concurrent_copying_collector_ = new collector::ConcurrentCopying(this,
-                                                                       /*young_gen=*/false,
+                                                                       collector::kGcTypePartial,
                                                                        use_generational_cc_,
+                                                                       use_midterm_cc_,
+                                                                       dyn_threshold,
                                                                        "",
                                                                        measure_gc_performance);
       if (use_generational_cc_) {
         young_concurrent_copying_collector_ = new collector::ConcurrentCopying(
             this,
-            /*young_gen=*/true,
+            collector::kGcTypeSticky,
             use_generational_cc_,
+            use_midterm_cc_,
+            dyn_threshold,
             "young",
             measure_gc_performance);
+        if (use_midterm_cc_) {
+          midterm_concurrent_copying_collector_ = new collector::ConcurrentCopying(
+            this,
+            collector::kGcTypeMidTerm,
+            use_generational_cc_,
+            use_midterm_cc_,
+            dyn_threshold,
+            "mid-term",
+            measure_gc_performance);
+        }
       }
       active_concurrent_copying_collector_ = concurrent_copying_collector_;
       DCHECK(region_space_ != nullptr);
@@ -682,10 +711,21 @@ Heap::Heap(size_t initial_size,
         // At this point, non-moving space should be created.
         DCHECK(non_moving_space_ != nullptr);
         concurrent_copying_collector_->CreateInterRegionRefBitmaps();
+        if (use_midterm_cc_) {
+          midterm_concurrent_copying_collector_->SetRegionSpace(region_space_);
+          midterm_concurrent_copying_collector_->CreateInterRegionRefBitmaps();
+          accounting::RememberedSet* region_space_rem_set =
+              new accounting::RememberedSet("Region space remembered set", this, region_space_);
+          CHECK(region_space_rem_set != nullptr) << "Failed to create region space remembered set";
+          AddRememberedSet(region_space_rem_set);
+        }
       }
       garbage_collectors_.push_back(concurrent_copying_collector_);
       if (use_generational_cc_) {
         garbage_collectors_.push_back(young_concurrent_copying_collector_);
+        if (use_midterm_cc_) {
+          garbage_collectors_.push_back(midterm_concurrent_copying_collector_);
+        }
       }
     }
   }
@@ -2281,6 +2321,9 @@ void Heap::ChangeCollector(CollectorType collector_type) {
       case kCollectorTypeCC: {
         if (use_generational_cc_) {
           gc_plan_.push_back(collector::kGcTypeSticky);
+          if (use_midterm_cc_) {
+            gc_plan_.push_back(collector::kGcTypeMidTerm);
+          }
         }
         gc_plan_.push_back(collector::kGcTypeFull);
         if (use_tlab_) {
@@ -2759,8 +2802,13 @@ collector::GcType Heap::CollectGarbageInternal(collector::GcType gc_type,
         if (use_generational_cc_) {
           // TODO: Other threads must do the flip checkpoint before they start poking at
           // active_concurrent_copying_collector_. So we should not concurrency here.
-          active_concurrent_copying_collector_ = (gc_type == collector::kGcTypeSticky) ?
-              young_concurrent_copying_collector_ : concurrent_copying_collector_;
+          if (gc_type == collector::kGcTypeSticky) {
+            active_concurrent_copying_collector_ = young_concurrent_copying_collector_;
+          } else if (use_midterm_cc_ && gc_type == collector::kGcTypeMidTerm) {
+            active_concurrent_copying_collector_ = midterm_concurrent_copying_collector_;
+          } else {
+            active_concurrent_copying_collector_ = concurrent_copying_collector_;
+          }
           DCHECK(active_concurrent_copying_collector_->RegionSpace() == region_space_);
         }
         collector = active_concurrent_copying_collector_;
@@ -3647,6 +3695,45 @@ void Heap::GrowForUtilization(collector::GarbageCollector* collector_ran,
   const size_t adjusted_min_free = static_cast<size_t>(min_free_ * multiplier);
   const size_t adjusted_max_free = static_cast<size_t>(max_free_ * multiplier);
   if (gc_type != collector::kGcTypeSticky) {
+    if (use_midterm_cc_) {
+      // We want to determine if the GC we ran now, is doing a good job and 
+      // choose a preference for non_sticky_type
+      collector::GarbageCollector* mid_term_collector =
+        FindCollectorByGcType(collector::kGcTypeMidTerm);
+      // remove preference, to get default non sticky type collector
+      preferred_non_sticky_gc_type_ = collector::kGcTypeNone;
+      double midterm_gc_throughput_adjustment = GetMidTermGcThroughputAdjustment();
+      if (mid_term_collector != nullptr && !ignore_mid_term_) {
+        // Find what the next non sticky collector will be.
+        collector::GarbageCollector* non_sticky_collector =
+            FindCollectorByGcType(NonStickyGcType());
+        if (use_generational_cc_) {
+          if (non_sticky_collector == nullptr) {
+            non_sticky_collector = FindCollectorByGcType(collector::kGcTypePartial);
+          }
+          CHECK(non_sticky_collector != nullptr);
+        }
+        if (gc_type == collector::kGcTypeMidTerm) {
+          //If we did a good job, restore the preference
+          if (non_sticky_collector->NumberOfIterations() >= kRemainingFullCollectorBeforeMidTerm &&
+              current_gc_iteration_.GetEstimatedThroughput() * midterm_gc_throughput_adjustment  >=
+              non_sticky_collector->GetEstimatedMeanThroughput()) {
+            preferred_non_sticky_gc_type_ = collector::kGcTypeMidTerm;
+          } else {
+            preferred_non_sticky_remaining_count_ = kRemainingFullCollectorBeforeMidTerm;
+          }
+        } else {
+          //Set the preference if mid_term has been doing a good job or if its time to try it out
+          if (--preferred_non_sticky_remaining_count_ == 0 ||
+              (mid_term_collector->NumberOfIterations() > 0 &&
+              non_sticky_collector->NumberOfIterations() >= kRemainingFullCollectorBeforeMidTerm &&
+              mid_term_collector->GetEstimatedMeanThroughput() * midterm_gc_throughput_adjustment >
+              non_sticky_collector->GetEstimatedMeanThroughput())) {
+            preferred_non_sticky_gc_type_ = collector::kGcTypeMidTerm;
+          }
+        }
+      }
+    }
     // Grow the heap for non sticky GC.
     uint64_t delta = bytes_allocated * (1.0 / GetTargetHeapUtilization() - 1.0);
     DCHECK_LE(delta, std::numeric_limits<size_t>::max()) << "bytes_allocated=" << bytes_allocated
@@ -3681,6 +3768,10 @@ void Heap::GrowForUtilization(collector::GarbageCollector* collector_ran,
         non_sticky_collector->NumberOfIterations() > 0 &&
         bytes_allocated <= (IsGcConcurrent() ? concurrent_start_bytes_ : target_footprint)) {
       next_gc_type_ = collector::kGcTypeSticky;
+      if (use_midterm_cc_ && !ignore_mid_term_) {
+        //Sticky GC seems to be doing a good job, mid-term might do a good job as well.
+        preferred_non_sticky_gc_type_ = collector::kGcTypeMidTerm;
+      }
     } else {
       next_gc_type_ = non_sticky_gc_type;
     }
@@ -4358,6 +4449,7 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
         if (!region_space_->AllocNewTlab(self, new_tlab_size)) {
           // Failed to allocate a tlab. Try non-tlab.
           return region_space_->AllocNonvirtual<false>(alloc_size,
+                                                       space::RegionSpace::kRegionAgeNewlyAllocated,
                                                        bytes_allocated,
                                                        usable_size,
                                                        bytes_tl_bulk_allocated);
@@ -4368,6 +4460,7 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
         // Check OOME for a non-tlab allocation.
         if (!IsOutOfMemoryOnAllocation(allocator_type, alloc_size, grow)) {
           return region_space_->AllocNonvirtual<false>(alloc_size,
+                                                       space::RegionSpace::kRegionAgeNewlyAllocated,
                                                        bytes_allocated,
                                                        usable_size,
                                                        bytes_tl_bulk_allocated);
@@ -4379,6 +4472,7 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
       // Large. Check OOME.
       if (LIKELY(!IsOutOfMemoryOnAllocation(allocator_type, alloc_size, grow))) {
         return region_space_->AllocNonvirtual<false>(alloc_size,
+                                                     space::RegionSpace::kRegionAgeNewlyAllocated,
                                                      bytes_allocated,
                                                      usable_size,
                                                      bytes_tl_bulk_allocated);
diff --git a/runtime/gc/heap.h b/runtime/gc/heap.h
index 5cf197869d..99a69a557a 100644
--- a/runtime/gc/heap.h
+++ b/runtime/gc/heap.h
@@ -214,6 +214,8 @@ class Heap {
        bool measure_gc_performance,
        bool use_homogeneous_space_compaction,
        bool use_generational_cc,
+       bool use_midterm_cc,
+       size_t tenure_threshold,
        uint64_t min_interval_homogeneous_space_compaction_by_oom,
        bool dump_region_info_before_gc,
        bool dump_region_info_after_gc,
@@ -539,6 +541,10 @@ class Heap {
     return use_generational_cc_;
   }
 
+  bool GetUseMidTermCC() const {
+    return use_midterm_cc_;
+  }
+
   // Returns the number of objects currently allocated.
   size_t GetObjectsAllocated() const
       REQUIRES(!Locks::heap_bitmap_lock_);
@@ -776,8 +782,14 @@ class Heap {
   // Returns the active concurrent copying collector.
   collector::ConcurrentCopying* ConcurrentCopyingCollector() {
     if (use_generational_cc_) {
-      DCHECK((active_concurrent_copying_collector_ == concurrent_copying_collector_) ||
-             (active_concurrent_copying_collector_ == young_concurrent_copying_collector_));
+      if (use_midterm_cc_) {
+        DCHECK((active_concurrent_copying_collector_ == concurrent_copying_collector_) ||
+              (active_concurrent_copying_collector_ == young_concurrent_copying_collector_) ||
+              (active_concurrent_copying_collector_ == midterm_concurrent_copying_collector_));
+      } else {
+        DCHECK((active_concurrent_copying_collector_ == concurrent_copying_collector_) ||
+              (active_concurrent_copying_collector_ == young_concurrent_copying_collector_));
+      }
     } else {
       DCHECK_EQ(active_concurrent_copying_collector_, concurrent_copying_collector_);
     }
@@ -896,6 +908,8 @@ class Heap {
   const Verification* GetVerification() const;
 
   void PostForkChildAction(Thread* self);
+  ALWAYS_INLINE void SetIgnoreMidTerm(bool bIgnore) { ignore_mid_term_ = bIgnore; }
+  ALWAYS_INLINE bool IgnoreMidTerm() { return ignore_mid_term_; }
 
  private:
   class ConcurrentGCTask;
@@ -1162,6 +1176,9 @@ class Heap {
       REQUIRES(!*gc_complete_lock_, !*pending_task_lock_, !*backtrace_lock_);
 
   collector::GcType NonStickyGcType() const {
+    if (preferred_non_sticky_gc_type_ != collector::kGcTypeNone) {
+      return preferred_non_sticky_gc_type_;
+    }
     return HasZygoteSpace() ? collector::kGcTypePartial : collector::kGcTypeFull;
   }
 
@@ -1310,6 +1327,9 @@ class Heap {
   // Last Gc type we ran. Used by WaitForConcurrentGc to know which Gc was waited on.
   volatile collector::GcType last_gc_type_ GUARDED_BY(gc_complete_lock_);
   collector::GcType next_gc_type_;
+  collector::GcType preferred_non_sticky_gc_type_;
+  size_t preferred_non_sticky_remaining_count_;
+  bool ignore_mid_term_;
 
   // Maximum size that the heap can reach.
   size_t capacity_;
@@ -1460,6 +1480,7 @@ class Heap {
   collector::SemiSpace* semi_space_collector_;
   collector::ConcurrentCopying* active_concurrent_copying_collector_;
   collector::ConcurrentCopying* young_concurrent_copying_collector_;
+  collector::ConcurrentCopying* midterm_concurrent_copying_collector_;
   collector::ConcurrentCopying* concurrent_copying_collector_;
 
   const bool is_running_on_memory_tool_;
@@ -1501,6 +1522,7 @@ class Heap {
   // (CC) collector, i.e. use sticky-bit CC for minor collections and (full) CC
   // for major collections. Set in Heap constructor.
   const bool use_generational_cc_;
+  const bool use_midterm_cc_;
 
   // True if the currently running collection has made some thread wait.
   bool running_collection_is_blocking_ GUARDED_BY(gc_complete_lock_);
diff --git a/runtime/gc/space/region_space-inl.h b/runtime/gc/space/region_space-inl.h
index 86a0a6e418..e9eb382eec 100644
--- a/runtime/gc/space/region_space-inl.h
+++ b/runtime/gc/space/region_space-inl.h
@@ -34,7 +34,7 @@ inline mirror::Object* RegionSpace::Alloc(Thread* self ATTRIBUTE_UNUSED,
                                           /* out */ size_t* usable_size,
                                           /* out */ size_t* bytes_tl_bulk_allocated) {
   num_bytes = RoundUp(num_bytes, kAlignment);
-  return AllocNonvirtual<false>(num_bytes, bytes_allocated, usable_size,
+  return AllocNonvirtual<false>(num_bytes, kRegionAgeNewlyAllocated, bytes_allocated, usable_size,
                                 bytes_tl_bulk_allocated);
 }
 
@@ -49,14 +49,18 @@ inline mirror::Object* RegionSpace::AllocThreadUnsafe(Thread* self,
 
 template<bool kForEvac>
 inline mirror::Object* RegionSpace::AllocNonvirtual(size_t num_bytes,
+                                                    uint8_t age,
                                                     /* out */ size_t* bytes_allocated,
                                                     /* out */ size_t* usable_size,
                                                     /* out */ size_t* bytes_tl_bulk_allocated) {
   DCHECK_ALIGNED(num_bytes, kAlignment);
+  CHECK(age <= kRegionMaxAgeTenured);
+  DCHECK(!use_midterm_cc_ || kForEvac == (age != kRegionAgeNewlyAllocated));
+  DCHECK(use_midterm_cc_ || (age == kRegionAgeNewlyAllocated));
   mirror::Object* obj;
   if (LIKELY(num_bytes <= kRegionSize)) {
     // Non-large object.
-    obj = (kForEvac ? evac_region_ : current_region_)->Alloc(num_bytes,
+    obj = (kForEvac ? evac_region_[age] : current_region_)->Alloc(num_bytes,
                                                              bytes_allocated,
                                                              usable_size,
                                                              bytes_tl_bulk_allocated);
@@ -66,7 +70,7 @@ inline mirror::Object* RegionSpace::AllocNonvirtual(size_t num_bytes,
     MutexLock mu(Thread::Current(), region_lock_);
     // Retry with current region since another thread may have updated
     // current_region_ or evac_region_.  TODO: fix race.
-    obj = (kForEvac ? evac_region_ : current_region_)->Alloc(num_bytes,
+    obj = (kForEvac ? evac_region_[age] : current_region_)->Alloc(num_bytes,
                                                              bytes_allocated,
                                                              usable_size,
                                                              bytes_tl_bulk_allocated);
@@ -80,7 +84,8 @@ inline mirror::Object* RegionSpace::AllocNonvirtual(size_t num_bytes,
       // Do our allocation before setting the region, this makes sure no threads race ahead
       // and fill in the region before we allocate the object. b/63153464
       if (kForEvac) {
-        evac_region_ = r;
+        evac_region_[age] = r;
+        r->SetAge(age);
       } else {
         current_region_ = r;
       }
@@ -126,70 +131,27 @@ inline mirror::Object* RegionSpace::Region::Alloc(size_t num_bytes,
 template<RegionSpace::RegionType kRegionType>
 inline uint64_t RegionSpace::GetBytesAllocatedInternal() {
   uint64_t bytes = 0;
-  MutexLock mu(Thread::Current(), region_lock_);
-  for (size_t i = 0; i < num_regions_; ++i) {
-    Region* r = &regions_[i];
-    if (r->IsFree()) {
-      continue;
-    }
-    switch (kRegionType) {
-      case RegionType::kRegionTypeAll:
-        bytes += r->BytesAllocated();
-        break;
-      case RegionType::kRegionTypeFromSpace:
-        if (r->IsInFromSpace()) {
-          bytes += r->BytesAllocated();
-        }
-        break;
-      case RegionType::kRegionTypeUnevacFromSpace:
-        if (r->IsInUnevacFromSpace()) {
-          bytes += r->BytesAllocated();
-        }
-        break;
-      case RegionType::kRegionTypeToSpace:
-        if (r->IsInToSpace()) {
-          bytes += r->BytesAllocated();
-        }
-        break;
-      default:
-        LOG(FATAL) << "Unexpected space type : " << kRegionType;
-    }
-  }
+  VisitRegionsInternal<kRegionType> (static_cast<uint8_t>(RegionGen::kRegionGenYoung) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenAged) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenuring) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenured),
+                              [&bytes](Region* r) {
+                                bytes +=  r->BytesAllocated();
+                              }
+                            );
   return bytes;
 }
 
 template<RegionSpace::RegionType kRegionType>
 inline uint64_t RegionSpace::GetObjectsAllocatedInternal() {
   uint64_t bytes = 0;
-  MutexLock mu(Thread::Current(), region_lock_);
-  for (size_t i = 0; i < num_regions_; ++i) {
-    Region* r = &regions_[i];
-    if (r->IsFree()) {
-      continue;
-    }
-    switch (kRegionType) {
-      case RegionType::kRegionTypeAll:
-        bytes += r->ObjectsAllocated();
-        break;
-      case RegionType::kRegionTypeFromSpace:
-        if (r->IsInFromSpace()) {
-          bytes += r->ObjectsAllocated();
-        }
-        break;
-      case RegionType::kRegionTypeUnevacFromSpace:
-        if (r->IsInUnevacFromSpace()) {
-          bytes += r->ObjectsAllocated();
-        }
-        break;
-      case RegionType::kRegionTypeToSpace:
-        if (r->IsInToSpace()) {
-          bytes += r->ObjectsAllocated();
-        }
-        break;
-      default:
-        LOG(FATAL) << "Unexpected space type : " << kRegionType;
-    }
-  }
+  VisitRegionsInternal<kRegionType> (static_cast<uint8_t>(RegionGen::kRegionGenYoung) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenAged) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenuring) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenured),
+                              [&bytes](Region* r) {
+                                bytes +=  r->ObjectsAllocated();
+                              });
   return bytes;
 }
 
@@ -228,14 +190,15 @@ inline void RegionSpace::ScanUnevacFromSpace(accounting::ContinuousSpaceBitmap*
 }
 
 template<bool kToSpaceOnly, typename Visitor>
-inline void RegionSpace::WalkInternal(Visitor&& visitor) {
+inline void RegionSpace::WalkInternal(const uint8_t regionGenFlags, Visitor&& visitor) {
   // TODO: MutexLock on region_lock_ won't work due to lock order
   // issues (the classloader classes lock and the monitor lock). We
   // call this with threads suspended.
   Locks::mutator_lock_->AssertExclusiveHeld(Thread::Current());
   for (size_t i = 0; i < num_regions_; ++i) {
     Region* r = &regions_[i];
-    if (r->IsFree() || (kToSpaceOnly && !r->IsInToSpace())) {
+    if (r->IsFree() || (kToSpaceOnly && !r->IsInToSpace())
+        || !(static_cast<uint8_t>(GetGenInternal(r)) & regionGenFlags)) {
       continue;
     }
     if (r->IsLarge()) {
@@ -297,11 +260,22 @@ inline void RegionSpace::WalkNonLargeRegion(Visitor&& visitor, const Region* r)
 
 template <typename Visitor>
 inline void RegionSpace::Walk(Visitor&& visitor) {
-  WalkInternal</* kToSpaceOnly= */ false>(visitor);
+  WalkInternal</* kToSpaceOnly= */ false>(static_cast<uint8_t>(RegionGen::kRegionGenYoung) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenAged) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenuring) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenured), visitor);
 }
 template <typename Visitor>
 inline void RegionSpace::WalkToSpace(Visitor&& visitor) {
-  WalkInternal</* kToSpaceOnly= */ true>(visitor);
+  WalkInternal</* kToSpaceOnly= */ true>(static_cast<uint8_t>(RegionGen::kRegionGenYoung) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenAged) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenuring) |
+                             static_cast<uint8_t>(RegionGen::kRegionGenTenured), visitor);
+}
+template <typename Visitor>
+inline void RegionSpace::WalkTenuredGen(Visitor&& visitor) {
+  WalkInternal</* kToSpaceOnly= */ false>(static_cast<uint8_t>(RegionGen::kRegionGenTenured),
+                                          visitor);
 }
 
 inline mirror::Object* RegionSpace::GetNextObject(mirror::Object* obj) {
@@ -416,30 +390,36 @@ inline mirror::Object* RegionSpace::AllocLargeInRange(size_t begin,
       DCHECK(first_reg->IsFree());
       first_reg->UnfreeLarge(this, time_);
       if (kForEvac) {
+        if (use_midterm_cc_) {
+          first_reg->SetAge(kRegionMaxAgeTenured);
+        } else {
+          first_reg->SetAge(0);
+        }
         ++num_evac_regions_;
       } else {
+        // Evac doesn't count as newly allocated.
+        first_reg->SetNewlyAllocated();
         ++num_non_free_regions_;
       }
       size_t allocated = num_regs_in_large_region * kRegionSize;
       // We make 'top' all usable bytes, as the caller of this
       // allocation may use all of 'usable_size' (see mirror::Array::Alloc).
       first_reg->SetTop(first_reg->Begin() + allocated);
-      if (!kForEvac) {
-        // Evac doesn't count as newly allocated.
-        first_reg->SetNewlyAllocated();
-      }
       for (size_t p = left + 1; p < right; ++p) {
         DCHECK_LT(p, num_regions_);
         DCHECK(regions_[p].IsFree());
         regions_[p].UnfreeLargeTail(this, time_);
         if (kForEvac) {
+          if (use_midterm_cc_) {
+            regions_[p].SetAge(kRegionMaxAgeTenured);
+          } else {
+            regions_[p].SetAge(0);
+          }
           ++num_evac_regions_;
         } else {
-          ++num_non_free_regions_;
-        }
-        if (!kForEvac) {
           // Evac doesn't count as newly allocated.
           regions_[p].SetNewlyAllocated();
+          ++num_non_free_regions_;
         }
       }
       *bytes_allocated = allocated;
@@ -527,6 +507,67 @@ inline size_t RegionSpace::Region::ObjectsAllocated() const {
   }
 }
 
+template <RegionSpace::RegionType kRegionType,
+          typename Visitor>
+void RegionSpace::VisitRegions(const uint8_t regionGenFlags, const Visitor& visitor) {
+  uint8_t* prev_begin = nullptr;
+  uint8_t* prev_end = nullptr;
+  VisitRegionsInternal<kRegionType> (regionGenFlags,
+      [&visitor, &prev_begin, &prev_end](Region *r) {
+        if (r->Begin() == prev_end) {
+          prev_end = r->End();
+        } else {
+          if (LIKELY(prev_begin != nullptr)) {
+            visitor(prev_begin, prev_end);
+          }
+          prev_begin = r->Begin();
+          prev_end = r->End();
+        }
+      });
+  // Flush out if we have any regions left
+  if (prev_begin != nullptr) {
+    visitor(prev_begin, prev_end);
+  }
+}
+
+template <RegionSpace::RegionType kRegionType, typename Visitor>
+void RegionSpace::VisitRegionsInternal(const uint8_t regionGenFlags, const Visitor& visitor) {
+  MutexLock mu(Thread::Current(), region_lock_);
+  const size_t iter_limit = kUseTableLookupReadBarrier
+     ? num_regions_ : std::min(num_regions_, non_free_region_index_limit_);
+  for (size_t i = 0; i < iter_limit; ++i) {
+    Region* r = &regions_[i];
+    if (r->IsFree()) {
+      continue;
+    }
+    switch(kRegionType) {
+      case RegionType::kRegionTypeAll:
+        if ((static_cast<uint8_t>(GetGenInternal(r)) & regionGenFlags) != 0) {
+          visitor(r);
+        }
+        break;
+      default:
+        if (r->Type() == kRegionType &&
+            (static_cast<uint8_t>(GetGenInternal(r)) & regionGenFlags) != 0) {
+          visitor(r);
+        }
+    }
+  }
+}
+
+inline bool operator>(RegionSpace::RegionGen left, RegionSpace::RegionGen right) {
+  return static_cast<uint8_t>(left) > static_cast<uint8_t>(right);
+}
+inline bool operator>=(RegionSpace::RegionGen left, RegionSpace::RegionGen right) {
+  return static_cast<uint8_t>(left) >= static_cast<uint8_t>(right);
+}
+inline bool operator<(RegionSpace::RegionGen left, RegionSpace::RegionGen right) {
+  return static_cast<uint8_t>(left) < static_cast<uint8_t>(right);
+}
+inline bool operator<=(RegionSpace::RegionGen left, RegionSpace::RegionGen right) {
+  return static_cast<uint8_t>(left) <= static_cast<uint8_t>(right);
+}
+
 }  // namespace space
 }  // namespace gc
 }  // namespace art
diff --git a/runtime/gc/space/region_space.cc b/runtime/gc/space/region_space.cc
index 823043ec75..480e2a84f7 100644
--- a/runtime/gc/space/region_space.cc
+++ b/runtime/gc/space/region_space.cc
@@ -28,10 +28,6 @@ namespace art {
 namespace gc {
 namespace space {
 
-// If a region has live objects whose size is less than this percent
-// value of the region size, evaculate the region.
-static constexpr uint kEvacuateLivePercentThreshold = 75U;
-
 // Whether we protect the unused and cleared regions.
 static constexpr bool kProtectClearedRegions = true;
 
@@ -47,6 +43,9 @@ static constexpr uint32_t kPoisonDeadObject = 0xBADDB01D;  // "BADDROID"
 // Whether we check a region's live bytes count against the region bitmap.
 static constexpr bool kCheckLiveBytesAgainstRegionBitmap = kIsDebugBuild;
 
+uint8_t RegionSpace::tenure_threshold_ = RegionSpace::kRegionMaxAgeTenureThreshold;
+bool RegionSpace::use_midterm_cc_ = true;
+
 MemMap RegionSpace::CreateMemMap(const std::string& name,
                                  size_t capacity,
                                  uint8_t* requested_begin) {
@@ -95,11 +94,11 @@ MemMap RegionSpace::CreateMemMap(const std::string& name,
 }
 
 RegionSpace* RegionSpace::Create(
-    const std::string& name, MemMap&& mem_map, bool use_generational_cc) {
-  return new RegionSpace(name, std::move(mem_map), use_generational_cc);
+    const std::string& name, MemMap&& mem_map, bool use_generational_cc, bool use_midterm_cc) {
+  return new RegionSpace(name, std::move(mem_map), use_generational_cc, use_midterm_cc);
 }
 
-RegionSpace::RegionSpace(const std::string& name, MemMap&& mem_map, bool use_generational_cc)
+RegionSpace::RegionSpace(const std::string& name, MemMap&& mem_map, bool use_generational_cc, bool use_midterm_cc)
     : ContinuousMemMapAllocSpace(name,
                                  std::move(mem_map),
                                  mem_map.Begin(),
@@ -115,10 +114,10 @@ RegionSpace::RegionSpace(const std::string& name, MemMap&& mem_map, bool use_gen
       max_peak_num_non_free_regions_(0U),
       non_free_region_index_limit_(0U),
       current_region_(&full_region_),
-      evac_region_(nullptr),
       cyclic_alloc_region_index_(0U) {
   CHECK_ALIGNED(mem_map_.Size(), kRegionSize);
   CHECK_ALIGNED(mem_map_.Begin(), kRegionSize);
+  use_midterm_cc_ = use_midterm_cc;
   DCHECK_GT(num_regions_, 0U);
   regions_.reset(new Region[num_regions_]);
   uint8_t* region_addr = mem_map_.Begin();
@@ -140,6 +139,9 @@ RegionSpace::RegionSpace(const std::string& name, MemMap&& mem_map, bool use_gen
   }
   DCHECK(!full_region_.IsFree());
   DCHECK(full_region_.IsAllocated());
+  for (uint8_t age = 0; age < kAgeRegionMapSize; ++age) {
+    evac_region_[age] = nullptr;
+  }
   size_t ignored;
   DCHECK(full_region_.Alloc(kAlignment, &ignored, nullptr, &ignored) == nullptr);
   // Protect the whole region space from the start.
@@ -182,11 +184,27 @@ size_t RegionSpace::ToSpaceSize() {
   return num_regions * kRegionSize;
 }
 
+size_t RegionSpace::TenuredGenSize(int threshold) {
+  uint64_t num_regions = 0;
+  MutexLock mu(Thread::Current(), region_lock_);
+  for (size_t i = 0; i < num_regions_; ++i) {
+    Region* r = &regions_[i];
+    if (r->Age() > threshold) {
+      ++num_regions;
+    }
+  }
+  return num_regions * kRegionSize;
+}
+
 void RegionSpace::Region::SetAsUnevacFromSpace(bool clear_live_bytes) {
   // Live bytes are only preserved (i.e. not cleared) during sticky-bit CC collections.
   DCHECK(GetUseGenerationalCC() || clear_live_bytes);
   DCHECK(!IsFree() && IsInToSpace());
-  type_ = RegionType::kRegionTypeUnevacFromSpace;
+  if (use_midterm_cc_ && IsTenured()) {
+    type_ = RegionType::kRegionTypeUnevacFromSpaceTenured;
+  } else {
+    type_ = RegionType::kRegionTypeUnevacFromSpace;
+  }
   if (IsNewlyAllocated()) {
     // A newly allocated region set as unevac from-space must be
     // a large or large tail region.
@@ -264,7 +282,8 @@ inline bool RegionSpace::Region::ShouldBeEvacuated(EvacMode evac_mode) {
       // so we prefer not to evacuate it.
       result = false;
     }
-  } else if (evac_mode == kEvacModeLivePercentNewlyAllocated) {
+  } else if (evac_mode == kEvacModeLivePercentNewlyAllocated ||
+             (IsAged() && evac_mode == kEvacModeAgedLivePercentNewlyAllocated)) {
     bool is_live_percent_valid = (live_bytes_ != static_cast<size_t>(-1));
     if (is_live_percent_valid) {
       DCHECK(IsInToSpace());
@@ -404,7 +423,9 @@ void RegionSpace::SetFromSpace(accounting::ReadBarrierTable* rb_table,
   }
   DCHECK_EQ(num_expected_large_tails, 0U);
   current_region_ = &full_region_;
-  evac_region_ = &full_region_;
+  for (uint8_t age = 0; age < kAgeRegionMapSize; ++age) {
+    evac_region_[age] = &full_region_;
+  }
 }
 
 static void ZeroAndProtectRegion(uint8_t* begin, uint8_t* end) {
@@ -416,7 +437,8 @@ static void ZeroAndProtectRegion(uint8_t* begin, uint8_t* end) {
 
 void RegionSpace::ClearFromSpace(/* out */ uint64_t* cleared_bytes,
                                  /* out */ uint64_t* cleared_objects,
-                                 const bool clear_bitmap) {
+                                 const bool clear_bitmap,
+                                 bool mid_term) {
   DCHECK(cleared_bytes != nullptr);
   DCHECK(cleared_objects != nullptr);
   *cleared_bytes = 0;
@@ -544,7 +566,7 @@ void RegionSpace::ClearFromSpace(/* out */ uint64_t* cleared_bytes,
         }
         continue;
       }
-      r->SetUnevacFromSpaceAsToSpace();
+      r->SetUnevacFromSpaceAsToSpace(!clear_bitmap, mid_term);
       if (r->AllAllocatedBytesAreLive()) {
         // Try to optimize the number of ClearRange calls by checking whether the next regions
         // can also be cleared.
@@ -556,7 +578,7 @@ void RegionSpace::ClearFromSpace(/* out */ uint64_t* cleared_bytes,
             break;
           }
           CHECK(cur->IsInUnevacFromSpace());
-          cur->SetUnevacFromSpaceAsToSpace();
+          cur->SetUnevacFromSpaceAsToSpace(!clear_bitmap, mid_term);
           ++regions_to_clear_bitmap;
         }
 
@@ -621,7 +643,9 @@ void RegionSpace::ClearFromSpace(/* out */ uint64_t* cleared_bytes,
   }
   // Update non_free_region_index_limit_.
   SetNonFreeRegionLimit(new_non_free_region_index_limit);
-  evac_region_ = nullptr;
+  for (uint8_t age = 0; age < kAgeRegionMapSize; ++age) {
+    evac_region_[age] = nullptr;
+  }
   num_non_free_regions_ += num_evac_regions_;
   num_evac_regions_ = 0;
 }
@@ -764,7 +788,9 @@ void RegionSpace::Clear() {
   SetNonFreeRegionLimit(0);
   DCHECK_EQ(num_non_free_regions_, 0u);
   current_region_ = &full_region_;
-  evac_region_ = &full_region_;
+  for (uint8_t age = 0; age < kAgeRegionMapSize; ++age) {
+    evac_region_[age] = &full_region_;
+  }
 }
 
 void RegionSpace::Protect() {
@@ -817,6 +843,74 @@ void RegionSpace::DumpRegions(std::ostream& os) {
   }
 }
 
+uint8_t RegionSpace::ComputeTenureThreshold() {
+  int newThreshold = tenure_threshold_;
+  size_t cumulative_ratio[kAgeRegionMapSize];
+  // Tolerance, small value results in volatility, large value results in stagnation
+  // Optimum is b/w 0.05 - 0.10 to keep results adaptive but less volatile
+  float kUpperGamma = 0.10f;
+  float kLowerGamma = 0.05f;
+  size_t age_min_cumulative = 1;
+
+  size_t live_ratio[kAgeRegionMapSize];
+  size_t num_regions[kAgeRegionMapSize];
+  CHECK(use_midterm_cc_);
+  for (int age = 0; age < kAgeRegionMapSize; ++age) {
+    live_ratio[age] = 0;
+    num_regions[age] = 0;
+  }
+  {
+    MutexLock mu(Thread::Current(), region_lock_);
+    for (size_t i = 0; i < num_regions_; ++i) {
+      Region* reg = &regions_[i];
+      if (!reg->IsFree() && !reg->IsLargeTail()) {
+        ++num_regions[reg->Age()];
+        size_t bytes_allocated = RoundUp(reg->BytesAllocated(), kRegionSize);
+        live_ratio[reg->Age()] += (reg->live_bytes_ * 100U <
+                                   bytes_allocated * kEvacuateLivePercentThreshold) ?
+                                    (reg->live_bytes_ * 100U / bytes_allocated) : 100U;
+      }
+    }
+  }
+  size_t sum_of_ratio = 0;
+  size_t sum_of_regions = 0;
+  for (int age = 1; age < kAgeRegionMapSize; ++age) {
+    sum_of_ratio += live_ratio[age];
+    sum_of_regions += num_regions[age];
+    cumulative_ratio[age] = (sum_of_regions > 0) ? (sum_of_ratio/sum_of_regions) : 0;
+    age_min_cumulative = (cumulative_ratio[age] < cumulative_ratio[age_min_cumulative]) ?
+                            age : age_min_cumulative;
+  }
+
+  size_t r_min = cumulative_ratio[age_min_cumulative];
+  size_t r_maxTenured = cumulative_ratio[kRegionMaxAgeTenured];
+  size_t target_ratio = r_min + (((r_maxTenured - r_min) * kRegionAgeMidTermAdjustment)/100);
+  // Search for new threshold, if our existing threshold is not-useful (or)
+  // the cumulative ratio of the existing threshold is within tolerance limits to target ratio.
+  if (tenure_threshold_ >= kRegionMaxAgeTenureThreshold ||
+      cumulative_ratio[tenure_threshold_] * (1 - kLowerGamma) >= target_ratio ||
+      cumulative_ratio[tenure_threshold_] * (1 + kUpperGamma) <= target_ratio) {
+    newThreshold = age_min_cumulative;
+    for (int age = 1; age < kRegionMaxAgeTenured; ++age) {
+      if (cumulative_ratio[age] > target_ratio) {
+        break;
+      }
+      newThreshold = (num_regions[age] > 0) ? age : newThreshold;
+    }
+  }
+  // If TenuredGen is empty, this threshold is no good as mid-term GC = Full-GC
+  newThreshold = (TenuredGenSize(newThreshold) > 0) ? newThreshold : 0;
+  return newThreshold;
+}
+
+void RegionSpace::ClearBitmap(const uint8_t regionGenFlags) {
+  VisitRegions<RegionType::kRegionTypeAll> (regionGenFlags,
+                    [this](uint8_t* begin, uint8_t* end) {
+                      mark_bitmap_->ClearRange(reinterpret_cast<mirror::Object*>(begin),
+                                              reinterpret_cast<mirror::Object*>(end));
+                    });
+}
+
 void RegionSpace::DumpNonFreeRegions(std::ostream& os) {
   MutexLock mu(Thread::Current(), region_lock_);
   for (size_t i = 0; i < num_regions_; ++i) {
@@ -840,6 +934,7 @@ bool RegionSpace::AllocNewTlab(Thread* self, size_t min_bytes) {
 
   Region* r = AllocateRegion(/*for_evac=*/ false);
   if (r != nullptr) {
+    DCHECK(r->Age() == kRegionAgeNewlyAllocated);
     r->is_a_tlab_ = true;
     r->thread_ = self;
     r->SetTop(r->End());
@@ -905,6 +1000,7 @@ void RegionSpace::Region::Dump(std::ostream& os) const {
      << reinterpret_cast<void*>(begin_)
      << "-" << reinterpret_cast<void*>(Top())
      << "-" << reinterpret_cast<void*>(end_)
+     << " age=" << (size_t)age_
      << " state=" << state_
      << " type=" << type_
      << " objects_allocated=" << objects_allocated_
@@ -964,6 +1060,7 @@ size_t RegionSpace::AllocationSizeNonvirtual(mirror::Object* obj, size_t* usable
 
 void RegionSpace::Region::Clear(bool zero_and_release_pages) {
   top_.store(begin_, std::memory_order_relaxed);
+  age_ = kRegionAgeNewlyAllocated;
   state_ = RegionState::kRegionStateFree;
   type_ = RegionType::kRegionTypeNone;
   objects_allocated_.store(0, std::memory_order_relaxed);
diff --git a/runtime/gc/space/region_space.h b/runtime/gc/space/region_space.h
index 26af6331cc..5533838fc0 100644
--- a/runtime/gc/space/region_space.h
+++ b/runtime/gc/space/region_space.h
@@ -21,6 +21,7 @@
 #include "base/mutex.h"
 #include "space.h"
 #include "thread.h"
+#include "gc/heap.h"
 
 namespace art {
 namespace gc {
@@ -48,6 +49,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   enum EvacMode {
     kEvacModeNewlyAllocated,
     kEvacModeLivePercentNewlyAllocated,
+    kEvacModeAgedLivePercentNewlyAllocated,
     kEvacModeForceAll,
   };
 
@@ -59,7 +61,8 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   // guaranteed to be granted, if it is required, the caller should call Begin on the returned
   // space to confirm the request was granted.
   static MemMap CreateMemMap(const std::string& name, size_t capacity, uint8_t* requested_begin);
-  static RegionSpace* Create(const std::string& name, MemMap&& mem_map, bool use_generational_cc);
+  static RegionSpace* Create(const std::string& name, MemMap&& mem_map,
+                             bool use_generational_cc, bool use_midterm_cc);
 
   // Allocate `num_bytes`, returns null if the space is full.
   mirror::Object* Alloc(Thread* self,
@@ -78,6 +81,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   // The main allocation routine.
   template<bool kForEvac>
   ALWAYS_INLINE mirror::Object* AllocNonvirtual(size_t num_bytes,
+                                                uint8_t age,
                                                 /* out */ size_t* bytes_allocated,
                                                 /* out */ size_t* usable_size,
                                                 /* out */ size_t* bytes_tl_bulk_allocated)
@@ -152,6 +156,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     kRegionTypeAll,              // All types.
     kRegionTypeFromSpace,        // From-space. To be evacuated.
     kRegionTypeUnevacFromSpace,  // Unevacuated from-space. Not to be evacuated.
+    kRegionTypeUnevacFromSpaceTenured, //Unevacuated from-space which is tenured.
     kRegionTypeToSpace,          // To-space.
     kRegionTypeNone,             // None.
   };
@@ -163,6 +168,14 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     kRegionStateLargeTail,       // Large tail (non-first regions of a large allocation).
   };
 
+  enum class RegionGen : uint8_t {
+    kRegionGenYoung = 0x1,    // Gen-0
+    kRegionGenAged = 0x2,     // Gen-1
+    kRegionGenTenuring = 0x4, // Gen-1.5 (Gen-1 but going to be Gen-2 in this cycle).
+    kRegionGenTenured = 0x8,  // Gen-2
+    kRegionGenNone = 0x10,    // Gen-? (non-region space objects considered as Gen-2).
+  };
+
   template<RegionType kRegionType> uint64_t GetBytesAllocatedInternal() REQUIRES(!region_lock_);
   template<RegionType kRegionType> uint64_t GetObjectsAllocatedInternal() REQUIRES(!region_lock_);
   uint64_t GetBytesAllocated() override REQUIRES(!region_lock_) {
@@ -178,11 +191,21 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     return GetObjectsAllocatedInternal<RegionType::kRegionTypeFromSpace>();
   }
   uint64_t GetBytesAllocatedInUnevacFromSpace() REQUIRES(!region_lock_) {
-    return GetBytesAllocatedInternal<RegionType::kRegionTypeUnevacFromSpace>();
+    if (use_midterm_cc_) {
+      return (GetBytesAllocatedInternal<RegionType::kRegionTypeUnevacFromSpace>() + GetBytesAllocatedInternal<RegionType::kRegionTypeUnevacFromSpaceTenured>());
+    } else {
+      return (GetBytesAllocatedInternal<RegionType::kRegionTypeUnevacFromSpace>());
+    }
   }
   uint64_t GetObjectsAllocatedInUnevacFromSpace() REQUIRES(!region_lock_) {
-    return GetObjectsAllocatedInternal<RegionType::kRegionTypeUnevacFromSpace>();
+    if (use_midterm_cc_) {
+      return (GetObjectsAllocatedInternal<RegionType::kRegionTypeUnevacFromSpace>() + GetObjectsAllocatedInternal<RegionType::kRegionTypeUnevacFromSpaceTenured>());
+    } else {
+      return (GetObjectsAllocatedInternal<RegionType::kRegionTypeUnevacFromSpace>());
+    }
   }
+  template <RegionType kRegionType, typename Visitor>
+  void VisitRegions(const uint8_t regionGenFlags, const Visitor& visitor);
   size_t GetMaxPeakNumNonFreeRegions() const {
     return max_peak_num_non_free_regions_;
   }
@@ -208,6 +231,8 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   ALWAYS_INLINE void Walk(Visitor&& visitor) REQUIRES(Locks::mutator_lock_);
   template <typename Visitor>
   ALWAYS_INLINE void WalkToSpace(Visitor&& visitor) REQUIRES(Locks::mutator_lock_);
+  template <typename Visitor>
+  ALWAYS_INLINE void WalkTenuredGen(Visitor&& visitor) REQUIRES(Locks::mutator_lock_);
 
   // Scans regions and calls visitor for objects in unevac-space corresponding
   // to the bits set in 'bitmap'.
@@ -228,6 +253,29 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   static constexpr size_t kAlignment = kObjectAlignment;
   // The region size.
   static constexpr size_t kRegionSize = 256 * KB;
+  // Age of NewlyAllocated Region
+  static constexpr uint8_t kRegionAgeNewlyAllocated = 0;
+  // Max Tenure Threshold
+  static constexpr uint8_t kRegionMaxAgeTenureThreshold = 20;
+  // Max Age of Tenured region
+  static constexpr uint8_t kRegionMaxAgeTenured = kRegionMaxAgeTenureThreshold + 1;
+  // Age region map size
+  static constexpr uint8_t kAgeRegionMapSize = kRegionMaxAgeTenured + 1;
+  // If a region has live objects whose size is less than this percent
+  // value of the region size, evaculate the region.
+  static constexpr uint kEvacuateLivePercentThreshold = 75U;
+  static constexpr uint kRegionAgeMidTermAdjustment = 75U;
+
+  static bool SetTenureThreshold(uint8_t threshold) {
+    if(threshold != tenure_threshold_){
+      tenure_threshold_=threshold;
+      // LOG(INFO)<<"[VK] Tenure Threshold="<<(int)threshold;
+      return true;
+    }
+    return false;
+  }
+  uint8_t ComputeTenureThreshold();
+  static uint8_t GetTenureThreshold() { return tenure_threshold_; }
 
   bool IsInFromSpace(mirror::Object* ref) {
     if (HasAddress(ref)) {
@@ -274,6 +322,41 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     return false;
   }
 
+  uint8_t GetAgeForForwarding(mirror::Object* ref) {
+    CHECK(HasAddress(ref));
+    Region* r = RefToRegionUnlocked(ref);
+    DCHECK(r->IsInFromSpace());
+    if (use_midterm_cc_) {
+      if (UNLIKELY(r->IsLarge())) {
+        return kRegionMaxAgeTenured;
+      } else {
+        DCHECK(!r->IsLargeTail());
+        return (r->age_ < kRegionMaxAgeTenured) ? r->age_+1 : kRegionMaxAgeTenured;
+      }
+    } else {
+      return kRegionAgeNewlyAllocated;
+    }
+  }
+
+  bool IsLargeAndYoungObjectUnsafe(mirror::Object* ref) {
+    DCHECK(HasAddress(ref)) << ref;
+    Region* r = RefToRegionUnlocked(ref);
+    return r->IsLarge() && r->IsYoung();
+  }
+
+  template <bool kEvacuating>
+  RegionGen GetGen(mirror::Object* ref) {
+    if (HasAddress(ref)) {
+      Region* r = RefToRegionUnlocked(ref);
+      DCHECK_EQ(kEvacuating, (evac_region_[0] != nullptr));
+      return r->GetGen<kEvacuating>();
+    } else {
+      return RegionGen::kRegionGenNone;
+    }
+  }
+
+  void ClearBitmap(uint8_t kRegionGenFlags);
+
   // If `ref` is in the region space, return the type of its region;
   // otherwise, return `RegionType::kRegionTypeNone`.
   RegionType GetRegionType(mirror::Object* ref) {
@@ -305,9 +388,11 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   size_t FromSpaceSize() REQUIRES(!region_lock_);
   size_t UnevacFromSpaceSize() REQUIRES(!region_lock_);
   size_t ToSpaceSize() REQUIRES(!region_lock_);
+  size_t TenuredGenSize(int threshold) REQUIRES(!region_lock_);
   void ClearFromSpace(/* out */ uint64_t* cleared_bytes,
                       /* out */ uint64_t* cleared_objects,
-                      const bool clear_bitmap)
+                      const bool clear_bitmap,
+                      bool young_only)
       REQUIRES(!region_lock_);
 
   void AddLiveBytes(mirror::Object* ref, size_t alloc_size) {
@@ -315,28 +400,32 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     reg->AddLiveBytes(alloc_size);
   }
 
-  void AssertAllRegionLiveBytesZeroOrCleared() REQUIRES(!region_lock_) {
+  void AssertAllRegionLiveBytesZeroOrCleared(bool exclude_tenured = false) REQUIRES(!region_lock_) {
     if (kIsDebugBuild) {
       MutexLock mu(Thread::Current(), region_lock_);
       for (size_t i = 0; i < num_regions_; ++i) {
         Region* r = &regions_[i];
-        size_t live_bytes = r->LiveBytes();
-        CHECK(live_bytes == 0U || live_bytes == static_cast<size_t>(-1)) << live_bytes;
+        if (!(exclude_tenured && r->IsTenured())) {
+          size_t live_bytes = r->LiveBytes();
+          CHECK(live_bytes == 0U || live_bytes == static_cast<size_t>(-1)) << live_bytes;
+        }
       }
     }
   }
 
-  void SetAllRegionLiveBytesZero() REQUIRES(!region_lock_) {
+  void SetAllRegionLiveBytesZero(bool exclude_tenured = false) REQUIRES(!region_lock_) {
     MutexLock mu(Thread::Current(), region_lock_);
     const size_t iter_limit = kUseTableLookupReadBarrier
         ? num_regions_
         : std::min(num_regions_, non_free_region_index_limit_);
     for (size_t i = 0; i < iter_limit; ++i) {
       Region* r = &regions_[i];
-      // Newly allocated regions don't need up-to-date live_bytes_ for deciding
-      // whether to be evacuated or not. See Region::ShouldBeEvacuated().
-      if (!r->IsFree() && !r->IsNewlyAllocated()) {
-        r->ZeroLiveBytes();
+      if (!(exclude_tenured && r->IsTenured())) {
+        // Newly allocated regions don't need up-to-date live_bytes_ for deciding
+        // whether to be evacuated or not. See Region::ShouldBeEvacuated().
+        if (!r->IsFree() && !r->IsNewlyAllocated()) {
+          r->ZeroLiveBytes();
+        }
       }
     }
   }
@@ -370,7 +459,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   }
 
  private:
-  RegionSpace(const std::string& name, MemMap&& mem_map, bool use_generational_cc);
+  RegionSpace(const std::string& name, MemMap&& mem_map, bool use_generational_cc, bool use_midterm_cc);
 
   class Region {
    public:
@@ -385,6 +474,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
           alloc_time_(0),
           is_newly_allocated_(false),
           is_a_tlab_(false),
+          age_(kRegionAgeNewlyAllocated),
           state_(RegionState::kRegionStateAllocated),
           type_(RegionType::kRegionTypeToSpace) {}
 
@@ -393,6 +483,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
       begin_ = begin;
       top_.store(begin, std::memory_order_relaxed);
       end_ = end;
+      age_ = kRegionAgeNewlyAllocated;
       state_ = RegionState::kRegionStateFree;
       type_ = RegionType::kRegionTypeNone;
       objects_allocated_.store(0, std::memory_order_relaxed);
@@ -447,6 +538,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
 
     void SetNewlyAllocated() {
       is_newly_allocated_ = true;
+      age_ = kRegionAgeNewlyAllocated;
     }
 
     // Non-large, non-large-tail allocated.
@@ -497,7 +589,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     }
 
     bool IsInUnevacFromSpace() const {
-      return type_ == RegionType::kRegionTypeUnevacFromSpace;
+      return type_ == RegionType::kRegionTypeUnevacFromSpace || type_ == RegionType::kRegionTypeUnevacFromSpaceTenured;
     }
 
     bool IsInNoSpace() const {
@@ -533,9 +625,18 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
 
     // Set this region as to-space. Used by RegionSpace::ClearFromSpace.
     // This is only valid if it is currently an unevac from-space region.
-    void SetUnevacFromSpaceAsToSpace() {
+    void SetUnevacFromSpaceAsToSpace(bool young_gen, bool mid_term) {
       DCHECK(!IsFree() && IsInUnevacFromSpace());
       type_ = RegionType::kRegionTypeToSpace;
+      if (use_midterm_cc_) {
+        //Avoid it for tenured objects during mid-term
+        // and non-young objects during young GC
+        if (!(mid_term && IsTenured()) &&
+            //We can have LargeObjects as UnevacFromSpace during a young_gen collector
+            !(young_gen && !IsYoung())) {
+          SetAge((age_ == kRegionMaxAgeTenured) ? age_ : age_ + 1);
+        }
+      }
     }
 
     // Return whether this region should be evacuated. Used by RegionSpace::SetFromSpace.
@@ -558,6 +659,12 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
       return live_bytes_;
     }
 
+    void SetAge(uint8_t age) {
+      CHECK_LE(age,kRegionMaxAgeTenured);
+      DCHECK_GE(age,age_);
+      age_ = age;
+    }
+
     // Returns the number of allocated bytes.  "Bulk allocated" bytes in active TLABs are excluded.
     size_t BytesAllocated() const;
 
@@ -596,6 +703,34 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
 
     uint64_t GetLongestConsecutiveFreeBytes() const;
 
+    inline bool IsAged() {
+      return (age_ > 0 && !IsTenured());
+    }
+
+    inline bool IsYoung() {
+      return age_ == 0;
+    }
+
+    inline bool IsTenured() { return age_ > tenure_threshold_; }
+
+    template <bool kEvacuating>
+    inline RegionGen GetGen() {
+      if (LIKELY(IsYoung())) {
+        return RegionGen::kRegionGenYoung;
+      } else if (IsAged()) {
+        // Handle unevacfromspace refs which are going to crossover a gen
+        if (age_ == tenure_threshold_ &&
+            (!kEvacuating || type_ == RegionType::kRegionTypeUnevacFromSpace)) {
+          return RegionGen::kRegionGenTenuring;
+        } else {
+          return RegionGen::kRegionGenAged;
+        }
+      }
+      return RegionGen::kRegionGenTenured;
+    }
+
+    inline uint8_t Age() { return age_; }
+
    private:
     static bool GetUseGenerationalCC();
 
@@ -616,14 +751,28 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
     // special value for `live_bytes_`.
     bool is_newly_allocated_;           // True if it's allocated after the last collection.
     bool is_a_tlab_;                    // True if it's a tlab.
+    // age ranges from 0 to kRegionMaxAgeTenured
+    uint8_t age_;                       // Age of the region.
     RegionState state_;                 // The region state (see RegionState).
     RegionType type_;                   // The region type (see RegionType).
 
     friend class RegionSpace;
   };
 
+  RegionGen GetGenInternal(Region* r) {
+    DCHECK(r != nullptr);
+    if (evac_region_[0] == nullptr) {
+      return r->GetGen</*kEvacuating=*/ false>();
+    } else {
+      return r->GetGen</*kEvacuating=*/ true>();
+    }
+  }
+
   template<bool kToSpaceOnly, typename Visitor>
-  ALWAYS_INLINE void WalkInternal(Visitor&& visitor) NO_THREAD_SAFETY_ANALYSIS;
+  ALWAYS_INLINE void WalkInternal(const uint8_t regionGenFlags, Visitor&& visitor) NO_THREAD_SAFETY_ANALYSIS;
+
+  template <RegionType kRegionType, typename Visitor>
+  void VisitRegionsInternal(const uint8_t regionGenFlags, const Visitor& visitor);
 
   // Visitor will be iterating on objects in increasing address order.
   template<typename Visitor>
@@ -723,6 +872,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
 
   // Cached version of Heap::use_generational_cc_.
   const bool use_generational_cc_;
+  static bool use_midterm_cc_;
   uint32_t time_;                  // The time as the number of collections since the startup.
   size_t num_regions_;             // The number of regions in this space.
   // The number of non-free regions in this space.
@@ -747,9 +897,11 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   size_t non_free_region_index_limit_ GUARDED_BY(region_lock_);
 
   Region* current_region_;         // The region currently used for allocation.
-  Region* evac_region_;            // The region currently used for evacuation.
+  Region* evac_region_[kAgeRegionMapSize];  // The regions currently used for evacuation.
   Region full_region_;             // The dummy/sentinel region that looks full.
 
+  // Tenure Threshold
+  static uint8_t tenure_threshold_;
   // Index into the region array pointing to the starting region when
   // trying to allocate a new region. Only used when
   // `kCyclicRegionAllocation` is true.
@@ -763,6 +915,11 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
 
 std::ostream& operator<<(std::ostream& os, const RegionSpace::RegionState& value);
 std::ostream& operator<<(std::ostream& os, const RegionSpace::RegionType& value);
+std::ostream& operator<<(std::ostream& os, RegionSpace::RegionGen value);
+bool operator>(RegionSpace::RegionGen left, RegionSpace::RegionGen right);
+bool operator>=(RegionSpace::RegionGen left, RegionSpace::RegionGen right);
+bool operator<(RegionSpace::RegionGen left, RegionSpace::RegionGen right);
+bool operator<=(RegionSpace::RegionGen left, RegionSpace::RegionGen right);
 
 }  // namespace space
 }  // namespace gc
diff --git a/runtime/parsed_options.cc b/runtime/parsed_options.cc
index 7117e93e6d..d4d6cb7f71 100644
--- a/runtime/parsed_options.cc
+++ b/runtime/parsed_options.cc
@@ -150,6 +150,10 @@ std::unique_ptr<RuntimeParser> ParsedOptions::MakeParser(bool ignore_unrecognize
       .Define("-XX:ConcGCThreads=_")
           .WithType<unsigned int>()
           .IntoKey(M::ConcGCThreads)
+      .Define("-XX:TenureThreshold=_")
+          .WithType<unsigned int>()
+          .WithRange(1, gc::space::RegionSpace::kRegionMaxAgeTenureThreshold)
+          .IntoKey(M::TenureThreshold)
       .Define("-XX:FinalizerTimeoutMs=_")
           .WithType<unsigned int>()
           .IntoKey(M::FinalizerTimeoutMs)
diff --git a/runtime/runtime.cc b/runtime/runtime.cc
index 51a40e78c6..ecd2de5c9f 100644
--- a/runtime/runtime.cc
+++ b/runtime/runtime.cc
@@ -666,6 +666,9 @@ void Runtime::PostZygoteFork() {
 void Runtime::CallExitHook(jint status) {
   if (exit_ != nullptr) {
     ScopedThreadStateChange tsc(Thread::Current(), kNative);
+    if (dump_gc_performance_on_shutdown_) {
+      heap_->DumpGcPerformanceInfo(LOG_STREAM(INFO));
+    }
     exit_(status);
     LOG(WARNING) << "Exit hook returned instead of exiting!";
   }
@@ -1302,6 +1305,7 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
 
   // Generational CC collection is currently only compatible with Baker read barriers.
   bool use_generational_cc = kUseBakerReadBarrier && xgc_option.generational_cc;
+  bool use_midterm_cc = use_generational_cc && xgc_option.midterm_cc;
 
   image_space_loading_order_ = runtime_options.GetOrDefault(Opt::ImageSpaceLoadingOrder);
 
@@ -1340,6 +1344,8 @@ bool Runtime::Init(RuntimeArgumentMap&& runtime_options_in) {
                        xgc_option.measure_,
                        runtime_options.GetOrDefault(Opt::EnableHSpaceCompactForOOM),
                        use_generational_cc,
+                       use_midterm_cc,
+                       runtime_options.GetOrDefault(Opt::TenureThreshold),
                        runtime_options.GetOrDefault(Opt::HSpaceCompactForOOMMinIntervalsMs),
                        runtime_options.Exists(Opt::DumpRegionInfoBeforeGC),
                        runtime_options.Exists(Opt::DumpRegionInfoAfterGC),
diff --git a/runtime/runtime.h b/runtime/runtime.h
index 6df9e3e37d..a465292891 100644
--- a/runtime/runtime.h
+++ b/runtime/runtime.h
@@ -164,6 +164,14 @@ class Runtime {
     return is_zygote_;
   }
 
+  unsigned int GetTenureThreshold() const {
+    return tenure_threshold_;
+  }
+
+  unsigned int GetMidTermGcAdjustment() const {
+    return mid_term_gc_adjustment_;
+  }
+
   bool IsSystemServer() const {
     return is_system_server_;
   }
@@ -1214,6 +1222,9 @@ class Runtime {
   gc::space::ImageSpaceLoadingOrder image_space_loading_order_ =
       gc::space::ImageSpaceLoadingOrder::kSystemFirst;
 
+  unsigned int tenure_threshold_;
+  unsigned int mid_term_gc_adjustment_;
+
   // Note: See comments on GetFaultMessage.
   friend std::string GetFaultMessageForAbortLogging();
   friend class ScopedThreadPoolUsage;
diff --git a/runtime/runtime_options.def b/runtime/runtime_options.def
index 4488680374..df57693891 100644
--- a/runtime/runtime_options.def
+++ b/runtime/runtime_options.def
@@ -55,6 +55,7 @@ RUNTIME_OPTIONS_KEY (double,              HeapTargetUtilization,          gc::He
 RUNTIME_OPTIONS_KEY (double,              ForegroundHeapGrowthMultiplier, gc::Heap::kDefaultHeapGrowthMultiplier)
 RUNTIME_OPTIONS_KEY (unsigned int,        ParallelGCThreads,              0u)
 RUNTIME_OPTIONS_KEY (unsigned int,        ConcGCThreads)
+RUNTIME_OPTIONS_KEY (unsigned int,        TenureThreshold,                gc::space::RegionSpace::kRegionMaxAgeTenureThreshold)
 RUNTIME_OPTIONS_KEY (unsigned int,        FinalizerTimeoutMs,             10000u)
 RUNTIME_OPTIONS_KEY (Memory<1>,           StackSize)  // -Xss
 RUNTIME_OPTIONS_KEY (unsigned int,        MaxSpinsBeforeThinLockInflation,Monitor::kDefaultMaxSpinsBeforeThinLockInflation)
-- 
2.17.1

