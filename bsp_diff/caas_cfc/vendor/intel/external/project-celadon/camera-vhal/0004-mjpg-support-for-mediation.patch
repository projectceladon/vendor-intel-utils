From c6e6cdbc2b3e562883163f19714b0e5ff10571ba Mon Sep 17 00:00:00 2001
From: shivasku82 <shiva.kumara.rudrappa@intel.com>
Date: Fri, 20 May 2022 10:03:17 +0530
Subject: [PATCH] mjpg support for mediation

---
 include/CameraSocketCommand.h      |  20 +-
 include/CameraSocketServerThread.h |   1 +
 include/VirtualBaseCamera.h        |   2 +
 include/VirtualBuffer.h            |   1 +
 include/VirtualCamera3.h           |   2 +
 include/VirtualCameraFactory.h     |  14 ++
 include/VirtualFakeCamera3.h       |   7 +-
 include/fake-pipeline2/Sensor.h    |  36 ++--
 src/CameraSocketServerThread.cpp   | 300 +++++++++++++++++------------
 src/VirtualBaseCamera.cpp          |   6 +
 src/VirtualCamera3.cpp             |   4 +
 src/VirtualCameraFactory.cpp       |  46 +++--
 src/VirtualCameraHal.cpp           |   7 +-
 src/VirtualFakeCamera3.cpp         | 278 +++++++++++++-------------
 src/fake-pipeline2/Sensor.cpp      | 147 ++++++--------
 15 files changed, 462 insertions(+), 409 deletions(-)

diff --git a/include/CameraSocketCommand.h b/include/CameraSocketCommand.h
index b80509b..50f9026 100644
--- a/include/CameraSocketCommand.h
+++ b/include/CameraSocketCommand.h
@@ -32,16 +32,9 @@ namespace android {
 
 namespace socket {
 
-enum class VideoCodecType { kH264 = 1, kH265 = 2,  kI420 = 3, kAll = 4 };
+enum class VideoCodecType { kH264 = 1, kH265 = 2,kI420 = 4, kAll = 3 };
 enum class FrameResolution { k480p = 1, k720p = 2, k1080p = 4, kAll = 7 };
 
-struct CameraFrameInfo {
-    VideoCodecType codec_type = VideoCodecType::kI420;
-    FrameResolution resolution = FrameResolution::k480p;
-    uint32_t reserved[4];
-};
-
-enum class CameraOperation { kOpen = 11, kClose = 12, kNone = 13 };
 enum class SensorOrientation {
     ORIENTATION_0 = 0,
     ORIENTATION_90 = 90,
@@ -123,21 +116,10 @@ typedef struct _camera_packet {
     camera_header_t header;
     uint8_t payload[0];
 } camera_packet_t;
-enum class CameraVHalVersion {
-    kV1 = 0,  // decode out of camera vhal
-    kV2 = 1,  // decode in camera vhal
-};
-// has default values.
-struct CameraConfig {
-    CameraVHalVersion version = CameraVHalVersion::kV2;
-    CameraOperation operation = CameraOperation::kNone;
-    CameraFrameInfo frame_info;
-};
 
 const char* camera_type_to_str(int type);
 const char* codec_type_to_str(uint32_t type);
 const char* resolution_to_str(uint32_t resolution);
-
 }  // namespace socket
 }  // namespace android
 
diff --git a/include/CameraSocketServerThread.h b/include/CameraSocketServerThread.h
index 54a3a61..ca5ab40 100644
--- a/include/CameraSocketServerThread.h
+++ b/include/CameraSocketServerThread.h
@@ -86,6 +86,7 @@ private:
     // Source: https://tools.ietf.org/html/rfc6184#page-13
     std::array<uint8_t, 200 * 1024> mSocketBuffer = {};
     size_t mSocketBufferSize = 0;
+
     struct ValidateClientCapability {
         bool validCodecType = false;
         bool validResolution = false;
diff --git a/include/VirtualBaseCamera.h b/include/VirtualBaseCamera.h
index d1bb17e..96944f6 100644
--- a/include/VirtualBaseCamera.h
+++ b/include/VirtualBaseCamera.h
@@ -77,6 +77,8 @@ public:
      */
     virtual status_t getCameraInfo(struct camera_info *info) = 0;
 
+    virtual status_t setTorchMode(const char* camera_id, bool enable) =0;
+
     /****************************************************************************
      * Data members
      ***************************************************************************/
diff --git a/include/VirtualBuffer.h b/include/VirtualBuffer.h
index 23776d1..22ffb08 100644
--- a/include/VirtualBuffer.h
+++ b/include/VirtualBuffer.h
@@ -9,6 +9,7 @@ namespace android {
 
 extern bool gIsInFrameI420;
 extern bool gIsInFrameH264;
+extern bool gIsInFrameMJPG;
 extern bool gUseVaapi;
 
 // Max no of cameras supported based on client device request.
diff --git a/include/VirtualCamera3.h b/include/VirtualCamera3.h
index 4ffd714..132abb4 100644
--- a/include/VirtualCamera3.h
+++ b/include/VirtualCamera3.h
@@ -97,6 +97,8 @@ public:
     virtual status_t closeCamera();
 
     virtual status_t getCameraInfo(struct camera_info *info);
+    
+    virtual status_t setTorchMode(const char* camera_id, bool enable); 
 
     /****************************************************************************
      * Camera API implementation.
diff --git a/include/VirtualCameraFactory.h b/include/VirtualCameraFactory.h
index 719f598..2c17066 100644
--- a/include/VirtualCameraFactory.h
+++ b/include/VirtualCameraFactory.h
@@ -31,6 +31,7 @@
 #endif
 
 #define MAX_NUMBER_OF_SUPPORTED_CAMERAS 2  // Max restricted to two, but can be extended.
+
 namespace android {
 
 class CameraSocketServerThread;
@@ -93,6 +94,13 @@ public:
      * callback.
      */
     int getCameraInfo(int camera_id, struct camera_info *info);
+/*
+     * Gets virtual camera torch mode support.
+     *
+     * This method is called in response to camera_module_t::isSetTorchModeSupported
+     * callback.
+     */
+    int setTorchMode(const char* camera_id, bool enable); 
 
     /*
      * Sets virtual camera callbacks.
@@ -125,6 +133,12 @@ public:
      */
     static int get_camera_info(int camera_id, struct camera_info *info);
 
+
+    /*
+     * camera_module_t::get_torch_support_info callback entry point.
+     */
+    static int set_torch_mode(const char* camera_id, bool enable); 
+
     /*
      * camera_module_t::set_callbacks callback entry point.
      */
diff --git a/include/VirtualFakeCamera3.h b/include/VirtualFakeCamera3.h
index c05c666..adf6da8 100644
--- a/include/VirtualFakeCamera3.h
+++ b/include/VirtualFakeCamera3.h
@@ -67,13 +67,13 @@ public:
     virtual ~VirtualFakeCamera3();
 
     /****************************************************************************
-* VirtualCamera3 virtual overrides
+     * VirtualCamera3 virtual overrides
      ***************************************************************************/
+
 public:
     virtual status_t Initialize();
 
     /****************************************************************************
-     
      * Camera module API and generic hardware device API implementation
      ***************************************************************************/
 
@@ -84,6 +84,9 @@ public:
 
     virtual status_t getCameraInfo(struct camera_info *info);
 
+    virtual status_t setTorchMode(const char* camera_id, bool enable); 
+
+
     /****************************************************************************
      * VirtualCamera3 abstract API implementation
      ***************************************************************************/
diff --git a/include/fake-pipeline2/Sensor.h b/include/fake-pipeline2/Sensor.h
index cd26a2a..a0db84e 100644
--- a/include/fake-pipeline2/Sensor.h
+++ b/include/fake-pipeline2/Sensor.h
@@ -91,9 +91,6 @@
 
 using namespace std::chrono_literals;
 
-#define FRAME_SIZE_240P 320 * 240 * 1.5
-#define FRAME_SIZE_480P 640 * 480 * 1.5
-
 namespace android {
 
 class Sensor : private Thread, public virtual RefBase {
@@ -242,7 +239,7 @@ private:
     void captureRGBA(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height);
     void captureRGB(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height);
     void captureNV12(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height);
-    void captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height);
+    void captureNV21(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height);
     void captureDepth(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height);
     void captureDepthCloud(uint8_t *img);
     void saveNV21(uint8_t *img, uint32_t size);
@@ -254,7 +251,8 @@ private:
 
     // Max supported resolution and size of client/source camera HW.
     // HAL supports max 1080p resolution.
-  
+    int mSrcWidth = 0;
+    int mSrcHeight = 0;
     uint32_t mSrcFrameSize = 0;
 
     /**
@@ -262,29 +260,23 @@ private:
      * Hence allocating buffers for max supported resolution, that is 1080p.
      */
 
-    static const size_t maxSupportedResWidth = 640;
-    static const size_t maxSupportedResHeight = 480;
+    static const size_t maxSupportedResWidth = 1920;
+    static const size_t maxSupportedResHeight = 1080;
     static const size_t bpp = 2;  // 12 bpp for NV12/NV21 and 4 bits extra for FHD operations.
     static const size_t buffSize = maxSupportedResWidth * maxSupportedResHeight * bpp;
 
-    // memories for preview usecases
-    uint32_t destPrevBufSize = FRAME_SIZE_480P;
-    std::array<uint8_t, 640 * 480 * 3 / 2> mDstTempPrevBuf = {};
-    std::array<uint8_t, 640 * 480 * 3 / 2> mDstPrevBuf = {};
+    // Allocate memories for resolution scaling operation in preview.
+    std::array<uint8_t, buffSize> mDstTempPrevBuf = {};
+    std::array<uint8_t, buffSize> mDstPrevBuf = {};
 
-    // memories for capture/record usecases
-    uint32_t mDstBufSize = FRAME_SIZE_480P;
-    std::array<uint8_t, 640 * 480 * 3 / 2> mDstTempBuf = {};
-    std::array<uint8_t, 640 * 480 * 3 / 2> mDstBuf = {};
+    // Allocate memories for resolution scaling operation in capture/record.
+    std::array<uint8_t, buffSize> mDstTempBuf = {};
+    std::array<uint8_t, buffSize> mDstBuf = {};
 
-    //memories for JPEG/BLOB capture usecases
-    uint32_t mDstJpegBufSize = FRAME_SIZE_480P;
-    std::array<uint8_t, 640 * 480 * 3 / 2> mDstJpegTempBuf = {};
-    std::array<uint8_t, 640 * 480 * 3 / 2> mDstJpegBuf = {};
+    // Allocate memories for resolution scaling operation in JPEG capture.
+    std::array<uint8_t, buffSize> mDstJpegTempBuf = {};
+    std::array<uint8_t, buffSize> mDstJpegBuf = {};
 
-    // vHAL buffer
-    int mSrcWidth = 640;
-    int mSrcHeight = 480;
 #ifdef ENABLE_FFMPEG
     std::shared_ptr<CGVideoDecoder> mDecoder = {};
 #endif
diff --git a/src/CameraSocketServerThread.cpp b/src/CameraSocketServerThread.cpp
index 72ff4e8..3fc1228 100644
--- a/src/CameraSocketServerThread.cpp
+++ b/src/CameraSocketServerThread.cpp
@@ -13,13 +13,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-//#define LOG_NDEBUG 0
+#define LOG_NDEBUG 0
 //#define LOG_NNDEBUG 0
-#define LOG_TAG "CameraSocketServerThread:"
+#define LOG_TAG "CameraSocketServerThread: "
 #include <log/log.h>
+#define HAVE_JPEG // required for libyuv.h to export MJPEG decode APIs
 
-#if defined(LOG_NNDEBUG) && LOG_NNDEBUG == 0
-#define ALOGVV ALOGV
+#include <libyuv.h>
+#ifdef LOG_NNDEBUG
+#define ALOGVV(...) ALOGV(__VA_ARGS__)
 #else
 #define ALOGVV(...) ((void)0)
 #endif
@@ -82,15 +84,8 @@ CameraSocketServerThread::CameraSocketServerThread(std::string suffix,
     char *k8s_env_value = getenv("K8S_ENV");
     mSocketPath = (k8s_env_value != NULL && !strcmp(k8s_env_value, "true")) ? "/conn/camera-socket"
                                                                             : sock_path.c_str();
-    mNumOfCamerasRequested = 2;
-    //D TODO
-    gCameraMaxWidth = 640;
-    gCameraMaxHeight = 480;
-    gMaxNumOfCamerasSupported = 2;
-    //mNumOfCamerasRequested = 2;
-    /*gCameraFacingBack = false;
-    gCameraSensorOrientation = (uint32_t)SensorOrientation::ORIENTATION_270;*/
-    gCodecType = (uint32_t)VideoCodecType::kI420;
+    ALOGI("%s camera socket server path is %s", __FUNCTION__, mSocketPath.c_str());
+    mNumOfCamerasRequested = 0;
 }
 
 CameraSocketServerThread::~CameraSocketServerThread() {
@@ -106,7 +101,7 @@ CameraSocketServerThread::~CameraSocketServerThread() {
 }
 
 status_t CameraSocketServerThread::requestExitAndWait() {
-    ALOGV("%s: Not implemented. Use requestExit + join instead", __FUNCTION__);
+    ALOGE("%s: Not implemented. Use requestExit + join instead", __FUNCTION__);
     return INVALID_OPERATION;
 }
 
@@ -130,7 +125,7 @@ status_t CameraSocketServerThread::readyToRun() {
 }
 
 void CameraSocketServerThread::setCameraMaxSupportedResolution(int32_t width, int32_t height) {
-    ALOGV(LOG_TAG "%s: E", __FUNCTION__);
+    ALOGVV(LOG_TAG "%s: E", __FUNCTION__);
 
     if (gMaxSupportedWidth < width && gMaxSupportedHeight < height) {
         gMaxSupportedWidth = width;
@@ -141,7 +136,7 @@ void CameraSocketServerThread::setCameraMaxSupportedResolution(int32_t width, in
 }
 
 void CameraSocketServerThread::setCameraResolution(uint32_t resolution) {
-    ALOGV(LOG_TAG "%s: E", __FUNCTION__);
+    ALOGVV(LOG_TAG "%s: E", __FUNCTION__);
 
     switch (resolution) {
         case uint32_t(FrameResolution::k480p):
@@ -166,7 +161,7 @@ void CameraSocketServerThread::setCameraResolution(uint32_t resolution) {
 }
 
 bool CameraSocketServerThread::configureCapabilities() {
-    ALOGV(LOG_TAG " %s Enter", __FUNCTION__);
+    ALOGVV(LOG_TAG " %s Enter", __FUNCTION__);
 
     bool status = false;
     bool valid_client_cap_info = false;
@@ -183,21 +178,21 @@ bool CameraSocketServerThread::configureCapabilities() {
     camera_packet_t *cap_packet = NULL;
     camera_packet_t *ack_packet = NULL;
     camera_header_t header = {};
-/*
+
     if ((recv_size = recv(mClientFd, (char *)&header, sizeof(camera_header_t), MSG_WAITALL)) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to receive header, err: %s ", __FUNCTION__, strerror(errno));
+        ALOGE(LOG_TAG "%s: Failed to receive header, err: %s ", __FUNCTION__, strerror(errno));
         goto out;
     }
 
     if (header.type != REQUEST_CAPABILITY) {
-        ALOGV(LOG_TAG "%s: Invalid packet type\n", __FUNCTION__);
+        ALOGE(LOG_TAG "%s: Invalid packet type\n", __FUNCTION__);
         goto out;
     }
     ALOGI(LOG_TAG "%s: Received REQUEST_CAPABILITY header from client", __FUNCTION__);
-*/
+
     cap_packet = (camera_packet_t *)malloc(cap_packet_size);
     if (cap_packet == NULL) {
-        ALOGV(LOG_TAG "%s: cap camera_packet_t allocation failed: %d ", __FUNCTION__, __LINE__);
+        ALOGE(LOG_TAG "%s: cap camera_packet_t allocation failed: %d ", __FUNCTION__, __LINE__);
         return false;
     }
 
@@ -208,67 +203,55 @@ bool CameraSocketServerThread::configureCapabilities() {
     capability.maxNumberOfCameras = MAX_NUMBER_OF_SUPPORTED_CAMERAS;
 
     memcpy(cap_packet->payload, &capability, sizeof(camera_capability_t));
- /*   if (send(mClientFd, cap_packet, cap_packet_size, 0) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to send camera capabilities, err: %s ", __FUNCTION__,
+    if (send(mClientFd, cap_packet, cap_packet_size, 0) < 0) {
+        ALOGE(LOG_TAG "%s: Failed to send camera capabilities, err: %s ", __FUNCTION__,
               strerror(errno));
         goto out;
-    }*/
+    }
     ALOGI(LOG_TAG "%s: Sent CAPABILITY packet to client", __FUNCTION__);
-/*
+
     if ((recv_size = recv(mClientFd, (char *)&header, sizeof(camera_header_t), MSG_WAITALL)) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to receive header, err: %s ", __FUNCTION__, strerror(errno));
+        ALOGE(LOG_TAG "%s: Failed to receive header, err: %s ", __FUNCTION__, strerror(errno));
         goto out;
     }
 
     if (header.type != CAMERA_INFO) {
-        ALOGV(LOG_TAG "%s: invalid camera_packet_type: %s", __FUNCTION__,
+        ALOGE(LOG_TAG "%s: invalid camera_packet_type: %s", __FUNCTION__,
               camera_type_to_str(header.type));
         goto out;
     }
-*/
+
     // Get the number fo cameras requested to support from client.
-/*    for (int i = 1; i <= MAX_NUMBER_OF_SUPPORTED_CAMERAS; i++) {
+    for (int i = 1; i <= MAX_NUMBER_OF_SUPPORTED_CAMERAS; i++) {
         if (header.size == i * sizeof(camera_info_t)) {
             mNumOfCamerasRequested = i;
             break;
         } else if (mNumOfCamerasRequested == 0 && i == MAX_NUMBER_OF_SUPPORTED_CAMERAS) {
-            ALOGV(LOG_TAG
+            ALOGE(LOG_TAG
                   "%s: Failed to support number of cameras requested by client "
                   "which is higher than the max number of cameras supported in the HAL",
                   __FUNCTION__);
             goto out;
         }
     }
-*/
-    /*if (mNumOfCamerasRequested == 0) {
-        ALOGV(LOG_TAG "%s: invalid header size received, size = %zu", __FUNCTION__, recv_size);
+
+    if (mNumOfCamerasRequested == 0) {
+        ALOGE(LOG_TAG "%s: invalid header size received, size = %zu", __FUNCTION__, recv_size);
         goto out;
     } else {
         // Update the number of cameras globally to create camera pipeline.
         gMaxNumOfCamerasSupported = mNumOfCamerasRequested;
-    }*/
-/*
+    }
     if ((recv_size = recv(mClientFd, (char *)&camera_info,
                           mNumOfCamerasRequested * sizeof(camera_info_t), MSG_WAITALL)) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to receive camera info, err: %s ", __FUNCTION__, strerror(errno));
+        ALOGE(LOG_TAG "%s: Failed to receive camera info, err: %s ", __FUNCTION__, strerror(errno));
         goto out;
     }
-*/
-    camera_info[0].cameraId = 0;
-    camera_info[0].codec_type = (uint32_t)VideoCodecType::kI420;
-    camera_info[0].sensorOrientation =  (uint32_t)SensorOrientation::ORIENTATION_90;
-    camera_info[0].facing =       (uint32_t)CameraFacing::BACK_FACING;
-    camera_info[0].resolution =  (uint32_t)FrameResolution::k480p;
-    camera_info[1].cameraId = 1;
-    camera_info[1].codec_type = (uint32_t)VideoCodecType::kI420;
-    camera_info[1].sensorOrientation =  (uint32_t)SensorOrientation::ORIENTATION_90;
-    camera_info[1].facing =       (uint32_t)CameraFacing::FRONT_FACING;
-    camera_info[1].resolution =  (uint32_t)FrameResolution::k480p;
-    mNumOfCamerasRequested =2;
+
     ALOGI(LOG_TAG "%s: Received CAMERA_INFO packet from client with recv_size: %zd ", __FUNCTION__,
           recv_size);
     ALOGI(LOG_TAG "%s: Number of cameras requested = %d", __FUNCTION__, mNumOfCamerasRequested);
-   gMaxNumOfCamerasSupported = mNumOfCamerasRequested;
+
     // Update status globally after received successful capability info.
     gCapabilityInfoReceived = true;
 
@@ -276,7 +259,7 @@ bool CameraSocketServerThread::configureCapabilities() {
     for (int i = 0; i < mNumOfCamerasRequested; i++) {
         expctd_cam_id = i;
         if (expctd_cam_id == (int)camera_info[i].cameraId)
-            ALOGV(LOG_TAG
+            ALOGVV(LOG_TAG
                    "%s: Camera Id number %u received from client is matching with expected Id",
                    __FUNCTION__, camera_info[i].cameraId);
         else
@@ -287,8 +270,8 @@ bool CameraSocketServerThread::configureCapabilities() {
 
         switch (camera_info[i].codec_type) {
             case uint32_t(VideoCodecType::kH264):
-            case uint32_t(VideoCodecType::kH265):
             case uint32_t(VideoCodecType::kI420):
+            case uint32_t(VideoCodecType::kH265):
                 val_client_cap[i].validCodecType = true;
                 break;
             default:
@@ -332,28 +315,27 @@ bool CameraSocketServerThread::configureCapabilities() {
 
     // Check whether recceived any invalid capability info or not.
     // ACK packet to client would be updated based on this verification.
-    /*for (int i = 0; i < mNumOfCamerasRequested; i++) {
+    for (int i = 0; i < mNumOfCamerasRequested; i++) {
         if (!val_client_cap[i].validCodecType || !val_client_cap[i].validResolution ||
             !val_client_cap[i].validOrientation || !val_client_cap[i].validCameraFacing) {
             valid_client_cap_info = false;
-            ALOGV("%s: capability info received from client is not completely correct and expected",
+            ALOGE("%s: capability info received from client is not completely correct and expected",
                   __FUNCTION__);
             break;
-        else {
-            ALOGV("%s: capability info received from client is correct and expected",
+        } else {
+            ALOGVV("%s: capability info received from client is correct and expected",
                    __FUNCTION__);
             valid_client_cap_info = true;
         }
-    }*/
+    }
 
-            valid_client_cap_info = true;
     // Updating metadata for each camera seperately with its capability info received.
     for (int i = 0; i < mNumOfCamerasRequested; i++) {
         // Going to update metadata for each camera, so update the status.
         gStartMetadataUpdate = false;
         gDoneMetadataUpdate = false;
         camera_id = i;
-        ALOGV(LOG_TAG
+        ALOGI(LOG_TAG
               "%s - Client requested for codec_type: %s, resolution: %s, orientation: %u, and "
               "facing: %u for camera Id %d",
               __FUNCTION__, codec_type_to_str(camera_info[i].codec_type),
@@ -367,7 +349,7 @@ bool CameraSocketServerThread::configureCapabilities() {
             // Set default resolution if receive invalid capability info from client.
             // Default resolution would be 480p.
             setCameraResolution((uint32_t)FrameResolution::k480p);
-            ALOGV(LOG_TAG
+            ALOGE(LOG_TAG
                   "%s: Not received valid resolution, "
                   "hence selected 480p as default",
                   __FUNCTION__);
@@ -380,7 +362,7 @@ bool CameraSocketServerThread::configureCapabilities() {
             // Set default codec type if receive invalid capability info from client.
             // Default codec type would be H264.
             gCodecType = (uint32_t)VideoCodecType::kH264;
-            ALOGV(LOG_TAG "%s: Not received valid codec type, hence selected H264 as default",
+            ALOGE(LOG_TAG "%s: Not received valid codec type, hence selected H264 as default",
                   __FUNCTION__);
         }
 
@@ -391,8 +373,8 @@ bool CameraSocketServerThread::configureCapabilities() {
             // Set default camera sensor orientation if received invalid orientation data from
             // client. Default sensor orientation would be zero deg and consider as landscape
             // display.
-            gCameraSensorOrientation = (uint32_t)SensorOrientation::ORIENTATION_270;
-            ALOGV(LOG_TAG
+            gCameraSensorOrientation = (uint32_t)SensorOrientation::ORIENTATION_0;
+            ALOGE(LOG_TAG
                   "%s: Not received valid sensor orientation, "
                   "hence selected ORIENTATION_0 as default",
                   __FUNCTION__);
@@ -411,7 +393,7 @@ bool CameraSocketServerThread::configureCapabilities() {
                 gCameraFacingBack = false;
             else
                 gCameraFacingBack = true;
-            ALOGV(LOG_TAG
+            ALOGE(LOG_TAG
                   "%s: Not received valid camera facing info, "
                   "hence selected default",
                   __FUNCTION__);
@@ -422,7 +404,7 @@ bool CameraSocketServerThread::configureCapabilities() {
 
         // Wait till complete the metadata update for a camera.
         while (!gDoneMetadataUpdate) {
-            ALOGV("%s: wait till complete the metadata update for a camera", __FUNCTION__);
+            ALOGVV("%s: wait till complete the metadata update for a camera", __FUNCTION__);
             // 200us sleep for this thread.
             std::this_thread::sleep_for(std::chrono::microseconds(200));
         }
@@ -430,7 +412,7 @@ bool CameraSocketServerThread::configureCapabilities() {
 
     ack_packet = (camera_packet_t *)malloc(ack_packet_size);
     if (ack_packet == NULL) {
-        ALOGV(LOG_TAG "%s: ack camera_packet_t allocation failed: %d ", __FUNCTION__, __LINE__);
+        ALOGE(LOG_TAG "%s: ack camera_packet_t allocation failed: %d ", __FUNCTION__, __LINE__);
         goto out;
     }
     ack_payload = (valid_client_cap_info) ? ACK_CONFIG : NACK_CONFIG;
@@ -439,11 +421,11 @@ bool CameraSocketServerThread::configureCapabilities() {
     ack_packet->header.size = sizeof(camera_ack_t);
 
     memcpy(ack_packet->payload, &ack_payload, sizeof(camera_ack_t));
-/*    if (send(mClientFd, ack_packet, ack_packet_size, 0) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to send camera capabilities, err: %s ", __FUNCTION__,
+    if (send(mClientFd, ack_packet, ack_packet_size, 0) < 0) {
+        ALOGE(LOG_TAG "%s: Failed to send camera capabilities, err: %s ", __FUNCTION__,
               strerror(errno));
         goto out;
-    }*/
+    }
     ALOGI(LOG_TAG "%s: Sent ACK packet to client with ack_size: %zu ", __FUNCTION__,
           ack_packet_size);
 
@@ -451,7 +433,7 @@ bool CameraSocketServerThread::configureCapabilities() {
 out:
     free(ack_packet);
     free(cap_packet);
-    ALOGV(LOG_TAG " %s: Exit", __FUNCTION__);
+    ALOGVV(LOG_TAG " %s: Exit", __FUNCTION__);
     return status;
 }
 
@@ -490,7 +472,7 @@ bool CameraSocketServerThread::threadLoop() {
             ALOGV("%s:%d Fail to construct camera socket with error: %s", __FUNCTION__, __LINE__,
               strerror(errno));
         return false;
-        }
+    }
 
     struct sockaddr_un addr_un;
     memset(&addr_un, 0, sizeof(addr_un));
@@ -502,7 +484,7 @@ bool CameraSocketServerThread::threadLoop() {
         ALOGI(" %s camera socket server file is %s", __FUNCTION__, mSocketPath.c_str());
         ret = unlink(mSocketPath.c_str());
         if (ret < 0) {
-            ALOGV(LOG_TAG " %s Failed to unlink %s address %d, %s", __FUNCTION__,
+            ALOGE(LOG_TAG " %s Failed to unlink %s address %d, %s", __FUNCTION__,
                   mSocketPath.c_str(), ret, strerror(errno));
             return false;
         }
@@ -514,7 +496,7 @@ bool CameraSocketServerThread::threadLoop() {
     ret = ::bind(mSocketServerFd, (struct sockaddr *)&addr_un,
                  sizeof(sa_family_t) + strlen(mSocketPath.c_str()) + 1);
     if (ret < 0) {
-        ALOGV(LOG_TAG " %s Failed to bind %s address %d, %s", __FUNCTION__, mSocketPath.c_str(),
+        ALOGE(LOG_TAG " %s Failed to bind %s address %d, %s", __FUNCTION__, mSocketPath.c_str(),
               ret, strerror(errno));
         return false;
     }
@@ -529,7 +511,7 @@ bool CameraSocketServerThread::threadLoop() {
 
         ret = listen(mSocketServerFd, 5);
         if (ret < 0) {
-            ALOGV("%s Failed to listen on %s", __FUNCTION__, mSocketPath.c_str());
+            ALOGE("%s Failed to listen on %s", __FUNCTION__, mSocketPath.c_str());
             return false;
         }
     }
@@ -571,10 +553,7 @@ bool CameraSocketServerThread::threadLoop() {
         addr_vm.svm_family = AF_VSOCK;
         addr_vm.svm_port = 1982;
         addr_vm.svm_cid = 3;
-        //addr_vm.svm_port = htons(1234);
-        //addr_vm.svm_cid = 4;
         int ret = 0;
-        int port = 1234;
         int so_reuseaddr = 1;
         size_update = 0;
         mSocketServerFd = ::socket(AF_VSOCK, SOCK_STREAM, 0);
@@ -586,7 +565,7 @@ bool CameraSocketServerThread::threadLoop() {
         ret = ::bind(mSocketServerFd, (struct sockaddr *)&addr_vm,
             sizeof(struct sockaddr_vm));
         if (ret < 0) {
-            ALOGV(LOG_TAG " %s Failed to bind port(%d). ret: %d, %s", __func__, port, ret,
+            ALOGV(LOG_TAG " %s Failed to bind port(%d). ret: %d, %s", __func__, addr_vm.svm_port, ret,
             strerror(errno));
             return false;
         }
@@ -598,7 +577,7 @@ bool CameraSocketServerThread::threadLoop() {
 
     }
     while (mRunning) {
-        ALOGV(LOG_TAG " %s: Wait for camera client to connect. . .", __FUNCTION__);
+        ALOGI(LOG_TAG " %s: Wait for camera client to connect. . .", __FUNCTION__);
 
         if (trans_mode == TCP) {
             socklen_t alen = sizeof(struct sockaddr_in);
@@ -615,10 +594,11 @@ bool CameraSocketServerThread::threadLoop() {
         }
         ALOGI(LOG_TAG " %s: Accepted client: [%d]", __FUNCTION__, new_client_fd);
         if (new_client_fd < 0) {
-            ALOGV(LOG_TAG " %s: Fail to accept client. Error: [%s]", __FUNCTION__, strerror(errno));
+            ALOGE(LOG_TAG " %s: Fail to accept client. Error: [%s]", __FUNCTION__, strerror(errno));
             continue;
         }
- 
+        mClientFd = new_client_fd;
+
         bool status = false;
         status = configureCapabilities();
         if (status) {
@@ -627,19 +607,18 @@ bool CameraSocketServerThread::threadLoop() {
                   "for %d camera(s) completed successfully..",
                   __FUNCTION__, mNumOfCamerasRequested);
         }
-        mClientFd =  new_client_fd; 
 
         ClientVideoBuffer *handle = ClientVideoBuffer::getClientInstance();
-        char *fbuffer = (char *)handle->clientBuf[handle->clientRevCount % 1].buffer;
+        uint8_t *fbuffer = (uint8_t *)handle->clientBuf[handle->clientRevCount % 1].buffer;
         // Reset and clear the input buffer before receiving the frames.
         handle->reset();
 
         struct pollfd fd;
         int event;
+
         fd.fd = mClientFd;  // your socket handler
         fd.events = POLLIN | POLLHUP;
 
-
         while (true) {
             // check if there are any events on fd.
             int ret = poll(&fd, 1, 3000);  // 3 seconds for timeout
@@ -649,25 +628,24 @@ bool CameraSocketServerThread::threadLoop() {
             if (event & POLLHUP) {
                 // connnection disconnected => socket is closed at the other end => close the
                 // socket.
-                ALOGV(LOG_TAG " %s: POLLHUP: Close camera socket connection", __FUNCTION__);
+                ALOGE(LOG_TAG " %s: POLLHUP: Close camera socket connection", __FUNCTION__);
                 shutdown(mClientFd, SHUT_RDWR);
                 close(mClientFd);
                 mClientFd = -1;
                 handle->reset();
                 break;
-            } else if ((event & POLLIN)) {  // preview / record
+            } else if (event & POLLIN) {  // preview / record
                 // data is available in socket => read data
                 if (gIsInFrameI420) {
-                     
                      if(trans_mode == VSOCK){
-                        char buffer_header[5];
+
                         int size_header =0; 
                         ssize_t size_pending =0; 
-                        size_header = recv(mClientFd, (char *)buffer_header, 2, 0);
-                        if(size_header < 0)
-                          ALOGE("header recv error %d",size_header); 
-                        if(!strncmp(buffer_header,"OK",2)){
-                        size_pending = 460800;  
+                        //Check if the header type is data
+                        camera_header_t buffer_header = {};
+                        size_header = recv(mClientFd, (char *)&buffer_header, sizeof(camera_header_t), 0);
+                        if(buffer_header.type == CAMERA_DATA){
+                          size_pending = buffer_header.size;
                         while(size_pending != 0){
                             ssize_t size_data = 0;
                             size_data = recv(mClientFd, (char *)fbuffer+size_update, size_pending, 0);
@@ -701,81 +679,159 @@ bool CameraSocketServerThread::threadLoop() {
                        
                     }else{
                     ssize_t size = 0;
+
                     if ((size = recv(mClientFd, (char *)fbuffer, 460800, MSG_WAITALL)) > 0) {
                         handle->clientRevCount++;
-                        ALOGV(LOG_TAG
-                               "[I420] %s: Packet rev %d and "
+                        ALOGVV(LOG_TAG
+                               "[I420] %s: Pocket rev %d and "
                                "size %zd",
                                __FUNCTION__, handle->clientRevCount, size);
                         } 
                     }
+                } else if (gIsInFrameMJPG) { 
+
+                        int size_header =0; 
+                        ssize_t size_pending =0; 
+                        ALOGE("Deepa it is MJPG irecv the header"); 
+                        camera_header_t buffer_header = {};
+                        size_header = recv(mClientFd, (char *)&buffer_header, sizeof(camera_header_t), 0);
+			unsigned char * tempBuffer = (unsigned char*)malloc(buffer_header.size);
+                        if(buffer_header.type == CAMERA_DATA){
+                          size_pending = buffer_header.size;
+                           ALOGE("Deepa it is MJPG recv buffer size %zd",size_pending); 
+                        while(size_pending != 0){
+                            ssize_t size_data = 0;
+                           ALOGE("Deepa it is MJPG recv buffer %zd",size_pending); 
+                            size_data = recv(mClientFd, (char *)tempBuffer+size_update, size_pending, 0);
+                            if(size_data < 0){
+                               //error handling while in preview
+                               ALOGE(LOG_TAG "entered into recv error, break to recover");
+                               continue;
+                            }
+                            size_update += size_data;
+                            size_pending -= size_data;
+                            if (size_pending == 0){
+                                handle->clientRevCount++;
+                            
+                                size_update = 0;
+                                
+                                ALOGV(LOG_TAG
+                                   "[I420] %s: Packet rev %d and "
+                                   "size %zd",    
+                                   __FUNCTION__, handle->clientRevCount, size_data);
+                               break;
+                            }
+                        }
+
+        int res = libyuv::MJPGToI420(
+            tempBuffer, buffer_header.size, static_cast<uint8_t*>(fbuffer), gCameraMaxWidth,
+            static_cast<uint8_t*>(fbuffer + (gCameraMaxWidth * gCameraMaxHeight)), (gCameraMaxWidth / 2),
+            static_cast<uint8_t*>(fbuffer + (gCameraMaxWidth * gCameraMaxHeight) + ((gCameraMaxWidth * gCameraMaxHeight) / 4)), (gCameraMaxWidth / 2),
+            gCameraMaxWidth, gCameraMaxHeight, gCameraMaxWidth, gCameraMaxHeight);
+        if (res != 0) {
+            ALOGE("Shiva updated fail to convert MJPG to I420 ret %d  and sz %d", res, buffer_header.size);
+        }
+                     }else
+                         ALOGE("received NOT OK");
+                      
+
                 } else if (gIsInFrameH264) {  // default H264
 #ifdef ENABLE_FFMPEG
-                    size_t recv_frame_size = 0;
                     ssize_t size = 0;
-                    if ((size = recv(mClientFd, (char *)&recv_frame_size, sizeof(size_t),
+                    camera_header_t header = {};
+                    if ((size = recv(mClientFd, (char *)&header, sizeof(camera_header_t),
                                      MSG_WAITALL)) > 0) {
-                        ALOGV("[H264] Received Header %zd bytes. Payload size: %zu", size,
-                              recv_frame_size);
-                        if (recv_frame_size > mSocketBuffer.size()) {
+                        ALOGVV("%s: Received Header %zd bytes. Payload size: %u", __FUNCTION__,
+                               size, header.size);
+                        if (header.type == REQUEST_CAPABILITY) {
+                            ALOGI(LOG_TAG
+                                  "%s: [Warning] Capability negotiation was already "
+                                  "done for %d camera(s); Can't do re-negotiation again!!!",
+                                  __FUNCTION__, mNumOfCamerasRequested);
+                            continue;
+                        } else if (header.type != CAMERA_DATA) {
+                            ALOGE(LOG_TAG "%s: invalid camera_packet_type: %s", __FUNCTION__,
+                                  camera_type_to_str(header.type));
+                            continue;
+                        }
+
+                        if (header.size > mSocketBuffer.size()) {
                             // maximum size of a H264 packet in any aggregation packet is 65535
                             // bytes. Source: https://tools.ietf.org/html/rfc6184#page-13
-                            ALOGV(
-                                "%s Fatal: Unusual H264 packet size detected: %zu! Max is %zu, ...",
-                                __func__, recv_frame_size, mSocketBuffer.size());
+                            ALOGE(
+                                "%s Fatal: Unusual encoded packet size detected: %u! Max is %zu, "
+                                "...",
+                                __func__, header.size, mSocketBuffer.size());
                             continue;
                         }
 
                         // recv frame
-                        if ((size = recv(mClientFd, (char *)mSocketBuffer.data(), recv_frame_size,
+                        if ((size = recv(mClientFd, (char *)mSocketBuffer.data(), header.size,
                                          MSG_WAITALL)) > 0) {
-                            mSocketBufferSize = recv_frame_size;
-                            ALOGV("%s [H264] Camera session state: %s", __func__,
-                                  kCameraSessionStateNames.at(mCameraSessionState).c_str());
+                            if (size < header.size) {
+                                ALOGW("%s : Incomplete data read %zd/%u bytes", __func__, size,
+                                      header.size);
+                                size_t remaining_size = header.size;
+                                remaining_size -= size;
+                                while (remaining_size > 0) {
+                                    if ((size = recv(mClientFd, (char *)mSocketBuffer.data() + size,
+                                                     remaining_size, MSG_WAITALL)) > 0) {
+                                        remaining_size -= size;
+                                        ALOGI("%s : Read-%zd after Incomplete data, remaining-%lu",
+                                              __func__, size, remaining_size);
+                                    }
+                                }
+                                size = header.size;
+                            }
+
+                            mSocketBufferSize = header.size;
+                            ALOGVV("%s: Camera session state: %s", __func__,
+                                   kCameraSessionStateNames.at(mCameraSessionState).c_str());
                             switch (mCameraSessionState) {
                                 case CameraSessionState::kCameraOpened:
                                     mCameraSessionState = CameraSessionState::kDecodingStarted;
-                                    ALOGV("%s [H264] Decoding started now.", __func__);
-                                    [[fallthrough]];
+                                    ALOGVV("%s: Decoding started now.", __func__);
                                 case CameraSessionState::kDecodingStarted:
                                     mVideoDecoder->decode(mSocketBuffer.data(), mSocketBufferSize);
                                     handle->clientRevCount++;
-                                    ALOGV("%s [H264] Received Payload #%d %zd/%zu bytes", __func__,
-                                          handle->clientRevCount, size, recv_frame_size);
+                                    ALOGVV("%s: Received Payload #%d %zd/%u bytes", __func__,
+                                           handle->clientRevCount, size, header.size);
+                                    mSocketBuffer.fill(0);
                                     break;
                                 case CameraSessionState::kCameraClosed:
-                                    mVideoDecoder->flush_decoder();
-                                    mVideoDecoder->destroy();
+                                    ALOGI("%s: Decoding stopping and flushing decoder.", __func__);
                                     mCameraSessionState = CameraSessionState::kDecodingStopped;
-                                    ALOGI("%s [H264] Decoding stopped now.", __func__);
+                                    ALOGI("%s: Decoding stopped now.", __func__);
                                     break;
                                 case CameraSessionState::kDecodingStopped:
-                                    ALOGV("%s [H264] Decoding is already stopped, skip the packets",
-                                          __func__);
-                                    [[fallthrough]];
+                                    ALOGVV("%s: Decoding is already stopped, skip the packets",
+                                           __func__);
+                                    mSocketBuffer.fill(0);
+                                    break;
                                 default:
-                                    ALOGV("%s [H264] Invalid Camera session state!", __func__);
+                                    ALOGE("%s: Invalid Camera session state!", __func__);
                                     break;
                             }
                         }
                     }
 #endif
                 } else {
-                    ALOGV("%s: only H264, I420 input frames supported", __FUNCTION__);
+                    ALOGE(
+                        "%s: Only H264, H265, I420 Input frames are supported. Check Input format",
+                        __FUNCTION__);
                 }
             } else {
-                //    ALOGV("%s: continue polling..", __FUNCTION__);
+                //    ALOGE("%s: continue polling..", __FUNCTION__);
             }
         }
     }
-    ALOGV(" %s: Quit CameraSocketServerThread... %s(%d)", __FUNCTION__, mSocketPath.c_str(),
+    ALOGE(" %s: Quit CameraSocketServerThread... %s(%d)", __FUNCTION__, mSocketPath.c_str(),
           mClientFd);
     shutdown(mClientFd, SHUT_RDWR);
     close(mClientFd);
     mClientFd = -1;
     close(mSocketServerFd);
     mSocketServerFd = -1;
-    size_update = 0;
     return true;
 }
 
diff --git a/src/VirtualBaseCamera.cpp b/src/VirtualBaseCamera.cpp
index 747a429..9d65978 100644
--- a/src/VirtualBaseCamera.cpp
+++ b/src/VirtualBaseCamera.cpp
@@ -61,6 +61,12 @@ status_t VirtualBaseCamera::getCameraInfo(struct camera_info *info) {
     return NO_ERROR;
 }
 
+status_t  VirtualBaseCamera::setTorchMode(const char* camera_id, bool enable){
+    ALOGV("%s", __FUNCTION__);
+
+    return OK;
+}
+
 status_t VirtualBaseCamera::setCameraFD(int socketFd) {
     mCameraSocketFD = socketFd;
     ALOGV("%s mCameraSocketFD = %d", __FUNCTION__, mCameraSocketFD);
diff --git a/src/VirtualCamera3.cpp b/src/VirtualCamera3.cpp
index a8c246e..1285afe 100644
--- a/src/VirtualCamera3.cpp
+++ b/src/VirtualCamera3.cpp
@@ -94,6 +94,10 @@ status_t VirtualCamera3::getCameraInfo(struct camera_info *info) {
     return VirtualBaseCamera::getCameraInfo(info);
 }
 
+status_t VirtualCamera3::setTorchMode(const char* camera_id, bool enable) {
+    return VirtualBaseCamera::setTorchMode(camera_id,enable);
+}
+
 /****************************************************************************
  * Camera Device API implementation.
  * These methods are called from the camera API callback routines.
diff --git a/src/VirtualCameraFactory.cpp b/src/VirtualCameraFactory.cpp
index 478fa68..7a35a28 100644
--- a/src/VirtualCameraFactory.cpp
+++ b/src/VirtualCameraFactory.cpp
@@ -19,9 +19,9 @@
  * available for emulation.
  */
 
-#define LOG_NDEBUG 0
-#define LOG_TAG "VirtualCamera_Factory "
-#include "VirtualBuffer.h"
+//#define LOG_NDEBUG 0
+#define LOG_TAG "VirtualCamera_Factory"
+
 #include "VirtualCameraFactory.h"
 #include "VirtualFakeCamera3.h"
 #include "CameraSocketServerThread.h"
@@ -30,7 +30,7 @@
 #endif
 #include <log/log.h>
 #include <cutils/properties.h>
-
+#include "VirtualBuffer.h"
 extern camera_module_t HAL_MODULE_INFO_SYM;
 
 /*
@@ -43,6 +43,7 @@ namespace android {
 
 bool gIsInFrameI420;
 bool gIsInFrameH264;
+bool gIsInFrameMJPG;
 bool gUseVaapi;
 
 void VirtualCameraFactory::readSystemProperties() {
@@ -58,6 +59,9 @@ void VirtualCameraFactory::readSystemProperties() {
     property_get("ro.vendor.camera.decode.vaapi", prop_val, "false");
     gUseVaapi = !strcmp(prop_val, "true");
 
+gIsInFrameH264 = false;
+gIsInFrameI420 = false;
+gIsInFrameMJPG = true;
     ALOGI("%s - gIsInFrameH264: %d, gIsInFrameI420: %d, gUseVaapi: %d", __func__, gIsInFrameH264,
           gIsInFrameI420, gUseVaapi);
 }
@@ -76,6 +80,7 @@ VirtualCameraFactory::VirtualCameraFactory()
         mDecoder = std::make_shared<CGVideoDecoder>();
 #endif
     }
+
     // Create socket server which is used to communicate with client device.
 #ifdef ENABLE_FFMPEG
     createSocketServer(mDecoder);
@@ -103,7 +108,7 @@ VirtualCameraFactory::VirtualCameraFactory()
     // Allocate space for each cameras requested.
     mVirtualCameras = new VirtualBaseCamera *[mNumOfCamerasSupported];
     if (mVirtualCameras == nullptr) {
-        ALOGV("%s: Unable to allocate virtual camera array", __FUNCTION__);
+        ALOGE("%s: Unable to allocate virtual camera array", __FUNCTION__);
         return;
     } else {
         for (int n = 0; n < mNumOfCamerasSupported; n++) {
@@ -115,7 +120,7 @@ VirtualCameraFactory::VirtualCameraFactory()
     for (int cameraId = 0; cameraId < mNumOfCamerasSupported; cameraId++) {
         // Wait until start updating metadata for each camera.
         while (!gStartMetadataUpdate) {
-            //ALOGV("%s: wait until start updating metadata for a single camera", __func__);
+            ALOGV("%s: wait until start updating metadata for a single camera", __func__);
             // 200us sleep for this thread.
             std::this_thread::sleep_for(std::chrono::microseconds(200));
         }
@@ -128,7 +133,6 @@ VirtualCameraFactory::VirtualCameraFactory()
         // Created a camera successfully hence update the status.
         gDoneMetadataUpdate = true;
         gStartMetadataUpdate = false;
-      
     }
 
     ALOGI("%s: Total number of cameras supported: %d", __FUNCTION__, mNumOfCamerasSupported);
@@ -148,8 +152,10 @@ bool VirtualCameraFactory::createSocketServer() {
     if (property_get("ro.boot.container.id", id, "") > 0) {
         mSocketServer =
             std::make_shared<CameraSocketServerThread>(id, decoder, std::ref(mCameraSessionState));
+
+        mSocketServer->run("FrontBackCameraSocketServerThread");
     } else
-        ALOGV("%s: FATAL: container id is not set!!", __func__);
+        ALOGE("%s: FATAL: container id is not set!!", __func__);
 
     ALOGV("%s: X", __FUNCTION__);
 #else
@@ -191,12 +197,12 @@ int VirtualCameraFactory::cameraDeviceOpen(int cameraId, hw_device_t **device) {
     *device = nullptr;
 
     if (!isConstructedOK()) {
-        ALOGV("%s: VirtualCameraFactory has failed to initialize", __FUNCTION__);
+        ALOGE("%s: VirtualCameraFactory has failed to initialize", __FUNCTION__);
         return -EINVAL;
     }
 
     if (cameraId < 0 || cameraId >= getVirtualCameraNum()) {
-        ALOGV("%s: Camera id %d is out of bounds (%d)", __FUNCTION__, cameraId,
+        ALOGE("%s: Camera id %d is out of bounds (%d)", __FUNCTION__, cameraId,
               getVirtualCameraNum());
         return -ENODEV;
     }
@@ -208,18 +214,23 @@ int VirtualCameraFactory::getCameraInfo(int cameraId, struct camera_info *info)
     ALOGI("%s: id = %d", __FUNCTION__, cameraId);
 
     if (!isConstructedOK()) {
-        ALOGV("%s: VirtualCameraFactory has failed to initialize", __FUNCTION__);
+        ALOGE("%s: VirtualCameraFactory has failed to initialize", __FUNCTION__);
         return -EINVAL;
     }
 
     if (cameraId < 0 || cameraId >= getVirtualCameraNum()) {
-        ALOGV("%s: Camera id %d is out of bounds (%d)", __FUNCTION__, cameraId,
+        ALOGE("%s: Camera id %d is out of bounds (%d)", __FUNCTION__, cameraId,
               getVirtualCameraNum());
         return -ENODEV;
     }
 
     return mVirtualCameras[cameraId]->getCameraInfo(info);
 }
+int  VirtualCameraFactory::setTorchMode(const char* camera_id, bool enable){
+    ALOGI("%s: ", __FUNCTION__);
+    enable = !enable;
+    return -ENOSYS;
+}
 
 int VirtualCameraFactory::setCallbacks(const camera_module_callbacks_t *callbacks) {
     ALOGV("%s: callbacks = %p", __FUNCTION__, callbacks);
@@ -246,12 +257,12 @@ int VirtualCameraFactory::device_open(const hw_module_t *module, const char *nam
      */
 
     if (module != &HAL_MODULE_INFO_SYM.common) {
-        ALOGV("%s: Invalid module %p expected %p", __FUNCTION__, module,
+        ALOGE("%s: Invalid module %p expected %p", __FUNCTION__, module,
               &HAL_MODULE_INFO_SYM.common);
         return -EINVAL;
     }
     if (name == nullptr) {
-        ALOGV("%s: NULL name is not expected here", __FUNCTION__);
+        ALOGE("%s: NULL name is not expected here", __FUNCTION__);
         return -EINVAL;
     }
 
@@ -265,6 +276,9 @@ int VirtualCameraFactory::get_number_of_cameras() {
 int VirtualCameraFactory::get_camera_info(int camera_id, struct camera_info *info) {
     return gVirtualCameraFactory.getCameraInfo(camera_id, info);
 }
+int VirtualCameraFactory::set_torch_mode(const char* camera_id, bool enable){
+    return gVirtualCameraFactory.setTorchMode(camera_id, enable);
+}
 
 int VirtualCameraFactory::set_callbacks(const camera_module_callbacks_t *callbacks) {
     return gVirtualCameraFactory.setCallbacks(callbacks);
@@ -302,7 +316,7 @@ void VirtualCameraFactory::createVirtualRemoteCamera(
                                std::ref(mCameraSessionState));
 #endif							   
     if (mVirtualCameras[cameraId] == nullptr) {
-        ALOGV("%s: Unable to instantiate fake camera class", __FUNCTION__);
+        ALOGE("%s: Unable to instantiate fake camera class", __FUNCTION__);
     } else {
         status_t res = mVirtualCameras[cameraId]->Initialize();
         if (res == NO_ERROR) {
@@ -310,7 +324,7 @@ void VirtualCameraFactory::createVirtualRemoteCamera(
                   gCameraFacingBack ? "Back" : "Front", cameraId);
             // Camera creation and initialization was successful.
         } else {
-            ALOGV("%s: Unable to initialize %s camera %d: %s (%d)", __FUNCTION__,
+            ALOGE("%s: Unable to initialize %s camera %d: %s (%d)", __FUNCTION__,
                   gCameraFacingBack ? "back" : "front", cameraId, strerror(-res), res);
             delete mVirtualCameras[cameraId];
         }
diff --git a/src/VirtualCameraHal.cpp b/src/VirtualCameraHal.cpp
index 5d83909..29b361e 100644
--- a/src/VirtualCameraHal.cpp
+++ b/src/VirtualCameraHal.cpp
@@ -32,7 +32,11 @@ camera_module_t HAL_MODULE_INFO_SYM = {
     .common =
         {
             .tag = HARDWARE_MODULE_TAG,
-            .module_api_version = CAMERA_MODULE_API_VERSION_2_3,
+            //the camera module api version is changed to 2.4 as the android expects the
+            //api version to be 2.4 and higher for android version greater than Q
+            // this fix was added as part of VTS cases execution
+            //.module_api_version = CAMERA_MODULE_API_VERSION_2_3,
+            .module_api_version = CAMERA_MODULE_API_VERSION_2_4,
             .hal_api_version = HARDWARE_HAL_API_VERSION,
             .id = CAMERA_HARDWARE_MODULE_ID,
             .name = "Virtual Camera Module",
@@ -43,6 +47,7 @@ camera_module_t HAL_MODULE_INFO_SYM = {
         },
     .get_number_of_cameras = android::VirtualCameraFactory::get_number_of_cameras,
     .get_camera_info = android::VirtualCameraFactory::get_camera_info,
+    .set_torch_mode = android::VirtualCameraFactory::set_torch_mode,
     .set_callbacks = android::VirtualCameraFactory::set_callbacks,
     .get_vendor_tag_ops = android::VirtualCameraFactory::get_vendor_tag_ops,
     .open_legacy = android::VirtualCameraFactory::open_legacy};
diff --git a/src/VirtualFakeCamera3.cpp b/src/VirtualFakeCamera3.cpp
index df4ccc5..bf1c0b7 100644
--- a/src/VirtualFakeCamera3.cpp
+++ b/src/VirtualFakeCamera3.cpp
@@ -22,7 +22,7 @@
 #include <inttypes.h>
 
 //#define LOG_NNDEBUG 0
-//#define LOG_NDEBUG 0
+#define LOG_NDEBUG 0
 #define LOG_TAG "VirtualFakeCamera3: "
 #include <cutils/properties.h>
 #include <log/log.h>
@@ -72,7 +72,7 @@ const int32_t VirtualFakeCamera3::kHalSupportedFormats[] = {
     HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED,  // defined as RGB32
     HAL_PIXEL_FORMAT_RGBA_8888,               // RGB32
     HAL_PIXEL_FORMAT_YCbCr_420_888,           // NV12
-    //HAL_PIXEL_FORMAT_YCrCb_420_SP,            // NV21
+    HAL_PIXEL_FORMAT_YCrCb_420_SP,            // NV21
     // HAL_PIXEL_FORMAT_YV12 /* Not supporting now*/
 };
 
@@ -126,11 +126,10 @@ VirtualFakeCamera3::VirtualFakeCamera3(int cameraId, struct hw_module_t *module,
     mAeTargetExposureTime = kNormalExposureTime;
     mAeCurrentExposureTime = kNormalExposureTime;
     mAeCurrentSensitivity = kNormalSensitivity;
-    mSensorWidth = 640;
-    mSensorHeight = 480;
-    
-	mSrcWidth = 640;
-    mSrcHeight = 480;
+    mSensorWidth = 0;
+    mSensorHeight = 0;
+    mSrcWidth = gCameraMaxWidth;
+    mSrcHeight = gCameraMaxHeight;
     mCodecType = 0;
     mDecoderResolution = 0;
     mFacingBack = false;
@@ -150,23 +149,23 @@ VirtualFakeCamera3::~VirtualFakeCamera3() {
 }
 
 status_t VirtualFakeCamera3::Initialize() {
-    ALOGV("%s: E", __FUNCTION__);
+    ALOGVV("%s: E", __FUNCTION__);
     status_t res;
 
     if (mStatus != STATUS_ERROR) {
-        ALOGV("%s: Already initialized!", __FUNCTION__);
+        ALOGE("%s: Already initialized!", __FUNCTION__);
         return INVALID_OPERATION;
     }
 
     res = getCameraCapabilities();
     if (res != OK) {
-        ALOGV("%s: Unable to get camera capabilities: %s (%d)", __FUNCTION__, strerror(-res), res);
+        ALOGE("%s: Unable to get camera capabilities: %s (%d)", __FUNCTION__, strerror(-res), res);
         return res;
     }
 
     res = constructStaticInfo();
     if (res != OK) {
-        ALOGV("%s: Unable to allocate static info: %s (%d)", __FUNCTION__, strerror(-res), res);
+        ALOGE("%s: Unable to allocate static info: %s (%d)", __FUNCTION__, strerror(-res), res);
         return res;
     }
 
@@ -181,7 +180,7 @@ status_t VirtualFakeCamera3::openCamera(hw_device_t **device) {
 }
 
 uint32_t VirtualFakeCamera3::setDecoderResolution(uint32_t resolution) {
-    ALOGV(LOG_TAG "%s: E", __FUNCTION__);
+    ALOGVV(LOG_TAG "%s: E", __FUNCTION__);
     uint32_t res = 0;
     switch (resolution) {
         case DECODER_SUPPORTED_RESOLUTION_480P:
@@ -204,6 +203,7 @@ uint32_t VirtualFakeCamera3::setDecoderResolution(uint32_t resolution) {
 }
 status_t VirtualFakeCamera3::sendCommandToClient(camera_cmd_t cmd) {
     ALOGI("%s E", __func__);
+
     status_t status = INVALID_OPERATION;
     size_t config_cmd_packet_size = sizeof(camera_header_t) + sizeof(camera_config_cmd_t);
     camera_config_cmd_t config_cmd = {};
@@ -214,17 +214,16 @@ status_t VirtualFakeCamera3::sendCommandToClient(camera_cmd_t cmd) {
     config_cmd.config.resolution = mDecoderResolution;
 
     camera_packet_t *config_cmd_packet = NULL;
-#if 0
 
     int client_fd = mSocketServer->getClientFd();
     if (client_fd < 0) {
-        ALOGV("%s: We're not connected to client yet!", __FUNCTION__);
+        ALOGE("%s: We're not connected to client yet!", __FUNCTION__);
         return status;
     }
 
     config_cmd_packet = (camera_packet_t *)malloc(config_cmd_packet_size);
     if (config_cmd_packet == NULL) {
-        ALOGV(LOG_TAG "%s: config camera_packet_t allocation failed: %d ", __FUNCTION__, __LINE__);
+        ALOGE(LOG_TAG "%s: config camera_packet_t allocation failed: %d ", __FUNCTION__, __LINE__);
         goto out;
     }
 
@@ -234,7 +233,7 @@ status_t VirtualFakeCamera3::sendCommandToClient(camera_cmd_t cmd) {
 
     ALOGI("%s: Camera client fd %d!", __FUNCTION__, client_fd);
     if (send(client_fd, config_cmd_packet, config_cmd_packet_size, 0) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to send Camera %s command to client, err %s ", __FUNCTION__,
+        ALOGE(LOG_TAG "%s: Failed to send Camera %s command to client, err %s ", __FUNCTION__,
               (cmd == camera_cmd_t::CMD_CLOSE) ? "CloseCamera" : "OpenCamera", strerror(errno));
         goto out;
     }
@@ -242,40 +241,6 @@ status_t VirtualFakeCamera3::sendCommandToClient(camera_cmd_t cmd) {
     ALOGI("%s: Sent cmd %s to client %d!", __FUNCTION__,
           (cmd == camera_cmd_t::CMD_CLOSE) ? "CloseCamera" : "OpenCamera", client_fd);
     status = OK;
-#endif
-    CameraConfig camera_config = {};
-    if(cmd == CMD_OPEN)
-    {
-    camera_config.operation = socket::CameraOperation::kOpen;
-    } 
-    else
-    camera_config.operation = socket::CameraOperation::kClose;
-
-    int client_fd = mSocketServer->getClientFd();
-    if (client_fd < 0) {
-        ALOGV("%s: We're not connected to client yet!", __FUNCTION__);
-        return INVALID_OPERATION;
-    }
-    char mode[PROPERTY_VALUE_MAX];
-    //incase vsock add yuv command
-    //D :to do
-    //if ((property_get("ro.vendor.camera.transference", mode, nullptr) > 0))
-    {
-    //    if (!strcmp(mode, "VSOCK"))
-            ALOGV("%s:! sending Vsock ingo!", __FUNCTION__);
-            camera_config.frame_info.codec_type = VideoCodecType::kI420;
-    }
-    ALOGI("%s: Camera client fd %d!", __FUNCTION__, client_fd);
-    if (send(client_fd, &camera_config, sizeof(camera_config), 0) < 0) {
-        ALOGV(LOG_TAG "%s: Failed to send Camera Open command to client, err %s ", __FUNCTION__,
-              strerror(errno));
-        return INVALID_OPERATION;
-    }
-
-    std::string cmd_str =
-        (cmd == CMD_CLOSE) ? "CloseCamera" : "OpenCamera";
-    ALOGI("%s: Sent cmd %s to client %d!", __FUNCTION__, cmd_str.c_str(), client_fd);
-    return OK;
 out:
     free(config_cmd_packet);
     return status;
@@ -290,7 +255,7 @@ status_t VirtualFakeCamera3::connectCamera() {
         // initialize decoder
         if (mDecoder->init((android::socket::FrameResolution)mDecoderResolution, mCodecType,
                            device_name, 0) < 0) {
-            ALOGV("%s VideoDecoder init failed. %s decoding", __func__,
+            ALOGE("%s VideoDecoder init failed. %s decoding", __func__,
                   !device_name ? "SW" : device_name);
         } else {
             mDecoderInitDone = true;
@@ -305,12 +270,13 @@ status_t VirtualFakeCamera3::connectCamera() {
     ALOGI("%s Calling sendCommandToClient", __func__);
     status_t ret;
     if ((ret = sendCommandToClient(camera_cmd_t::CMD_OPEN)) != OK) {
-        ALOGV("%s sendCommandToClient failed", __func__);
+        ALOGE("%s sendCommandToClient failed", __func__);
         return ret;
     }
     ALOGI("%s Called sendCommandToClient", __func__);
     mCameraSessionState = CameraSessionState::kCameraOpened;
-
+    mSrcWidth = gCameraMaxWidth;
+    mSrcHeight = gCameraMaxHeight;
     // create sensor who gets decoded frames and forwards them to framework
 #ifdef ENABLE_FFMPEG
     mSensor = new Sensor(mSrcWidth, mSrcHeight, mDecoder);
@@ -364,7 +330,7 @@ status_t VirtualFakeCamera3::closeCamera() {
     // remote. If NO processCaptureRequest received between open and close then wait.
 
     if (!mprocessCaptureRequestFlag) {
-        ALOGV(LOG_TAG " %s: wait:..", __FUNCTION__);
+        ALOGE(LOG_TAG " %s: wait:..", __FUNCTION__);
         std::this_thread::sleep_for(2500ms);
     }
     mprocessCaptureRequestFlag = false;
@@ -379,7 +345,7 @@ status_t VirtualFakeCamera3::closeCamera() {
 
         auto ret = mSensor->shutDown();
         if (ret != NO_ERROR) {
-            ALOGV("%s: Unable to shut down sensor: %d", __FUNCTION__, ret);
+            ALOGE("%s: Unable to shut down sensor: %d", __FUNCTION__, ret);
         }
         mSensor.clear();
 
@@ -417,13 +383,13 @@ status_t VirtualFakeCamera3::closeCamera() {
     // Send close command to client
     status_t ret = sendCommandToClient(camera_cmd_t::CMD_CLOSE);
     if (ret != OK) {
-        ALOGV("%s sendCommandToClient failed", __func__);
+        ALOGE("%s sendCommandToClient failed", __func__);
     }
 
     // Set NULL or Zero to some local members which would be updated in the
     // next configure_streams call to support Dynamic multi-resolution.
-    mSrcWidth = 640;
-    mSrcHeight = 480;
+    mSrcWidth = 0;
+    mSrcHeight = 0;
     mDecoderResolution = 0;
     mDecoderInitDone = false;
     mSensor = NULL;
@@ -437,6 +403,11 @@ status_t VirtualFakeCamera3::getCameraInfo(struct camera_info *info) {
     return VirtualCamera3::getCameraInfo(info);
 }
 
+status_t  VirtualFakeCamera3::setTorchMode(const char* camera_id, bool enable){
+    return VirtualCamera3::setTorchMode(camera_id,enable);
+}
+
+
 /**
  * Camera3 interface methods
  */
@@ -446,7 +417,7 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
     status_t res;
 
     if (mStatus != STATUS_OPEN && mStatus != STATUS_READY) {
-        ALOGV("%s: Cannot configure streams in state %d", __FUNCTION__, mStatus);
+        ALOGE("%s: Cannot configure streams in state %d", __FUNCTION__, mStatus);
         return NO_INIT;
     }
 
@@ -454,19 +425,19 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
      * Sanity-check input list.
      */
     if (streamList == NULL) {
-        ALOGV("%s: NULL stream configuration", __FUNCTION__);
+        ALOGE("%s: NULL stream configuration", __FUNCTION__);
         return BAD_VALUE;
     }
 
     ALOGI("%s: %d streams", __FUNCTION__, streamList->num_streams);
     if (streamList->streams == NULL) {
-        ALOGV("%s: NULL stream list", __FUNCTION__);
+        ALOGE("%s: NULL stream list", __FUNCTION__);
         return BAD_VALUE;
     }
 
     ALOGI("%s: %d streams", __FUNCTION__, streamList->num_streams);
     if (streamList->num_streams < 1) {
-        ALOGV("%s: Bad number of streams requested: %d", __FUNCTION__, streamList->num_streams);
+        ALOGE("%s: Bad number of streams requested: %d", __FUNCTION__, streamList->num_streams);
         return BAD_VALUE;
     }
 
@@ -475,7 +446,7 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
         camera3_stream_t *newStream = streamList->streams[i];
 
         if (newStream == NULL) {
-            ALOGV("%s: Stream index %zu was NULL", __FUNCTION__, i);
+            ALOGE("%s: Stream index %zu was NULL", __FUNCTION__, i);
             return BAD_VALUE;
         }
 
@@ -488,7 +459,7 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
         if (newStream->stream_type == CAMERA3_STREAM_INPUT ||
             newStream->stream_type == CAMERA3_STREAM_BIDIRECTIONAL) {
             if (inputStream != NULL) {
-                ALOGV("%s: Multiple input streams requested!", __FUNCTION__);
+                ALOGE("%s: Multiple input streams requested!", __FUNCTION__);
                 return BAD_VALUE;
             }
             inputStream = newStream;
@@ -497,7 +468,7 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
         if (newStream->stream_type != CAMERA3_STREAM_INPUT) {
             if (newStream->rotation < CAMERA3_STREAM_ROTATION_0 ||
                 newStream->rotation > CAMERA3_STREAM_ROTATION_270) {
-                ALOGV("%s: Unsupported stream rotation 0x%x requested", __FUNCTION__,
+                ALOGE("%s: Unsupported stream rotation 0x%x requested", __FUNCTION__,
                       newStream->rotation);
                 return BAD_VALUE;
             }
@@ -506,7 +477,7 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
         if (newStream->width == 0 || newStream->height == 0 ||
             newStream->width > (uint32_t)mSensorWidth ||
             newStream->height > (uint32_t)mSensorHeight) {
-            ALOGV("%s: Unsupported stream width 0x%x height 0x%x", __FUNCTION__, newStream->width,
+            ALOGE("%s: Unsupported stream width 0x%x height 0x%x", __FUNCTION__, newStream->width,
                   newStream->height);
             return BAD_VALUE;
         }
@@ -520,18 +491,18 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
             }
         }
         if (!validFormat) {
-            ALOGV("%s: Unsupported stream format 0x%x requested", __FUNCTION__, newStream->format);
+            ALOGE("%s: Unsupported stream format 0x%x requested", __FUNCTION__, newStream->format);
             return BAD_VALUE;
         }
 
-       /* if (mSrcWidth < newStream->width && mSrcHeight < newStream->height) {
+        if (mSrcWidth < newStream->width && mSrcHeight < newStream->height) {
             // Update app's res request to local variable.
             mSrcWidth = newStream->width;
             mSrcHeight = newStream->height;
             // Update globally for clearing used buffers properly.
             gSrcWidth = mSrcWidth;
             gSrcHeight = mSrcHeight;
-        }*/
+        }
     }
     mInputStream = inputStream;
 
@@ -542,6 +513,7 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
      */
     for (StreamIterator s = mStreams.begin(); s != mStreams.end(); ++s) {
         PrivateStreamInfo *privStream = static_cast<PrivateStreamInfo *>((*s)->priv);
+        if(privStream != NULL) 
         privStream->alive = false;
     }
 
@@ -571,9 +543,13 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
                 // Workarroud: SG1:  HAL_PIXEL_FORMAT_RGBA_8888 &&
                 // GRALLOC_USAGE_HW_CAMERA_WRITE combination doesn't supported by minigbm
                 newStream->usage |= GRALLOC_USAGE_HW_CAMERA_WRITE;
-                ALOGV("%s: GRALLOC0", __FUNCTION__);
+                ALOGE("%s: GRALLOC0", __FUNCTION__);
 #else
-                ALOGV("%s: GRALLOC1", __FUNCTION__);
+                newStream->usage |= GRALLOC_USAGE_SW_WRITE_OFTEN;
+                ALOGE("%s: GRALLOC1 GRALLOC_USAGE_SW_WRITE_OFTEN", __FUNCTION__);
+                 //WA: configure usage when requrested for buffer overlay, WA provided during vts run
+                  // cases of configure single stream and flush 
+                //   newStream->usage = 0x100;
 #endif
                 break;
             case CAMERA3_STREAM_INPUT:
@@ -588,11 +564,17 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
 #ifndef USE_GRALLOC1
             if (newStream->usage & GRALLOC_USAGE_HW_CAMERA_WRITE) {
 #endif
-                if (newStream->usage & GRALLOC_USAGE_HW_TEXTURE) {
+                if ((newStream->usage & GRALLOC_USAGE_HW_TEXTURE) ||
+                    (newStream->usage & GRALLOC_USAGE_HW_VIDEO_ENCODER)) {
+                    // Both preview and video capture output format would
+                    // be RGB32 always if it is HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED.
                     newStream->format = HAL_PIXEL_FORMAT_RGBA_8888;
-                } else if (newStream->usage & GRALLOC_USAGE_HW_VIDEO_ENCODER) {
-                    newStream->format = HAL_PIXEL_FORMAT_YCbCr_420_888;
-                } else {
+                }
+				 //TODO: present in old VHAL, need to check video usecase
+				 //else if (newStream->usage & GRALLOC_USAGE_HW_VIDEO_ENCODER) {
+                   // newStream->format = HAL_PIXEL_FORMAT_YCbCr_420_888;
+                //}
+				else {
                     newStream->format = HAL_PIXEL_FORMAT_RGB_888;
                 }
 #ifndef USE_GRALLOC1
@@ -640,26 +622,26 @@ status_t VirtualFakeCamera3::configureStreams(camera3_stream_configuration *stre
 }
 
 status_t VirtualFakeCamera3::registerStreamBuffers(const camera3_stream_buffer_set *bufferSet) {
-    ALOGV("%s: E", __FUNCTION__);
+    ALOGVV("%s: E", __FUNCTION__);
     Mutex::Autolock l(mLock);
 
     // Should not be called in HAL versions >= 3.2
 
-    ALOGV("%s: Should not be invoked on new HALs!", __FUNCTION__);
+    ALOGE("%s: Should not be invoked on new HALs!", __FUNCTION__);
     return NO_INIT;
 }
 
 const camera_metadata_t *VirtualFakeCamera3::constructDefaultRequestSettings(int type) {
-    ALOGV("%s: E", __FUNCTION__);
+    ALOGVV("%s: E", __FUNCTION__);
     Mutex::Autolock l(mLock);
 
     if (type < 0 || type >= CAMERA3_TEMPLATE_COUNT) {
-        ALOGV("%s: Unknown request settings template: %d", __FUNCTION__, type);
+        ALOGE("%s: Unknown request settings template: %d", __FUNCTION__, type);
         return NULL;
     }
 
     if (!hasCapability(BACKWARD_COMPATIBLE) && type != CAMERA3_TEMPLATE_PREVIEW) {
-        ALOGV("%s: Template %d not supported w/o BACKWARD_COMPATIBLE capability", __FUNCTION__,
+        ALOGE("%s: Template %d not supported w/o BACKWARD_COMPATIBLE capability", __FUNCTION__,
               type);
         return NULL;
     }
@@ -975,29 +957,29 @@ const camera_metadata_t *VirtualFakeCamera3::constructDefaultRequestSettings(int
 }
 
 status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *request) {
-    ALOGV("%s: E", __FUNCTION__);
+    ALOGVV("%s: E", __FUNCTION__);
     Mutex::Autolock l(mLock);
     status_t res;
     mprocessCaptureRequestFlag = true;
     /** Validation */
 
     if (mStatus < STATUS_READY) {
-        ALOGV("%s: Can't submit capture requests in state %d", __FUNCTION__, mStatus);
+        ALOGE("%s: Can't submit capture requests in state %d", __FUNCTION__, mStatus);
         return INVALID_OPERATION;
     }
 
     if (request == NULL) {
-        ALOGV("%s: NULL request!", __FUNCTION__);
+        ALOGE("%s: NULL request!", __FUNCTION__);
         return BAD_VALUE;
     }
 
-    ALOGV("%s: Number of requested buffers = %u, Frame no: %u", __FUNCTION__,
+    ALOGVV("%s: Number of requested buffers = %u, Frame no: %u", __FUNCTION__,
            request->num_output_buffers, request->frame_number);
 
     uint32_t frameNumber = request->frame_number;
 
     if (request->settings == NULL && mPrevSettings.isEmpty()) {
-        ALOGV(
+        ALOGE(
             "%s: Request %d: NULL settings for first request after"
             "configureStreams()",
             __FUNCTION__, frameNumber);
@@ -1005,7 +987,7 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
     }
 
     if (request->input_buffer != NULL && request->input_buffer->stream != mInputStream) {
-        ALOGV("%s: Request %d: Input buffer not from input stream!", __FUNCTION__, frameNumber);
+        ALOGE("%s: Request %d: Input buffer not from input stream!", __FUNCTION__, frameNumber);
         ALOGV("%s: Bad stream %p, expected: %p", __FUNCTION__, request->input_buffer->stream,
               mInputStream);
         ALOGV("%s: Bad stream type %d, expected stream type %d", __FUNCTION__,
@@ -1016,7 +998,7 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
     }
 
     if (request->num_output_buffers < 1 || request->output_buffers == NULL) {
-        ALOGV("%s: Request %d: No output buffers provided!", __FUNCTION__, frameNumber);
+        ALOGE("%s: Request %d: No output buffers provided!", __FUNCTION__, frameNumber);
         return BAD_VALUE;
     }
 
@@ -1034,25 +1016,25 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
     do {
         PrivateStreamInfo *priv = static_cast<PrivateStreamInfo *>(b->stream->priv);
         if (priv == NULL) {
-            ALOGV("%s: Request %d: Buffer %zu: Unconfigured stream!", __FUNCTION__, frameNumber,
+            ALOGE("%s: Request %d: Buffer %zu: Unconfigured stream!", __FUNCTION__, frameNumber,
                   idx);
             return BAD_VALUE;
         }
         if (!priv->alive) {
-            ALOGV("%s: Request %d: Buffer %zu: Dead stream!", __FUNCTION__, frameNumber, idx);
+            ALOGE("%s: Request %d: Buffer %zu: Dead stream!", __FUNCTION__, frameNumber, idx);
             return BAD_VALUE;
         }
         if (b->status != CAMERA3_BUFFER_STATUS_OK) {
-            ALOGV("%s: Request %d: Buffer %zu: Status not OK!", __FUNCTION__, frameNumber, idx);
+            ALOGE("%s: Request %d: Buffer %zu: Status not OK!", __FUNCTION__, frameNumber, idx);
             return BAD_VALUE;
         }
         if (b->release_fence != -1) {
-            ALOGV("%s: Request %d: Buffer %zu: Has a release fence!", __FUNCTION__, frameNumber,
+            ALOGE("%s: Request %d: Buffer %zu: Has a release fence!", __FUNCTION__, frameNumber,
                   idx);
             return BAD_VALUE;
         }
         if (b->buffer == NULL) {
-            ALOGV("%s: Request %d: Buffer %zu: NULL buffer handle!", __FUNCTION__, frameNumber,
+            ALOGE("%s: Request %d: Buffer %zu: NULL buffer handle!", __FUNCTION__, frameNumber,
                   idx);
             return BAD_VALUE;
         }
@@ -1129,11 +1111,16 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
 #ifndef USE_GRALLOC1
             if (srcBuf.stream->usage & GRALLOC_USAGE_HW_CAMERA_WRITE) {
 #endif
-                if (srcBuf.stream->usage & GRALLOC_USAGE_HW_TEXTURE) {
+                if ((srcBuf.stream->usage & GRALLOC_USAGE_HW_TEXTURE) ||
+                    (srcBuf.stream->usage & GRALLOC_USAGE_HW_VIDEO_ENCODER)) {
+                    // Both preview and video capture output format would
+                    // be RGB32 always if it is HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED.
                     destBuf.format = HAL_PIXEL_FORMAT_RGBA_8888;
-                } else if (srcBuf.stream->usage & GRALLOC_USAGE_HW_VIDEO_ENCODER) {
-                    destBuf.format = HAL_PIXEL_FORMAT_YCbCr_420_888;
-                } else if ((srcBuf.stream->usage & GRALLOC_USAGE_HW_CAMERA_MASK) ==
+               //TODO: present in old VHAL
+			   //  } else if (srcBuf.stream->usage & GRALLOC_USAGE_HW_VIDEO_ENCODER) {
+                //    destBuf.format = HAL_PIXEL_FORMAT_YCbCr_420_888;
+                //}
+				 }else if ((srcBuf.stream->usage & GRALLOC_USAGE_HW_CAMERA_MASK) ==
                            GRALLOC_USAGE_HW_CAMERA_ZSL) {
                     // Note: Currently no support for ZSL mode
                     destBuf.format = HAL_PIXEL_FORMAT_RGB_888;
@@ -1151,20 +1138,22 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
         sp<Fence> bufferAcquireFence = new Fence(srcBuf.acquire_fence);
         res = bufferAcquireFence->wait(kFenceTimeoutMs);
         if (res == TIMED_OUT) {
-            ALOGV("%s: Request %d: Buffer %zu: Fence timed out after %d ms", __FUNCTION__,
+            ALOGE("%s: Request %d: Buffer %zu: Fence timed out after %d ms", __FUNCTION__,
                   frameNumber, i, kFenceTimeoutMs);
         }
         if (res == OK) {
             // Lock buffer for writing
-            if (srcBuf.stream->format == HAL_PIXEL_FORMAT_YCbCr_420_888) {
-                if (destBuf.format == HAL_PIXEL_FORMAT_YCbCr_420_888) {
+            if (srcBuf.stream->format == HAL_PIXEL_FORMAT_YCbCr_420_888 ||
+                srcBuf.stream->format == HAL_PIXEL_FORMAT_YCrCb_420_SP) {
+                if (destBuf.format == HAL_PIXEL_FORMAT_YCbCr_420_888 ||
+                    destBuf.format == HAL_PIXEL_FORMAT_YCrCb_420_SP) {
                     android_ycbcr ycbcr = android_ycbcr();
                     bufferHandle2 = native_handle_clone(*(destBuf.buffer));
 #ifdef GRALLOC_MAPPER4
                     res = GrallocModule::getInstance().importBuffer(bufferHandle2, &bufferHandle1);
                     //res = GrallocModule::getInstance().importBuffer(*(destBuf.buffer), &bufferHandle1);
                     if (res!= OK) {
-                      //  ALOGV("%s: Gralloc importBuffer failed",__FUNCTION__);
+                        ALOGV("%s: Gralloc importBuffer failed",__FUNCTION__);
                     }
                     res = GrallocModule::getInstance().lock_ycbcr(bufferHandle2,
                     //res = GrallocModule::getInstance().lock_ycbcr(bufferHandle1,
@@ -1181,7 +1170,7 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
                                                                   destBuf.height, &ycbcr);
                     destBuf.img = static_cast<uint8_t *>(ycbcr.y);
                 } else {
-                    ALOGV("Unexpected private format for flexible YUV: 0x%x", destBuf.format);
+                    ALOGE("Unexpected private format for flexible YUV: 0x%x", destBuf.format);
                     res = INVALID_OPERATION;
                 }
             } else {
@@ -1208,10 +1197,10 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
                                                         (void **)&(destBuf.img));
             }
             if (res != OK) {
-                ALOGV("%s: Request %d: Buffer %zu: Unable to lock buffer", __FUNCTION__,
+                ALOGE("%s: Request %d: Buffer %zu: Unable to lock buffer", __FUNCTION__,
                       frameNumber, i);
             } else {
-                ALOGV(" %s, stream format 0x%x width %d height %d buffer 0x%p img 0x%p",
+                ALOGVV(" %s, stream format 0x%x width %d height %d buffer 0x%p img 0x%p",
                        __FUNCTION__, destBuf.format, destBuf.width, destBuf.height, destBuf.buffer,
                        destBuf.img);
             }
@@ -1253,13 +1242,13 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
     if (needJpeg) {
         bool ready = mJpegCompressor->waitForDone(kJpegTimeoutNs);
         if (!ready) {
-            ALOGV("%s: Timeout waiting for JPEG compression to complete!", __FUNCTION__);
+            ALOGE("%s: Timeout waiting for JPEG compression to complete!", __FUNCTION__);
             res = NO_INIT;
             goto out;
         }
         res = mJpegCompressor->reserve();
         if (res != OK) {
-            ALOGV("%s: Error managing JPEG compressor resources, can't reserve it!", __FUNCTION__);
+            ALOGE("%s: Error managing JPEG compressor resources, can't reserve it!", __FUNCTION__);
             res = NO_INIT;
             goto out;
         }
@@ -1270,7 +1259,7 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
      */
     res = mReadoutThread->waitForReadout();
     if (res != OK) {
-        ALOGV("%s: Timeout waiting for previous requests to complete!", __FUNCTION__);
+        ALOGE("%s: Timeout waiting for previous requests to complete!", __FUNCTION__);
         res = NO_INIT;
         goto out;
     }
@@ -1286,7 +1275,7 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
             goto out;
         }
         if (syncTimeoutCount == kMaxSyncTimeoutCount) {
-            ALOGV("%s: Request %d: Sensor sync timed out after %" PRId64 " ms", __FUNCTION__,
+            ALOGE("%s: Request %d: Sensor sync timed out after %" PRId64 " ms", __FUNCTION__,
                   frameNumber, kSyncWaitTimeout * kMaxSyncTimeoutCount / 1000000);
             res = NO_INIT;
             goto out;
@@ -1309,7 +1298,7 @@ status_t VirtualFakeCamera3::processCaptureRequest(camera3_capture_request *requ
     r.buffers = buffers;
 
     mReadoutThread->queueCaptureRequest(r);
-    ALOGV("%s: Queued frame %d", __FUNCTION__, request->frame_number);
+    ALOGVV("%s: Queued frame %d", __FUNCTION__, request->frame_number);
 
     // Cache the settings for next time
     mPrevSettings.acquire(settings);
@@ -1322,7 +1311,7 @@ out:
 }
 
 status_t VirtualFakeCamera3::flush() {
-    ALOGV("%s: Not implemented; ignored", __FUNCTION__);
+    ALOGVV("%s: Not implemented; ignored", __FUNCTION__);
     return OK;
 }
 
@@ -1353,7 +1342,7 @@ status_t VirtualFakeCamera3::getCameraCapabilities() {
             cap = strtok_r(NULL, " ,", &saveptr);
         }
         if (mCapabilities.size() == 0) {
-            ALOGV("remote.sf.back_camera_caps had no valid capabilities: %s", prop);
+            ALOGE("remote.sf.back_camera_caps had no valid capabilities: %s", prop);
         }
     }
     // Default to FULL_LEVEL plus RAW if nothing is defined
@@ -1423,7 +1412,7 @@ status_t VirtualFakeCamera3::constructStaticInfo() {
     status_t res;
     int32_t width = 0, height = 0;
 
-    ALOGV("%s: Updating metadata for Camera %d", __func__, mCameraID);
+    ALOGVV("%s: Updating metadata for Camera %d", __func__, mCameraID);
 
     // Setting the max supported Camera resolution.
     setMaxSupportedResolution();
@@ -2161,10 +2150,11 @@ status_t VirtualFakeCamera3::constructStaticInfo() {
     }
 
     // android.info
-
-    const uint8_t supportedHardwareLevel = hasCapability(FULL_LEVEL)
-                                               ? ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL_FULL
-                                               : ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
+    //WA during vts case execution for burst mode, setting limited hardware level
+    const uint8_t supportedHardwareLevel = ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
+    //const uint8_t supportedHardwareLevel = hasCapability(FULL_LEVEL)
+                                           //    ? ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL_FULL
+                                             //  : ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED;
     ADD_STATIC_ENTRY(ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL, &supportedHardwareLevel,
                      /*count*/ 1);
 
@@ -2361,7 +2351,7 @@ status_t VirtualFakeCamera3::process3A(CameraMetadata &settings) {
 
     e = settings.find(ANDROID_CONTROL_MODE);
     if (e.count == 0) {
-        ALOGV("%s: No control mode entry!", __FUNCTION__);
+        ALOGE("%s: No control mode entry!", __FUNCTION__);
         return BAD_VALUE;
     }
     uint8_t controlMode = e.data.u8[0];
@@ -2377,13 +2367,13 @@ status_t VirtualFakeCamera3::process3A(CameraMetadata &settings) {
         return OK;
     } else if (controlMode == ANDROID_CONTROL_MODE_USE_SCENE_MODE) {
         if (!hasCapability(BACKWARD_COMPATIBLE)) {
-            ALOGV("%s: Can't use scene mode when BACKWARD_COMPATIBLE not supported!", __FUNCTION__);
+            ALOGE("%s: Can't use scene mode when BACKWARD_COMPATIBLE not supported!", __FUNCTION__);
             return BAD_VALUE;
         }
 
         e = settings.find(ANDROID_CONTROL_SCENE_MODE);
         if (e.count == 0) {
-            ALOGV("%s: No scene mode entry!", __FUNCTION__);
+            ALOGE("%s: No scene mode entry!", __FUNCTION__);
             return BAD_VALUE;
         }
         uint8_t sceneMode = e.data.u8[0];
@@ -2393,7 +2383,7 @@ status_t VirtualFakeCamera3::process3A(CameraMetadata &settings) {
                 mFacePriority = true;
                 break;
             default:
-                ALOGV("%s: Emulator doesn't support scene mode %d", __FUNCTION__, sceneMode);
+                ALOGE("%s: Emulator doesn't support scene mode %d", __FUNCTION__, sceneMode);
                 return BAD_VALUE;
         }
     } else {
@@ -2421,7 +2411,7 @@ status_t VirtualFakeCamera3::doFakeAE(CameraMetadata &settings) {
 
     e = settings.find(ANDROID_CONTROL_AE_MODE);
     if (e.count == 0 && hasCapability(BACKWARD_COMPATIBLE)) {
-        ALOGV("%s: No AE mode entry!", __FUNCTION__);
+        ALOGE("%s: No AE mode entry!", __FUNCTION__);
         return BAD_VALUE;
     }
     uint8_t aeMode = (e.count > 0) ? e.data.u8[0] : (uint8_t)ANDROID_CONTROL_AE_MODE_ON;
@@ -2517,7 +2507,7 @@ status_t VirtualFakeCamera3::doFakeAE(CameraMetadata &settings) {
                 mAeCounter = 0;
                 break;
             default:
-                ALOGV("%s: Emulator in unexpected AE state %d", __FUNCTION__, mAeState);
+                ALOGE("%s: Emulator in unexpected AE state %d", __FUNCTION__, mAeState);
                 return INVALID_OPERATION;
         }
     } else {
@@ -2533,7 +2523,7 @@ status_t VirtualFakeCamera3::doFakeAF(CameraMetadata &settings) {
 
     e = settings.find(ANDROID_CONTROL_AF_MODE);
     if (e.count == 0 && hasCapability(BACKWARD_COMPATIBLE)) {
-        ALOGV("%s: No AF mode entry!", __FUNCTION__);
+        ALOGE("%s: No AF mode entry!", __FUNCTION__);
         return BAD_VALUE;
     }
     uint8_t afMode = (e.count > 0) ? e.data.u8[0] : (uint8_t)ANDROID_CONTROL_AF_MODE_OFF;
@@ -2565,7 +2555,7 @@ status_t VirtualFakeCamera3::doFakeAF(CameraMetadata &settings) {
             }
             break;
         default:
-            ALOGV("%s: Emulator doesn't support AF mode %d", __FUNCTION__, afMode);
+            ALOGE("%s: Emulator doesn't support AF mode %d", __FUNCTION__, afMode);
             return BAD_VALUE;
     }
 
@@ -2590,12 +2580,12 @@ status_t VirtualFakeCamera3::doFakeAF(CameraMetadata &settings) {
             // Cancel trigger always transitions into INACTIVE
             mAfState = ANDROID_CONTROL_AF_STATE_INACTIVE;
 
-            ALOGV("%s: AF State transition to STATE_INACTIVE", __FUNCTION__);
+            ALOGVV("%s: AF State transition to STATE_INACTIVE", __FUNCTION__);
 
             // Stay in 'inactive' until at least next frame
             return OK;
         default:
-            ALOGV("%s: Unknown af trigger value %d", __FUNCTION__, afTrigger);
+            ALOGE("%s: Unknown af trigger value %d", __FUNCTION__, afTrigger);
             return BAD_VALUE;
     }
 
@@ -2712,7 +2702,7 @@ status_t VirtualFakeCamera3::doFakeAF(CameraMetadata &settings) {
             }
             break;
         default:
-            ALOGV("%s: Bad af state %d", __FUNCTION__, mAfState);
+            ALOGE("%s: Bad af state %d", __FUNCTION__, mAfState);
     }
 
     {
@@ -2727,7 +2717,7 @@ status_t VirtualFakeCamera3::doFakeAF(CameraMetadata &settings) {
         };
         camera_metadata_enum_snprint(ANDROID_CONTROL_AF_STATE, mAfState, afNewStateString,
                                      sizeof(afNewStateString));
-        ALOGV("%s: AF state transitioned from %s to %s", __FUNCTION__, afStateString,
+        ALOGVV("%s: AF state transitioned from %s to %s", __FUNCTION__, afStateString,
                afNewStateString);
     }
 
@@ -2739,7 +2729,7 @@ status_t VirtualFakeCamera3::doFakeAWB(CameraMetadata &settings) {
 
     e = settings.find(ANDROID_CONTROL_AWB_MODE);
     if (e.count == 0 && hasCapability(BACKWARD_COMPATIBLE)) {
-        ALOGV("%s: No AWB mode entry!", __FUNCTION__);
+        ALOGE("%s: No AWB mode entry!", __FUNCTION__);
         return BAD_VALUE;
     }
     uint8_t awbMode = (e.count > 0) ? e.data.u8[0] : (uint8_t)ANDROID_CONTROL_AWB_MODE_AUTO;
@@ -2763,7 +2753,7 @@ status_t VirtualFakeCamera3::doFakeAWB(CameraMetadata &settings) {
                 awbLocked ? ANDROID_CONTROL_AWB_STATE_LOCKED : ANDROID_CONTROL_AWB_STATE_CONVERGED;
             break;
         default:
-            ALOGV("%s: Emulator doesn't support AWB mode %d", __FUNCTION__, awbMode);
+            ALOGE("%s: Emulator doesn't support AWB mode %d", __FUNCTION__, awbMode);
             return BAD_VALUE;
     }
 
@@ -2847,7 +2837,7 @@ void VirtualFakeCamera3::signalReadoutIdle() {
 void VirtualFakeCamera3::onSensorEvent(uint32_t frameNumber, Event e, nsecs_t timestamp) {
     switch (e) {
         case Sensor::SensorListener::EXPOSURE_START: {
-            //          ALOGV("%s: Frame %d: Sensor started exposure at %lld",
+            //          ALOGVV("%s: Frame %d: Sensor started exposure at %lld",
             //               __FUNCTION__, frameNumber, timestamp);
             // Trigger shutter notify to framework
             camera3_notify_msg_t msg;
@@ -2900,11 +2890,11 @@ status_t VirtualFakeCamera3::ReadoutThread::waitForReadout() {
     while (mInFlightQueue.size() >= kMaxQueueSize) {
         res = mInFlightSignal.waitRelative(mLock, kWaitPerLoop);
         if (res != OK && res != TIMED_OUT) {
-            ALOGV("%s: Error waiting for in-flight queue to shrink", __FUNCTION__);
+            ALOGE("%s: Error waiting for in-flight queue to shrink", __FUNCTION__);
             return INVALID_OPERATION;
         }
         if (loopCount == kMaxWaitLoops) {
-            ALOGV("%s: Timed out waiting for in-flight queue to shrink", __FUNCTION__);
+            ALOGE("%s: Timed out waiting for in-flight queue to shrink", __FUNCTION__);
             return TIMED_OUT;
         }
         loopCount++;
@@ -2915,7 +2905,7 @@ status_t VirtualFakeCamera3::ReadoutThread::waitForReadout() {
 bool VirtualFakeCamera3::ReadoutThread::threadLoop() {
     status_t res = NO_ERROR;
 
-    ALOGV("%s: ReadoutThread waiting for request", __FUNCTION__);
+    ALOGVV("%s: ReadoutThread waiting for request", __FUNCTION__);
 
     // First wait for a request from the in-flight queue
 
@@ -2924,10 +2914,10 @@ bool VirtualFakeCamera3::ReadoutThread::threadLoop() {
         if (mInFlightQueue.empty()) {
             res = mInFlightSignal.waitRelative(mLock, kWaitPerLoop);
             if (res == TIMED_OUT) {
-                ALOGV("%s: ReadoutThread: Timed out waiting for request", __FUNCTION__);
+                ALOGVV("%s: ReadoutThread: Timed out waiting for request", __FUNCTION__);
                 return true;
             } else if (res != NO_ERROR) {
-                ALOGV("%s: Error waiting for capture requests: %d", __FUNCTION__, res);
+                ALOGE("%s: Error waiting for capture requests: %d", __FUNCTION__, res);
                 return false;
             }
         }
@@ -2938,20 +2928,20 @@ bool VirtualFakeCamera3::ReadoutThread::threadLoop() {
         mInFlightQueue.erase(mInFlightQueue.begin());
         mInFlightSignal.signal();
         mThreadActive = true;
-        ALOGV("%s: Beginning readout of frame %d", __FUNCTION__, mCurrentRequest.frameNumber);
+        ALOGVV("%s: Beginning readout of frame %d", __FUNCTION__, mCurrentRequest.frameNumber);
     }
 
     // Then wait for it to be delivered from the sensor
-    ALOGV("%s: ReadoutThread: Wait for frame to be delivered from sensor", __FUNCTION__);
+    ALOGVV("%s: ReadoutThread: Wait for frame to be delivered from sensor", __FUNCTION__);
 
     nsecs_t captureTime;
     bool gotFrame = mParent->mSensor->waitForNewFrame(kWaitPerLoop, &captureTime);
     if (!gotFrame) {
-        ALOGV("%s: ReadoutThread: Timed out waiting for sensor frame", __FUNCTION__);
+        ALOGVV("%s: ReadoutThread: Timed out waiting for sensor frame", __FUNCTION__);
         return true;
     }
 
-    //     ALOGV("Sensor done with readout for frame %d, captured at %lld ",
+    //     ALOGVV("Sensor done with readout for frame %d, captured at %lld ",
     //          mCurrentRequest.frameNumber, captureTime);
 
     // Check if we need to JPEG encode a buffer, and send it for async
@@ -2977,7 +2967,7 @@ bool VirtualFakeCamera3::ReadoutThread::threadLoop() {
             }
             if (goodBuffer) {
                 needJpeg = true;
-                ALOGV("Sensor done with readout for frame %d, needJpeg = %d",
+                ALOGVV("Sensor done with readout for frame %d, needJpeg = %d",
                        mCurrentRequest.frameNumber, needJpeg);
 
                 mJpegHalBuffer = *buf;
@@ -2989,7 +2979,7 @@ bool VirtualFakeCamera3::ReadoutThread::threadLoop() {
 
                 continue;
             }
-            ALOGV("%s: Error compressing output buffer: %s (%d)", __FUNCTION__, strerror(-res),
+            ALOGE("%s: Error compressing output buffer: %s (%d)", __FUNCTION__, strerror(-res),
                   res);
             // fallthrough for cleanup
         }
@@ -3066,7 +3056,7 @@ bool VirtualFakeCamera3::ReadoutThread::threadLoop() {
     if (signalIdle) mParent->signalReadoutIdle();
 
     // Send it off to the framework
-    ALOGV("%s: ReadoutThread: Send result to framework", __FUNCTION__);
+    ALOGVV("%s: ReadoutThread: Send result to framework", __FUNCTION__);
     mParent->sendCaptureResult(&result);
 
     // Clean up
@@ -3103,7 +3093,7 @@ void VirtualFakeCamera3::ReadoutThread::onJpegDone(const StreamBuffer &jpegBuffe
     result.partial_result = 0;
 
     if (!success) {
-        ALOGV(
+        ALOGE(
             "%s: Compression failure, returning error state buffer to"
             " framework",
             __FUNCTION__);
@@ -3117,7 +3107,7 @@ void VirtualFakeCamera3::ReadoutThread::onJpegDone(const StreamBuffer &jpegBuffe
 void VirtualFakeCamera3::ReadoutThread::onJpegInputDone(const StreamBuffer &inputBuffer) {
     // Should never get here, since the input buffer has to be returned
     // by end of processCaptureRequest
-    ALOGV("%s: Unexpected input buffer from JPEG compressor!", __FUNCTION__);
+    ALOGE("%s: Unexpected input buffer from JPEG compressor!", __FUNCTION__);
 }
 
 };  // namespace android
diff --git a/src/fake-pipeline2/Sensor.cpp b/src/fake-pipeline2/Sensor.cpp
index 17a4352..15c4757 100644
--- a/src/fake-pipeline2/Sensor.cpp
+++ b/src/fake-pipeline2/Sensor.cpp
@@ -131,10 +131,8 @@ Sensor::Sensor(uint32_t width, uint32_t height)
 {
     // Max supported resolution of the camera sensor.
     // It is based on client camera capability info.
-    //mSrcWidth = width;
-    //mSrcHeight = height;
-    mSrcWidth = 640;
-    mSrcHeight = 480;
+    mSrcWidth = width;
+    mSrcHeight = height;
     mSrcFrameSize = mSrcWidth * mSrcHeight * BPP_NV12;
 }
 
@@ -250,7 +248,7 @@ status_t Sensor::readyToRun() {
     return OK;
 }
 
-#define CROP_ROTATE
+//#define CROP_ROTATE
 #ifdef CROP_ROTATE
 void bufferCropAndRotate(unsigned char * buff, unsigned char * buff_out){
 //
@@ -333,7 +331,6 @@ bool Sensor::threadLoop() {
     uint32_t gain;
     Buffers *nextBuffers;
     uint32_t frameNumber;
-    bool needJpeg = false;
     ALOGVV("Sensor Thread stage E :1");
     SensorListener *listener = nullptr;
     {
@@ -443,24 +440,20 @@ bool Sensor::threadLoop() {
                         bAux.streamId = 0;
                         bAux.width = b.width;
                         bAux.height = b.height;
-                        bAux.format =
-                            HAL_PIXEL_FORMAT_YCbCr_420_888;
+                        bAux.format = HAL_PIXEL_FORMAT_YCrCb_420_SP;
                         bAux.stride = b.width;
                         bAux.buffer = nullptr;
                         bAux.img = new uint8_t[b.width * b.height * 3];
-                        needJpeg = true;
                         mNextCapturedBuffers->push_back(bAux);
                     } else {
                         captureDepthCloud(b.img);
                     }
                     break;
+                case HAL_PIXEL_FORMAT_YCrCb_420_SP:
+                    captureNV21(b.img, gain, b.width, b.height);
+                    break;
                 case HAL_PIXEL_FORMAT_YCbCr_420_888:
-                    if (!needJpeg) {
-                        captureNV12(b.img, gain, b.width, b.height);
-                    } else {
-                        needJpeg = false;
-                        captureJPEG(b.img, gain, b.width, b.height);
-                    }
+                    captureNV12(b.img, gain, b.width, b.height);
                     break;
                 case HAL_PIXEL_FORMAT_YV12:
                     // TODO:
@@ -495,7 +488,6 @@ bool Sensor::threadLoop() {
     }
 
     ALOGVV("Sensor Thread stage X :4");
-    ClientVideoBuffer *handle = ClientVideoBuffer::getClientInstance();
     ALOGVV("Frame No: %d took %d ms, target %d ms", frameNumber,
            (int)(workDoneRealTime - startRealTime) / 1000000, (int)(frameDuration / 1000000));
     return true;
@@ -617,7 +609,7 @@ void Sensor::captureRGBA(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
     #endif
     int cameraInputDataSize;
 
-    if (!gIsInFrameI420 && !gIsInFrameH264) {
+    if (!gIsInFrameI420 && !gIsInFrameH264 && !gIsInFrameMJPG) {
         ALOGE("%s Exit - only H264, H265, I420 input frames supported", __FUNCTION__);
         return;
     }
@@ -647,7 +639,7 @@ void Sensor::captureRGBA(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
 
     // For Max supported Resolution.
     if (width == (uint32_t)mSrcWidth && height == (uint32_t)mSrcHeight) {
-        if (gIsInFrameI420) {
+        if (gIsInFrameI420 || gIsInFrameMJPG) {
             ALOGVV(LOG_TAG " %s: I420, scaling not required: Size = %dx%d", __FUNCTION__, width,
                    height);
             const uint8_t *src_y = bufData;
@@ -679,7 +671,7 @@ void Sensor::captureRGBA(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
         }
         // For upscaling and downscaling all other resolutions below max supported resolution.
     } else {
-        if (gIsInFrameI420) {
+        if (gIsInFrameI420 || gIsInFrameMJPG) {
             ALOGVV(LOG_TAG " %s: I420, need to scale: Size = %dx%d", __FUNCTION__, width, height);
             int destFrameSize = width * height;
 
@@ -862,7 +854,7 @@ void Sensor::captureNV12(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
     ALOGVV(LOG_TAG " %s: bufData[%p] img[%p] resolution[%d:%d]", __func__, bufData, img, width,
            height);
 
-    if (!gIsInFrameI420 && !gIsInFrameH264) {
+    if (!gIsInFrameI420 && !gIsInFrameH264 && !gIsInFrameMJPG) {
         ALOGE("%s Exit - only H264, I420 input frames supported", __FUNCTION__);
         return;
     }
@@ -892,7 +884,7 @@ void Sensor::captureNV12(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
 
     // For Max supported Resolution.
     if (width == (uint32_t)mSrcWidth && height == (uint32_t)mSrcHeight) {
-        if (gIsInFrameI420) {
+        if (gIsInFrameI420 || gIsInFrameMJPG) {
             // For I420 input support
             ALOGVV(LOG_TAG " %s: I420 no scaling required Size = %dx%d", __FUNCTION__, width,
                    height);
@@ -927,7 +919,7 @@ void Sensor::captureNV12(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
         }
         // For upscaling and downscaling all other resolutions below max supported resolution.
     } else {
-        if (gIsInFrameI420) {
+        if (gIsInFrameI420 || gIsInFrameMJPG) {
             // For I420 input support
             ALOGVV(LOG_TAG " %s: I420 with scaling: Size = %dx%d", __FUNCTION__, width, height);
 
@@ -1054,7 +1046,7 @@ void Sensor::captureNV12(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
     ALOGVV(LOG_TAG " %s: Captured NV12 image sucessfully..", __FUNCTION__);
 }
 
-void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height) {
+void Sensor::captureNV21(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height) {
     ALOGVV("%s: E", __FUNCTION__);
 
     ClientVideoBuffer *handle = ClientVideoBuffer::getClientInstance();
@@ -1068,50 +1060,40 @@ void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
     int src_size = mSrcWidth * mSrcHeight;
     int dstFrameSize = width * height;
 
-    int out_size;
+    int cameraInputDataSize;
 
     if (!gIsInFrameI420 && !gIsInFrameH264) {
-	ALOGE("%s Exit - only H264, I420 input frames supported", __FUNCTION__);
-	return;
-    }
-
-    //TODO: handle other resolutions as required
-    if (width == 320 && height == 240) {
-        mDstJpegBufSize = FRAME_SIZE_240P;
-    } else if (width == 640 && height == 480) {
-        mDstJpegBufSize = FRAME_SIZE_480P;
-    } else {
-        //TODO: adjust default
-        mDstJpegBufSize = FRAME_SIZE_480P;
+        ALOGE("%s Exit - only H264, H265, I420 input frames supported", __FUNCTION__);
+        return;
     }
 
-    //Initialize to the size based on resolution.
-    out_size = mDstJpegBufSize;
+    // Initialize the input data size based on client camera resolution.
+    cameraInputDataSize = mSrcFrameSize;
 
 #ifdef ENABLE_FFMPEG
     if (gIsInFrameH264) {
         if (handle->clientBuf[handle->clientRevCount % 1].decoded) {
-	   //Note: bufData already assigned in the function start
-	   ALOGVV("%s - Already Decoded", __FUNCTION__);
-	   out_size = mDstJpegBufSize;
-	} else {
-	   getNV12Frames(bufData, &out_size);
-	   handle->clientBuf[handle->clientRevCount % 1].decoded = true;
-	   ALOGVV("%s - getNV12Framesout_size: %d\n", __func__, out_size);
-	   std::unique_lock<std::mutex> ulock(client_buf_mutex);
-	   handle->decodedFrameNo++;
-	   ALOGVV("%s Decoded frame #[%zd]", __FUNCTION__, handle->decodedFrameNo);
-	   ulock.unlock();
-	}
+            // If already decoded camera input frame.
+            ALOGVV("%s - Already Decoded Camera Input frame", __FUNCTION__);
+        } else {
+            // To get the decoded frame.
+            getNV12Frames(bufData, &cameraInputDataSize);
+            handle->clientBuf[handle->clientRevCount % 1].decoded = true;
+            std::unique_lock<std::mutex> ulock(client_buf_mutex);
+            handle->decodedFrameNo++;
+            ALOGVV("%s Decoded Camera Input Frame No: %zd with size of %d", __FUNCTION__,
+                   handle->decodedFrameNo, cameraInputDataSize);
+            ulock.unlock();
+        }
     }
 #endif
 
     //For default resolution 640x480p
     if (width == (uint32_t)mSrcWidth && height == (uint32_t)mSrcHeight) {
-	//For I420 input
+        // For I420 input
         if (gIsInFrameI420) {
-            ALOGVV(LOG_TAG "%s: I420 input without scaling required Size = %dx%d for JPEG conversion",
-			    __FUNCTION__, width, height);
+            ALOGVV(LOG_TAG "%s: I420 to NV21 conversion without scaling: Size = %dx%d",
+                   __FUNCTION__, width, height);
 
             const uint8_t *src_y = bufData;
             int src_stride_y = mSrcWidth;
@@ -1131,8 +1113,8 @@ void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
             }
             // For NV12 input
         } else {
-	    ALOGVV(LOG_TAG "%s: NV12 to NV21 conversion for JPEG conversion: Size = %dx%d",
-			   __FUNCTION__, width, height);
+            ALOGVV(LOG_TAG "%s: NV12 to NV21 conversion without scaling: Size = %dx%d",
+                   __FUNCTION__, width, height);
 
             const uint8_t *src_y = bufData;
             int src_stride_y = mSrcWidth;
@@ -1146,10 +1128,10 @@ void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
             uint8_t *dst_v = mDstJpegBuf.data() + src_size + src_size / 4;
             int dst_stride_v = mSrcWidth >> 1;
 
-	    if (int ret = libyuv::NV12ToI420(src_y, src_stride_y, src_uv, src_stride_uv,
-				             dst_y,dst_stride_y, dst_u, dst_stride_u, dst_v, dst_stride_v,
-					     mSrcWidth, mSrcHeight)) {
-	    }
+            if (int ret = libyuv::NV12ToI420(src_y, src_stride_y, src_uv, src_stride_uv, dst_y,
+                                             dst_stride_y, dst_u, dst_stride_u, dst_v, dst_stride_v,
+                                             mSrcWidth, mSrcHeight)) {
+            }
 
             src_y = mDstJpegBuf.data();
             src_stride_y = mSrcWidth;
@@ -1172,8 +1154,8 @@ void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
     } else {
         // For I420 input
         if (gIsInFrameI420) {
-	    ALOGVV(LOG_TAG "%s: I420 with scaling: Size = %dx%d for JPEG conversion",
-			    __FUNCTION__, width, height);
+            ALOGVV(LOG_TAG "%s: I420 to NV21 with scaling: Size = %dx%d", __FUNCTION__, width,
+                   height);
 
             const uint8_t *src_y = bufData;
             int src_stride_y = mSrcWidth;
@@ -1194,33 +1176,32 @@ void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
             int dst_height = height;
             auto filtering = libyuv::kFilterNone;
 
-	    if (int ret = libyuv::I420Scale(src_y, src_stride_y, src_u, src_stride_u, src_v,
-				            src_stride_v, src_width, src_height, dst_y,
-					    dst_stride_y, dst_u, dst_stride_u, dst_v,
-					    dst_stride_v,dst_width, dst_height, filtering)) {
-	    }
-
-	    ALOGVV("%s: I420 Scaling done for JPEG conversion", __FUNCTION__);
+            if (int ret = libyuv::I420Scale(src_y, src_stride_y, src_u, src_stride_u, src_v,
+                                            src_stride_v, src_width, src_height, dst_y,
+                                            dst_stride_y, dst_u, dst_stride_u, dst_v, dst_stride_v,
+                                            dst_width, dst_height, filtering)) {
+            }
 
-	    src_y = mDstJpegBuf.data();
-	    src_stride_y = width;
-	    src_u = mDstJpegBuf.data() + dstFrameSize;
-	    src_stride_u = width >> 1;
-	    src_v = mDstJpegBuf.data() + dstFrameSize + dstFrameSize / 4;
-	    src_stride_v = width >> 1;
-	    dst_y = img;
-	    dst_stride_y = width;
+            src_y = mDstJpegBuf.data();
+            src_stride_y = width;
+            src_u = mDstJpegBuf.data() + dstFrameSize;
+            src_stride_u = width >> 1;
+            src_v = mDstJpegBuf.data() + dstFrameSize + dstFrameSize / 4;
+            src_stride_v = width >> 1;
+            dst_y = img;
+            dst_stride_y = width;
 
             uint8_t *dst_vu = dst_y + width * height;
             int dst_stride_vu = width;
 
-	    if (int ret = libyuv::I420ToNV21(src_y, src_stride_y, src_u, src_stride_u, src_v,
-				             src_stride_v, dst_y, dst_stride_y,
-					     dst_vu, dst_stride_vu, width, height)) {
+            if (int ret = libyuv::I420ToNV21(src_y, src_stride_y, src_u, src_stride_u, src_v,
+                                             src_stride_v, dst_y, dst_stride_y, dst_vu,
+                                             dst_stride_vu, width, height)) {
             }
-	//For NV12 input
-	} else {
-	    ALOGVV(LOG_TAG "%s: NV12 input with scaling Size = %dx%d for JPEG conversion", __FUNCTION__, width, height);
+            // For NV12 input
+        } else {
+            ALOGVV(LOG_TAG "%s: NV12 to NV21 conversion with scaling: Size = %dx%d", __FUNCTION__,
+                   width, height);
 
             const uint8_t *src_y = bufData;
             int src_stride_y = mSrcWidth;
@@ -1282,7 +1263,7 @@ void Sensor::captureJPEG(uint8_t *img, uint32_t gain, uint32_t width, uint32_t h
             }
         }
     }
-    ALOGVV("%s: Successfully Converted to NV21 for JPEG Capture!!!", __FUNCTION__);
+    ALOGVV("%s: Captured NV21 image sucessfully..", __FUNCTION__);
 }
 
 void Sensor::captureDepth(uint8_t *img, uint32_t gain, uint32_t width, uint32_t height) {
-- 
2.17.1

