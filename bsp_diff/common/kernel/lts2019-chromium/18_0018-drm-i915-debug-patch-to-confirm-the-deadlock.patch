From 52b23257b5447112fb94a0f247666f1f04ae8e8a Mon Sep 17 00:00:00 2001
From: feijiang1 <fei.jiang@intel.com>
Date: Wed, 24 Mar 2021 00:32:47 +0800
Subject: [PATCH] drm-i915: debug patch to confirm the deadlock - newnew

Tracked-On:
Signed-off-by: feijiang1 <fei.jiang@intel.com>
---
 drivers/gpu/drm/i915/i915_active.c | 20 +++++++++++++++++---
 1 file changed, 17 insertions(+), 3 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_active.c b/drivers/gpu/drm/i915/i915_active.c
index 5605f55bc2b7..25c9aea32f6a 100644
--- a/drivers/gpu/drm/i915/i915_active.c
+++ b/drivers/gpu/drm/i915/i915_active.c
@@ -43,7 +43,11 @@ node_from_active(struct i915_active_fence *active)
 
 static inline bool is_barrier(const struct i915_active_fence *active)
 {
-	return IS_ERR(rcu_access_pointer(active->fence));
+	bool ret;
+	printk("fei debug: call rcu_access_pointer start, cpu %d\n", smp_processor_id());
+	ret = IS_ERR(rcu_access_pointer(active->fence));
+	printk("fei debug: call rcu_access_pointer end, cpu %d\n", smp_processor_id());
+	return ret;
 }
 
 static inline struct llist_node *barrier_to_ll(struct active_node *node)
@@ -611,9 +615,11 @@ int i915_active_acquire_preallocate_barrier(struct i915_active *ref,
 
 	GEM_BUG_ON(i915_active_is_idle(ref));
 
+	printk("fei debug: check llist_empty start, cpu %d\n", smp_processor_id());
 	/* Wait until the previous preallocation is completed */
 	while (!llist_empty(&ref->preallocated_barriers))
 		cond_resched();
+	printk("fei debug: check llist_empty end, cpu %d\n", smp_processor_id());
 
 	/*
 	 * Preallocate a node for each physical engine supporting the target
@@ -687,7 +693,7 @@ int i915_active_acquire_preallocate_barrier(struct i915_active *ref,
 
 void i915_active_acquire_barrier(struct i915_active *ref)
 {
-	struct llist_node *pos, *next;
+	struct llist_node *pos, *next, *list;
 	unsigned long flags;
 
 	GEM_BUG_ON(i915_active_is_idle(ref));
@@ -698,13 +704,20 @@ void i915_active_acquire_barrier(struct i915_active *ref)
 	 * populated by i915_request_add_active_barriers() to point to the
 	 * request that will eventually release them.
 	 */
-	llist_for_each_safe(pos, next, take_preallocated_barriers(ref)) {
+	printk("fei debug: call take_preallocated_barriers, cpu %d\n", smp_processor_id());
+	list = take_preallocated_barriers(ref);
+	printk("fei debug: call llist_for_each_safe start, cpu %d\n", smp_processor_id());
+	llist_for_each_safe(pos, next, list) {
+	        printk("fei debug: call barrier_from_ll start, cpu %d\n", smp_processor_id());
 		struct active_node *node = barrier_from_ll(pos);
+	        printk("fei debug: call barrier_from_ll end, cpu %d\n", smp_processor_id());
 		struct intel_engine_cs *engine = barrier_to_engine(node);
 		struct rb_node **p, *parent;
 
+		printk("fei debug: call spin_lock_irqsave_nested start, cpu %d\n", smp_processor_id());
 		spin_lock_irqsave_nested(&ref->tree_lock, flags,
 					 SINGLE_DEPTH_NESTING);
+		printk("fei debug: call spin_lock_irqsave_nested end, cpu %d\n", smp_processor_id());
 		parent = NULL;
 		p = &ref->tree.rb_node;
 		while (*p) {
@@ -726,6 +739,7 @@ void i915_active_acquire_barrier(struct i915_active *ref)
 		llist_add(barrier_to_ll(node), &engine->barrier_tasks);
 		intel_engine_pm_put(engine);
 	}
+	printk("fei debug: call llist_for_each_safe end, cpu %d\n", smp_processor_id());
 }
 
 static struct dma_fence **ll_to_fence_slot(struct llist_node *node)
-- 
2.30.1

