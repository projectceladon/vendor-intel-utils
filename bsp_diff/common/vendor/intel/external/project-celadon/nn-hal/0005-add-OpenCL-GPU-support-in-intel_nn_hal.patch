From c7fc93edb7f2012c30ceffa3da9299857d4fabe1 Mon Sep 17 00:00:00 2001
From: dengliqiang <liqiangx.deng@intel.com>
Date: Thu, 26 Sep 2019 21:48:01 +0800
Subject: [PATCH 5/7] add OpenCL GPU support in intel_nn_hal

OpenVINO NN HAL GPU path will call cldnn plugin and cldd to utilize OpenCL
for accelerating NN tensor operations

Tracked-On: OAM-86770
Signed-off-by: dengliqiang <liqiangx.deng@intel.com>
---
 intel_nn_hal/Driver.cpp        |  21 ++-
 intel_nn_hal/PreparedModel.cpp | 336 ++++++++++++++++++++++++++++++++-
 intel_nn_hal/PreparedModel.h   |  13 +-
 3 files changed, 365 insertions(+), 5 deletions(-)

diff --git a/intel_nn_hal/Driver.cpp b/intel_nn_hal/Driver.cpp
index d65ab24..9e8e49b 100644
--- a/intel_nn_hal/Driver.cpp
+++ b/intel_nn_hal/Driver.cpp
@@ -36,6 +36,8 @@ static sp<PreparedModel> ModelFactory(const char* name, const Model& model) {
         preparedModel = new CpuPreparedModel(model);
     else if (strcmp(name, "VPU") == 0)
         preparedModel = new VpuPreparedModel(model);
+    else if (strcmp(name, "GPU") == 0)
+        preparedModel = new GpuPreparedModel(model);
 
     return preparedModel;
 }
@@ -91,13 +93,28 @@ Return<void> Driver::getCapabilities(getCapabilities_cb cb) {
 Return<void> Driver::getCapabilities_1_1(getCapabilities_1_1_cb cb) {
     ALOGI("Entering %s", __func__);
     if (mName.compare("CPU") == 0) {
-        ALOGI("Cpu driver getCapabilities()");
+        ALOGI("CPU driver getCapabilities()");
         Capabilities capabilities = {
             .float32Performance = {.execTime = 0.9f, .powerUsage = 0.9f},
             .quantized8Performance = {.execTime = 0.9f, .powerUsage = 0.9f},
             .relaxedFloat32toFloat16Performance = {.execTime = 0.9f, .powerUsage = 0.9f}};
+
+        ALOGI("CPU MKLDNN driver Capabilities .execTime = 0.9f, .powerUsage = 0.9f");
+        cb(ErrorStatus::NONE, capabilities);
+    }
+    else if (mName.compare("GPU") == 0)
+    {
+        ALOGI("GPU driver getCapabilities()");
+        Capabilities capabilities = {
+            .float32Performance = {.execTime = 0.95f, .powerUsage = 0.85f},
+            .quantized8Performance = {.execTime = 0.95f, .powerUsage = 0.85f}};
+
+        ALOGI("GPU clDNN driver Capabilities .execTime = 0.95f, .powerUsage = 0.85f");
         cb(ErrorStatus::NONE, capabilities);
-    } else { /* mName.compare("VPU") == 0 */
+    }
+    // mName.compare("VPU") == 0
+    else
+    {
         ALOGI("Myriad driver getCapabilities()");
 
         Capabilities capabilities = {
diff --git a/intel_nn_hal/PreparedModel.cpp b/intel_nn_hal/PreparedModel.cpp
index c747522..183aeec 100644
--- a/intel_nn_hal/PreparedModel.cpp
+++ b/intel_nn_hal/PreparedModel.cpp
@@ -1194,7 +1194,7 @@ bool PreparedModel::initialize() {
     std::string graphfile("/data/local/graphfile");
     if (mModel.operations.size() > 1) {
         mNet.save(graphfile);
-        VLOG(L1, "saving to IR if oepration count > 1");
+        VLOG(L1, "saving to IR if operation count > 1");
     } else
         VLOG(L1, "NOT Saving to IR as operation count is 1,appending TBD!!");
 
@@ -1823,7 +1823,7 @@ bool PreparedModel::isOperationSupported(const Operation& operation, const Model
             }
         } break;
         default:
-            VLOG(L1, "unsupport opration %d", operation.type);
+            VLOG(L1, "unsupport operation %d", operation.type);
             return false;
     }
 #ifdef DISABLE_ALL_QUANT
@@ -4034,6 +4034,338 @@ Blob::Ptr CpuPreparedModel::GetInOutOperandAsBlob(RunTimeOperandInfo& op, const
     return nullptr;
 }
 
+Blob::Ptr GpuPreparedModel::GetConstOperandAsTensor(int operand_idx, int operation_idx)
+{
+    dumpOperand(operand_idx);
+    const auto op = mModel.operands[operand_idx];
+    uint32_t len = 0;
+
+    const uint8_t* buf = GetOperandMemory(mModel, operand_idx, len);
+
+    if (OperandType::TENSOR_FLOAT32 == op.type || OperandType::FLOAT32 == op.type)
+    {
+        vec<unsigned int> order;
+        Layout layout;
+
+        if (op.dimensions.size() == 4)
+        {
+            order = {0, 3, 1, 2};  // nhwc -> nchw
+            layout = Layout::OIHW;  // weights layout
+        }
+        else if (op.dimensions.size() == 2)
+        {
+            order = {0, 1};
+            layout = Layout::NC;
+        }
+        else
+        {
+            order = {0};  //(op.dimensions.size() < 2)
+            layout = Layout::C;
+        }
+
+        auto inputDims = toDims(op.dimensions);
+        TensorDesc td(InferenceEngine::Precision::FP32, permuteDims(inputDims, order), layout);
+
+        if (nullptr == buf)
+        {
+            InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+            blob->allocate();
+            return blob;
+        }
+        else
+        {
+            if (inputDims.size() != 4)
+            {
+                InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td, (float*)buf, len);
+                return blob;
+            } else {
+                InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+                blob->allocate();
+
+                const auto& dims_ohwi = inputDims;  // toDims(op.dimensions);
+                const size_t out_depth = dims_ohwi[0];
+                const size_t in_depth = dims_ohwi[3];
+                const size_t height = dims_ohwi[1];
+                const size_t width = dims_ohwi[2];
+                const float* inputFilter = reinterpret_cast<const float*>(buf);  // OHWI memory layout
+                size_t offset = 0;  // blob->size() == o*i*h*w and simlar to nchw memory layout
+
+                for (size_t o = 0; o < out_depth; o++)
+                {
+                    for (size_t i = 0; i < in_depth; i++)
+                    {
+                        for (size_t h = 0; h < height; h++)
+                        {
+                            for (size_t w = 0; w < width; w++)
+                            {
+                                const size_t offset_ohwi = o * height * width * in_depth +
+                                                     h * width * in_depth + w * in_depth +
+                                                     i;  // similar to NHWC memory layout
+                                blob->buffer().as<float*>()[offset++] = inputFilter[offset_ohwi];
+                            }
+                        }
+                    }
+                }
+
+                return blob;
+            }
+        }
+    }
+    else if (OperandType::TENSOR_INT32 == op.type)
+    {
+        TensorDesc td(InferenceEngine::Precision::I32, toDims(op.dimensions), Layout::ANY);
+
+        if (nullptr == buf)
+        {
+            InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+            blob->allocate();
+            return blob;
+        }
+        else
+        {
+            InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td, (float*)buf, len);
+            return blob;
+        }
+    }
+    else
+    {
+        nnAssert(false);
+    }
+
+    return nullptr;
+}
+
+Blob::Ptr GpuPreparedModel::GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf, uint32_t& len)
+{
+    if (OperandType::TENSOR_FLOAT32 == op.type || OperandType::FLOAT32 == op.type)
+    {
+        if (op.lifetime == OperandLifeTime::MODEL_INPUT)
+        {
+            vec<unsigned int> order;
+            Layout layout;
+
+            if (op.dimensions.size() == 4)
+            {
+                order = {0, 3, 1, 2};  // nhwc -> nchw
+                layout = Layout::NCHW;
+                // layout = Layout::NHWC;
+            }
+            else if (op.dimensions.size() == 2)
+            {
+                order = {0, 1};
+                layout = Layout::NC;
+            }
+            else
+            {
+                order = {0};  //(op.dimensions.size() < 2)
+                layout = Layout::C;
+            }
+
+            auto inputDims = toDims(op.dimensions);
+            TensorDesc td(InferenceEngine::Precision::FP32, permuteDims(inputDims, order), layout);
+
+            if (buf == nullptr)
+            {
+                InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+                blob->allocate();
+                return blob;
+            }
+            else
+            {
+                if (inputDims.size() != 4)
+                {
+                    InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td, (float*)buf, len);
+                    return blob;
+                }
+                else
+                {
+                    InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+                    blob->allocate();
+
+                    const auto& dims_nhwc = inputDims;  // toDims(op.dimensions);
+                    const size_t batch = dims_nhwc[0];
+                    const size_t in_depth = dims_nhwc[3];  // channels
+                    const size_t height = dims_nhwc[1];
+                    const size_t width = dims_nhwc[2];
+                    const float* input = reinterpret_cast<const float*>(buf);  // OHWI memory layout
+                    size_t offset = 0;  // blob->size() == o*i*h*w and simlar to nchw memory layout
+
+                    // convert NHWC -> NCHW
+
+                    for (size_t b = 0; b < batch; b++)
+                    {
+                        for (size_t i = 0; i < in_depth; i++)
+                        {
+                            for (size_t h = 0; h < height; h++)
+                            {
+                                for (size_t w = 0; w < width; w++)
+                                {
+                                    const size_t offset_nhwc = b * height * width * in_depth +
+                                                         h * width * in_depth + w * in_depth +
+                                                         i;  // similar to NHWC memory layout
+                                    blob->buffer().as<float*>()[offset++] = input[offset_nhwc];
+                                }
+                            }
+                        }
+                    }
+
+                    return blob;
+                }
+            }
+        }
+        else if (op.lifetime == OperandLifeTime::MODEL_OUTPUT)
+        {
+            vec<unsigned int> order;
+            Layout layout;
+
+            if (op.dimensions.size() == 4)
+            {
+                // order = {0,3,1,2};  //nhwc -> nchw
+                layout = Layout::NHWC;
+            }
+            else if (op.dimensions.size() == 2)
+            {
+                // order = {0, 1};
+                layout = Layout::NC;
+            }
+            else
+            {
+                // order = {0}; //(op.dimensions.size() < 2)
+                layout = Layout::C;
+            }
+
+            TensorDesc td(InferenceEngine::Precision::FP32, toDims(op.dimensions), layout);  // nhwc
+            if (nullptr == buf)
+            {
+                InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+                blob->allocate();
+                return blob;
+            }
+            else
+            {
+                InferenceEngine::TBlob<float>::Ptr blob = InferenceEngine::make_shared_blob<float>(td, (float*)buf, len);
+                return blob;
+            }
+        }
+    }
+    else if (OperandType::TENSOR_INT32 == op.type)
+    {
+        TensorDesc td(InferenceEngine::Precision::I32, toDims(op.dimensions), Layout::ANY);
+        return std::make_shared<InferenceEngine::TBlob<int32_t> >(td, (int32_t*)buf, len);
+    }
+    else
+    {
+        nnAssert(false);
+    }
+
+    return nullptr;
+}
+
+Blob::Ptr GpuPreparedModel::GetConstWeightsOperandAsTensor(uint32_t index)
+{
+    dumpOperand(index);
+    const auto op = mModel.operands[index];
+    uint32_t len = 0;
+    const uint8_t* buf = GetOperandMemory(mModel, index, len);
+
+    if (OperandType::TENSOR_FLOAT32 == op.type || OperandType::FLOAT32 == op.type)
+    {
+        vec<unsigned int> order;
+        Layout layout;
+
+        if (op.dimensions.size() == 4)
+        {
+            order = {3, 0, 1, 2};  // IHWO -> OIHW for depth conv
+            layout = Layout::OIHW;  // weights layout
+        }
+        else if (op.dimensions.size() == 2)
+        {
+            order = {0, 1};
+            layout = Layout::NC;
+        }
+        else
+        {
+            order = {0};  //(op.dimensions.size() < 2)
+            layout = Layout::C;
+        }
+
+        auto inputDims = toDims(op.dimensions);
+        TensorDesc td(InferenceEngine::Precision::FP32, permuteDims(inputDims, order), layout);
+
+        if (buf == nullptr)
+        {
+            InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+            blob->allocate();
+            return blob;
+        }
+        else
+        {
+            if (inputDims.size() != 4)
+            {
+                InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td, (float*)buf, len);
+                return blob;
+            }
+            else
+            {
+                InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+                blob->allocate();
+
+                const auto& dims_ohwi = inputDims;  // toDims(op.dimensions);
+                const size_t out_depth = dims_ohwi[0];
+                const size_t in_depth = dims_ohwi[3];
+                const size_t height = dims_ohwi[1];
+                const size_t width = dims_ohwi[2];
+                const float* inputFilter = reinterpret_cast<const float*>(buf);  // OHWI memory layout
+                size_t offset = 0;  // blob->size() == o*i*h*w and simlar to nchw memory layout
+
+                // convert OHWI -> OIHW
+
+                // for depth conv need reorder as OIHW since for tflite O is always 1 and IE expects
+                // reorder to [in_channels, depth_multiplier, filter_height, filter_width]
+                for (size_t i = 0; i < in_depth; i++)
+                {
+                    for (size_t o = 0; o < out_depth; o++)
+                    {
+                        for (size_t h = 0; h < height; h++)
+                        {
+                            for (size_t w = 0; w < width; w++)
+                            {
+                                size_t offset_ohwi = o * height * width * in_depth +
+                                                     h * width * in_depth + w * in_depth +
+                                                     i;  // similar to NHWC memory layout
+                                blob->buffer().as<float*>()[offset++] = inputFilter[offset_ohwi];
+                            }
+                        }
+                    }
+                }
+
+                return blob;
+            }
+        }
+    }
+    else if (op.type == OperandType::TENSOR_INT32)
+    {
+        TensorDesc td(InferenceEngine::Precision::I32, toDims(op.dimensions), Layout::ANY);
+
+        if (buf == nullptr) {
+            InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td);
+            blob->allocate();
+            return blob;
+        }
+        else
+        {
+            InferenceEngine::TBlob<float>::Ptr blob = std::make_shared<InferenceEngine::TBlob<float> >(td, (float*)buf, len);
+            return blob;
+        }
+    }
+    else
+    {
+        nnAssert(false);
+    }
+
+    return nullptr;
+}
+
 }  // namespace nnhal
 }  // namespace neuralnetworks
 }  // namespace hardware
diff --git a/intel_nn_hal/PreparedModel.h b/intel_nn_hal/PreparedModel.h
index b033c51..54c17f6 100644
--- a/intel_nn_hal/PreparedModel.h
+++ b/intel_nn_hal/PreparedModel.h
@@ -115,7 +115,7 @@ public:
 
     PreparedModel(const TargetDevice device, const Model& model)
         : mTargetDevice(device), mModel(model), mNet("nnNet"), enginePtr(nullptr), mPadreq(EXPL_PAD) {
-        if (mTargetDevice == TargetDevice::eCPU)
+        if (mTargetDevice == TargetDevice::eCPU || mTargetDevice == TargetDevice::eGPU)
             g_layer_precision = InferenceEngine::Precision::FP32;
         else if (mTargetDevice == TargetDevice::eMYRIAD)
             g_layer_precision = InferenceEngine::Precision::FP16;
@@ -208,6 +208,17 @@ public:
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index) override;
 };
 
+class GpuPreparedModel : public PreparedModel {
+public:
+    GpuPreparedModel(const Model& model) : PreparedModel(TargetDevice::eGPU, model) {}
+
+    virtual Blob::Ptr GetConstOperandAsTensor(int operand_index, int operation_idx) override;
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf,
+                                            uint32_t& len) override;
+    virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index) override;
+};
+
+
 }  // namespace nnhal
 }  // namespace neuralnetworks
 }  // namespace hardware
-- 
2.23.0

