From 47f9bcd1c2e38180ab9aab0021d0ba771b2af558 Mon Sep 17 00:00:00 2001
From: Atul Vaish <atul.vaish@intel.com>
Date: Mon, 22 Jul 2019 11:08:16 +0530
Subject: [PATCH] Enabled 1.1 and ran coding style script

Tracked-On: OAM-84202
Signed-off-by: Nagamani Chennuboina <nagamani.chennuboina@intel.com>
Signed-off-by: Atul Vaish <atul.vaish@intel.com>
Signed-off-by: Lakshmishree C <lakshmishree.c@intel.com>
---
 intel_nn_hal/.clang-format                    |   2 +-
 intel_nn_hal/Android.mk                       |  18 +-
 intel_nn_hal/Driver.cpp                       |  77 +-
 intel_nn_hal/Driver.h                         |  42 +-
 intel_nn_hal/Executor.h                       | 109 +-
 intel_nn_hal/PreparedModel.cpp                | 684 ++++++++++---
 intel_nn_hal/PreparedModel.h                  | 101 +-
 ...oid.hardware.neuralnetworks@1.1-generic.rc |   4 +
 intel_nn_hal/coding_style.txt                 |   2 +-
 intel_nn_hal/dl/helpers.hpp                   |   2 +-
 intel_nn_hal/graphAPI/IENetwork.h             | 281 ++---
 intel_nn_hal/graphAPI/IRDocument.cpp          |  25 +-
 intel_nn_hal/graphAPI/IRDocument.h            |  41 +-
 intel_nn_hal/graphAPI/IRLayer.cpp             |  16 +-
 intel_nn_hal/graphAPI/IRLayer.h               |  90 +-
 intel_nn_hal/graphAPI/IRLayers.h              | 958 ++++++++----------
 intel_nn_hal/graphTests/helpers-test.hpp      |   4 +-
 intel_nn_hal/graphTests/main.cpp              |  30 +-
 intel_nn_hal/service.cpp                      |   4 +-
 19 files changed, 1400 insertions(+), 1090 deletions(-)
 create mode 100644 intel_nn_hal/android.hardware.neuralnetworks@1.1-generic.rc

diff --git a/intel_nn_hal/.clang-format b/intel_nn_hal/.clang-format
index aada964..98a66ee 100644
--- a/intel_nn_hal/.clang-format
+++ b/intel_nn_hal/.clang-format
@@ -1,7 +1,7 @@
 ---
 Language:        Cpp
 # BasedOnStyle:  Google
-AccessModifierOffset: -1
+AccessModifierOffset: -4
 AlignAfterOpenBracket: Align
 AlignConsecutiveAssignments: false
 AlignConsecutiveDeclarations: false
diff --git a/intel_nn_hal/Android.mk b/intel_nn_hal/Android.mk
index 3a2e038..68d726e 100644
--- a/intel_nn_hal/Android.mk
+++ b/intel_nn_hal/Android.mk
@@ -3,7 +3,7 @@ LOCAL_PATH := $(call my-dir)
 ##############################################################
 include $(CLEAR_VARS)
 
-LOCAL_MODULE := android.hardware.neuralnetworks@1.0-generic-impl
+LOCAL_MODULE := android.hardware.neuralnetworks@1.1-generic-impl
 LOCAL_PROPRIETARY_MODULE := true
 LOCAL_MODULE_OWNER := intel
 #LOCAL_MULTILIB := both
@@ -11,9 +11,7 @@ LOCAL_MULTILIB := 64
 
 LOCAL_SRC_FILES := \
 	Driver.cpp \
-	PreparedModel.cpp \
-	Executor.cpp
-
+	PreparedModel.cpp
 
 LOCAL_C_INCLUDES += \
 	$(LOCAL_PATH) \
@@ -40,7 +38,7 @@ LOCAL_C_INCLUDES += \
 	frameworks/ml/nn/common/include
 
 LOCAL_CFLAGS += \
-	-std=c++11 \
+	-std=c++14 \
 	-fPIC \
 	-fPIE \
 	-Wall \
@@ -61,7 +59,6 @@ LOCAL_CFLAGS +=  -DNN_DEBUG
 #LOCAL_CFLAGS +=  -DAT_RUNTIME
 #LOCAL_CFLAGS +=  -DNNLOG
 
-
 LOCAL_SHARED_LIBRARIES := \
 	libhidlbase \
 	libhidltransport \
@@ -73,6 +70,7 @@ LOCAL_SHARED_LIBRARIES := \
 	libhidlmemory \
 	android.hardware.neuralnetworks@1.0 \
 	android.hardware.neuralnetworks@1.1 \
+	android.hardware.neuralnetworks@1.2 \
 	android.hidl.allocator@1.0 \
 	android.hidl.memory@1.0 \
 	libinference_engine
@@ -82,8 +80,8 @@ LOCAL_STATIC_LIBRARIES := libgraphAPI libpugixml libneuralnetworks_common
 include $(BUILD_SHARED_LIBRARY)
 ###############################################################
 include $(CLEAR_VARS)
-LOCAL_MODULE := android.hardware.neuralnetworks@1.0-generic-service
-LOCAL_INIT_RC := android.hardware.neuralnetworks@1.0-generic.rc
+LOCAL_MODULE := android.hardware.neuralnetworks@1.1-generic-service
+LOCAL_INIT_RC := android.hardware.neuralnetworks@1.1-generic.rc
 LOCAL_MODULE_RELATIVE_PATH := hw
 LOCAL_PROPRIETARY_MODULE := true
 LOCAL_MODULE_OWNER := intel
@@ -102,8 +100,8 @@ LOCAL_SHARED_LIBRARIES := \
 	liblog \
 	libcutils \
 	libhardware \
-	android.hardware.neuralnetworks@1.0 \
-	android.hardware.neuralnetworks@1.0-generic-impl
+	android.hardware.neuralnetworks@1.1 \
+	android.hardware.neuralnetworks@1.1-generic-impl
 
 LOCAL_MULTILIB := 64
 
diff --git a/intel_nn_hal/Driver.cpp b/intel_nn_hal/Driver.cpp
index 491a1da..d65ab24 100644
--- a/intel_nn_hal/Driver.cpp
+++ b/intel_nn_hal/Driver.cpp
@@ -17,24 +17,18 @@
 #define LOG_TAG "Driver"
 
 #include "Driver.h"
-#ifndef AT_RUNTIME
-#include "PreparedModel.h"
-#else
-#include "Executor.h"
-#endif
 #include <android-base/logging.h>
 #include <thread>
+#include "PreparedModel.h"
 #include "ValidateHal.h"
 
 namespace android {
 namespace hardware {
 namespace neuralnetworks {
-namespace V1_0 {
-namespace driver {
+namespace nnhal {
 
 using namespace android::nn;
 
-#ifndef AT_RUNTIME
 static sp<PreparedModel> ModelFactory(const char* name, const Model& model) {
     sp<PreparedModel> preparedModel = NULL;
 
@@ -46,42 +40,28 @@ static sp<PreparedModel> ModelFactory(const char* name, const Model& model) {
     return preparedModel;
 }
 
-#else
-static sp<executor::PreparedModel> ModelFactory(const char* name, const Model& model) {
-    sp<executor::PreparedModel> preparedModel = NULL;
-
-    if (strcmp(name, "CPU") == 0)
-        preparedModel = new executor::CpuPreparedModel(model);
-    else if (strcmp(name, "VPU") == 0)
-        preparedModel = new executor::VpuPreparedModel(model);
+Return<ErrorStatus> Driver::prepareModel(const V10_Model& model,
+                                         const sp<IPreparedModelCallback>& callback) {
+    ALOGI("Entering %s", __func__);
 
-    return preparedModel;
+    return ErrorStatus::NONE;
 }
 
-#endif
-
-Return<ErrorStatus> Driver::prepareModel(const Model& model,
-                                         const sp<IPreparedModelCallback>& callback) {
-    ALOGI("Driver::prepareModel");
+Return<ErrorStatus> Driver::prepareModel_1_1(const Model& model, ExecutionPreference preference,
+                                             const sp<IPreparedModelCallback>& callback) {
+    ALOGI("Entering %s", __func__);
 
     if (callback.get() == nullptr) {
         ALOGI("invalid callback passed to prepareModel");
         return ErrorStatus::INVALID_ARGUMENT;
     }
-
-    if (!validateModel(model)) {
-        ALOGI("NNERR: %s failed at line no: %d\n", __func__, __LINE__);
+    if (!validateModel(model) || !validateExecutionPreference(preference)) {
         callback->notify(ErrorStatus::INVALID_ARGUMENT, nullptr);
         return ErrorStatus::INVALID_ARGUMENT;
     }
 
     // TODO: make asynchronous later
-#ifndef AT_RUNTIME
     sp<PreparedModel> preparedModel = ModelFactory(mName.c_str(), model);
-#else
-    sp<executor::PreparedModel> preparedModel = ModelFactory(mName.c_str(), model);
-#endif
-
     if (preparedModel == NULL) {
         ALOGI("failed to create preparedmodel");
         return ErrorStatus::INVALID_ARGUMENT;
@@ -103,13 +83,19 @@ Return<DeviceStatus> Driver::getStatus() {
 }
 
 Return<void> Driver::getCapabilities(getCapabilities_cb cb) {
+    ALOGI("Entering %s", __func__);
+
+    return Void();
+}
+
+Return<void> Driver::getCapabilities_1_1(getCapabilities_1_1_cb cb) {
+    ALOGI("Entering %s", __func__);
     if (mName.compare("CPU") == 0) {
         ALOGI("Cpu driver getCapabilities()");
         Capabilities capabilities = {
             .float32Performance = {.execTime = 0.9f, .powerUsage = 0.9f},
-            .quantized8Performance = {.execTime = 0.9f, .powerUsage = 0.9f}};
-
-        ALOGI("CPU MKLDNN driver Capabilities .execTime = 0.9f, .powerUsage = 0.9f");
+            .quantized8Performance = {.execTime = 0.9f, .powerUsage = 0.9f},
+            .relaxedFloat32toFloat16Performance = {.execTime = 0.9f, .powerUsage = 0.9f}};
         cb(ErrorStatus::NONE, capabilities);
     } else { /* mName.compare("VPU") == 0 */
         ALOGI("Myriad driver getCapabilities()");
@@ -124,10 +110,18 @@ Return<void> Driver::getCapabilities(getCapabilities_cb cb) {
     return Void();
 }
 
-Return<void> Driver::getSupportedOperations(const Model& model, getSupportedOperations_cb cb) {
-    ALOGI("Driver getSupportedOperations()");
+Return<void> Driver::getSupportedOperations(const V10_Model& model, getSupportedOperations_cb cb) {
+    ALOGI("Entering %s", __func__);
+
+    return Void();
+}
+
+Return<void> Driver::getSupportedOperations_1_1(const Model& model,
+                                                getSupportedOperations_1_1_cb cb) {
+    ALOGI("Entering %s", __func__);
+
     int count = model.operations.size();
-    std::vector<bool> supported(count, false);
+    std::vector<bool> supported(count, true);
 
     if (!validateModel(model)) {
         ALOGI("NNERR: %s failed at line no: %d\n", __func__, __LINE__);
@@ -135,24 +129,15 @@ Return<void> Driver::getSupportedOperations(const Model& model, getSupportedOper
         return Void();
     }
 
-#ifndef AT_RUNTIME
     for (int i = 0; i < count; i++) {
         const auto& operation = model.operations[i];
         supported[i] = PreparedModel::isOperationSupported(operation, model);
     }
-#else
-    for (int i = 0; i < count; i++) {
-        const auto& operation = model.operations[i];
-        supported[i] = executor::PreparedModel::isOperationSupported(operation, model);
-    }
-#endif
-
     cb(ErrorStatus::NONE, supported);
     return Void();
 }
 
-}  // namespace driver
-}  // namespace V1_0
+}  // namespace nnhal
 }  // namespace neuralnetworks
 }  // namespace hardware
 }  // namespace android
diff --git a/intel_nn_hal/Driver.h b/intel_nn_hal/Driver.h
index f3c46e4..d724ac7 100644
--- a/intel_nn_hal/Driver.h
+++ b/intel_nn_hal/Driver.h
@@ -17,43 +17,59 @@
 #ifndef ANDROID_ML_NN_VPU_DRIVER_H
 #define ANDROID_ML_NN_VPU_DRIVER_H
 
-#include <android/hardware/neuralnetworks/1.0/IDevice.h>
-#include <android/hardware/neuralnetworks/1.0/IPreparedModel.h>
+#include <android/hardware/neuralnetworks/1.0/types.h>
+#include <android/hardware/neuralnetworks/1.1/IDevice.h>
+#include <android/hardware/neuralnetworks/1.1/types.h>
 #include <hardware/hardware.h>
 #include <string>
 
 namespace android {
 namespace hardware {
 namespace neuralnetworks {
-namespace V1_0 {
-namespace driver {
+namespace nnhal {
+
+using Model = ::android::hardware::neuralnetworks::V1_1::Model;
+using V10_Model = ::android::hardware::neuralnetworks::V1_0::Model;
+using Operation = ::android::hardware::neuralnetworks::V1_1::Operation;
+using V10_Operation = ::android::hardware::neuralnetworks::V1_0::Operation;
+using OperationType = ::android::hardware::neuralnetworks::V1_1::OperationType;
+using OperandType = ::android::hardware::neuralnetworks::V1_0::OperandType;
+using Capabilities = ::android::hardware::neuralnetworks::V1_1::Capabilities;
+using V10_Capabilities = ::android::hardware::neuralnetworks::V1_0::Capabilities;
+
+using namespace ::android::hardware::neuralnetworks::V1_1;
+using namespace ::android::hardware::neuralnetworks::V1_0;
 
-using ::android::hardware::neuralnetworks::V1_0::IDevice;
 // Base class used to create vpu drivers for the NN HAL.  This class
 // provides some implementation of the more common functions.
 //
 // Since these drivers simulate hardware, they must run the computations
 // on the CPU.  An actual driver would not do that.
-class Driver : public IDevice {
+class Driver : public ::android::hardware::neuralnetworks::V1_1::IDevice {
 public:
     Driver() {}
     Driver(const char* name) : mName(name) {}
 
     ~Driver() override {}
-    Return<ErrorStatus> prepareModel(const Model& model,
+    Return<void> getCapabilities(getCapabilities_cb cb) override;
+    Return<void> getCapabilities_1_1(getCapabilities_1_1_cb cb) override;
+    Return<void> getSupportedOperations(const V10_Model& model,
+                                        getSupportedOperations_cb cb) override;
+    Return<void> getSupportedOperations_1_1(const Model& model,
+                                            getSupportedOperations_1_1_cb cb) override;
+    Return<ErrorStatus> prepareModel(const V10_Model& model,
                                      const sp<IPreparedModelCallback>& callback) override;
+    Return<ErrorStatus> prepareModel_1_1(const Model& model, ExecutionPreference preference,
+                                         const sp<IPreparedModelCallback>& callback) override;
     Return<DeviceStatus> getStatus() override;
-    Return<void> getCapabilities(getCapabilities_cb _hidl_cb) override;
-    Return<void> getSupportedOperations(const Model& model, getSupportedOperations_cb cb) override;
+
 protected:
     std::string mName;
 };
 
-
-}  // namespace driver
-}  // namespace V1_0
+}  // namespace nnhal
 }  // namespace neuralnetworks
 }  // namespace hardware
 }  // namespace android
 
-#endif // ANDROID_ML_NN_VPU_DRIVER_H
+#endif  // ANDROID_ML_NN_VPU_DRIVER_H
diff --git a/intel_nn_hal/Executor.h b/intel_nn_hal/Executor.h
index ff67395..94b2cf9 100644
--- a/intel_nn_hal/Executor.h
+++ b/intel_nn_hal/Executor.h
@@ -19,11 +19,11 @@
 
 #include <android/hardware/neuralnetworks/1.0/IPreparedModel.h>
 #include <android/hidl/memory/1.0/IMemory.h>
-#include <hidlmemory/mapping.h>
 #include <hardware/hardware.h>
+#include <hidlmemory/mapping.h>
 #include <sys/mman.h>
-#include <string>
 #include <fstream>
+#include <string>
 
 #include "IENetwork.h"
 
@@ -38,9 +38,9 @@ namespace V1_0 {
 namespace driver {
 namespace executor {
 
-
-template <class T> using  vec = std::vector<T>;
-typedef uint8_t * memory;
+template <class T>
+using vec = std::vector<T>;
+typedef uint8_t* memory;
 
 // The type and dimensions of an operand.
 struct Shape {
@@ -53,9 +53,9 @@ struct Shape {
 // Information we maintain about each operand during execution that
 // may change during execution.
 struct RunTimeOperandInfo {
-    //std::string name;
-    //uint32_t opIdx;
-    //void * opIdx;
+    // std::string name;
+    // uint32_t opIdx;
+    // void * opIdx;
 
     // TODO Storing the type here is redundant, as it won't change during execution.
     OperandType type;
@@ -86,7 +86,6 @@ struct RunTimeOperandInfo {
     }
 };
 
-
 // Used to keep a pointer to each of the memory pools.
 struct RunTimePoolInfo {
     sp<IMemory> memory;
@@ -97,32 +96,27 @@ struct RunTimePoolInfo {
     bool update();
 };
 
-
 bool setRunTimePoolInfosFromHidlMemories(std::vector<RunTimePoolInfo>* poolInfos,
                                          const hidl_vec<hidl_memory>& pools);
 
-
-
 // This class is used to execute a model
 class Executor {
 public:
-    Executor()
-          :mTargetDevice(TargetDevice::eMYRIAD), mNet("nnNet"), enginePtr(nullptr) {
+    Executor() : mTargetDevice(TargetDevice::eMYRIAD), mNet("nnNet"), enginePtr(nullptr) {
         IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
     }
 
-    Executor(const TargetDevice device)
-          :mTargetDevice(device), mNet("nnNet"), enginePtr(nullptr) {
+    Executor(const TargetDevice device) : mTargetDevice(device), mNet("nnNet"), enginePtr(nullptr) {
         if (mTargetDevice == TargetDevice::eCPU)
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::FP32;
+            IRBuilder::g_layer_precision = InferenceEngine::Precision::FP32;
         else if (mTargetDevice == TargetDevice::eMYRIAD)
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
+            IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
         else
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
+            IRBuilder::g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
     }
 
-    ~Executor() {deinitialize();}
-    //bool initialize();
+    ~Executor() { deinitialize(); }
+    // bool initialize();
     // Executes the model. The results will be stored at the locations
     // specified in the constructor.
     // The model must outlive the executor.  We prevent it from being modified
@@ -134,7 +128,7 @@ public:
 protected:
     void deinitialize();
     bool initializeRunTimeInfo(const std::vector<RunTimePoolInfo>& modelPoolInfos,
-                                            const std::vector<RunTimePoolInfo>& requestPoolInfos);
+                               const std::vector<RunTimePoolInfo>& requestPoolInfos);
 
     bool executeOperation(const Operation& operation);
 
@@ -148,7 +142,7 @@ protected:
     bool operationLRN(const Operation& operation);
     bool operationMaxPool2D(const Operation& operation);
     bool operationLogisticSigmoid(const Operation& operation);
-    //bool operationLSTM(const Operation& operation);
+    // bool operationLSTM(const Operation& operation);
     bool operationMUL(const Operation& operation);
     bool operationRELU(const Operation& operation);
     bool operationRELU1(const Operation& operation);
@@ -160,77 +154,73 @@ protected:
     void initializeInput();
     void finalizeOutput(/*RunTimeOperandInfo* output*/);
 
-    OutputPort handleFusion(const OutputPort &out, int32_t fusedOp);
-    template<typename T>
-    T GetConstFromBuffer(const uint8_t *buf, uint32_t len);
-    template<typename T>
-    std::vector<T> GetConstVecFromBuffer(const uint8_t *buf, uint32_t len);
-    const uint8_t *GetOperandMemory(const Model *model, uint32_t index, uint32_t &len_out);
+    OutputPort handleFusion(const OutputPort& out, int32_t fusedOp);
+    template <typename T>
+    T GetConstFromBuffer(const uint8_t* buf, uint32_t len);
+    template <typename T>
+    std::vector<T> GetConstVecFromBuffer(const uint8_t* buf, uint32_t len);
+    const uint8_t* GetOperandMemory(const Model* model, uint32_t index, uint32_t& len_out);
     template <typename T>
-    T ParseOperationInput(const Model *model, const Operation& operation, uint32_t index);
+    T ParseOperationInput(const Model* model, const Operation& operation, uint32_t index);
     template <typename T>
-    T GetConstOperand(const Model *model, uint32_t index);
+    T GetConstOperand(const Model* model, uint32_t index);
     template <typename T>
-    std::vector<T> GetConstVecOperand(const Model *model, uint32_t index);
+    std::vector<T> GetConstVecOperand(const Model* model, uint32_t index);
     virtual Blob::Ptr GetConstOperandAsTensor(uint32_t index);
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index);
-    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf, uint32_t& len);
-    void SetOperandMemory(const Model *model, uint32_t index, uint32_t &len_out, const uint8_t *buf);
-    void SetOperandFromTensor(uint8_t* buf, uint32_t &length, Blob::Ptr infOutput);
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
+                                            uint32_t& len);
+    void SetOperandMemory(const Model* model, uint32_t index, uint32_t& len_out,
+                          const uint8_t* buf);
+    void SetOperandFromTensor(uint8_t* buf, uint32_t& length, Blob::Ptr infOutput);
     bool isConst(int index);
     OutputPort getPort(int index);
 
     TargetDevice mTargetDevice;
     std::vector<RunTimeOperandInfo> mOperands;
     IRDocument mNet;
-    std::vector<OutputPort> mPorts;  //typedef std::shared_ptr<Data> DataPtr;
+    std::vector<OutputPort> mPorts;  // typedef std::shared_ptr<Data> DataPtr;
     ExecuteNetwork* enginePtr;
 
     // The model and the request that we'll execute. Only valid while run()
     // is being executed.
     const Model* mModel = nullptr;
     const Request* mRequest = nullptr;
-
-
 };
 
 class VpuExecutor : public Executor {
 public:
-    VpuExecutor()
-          :Executor(TargetDevice::eMYRIAD) {
-    }
+    VpuExecutor() : Executor(TargetDevice::eMYRIAD) {}
 
     virtual Blob::Ptr GetConstOperandAsTensor(uint32_t index) override;
-    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf, uint32_t& len) override;
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
+                                            uint32_t& len) override;
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index) override;
 };
 
 class CpuExecutor : public Executor {
 public:
-    CpuExecutor()
-          :Executor(TargetDevice::eCPU) {
-    }
+    CpuExecutor() : Executor(TargetDevice::eCPU) {}
 
     virtual Blob::Ptr GetConstOperandAsTensor(uint32_t index) override;
-    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf, uint32_t& len) override;
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
+                                            uint32_t& len) override;
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index) override;
 };
 
-
-
 class PreparedModel : public IPreparedModel {
 public:
     PreparedModel(const Model& model) : mModel(model), mTargetDevice(TargetDevice::eMYRIAD) {
         IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
     }
     PreparedModel(const TargetDevice device, const Model& model)
-          :mTargetDevice(device), mModel(model) {
+        : mTargetDevice(device), mModel(model) {
         if (mTargetDevice == TargetDevice::eCPU)
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::FP32;
+            IRBuilder::g_layer_precision = InferenceEngine::Precision::FP32;
         else if (mTargetDevice == TargetDevice::eMYRIAD)
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
+            IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
         else
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
+            IRBuilder::g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
     }
     ~PreparedModel() override {}
     bool initialize();
@@ -244,30 +234,23 @@ private:
     Model mModel;
     std::vector<RunTimePoolInfo> mPoolInfos;
     TargetDevice mTargetDevice;
-
 };
 
 class VpuPreparedModel : public PreparedModel {
 public:
-    VpuPreparedModel(const Model& model)
-          :PreparedModel(TargetDevice::eMYRIAD, model) {
-    }
-
+    VpuPreparedModel(const Model& model) : PreparedModel(TargetDevice::eMYRIAD, model) {}
 };
 
 class CpuPreparedModel : public PreparedModel {
 public:
-    CpuPreparedModel(const Model& model)
-          :PreparedModel(TargetDevice::eCPU, model) {
-    }
-
+    CpuPreparedModel(const Model& model) : PreparedModel(TargetDevice::eCPU, model) {}
 };
 
-}
+}  // namespace executor
 }  // namespace driver
 }  // namespace V1_0
 }  // namespace neuralnetworks
 }  // namespace hardware
 }  // namespace android
 
-#endif // ANDROID_ML_NN_PREPAREDMODEL_H
+#endif  // ANDROID_ML_NN_PREPAREDMODEL_H
diff --git a/intel_nn_hal/PreparedModel.cpp b/intel_nn_hal/PreparedModel.cpp
index 10bd1ac..b484855 100644
--- a/intel_nn_hal/PreparedModel.cpp
+++ b/intel_nn_hal/PreparedModel.cpp
@@ -75,12 +75,19 @@ unsigned int debugMask = ((1 << (L1 + 1)) - 1);
         ALOGI("---------------------------------------------");     \
     } while (0)
 
+#define dumpOperationParam(operation)             \
+    do {                                          \
+        ALOGI("dumping operation-params");        \
+        ALOGI("%s", toString(operation).c_str()); \
+    } while (0)
+
 #else
 #define VLOG(...)
 #define VLOGDIMS(l, d, header)
 #define dumpOperand(...)
 #define dumpOperation(operation)
 #define dumpOperationSupport(operation, support)
+#define dumpOperationParam(operation)
 #endif
 
 #define WRONG_DIM (-1)
@@ -94,11 +101,86 @@ unsigned int debugMask = ((1 << (L1 + 1)) - 1);
         }                                                                                      \
     } while (0)
 
+#define EXPL_PAD_PARAMS_CONV 10
+#define IMPL_PAD_PARAMS_CONV 7
+#define EXPL_PAD_PARAMS_DW_CONV 11
+#define IMPL_PAD_PARAMS_DW_CONV 8
+#define SOFTMAX_INPUT_PARAMS 2
+#define EXPL_PAD 1
+#define IMPL_PAD 2
+#define NHWC_DIM_NUM 4
+#define NHWC_CH_IDX 3
+#define NHWC_HT_IDX 1
+#define NHWC_WD_IDX 2
+// operand index as from  1.1/type.hal
+#define OP_INPUT_IDX_CONV 0
+#define OP_FILTER_IDX_CONV 1
+#define OP_BIAS_IDX_CONV 2
+#define OP_PADSCHEME_IDX_CONV 3
+#define OP_PADL_IDX_CONV 3
+#define OP_PADR_IDX_CONV 4
+#define OP_PADH_IDX_CONV 5
+#define OP_PADW_IDX_CONV 6
+#define OP_STRD_WD_IDX_EXPL_CONV 7
+#define OP_STRD_HT_IDX_EXPL_CONV 8
+#define OP_STRD_WD_IDX_IMPL_CONV 4
+#define OP_STRD_HT_IDX_IMPL_CONV 5
+#define OP_ACTV_FUNC_IDX_IMPL_CONV 6
+#define OP_ACTV_FUNC_IDX_EXPL_CONV 9
+#define OP_ACTV_FUNC_IDX_IMPL_DW_CONV 7
+#define OP_ACTV_FUNC_IDX_EXPL_DW_CONV 10
+#define OP_DW_CONV_DPM_IMPL 6  // depth multiplier
+#define OP_DW_CONV_DPM_EXPL 9
+#define OP_ADD_OPR1_IDX 0
+#define OP_ADD_OPR1_IDX 1
+
+// average_pooling_2d as in type.hal
+#define EXPL_PAD_PARAMS_POOL 10
+#define IMPL_PAD_PARAMS_POOL 7
+#define OP_INPUT_IDX_POOL 0
+#define OP_PADL_IDX_POOL 1
+#define OP_PADR_IDX_POOL 2
+#define OP_PADH_IDX_POOL 3
+#define OP_PADW_IDX_POOL 4
+#define OP_STRD_WD_IDX_EXPL_POOL 5
+#define OP_STRD_HT_IDX_EXPL_POOL 6
+#define OP_FLT_WD_IDX_EXPL_POOL 7
+#define OP_FLT_HT_IDX_EXPL_POOL 8
+#define OP_ACTV_FUNC_IDX_EXPL_POOL 9
+
+#define OP_PADSCHEME_IDX_POOL 1
+#define OP_STRD_WD_IDX_IMPL_POOL 2
+#define OP_STRD_HT_IDX_IMPL_POOL 3
+#define OP_FLT_WD_IDX_IMPL_POOL 4
+#define OP_FLT_HT_IDX_IMPL_POOL 5
+#define OP_ACTV_FUNC_IDX_IMPL_POOL 6
+
+// fully_connected as in type.hal
+#define OP_INPUT_IDX_FC 0
+#define OP_WGHT_IDX_FC 1
+#define OP_BIAS_IDX_FC 2
+#define OP_ACTV_IDX_FC 3
+#define FC_INPUT_PARAMS 4
+
+// ADD operation
+#define ADD_INPUT_PARAMS 3
+#define OP_INPUT0_IDX_ADD 0
+#define OP_INPUT1_IDX_ADD 1
+#define OP_ACTV_IDX_ADD 2
+
+#define CHECK_OPERAND_2D(params, idx_x, idx_y)                                                 \
+    do {                                                                                       \
+        VLOG(L1, "As found in %s", __func__);                                                  \
+        if (params.x < 0 || params.y < 0) {                                                    \
+            VLOG(L1, "Invalid Point2D Operands at index [%d ,%d] , aborting!!", idx_x, idx_y); \
+            return false;                                                                      \
+        }                                                                                      \
+    } while (0)
+
 namespace android {
 namespace hardware {
 namespace neuralnetworks {
-namespace V1_0 {
-namespace driver {
+namespace nnhal {
 
 using namespace android::nn;
 
@@ -151,7 +233,7 @@ int32_t computeOutSize(int32_t imageSize, int32_t filterSize, int32_t stride, in
     return (imageSize - filterSize + stride + paddingHead + paddingTail) / stride;
 }
 
-static inline size_t sizeOf(const TensorDims& dims) {
+inline size_t sizeOf(const TensorDims& dims) {
     size_t ret = dims[0];
     for (int i = 1; i < dims.size(); ++i) ret *= dims[i];
     return ret;
@@ -836,7 +918,9 @@ std::vector<T> PreparedModel::GetConstVecOperand(const Model& model, uint32_t in
 
 IRBlob::Ptr PreparedModel::GetConstWeightsOperandAsTensor(uint32_t index) { return nullptr; }
 
-IRBlob::Ptr PreparedModel::GetConstOperandAsTensor(uint32_t index) { return nullptr; }
+IRBlob::Ptr PreparedModel::GetConstOperandAsTensor(int operand_idx, int operation_idx) {
+    return nullptr;
+}
 
 Blob::Ptr PreparedModel::GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
                                                uint32_t& len) {
@@ -930,6 +1014,10 @@ void PreparedModel::SetOperandMemory(const Model& model, uint32_t index, uint32_
 bool PreparedModel::initializeRunTimeOperandInfo() {
     // initialize runtime operand info from model.
     const size_t count = mModel.operands.size();
+    if (!count) {
+        VLOG(L1, "NNERR:Operand Count is 0");
+        return false;
+    }
     mOperands.resize(count);
     mPorts.resize(count);
     // TensorDims dims;
@@ -946,7 +1034,6 @@ bool PreparedModel::initializeRunTimeOperandInfo() {
         }
 
         to.scale = from.scale;
-        nnAssert(from.zeroPoint == 0);
         switch (from.type) {
             case OperandType::TENSOR_FLOAT32:
             case OperandType::FLOAT32:
@@ -961,18 +1048,13 @@ bool PreparedModel::initializeRunTimeOperandInfo() {
             case OperandType::UINT32:
                 nnAssert(to.scale == 0);
             case OperandType::TENSOR_INT32:
-                to.type = OperandType::TENSOR_INT32;
-                // port->setPrecision(InferenceEngine::Precision::I32);
-                VLOG(L1, "OperandType::TENSOR_INT32 and operand scale value = %.1f", to.scale);
+                to.type = from.type;
                 break;
             case OperandType::TENSOR_QUANT8_ASYMM:
                 ALOGE("OperandType::TENSOR_QUANT8_ASYMM is not supported");
-                nnAssert(to.scale != 0);
-                // to.type = VpuDataType::U8;
                 break;
             default:
                 ALOGE("wrong operand type %d", from.type);
-                ;
                 return false;
         }
 
@@ -1102,7 +1184,8 @@ bool PreparedModel::initialize() {
     }
 
     initializeInput();
-    finalizeOutput();
+    success = finalizeOutput();
+    if (success == false) return success;
 
     // initialize IE operation input/output ports
     //    convertModel(mNet);
@@ -1111,8 +1194,14 @@ bool PreparedModel::initialize() {
     mNet.buildNetwork();
     std::fstream dot;
     std::string graphfile("/data/local/graphfile");
+    if (mModel.operations.size() > 1) {
+        mNet.save(graphfile);
+        VLOG(L1, "saving to IR if oepration count > 1");
+    } else
+        VLOG(L1, "NOT Saving to IR as operation count is 1,appending TBD!!");
+
     dot.open("/data/local/graph.dot", std::ios::out);
-    mNet.save(graphfile);
+
     mNet.crateDotFile(dot);
     dot.close();
 
@@ -1371,85 +1460,218 @@ bool PreparedModel::isOperationSupported(const Operation& operation, const Model
 
 #define VLOG_CHECKFAIL(fail) VLOG(L1, "Check failed: %s", fail)
 
-#ifdef DISABLE_ALL_QUANT
-    for (auto i : operation.inputs) {
-        const auto input = model.operands[i];
-        if (input.type == OperandType::TENSOR_QUANT8_ASYMM) {
-            VLOG_CHECKFAIL("input quant");
-            return false;
-        }
-    }
-    for (auto i : operation.outputs) {
-        const auto output = model.operands[i];
-        if (output.type == OperandType::TENSOR_QUANT8_ASYMM) {
-            VLOG_CHECKFAIL("output quant");
-            return false;
-        }
-    }
-#else
-    for (auto i : operation.inputs) {
-        const auto input = model.operands[i];
-        if (input.type == OperandType::TENSOR_QUANT8_ASYMM && input.zeroPoint != 0) {
-            VLOG_CHECKFAIL("input quant");
-            return false;
-        }
-    }
-    for (auto i : operation.outputs) {
-        const auto output = model.operands[i];
-        if (output.type == OperandType::TENSOR_QUANT8_ASYMM && output.zeroPoint != 0) {
-            VLOG_CHECKFAIL("output quant");
-            return false;
-        }
-    }
-#endif
-
-    const auto input0 = model.operands[operation.inputs[0]];
-    auto activationPass = [&model](const Operand& input) -> bool {
-        const FusedActivationFunc activation =
-            getOperandConstVal<FusedActivationFunc>(model, input);
-        /*
-        if (activation == FusedActivationFunc::RELU1) {
-            VLOG_CHECKFAIL("relu1 used");
-            return false;
-        }
-        */
-        return true;
-    };
-
-    const auto& inputn = model.operands[operation.inputs[operation.inputs.size() - 1]];
-
     switch (operation.type) {
         case OperationType::CONV_2D: {
-            const auto& input1 = model.operands[operation.inputs[1]];
+            int oper_size = operation.inputs.size();
+            const auto& input0 = model.operands[operation.inputs[OP_INPUT_IDX_CONV]];
+            const auto& input1 = model.operands[operation.inputs[OP_FILTER_IDX_CONV]];
+            const auto& input2 = model.operands[operation.inputs[OP_BIAS_IDX_CONV]];
+
+            VLOG(L1, "Validating CONV2D params");
             // filter in == channel
-            if (input0.dimensions[3] != input1.dimensions[3]) {
-                VLOG_CHECKFAIL("filter in not equals channel");
+            // Check Input/Filter  Operand type
+
+            if (oper_size == EXPL_PAD_PARAMS_CONV) {
+                VLOG(L1, "NNERR: Explicit padding not supported!!");
+                return false;
+            }
+
+            if (input0.type != OperandType::TENSOR_FLOAT32 ||
+                input1.type != OperandType::TENSOR_FLOAT32 ||
+                input2.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR: input0/input1/input2 invalid operand types");
+                return false;
+            }
+
+            if (input0.lifetime == input1.lifetime) {
+                VLOG(L1,
+                     "NNERR: Filter (index %d) as model_input (index %d) not "
+                     "supported,aborting!!",
+                     operation.inputs[OP_FILTER_IDX_CONV], operation.inputs[OP_INPUT_IDX_CONV]);
+                return false;
+            }
+
+            // Check Input Dimension size
+            if (input0.dimensions.size() != NHWC_DIM_NUM ||
+                input1.dimensions.size() != NHWC_DIM_NUM) {
+                VLOG(L1,
+                     "NNERR: input-0 dim-size %d  or input1 dim-size %d "
+                     "invalid,aborting!!",
+                     input0.dimensions.size(), input1.dimensions.size());
+                return false;
+            }
+
+            // Check Channel parameter for Input and filter/kernel
+            if (input0.dimensions[NHWC_CH_IDX] != input1.dimensions[NHWC_CH_IDX]) {
+                VLOG(L1,
+                     "NNERR: input-0 ch-size %d  and input-1 ch-size %d not "
+                     "equal,aborting!!",
+                     input0.dimensions.size(), input1.dimensions.size());
+                return false;
+            }
+
+            if (input1.dimensions[NHWC_HT_IDX] != input1.dimensions[NHWC_WD_IDX]) {
+                VLOG(L1, "NNERR: non-square Filter size(H:%d,W:%d) not supported,warning!!",
+                     input1.dimensions[NHWC_HT_IDX], input1.dimensions[NHWC_WD_IDX]);
+                return false;
+            }
+
+            // Check all other Input operand types for implicit/explicit Padding
+
+            if (oper_size == IMPL_PAD_PARAMS_CONV) {
+                const auto& input3 = model.operands[operation.inputs[OP_PADSCHEME_IDX_CONV]];
+                const auto& input4 = model.operands[operation.inputs[OP_STRD_WD_IDX_IMPL_CONV]];
+                const auto& input5 = model.operands[operation.inputs[OP_STRD_HT_IDX_IMPL_CONV]];
+                const auto& input6 = model.operands[operation.inputs[OP_ACTV_FUNC_IDX_IMPL_CONV]];
+
+                if (input3.type != OperandType::INT32 || input4.type != OperandType::INT32 ||
+                    input5.type != OperandType::INT32 || input6.type != OperandType::INT32) {
+                    VLOG(L1, "NNERR: inputs 3-6 invalid operand types");
+                    return false;
+                }
+            } else if (oper_size == EXPL_PAD_PARAMS_CONV) {
+                const auto& input3 = model.operands[operation.inputs[OP_PADL_IDX_CONV]];
+                const auto& input4 = model.operands[operation.inputs[OP_PADR_IDX_CONV]];
+                const auto& input5 = model.operands[operation.inputs[OP_PADH_IDX_CONV]];
+                const auto& input6 = model.operands[operation.inputs[OP_PADW_IDX_CONV]];
+                const auto& input7 = model.operands[operation.inputs[OP_STRD_WD_IDX_EXPL_CONV]];
+                const auto& input8 = model.operands[operation.inputs[OP_STRD_HT_IDX_EXPL_CONV]];
+                const auto& input9 = model.operands[operation.inputs[OP_ACTV_FUNC_IDX_EXPL_CONV]];
+
+                if (input3.type != OperandType::INT32 || input4.type != OperandType::INT32 ||
+                    input5.type != OperandType::INT32 || input6.type != OperandType::INT32 ||
+                    input7.type != OperandType::INT32 || input8.type != OperandType::INT32 ||
+                    input9.type != OperandType::INT32) {
+                    VLOG(L1, "NNERR:inputs 3-9 invalid operand types");
+                    return false;
+                }
+            }
+
+            const auto& output = model.operands[operation.outputs[0]];
+
+            if (output.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR:output operand types invalid,aborting!!");
                 return false;
             }
             break;
-            // continue to check actication.
+            // continue to check activation.
         }
 
         case OperationType::DEPTHWISE_CONV_2D: {
-            const auto& input1 = model.operands[operation.inputs[1]];
-            // channels_out must be channels * depth_mul
-            if ((input1.dimensions[3] % input0.dimensions[3]) != 0) {
-                VLOG_CHECKFAIL("dims not in group");
+            VLOG(L1, "Validating DEPTHWISE_CONV_2D params");
+            const auto& input0 = model.operands[operation.inputs[OP_INPUT_IDX_CONV]];
+            const auto& input1 = model.operands[operation.inputs[OP_FILTER_IDX_CONV]];
+            // depth_out = depth_in * depth_multiplier,input1 is depth_out and input0 is
+            // depth_in
+
+            const auto& input2 = model.operands[operation.inputs[OP_BIAS_IDX_CONV]];
+
+            int oper_size = operation.inputs.size();
+
+            if (oper_size == EXPL_PAD_PARAMS_DW_CONV) {
+                VLOG(L1, "NNERR: Explicit padding not supported!!");
+                return false;
+            }
+            // Check Input/Filter  Operand type
+            if (input0.type != OperandType::TENSOR_FLOAT32 ||
+                input1.type != OperandType::TENSOR_FLOAT32 ||
+                input2.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR: input 0-2 invalid operand types");
+                return false;
+            }
+
+            // Check Input Dimension size
+            if (input0.dimensions.size() != NHWC_DIM_NUM ||
+                input1.dimensions.size() != NHWC_DIM_NUM) {
+                VLOG(L1,
+                     "NNERR: input-0 dim-size %d  or input1 dim-size %d "
+                     "invalid,aborting!!",
+                     input0.dimensions.size(), input1.dimensions.size());
+                return false;
+            }
+
+            if (input0.lifetime == input1.lifetime) {
+                VLOG(L1,
+                     "NNERR: Filter (index %d) as model_input (index %d) not "
+                     "supported,aborting!!",
+                     operation.inputs[OP_FILTER_IDX_CONV], operation.inputs[OP_INPUT_IDX_CONV]);
+                return false;
+            }
+
+            if ((input1.dimensions[NHWC_CH_IDX] % input0.dimensions[NHWC_CH_IDX]) != 0) {
+                VLOG_CHECKFAIL(
+                    "NNERR:input/filter invalid depth leads to non-integer "
+                    "Depth Multiper");
                 return false;
             }
-            if (activationPass(inputn) == false) {
+
+            // Check all other Input operand types for implicit/explicit Padding
+
+            if (oper_size == IMPL_PAD_PARAMS_DW_CONV) {
+                const auto& input3 = model.operands[operation.inputs[OP_PADSCHEME_IDX_CONV]];
+                const auto& input4 = model.operands[operation.inputs[OP_STRD_WD_IDX_IMPL_CONV]];
+                const auto& input5 = model.operands[operation.inputs[OP_STRD_HT_IDX_IMPL_CONV]];
+                const auto& input6 = model.operands[operation.inputs[OP_DW_CONV_DPM_IMPL]];
+                const auto& input7 =
+                    model.operands[operation.inputs[OP_ACTV_FUNC_IDX_IMPL_DW_CONV]];
+
+                if (input3.type != OperandType::INT32 || input4.type != OperandType::INT32 ||
+                    input5.type != OperandType::INT32 || input6.type != OperandType::INT32 ||
+                    input7.type != OperandType::INT32) {
+                    VLOG(L1, "NNERR: inputs 3-7 invalid operand types");
+                    return false;
+                }
+            } else if (oper_size == EXPL_PAD_PARAMS_DW_CONV) {
+                const auto& input3 = model.operands[operation.inputs[OP_PADL_IDX_CONV]];
+                const auto& input4 = model.operands[operation.inputs[OP_PADR_IDX_CONV]];
+                const auto& input5 = model.operands[operation.inputs[OP_PADH_IDX_CONV]];
+                const auto& input6 = model.operands[operation.inputs[OP_PADW_IDX_CONV]];
+                const auto& input7 = model.operands[operation.inputs[OP_STRD_WD_IDX_EXPL_CONV]];
+                const auto& input8 = model.operands[operation.inputs[OP_STRD_HT_IDX_EXPL_CONV]];
+                const auto& input9 = model.operands[operation.inputs[OP_DW_CONV_DPM_EXPL]];
+                const auto& input10 =
+                    model.operands[operation.inputs[OP_ACTV_FUNC_IDX_EXPL_DW_CONV]];
+
+                if (input3.type != OperandType::INT32 || input4.type != OperandType::INT32 ||
+                    input5.type != OperandType::INT32 || input6.type != OperandType::INT32 ||
+                    input7.type != OperandType::INT32 || input8.type != OperandType::INT32 ||
+                    input9.type != OperandType::INT32 || input10.type != OperandType::INT32) {
+                    VLOG(L1, "NNERR:inputs 3-10 invalid operand types");
+                    return false;
+                }
+            }
+
+            const auto& output = model.operands[operation.outputs[0]];
+
+            if (output.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR:output operand types invalid,aborting!!");
                 return false;
             }
             break;
         }
 
         case OperationType::SOFTMAX: {
+            VLOG(L1, "Validating SOFTMAX operation params");
+            const auto& input0 = model.operands[operation.inputs[OP_INPUT_IDX_CONV]];
             const auto& input1 = model.operands[operation.inputs[1]];
             float beta = getOperandConstVal<float>(model, input1);
+
+            const auto& output = model.operands[operation.outputs[0]];
+
+            // Check Input/Filter  Operand type
+            if (input0.type != OperandType::TENSOR_FLOAT32 || input1.type != OperandType::FLOAT32) {
+                VLOG(L1, "NNERR: input0/input1 invalid operand types");
+                return false;
+            }
+
+            if (output.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR:output operand types invalid,aborting!!");
+                return false;
+            }
             // beta need = 1.0f
             // if (beta != 1.0f) {
-            if (beta <= 0.0f) {
-                VLOG_CHECKFAIL("beta must be positive for softmax");
+            if (beta != 1) {
+                VLOG_CHECKFAIL("NNERR:beta equal to 1 only supported");
                 return false;
             }
 
@@ -1457,42 +1679,181 @@ bool PreparedModel::isOperationSupported(const Operation& operation, const Model
         }
 
         case OperationType::AVERAGE_POOL_2D:
-        case OperationType::MAX_POOL_2D:
+        case OperationType::MAX_POOL_2D: {
+            int oper_size = operation.inputs.size();
+            const auto& input0 = model.operands[operation.inputs[OP_INPUT_IDX_POOL]];
+            VLOG(L1, "Validating AVG_POOL_2D params");
+
+            if (input0.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR: input0 invalid operand types");
+                return false;
+            }
+
+            // Check Input Dimension size
+            if (input0.dimensions.size() != NHWC_DIM_NUM) {
+                VLOG(L1, "NNERR: input-0 dim-size %d invalid,aborting!!", input0.dimensions.size());
+                return false;
+            }
+
+            // Check all other Input operand types for implicit/explicit Padding
+
+            if (oper_size == IMPL_PAD_PARAMS_POOL) {
+                const auto& input1 = model.operands[operation.inputs[OP_PADSCHEME_IDX_POOL]];
+                const auto& input2 = model.operands[operation.inputs[OP_STRD_WD_IDX_IMPL_POOL]];
+                const auto& input3 = model.operands[operation.inputs[OP_STRD_HT_IDX_IMPL_POOL]];
+                const auto& input4 = model.operands[operation.inputs[OP_FLT_WD_IDX_IMPL_POOL]];
+                const auto& input5 = model.operands[operation.inputs[OP_FLT_HT_IDX_IMPL_POOL]];
+                const auto& input6 = model.operands[operation.inputs[OP_ACTV_FUNC_IDX_IMPL_POOL]];
+
+                if (input1.type != OperandType::INT32 || input2.type != OperandType::INT32 ||
+                    input3.type != OperandType::INT32 || input4.type != OperandType::INT32 ||
+                    input5.type != OperandType::INT32 || input6.type != OperandType::INT32) {
+                    VLOG(L1, "NNERR: inputs 1-6 invalid operand types");
+                    return false;
+                }
+            } else if (oper_size == EXPL_PAD_PARAMS_POOL) {
+                const auto& input1 = model.operands[operation.inputs[OP_PADL_IDX_POOL]];
+                const auto& input2 = model.operands[operation.inputs[OP_PADR_IDX_POOL]];
+                const auto& input3 = model.operands[operation.inputs[OP_PADH_IDX_POOL]];
+                const auto& input4 = model.operands[operation.inputs[OP_PADW_IDX_POOL]];
+                const auto& input5 = model.operands[operation.inputs[OP_STRD_WD_IDX_EXPL_POOL]];
+                const auto& input6 = model.operands[operation.inputs[OP_STRD_HT_IDX_EXPL_POOL]];
+                const auto& input7 = model.operands[operation.inputs[OP_FLT_WD_IDX_EXPL_POOL]];
+                const auto& input8 = model.operands[operation.inputs[OP_FLT_HT_IDX_EXPL_POOL]];
+                const auto& input9 = model.operands[operation.inputs[OP_ACTV_FUNC_IDX_EXPL_POOL]];
+
+                if (input1.type != OperandType::INT32 || input2.type != OperandType::INT32 ||
+                    input3.type != OperandType::INT32 || input4.type != OperandType::INT32 ||
+                    input5.type != OperandType::INT32 || input6.type != OperandType::INT32 ||
+                    input7.type != OperandType::INT32 || input8.type != OperandType::INT32 ||
+                    input9.type != OperandType::INT32) {
+                    VLOG(L1, "NNERR:inputs 1-9 as invalid operand types");
+                    return false;
+                }
+            }
+
+            const auto& output = model.operands[operation.outputs[0]];
+
+            if (output.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR:output operand types invalid,aborting!!");
+                return false;
+            }
+        } break;
         case OperationType::FULLY_CONNECTED: {
-            if (activationPass(inputn) == false) {
+            const auto& input0 = model.operands[operation.inputs[OP_INPUT_IDX_FC]];
+
+            const auto& input1 = model.operands[operation.inputs[OP_WGHT_IDX_FC]];
+            const auto& input2 = model.operands[operation.inputs[OP_BIAS_IDX_FC]];
+
+            if (input0.type != OperandType::TENSOR_FLOAT32 ||
+                input1.type != OperandType::TENSOR_FLOAT32 ||
+                input2.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR: input0/input1/input2 invalid operand types");
                 return false;
             }
-            break;
-        }
 
+            if (input0.lifetime == input1.lifetime) {
+                VLOG(L1, "NNERR: Filter (index %d) as model_input not supported,aborting!!",
+                     operation.inputs[OP_FILTER_IDX_CONV]);
+                return false;
+            }
+
+            if (input0.dimensions.size() < 2 || input1.dimensions.size() < 2 ||
+                input2.dimensions.size() < 1) {
+                VLOG(L1, "NNERR: input 0-2 dimensions size invalid, aborting!!");
+                return false;
+            }
+            if (input0.dimensions[1] != input1.dimensions[1]) {
+                VLOG(L1,
+                     "NNERR: input0 and input1(weight) with unequal input-size "
+                     "value, aborting!!");
+                return false;
+            }
+
+            const auto& input3 = model.operands[operation.inputs[OP_ACTV_IDX_FC]];
+
+            if (input3.type != OperandType::INT32) {
+                VLOG(L1, "NNERR: input3  invalid operand types");
+                return false;
+            }
+
+            const auto& output = model.operands[operation.outputs[0]];
+
+            if (output.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR:invalid output operand types for FC ,aborting!!");
+                return false;
+            }
+        } break;
         case OperationType::RELU:
         case OperationType::RELU1:
         case OperationType::RELU6:
-            break;
-        case OperationType::LOGISTIC:
+        case OperationType::LOGISTIC: {
+            const auto& input0 = model.operands[operation.inputs[0]];
+            const auto& output = model.operands[operation.outputs[0]];
+
+            if (input0.dimensions.size() > 4 || input0.type != OperandType::TENSOR_FLOAT32 ||
+                operation.outputs.size() > 1 || operation.inputs.size() > 1 ||
+                output.type != OperandType::TENSOR_FLOAT32) {
+                VLOG(L1, "NNERR: input/output  params invalid for Relu/Logit, aborting!!");
+                return false;
+            }
+
+            if (input0.dimensions[0] > 1) {
+                VLOG(L1, "NNERR:batch size more than 1 not supported for relu/logit");
+                return false;
+            }
+        } break;
+
         case OperationType::TANH:
-        case OperationType::LOCAL_RESPONSE_NORMALIZATION:
-        case OperationType::CONCATENATION:
-        case OperationType::L2_NORMALIZATION:
-        case OperationType::RESHAPE:
             break;
 
         case OperationType::ADD: {
-            const auto& input1 = model.operands[operation.inputs[1]];
+            const auto& input0 = model.operands[operation.inputs[OP_INPUT0_IDX_ADD]];
+            const auto& input1 = model.operands[operation.inputs[OP_INPUT1_IDX_ADD]];
+            const auto& input2 = model.operands[operation.inputs[OP_ACTV_IDX_ADD]];
             if (input0.dimensions != input1.dimensions) {
-                VLOG_CHECKFAIL("dims not match");
+                VLOG(L1, "NNERR:dims not match");
                 return false;
             }
 
-            if (activationPass(inputn) == false) {
+            if (input0.type != input1.type) {
+                VLOG(L1, "NNERR:input0 and input1 type not equal,aborting!!");
                 return false;
             }
-            break;
-        }
+
+            if (input2.type != OperandType::INT32) {
+                VLOG(L1, "NNERR:input2 type invalid,aborting!!");
+                return false;
+            }
+
+            const auto& output = model.operands[operation.outputs[0]];
+
+            if (output.type != input0.type) {
+                VLOG(L1, "NNERR: output type not equalt to input0 type ,aborting!!");
+                return false;
+            }
+        } break;
         default:
             VLOG(L1, "unsupport opration %d", operation.type);
             return false;
     }
+#ifdef DISABLE_ALL_QUANT
+    for (auto i : operation.inputs) {
+        const auto input = model.operands[i];
+        if (input.type == OperandType::TENSOR_QUANT8_ASYMM) {
+            VLOG_CHECKFAIL("input quant");
+            return false;
+        }
+    }
+    for (auto i : operation.outputs) {
+        const auto output = model.operands[i];
+        if (output.type == OperandType::TENSOR_QUANT8_ASYMM) {
+            VLOG_CHECKFAIL("output quant");
+            return false;
+        }
+    }
+
+#endif
     VLOG(L1, "Operation %d supported by driver", operation.type);
 
     return true;
@@ -1513,8 +1874,8 @@ bool PreparedModel::isConst(int index) {
 bool PreparedModel::operationAdd(const Operation& operation) {
     VLOG(L1, "OperationType::ADD");
     OutputPort out;
-    bool isIn0Const = isConst(operation.inputs[0]);
-    bool isIn1Const = isConst(operation.inputs[1]);
+    bool isIn0Const = isConst(operation.inputs[OP_INPUT_IDX_CONV]);
+    bool isIn1Const = isConst(operation.inputs[OP_FILTER_IDX_CONV]);
     VLOG(L1, "isIn0Const = %d isIn1Const = %d \n", isIn0Const, isIn1Const);
     if (isIn0Const || isIn1Const) {
         if (isIn0Const && isIn1Const) {
@@ -1522,14 +1883,17 @@ bool PreparedModel::operationAdd(const Operation& operation) {
             nnAssert(true);
         }
         // this will use ScaleShift
-        if (isIn0Const)  // if op.inputs[1] is a Model input
-            out = AddConst(mNet, getPort(operation.inputs[1]),
-                           GetConstOperandAsTensor(operation.inputs[0]));
-        else  // isIn1Const is const //op.inputs[0] is a Model input
-            out = AddConst(mNet, getPort(operation.inputs[0]),
-                           GetConstOperandAsTensor(operation.inputs[1]));
-    } else {  // both inputs[0] & inputs[1] are model inputs
-        out = getPort(operation.inputs[0]) + getPort(operation.inputs[1]);
+        if (isIn0Const)  // if op.inputs[OP_FILTER_IDX] is a Model input
+            out = AddConst(
+                mNet, getPort(operation.inputs[OP_FILTER_IDX_CONV]),
+                GetConstOperandAsTensor(operation.inputs[OP_INPUT_IDX_CONV], OP_INPUT_IDX_CONV));
+        else  // isIn1Const is const //op.inputs[OP_INPUT_IDX_CONV] is a Model input
+            out = AddConst(
+                mNet, getPort(operation.inputs[OP_INPUT_IDX_CONV]),
+                GetConstOperandAsTensor(operation.inputs[OP_FILTER_IDX_CONV], OP_FILTER_IDX_CONV));
+    } else {  // both inputs[OP_INPUT_IDX_CONV] & inputs[OP_FILTER_IDX_CONV] aremodel inputs
+        out = getPort(operation.inputs[OP_INPUT_IDX_CONV]) +
+              getPort(operation.inputs[OP_FILTER_IDX_CONV]);
     }
     // check fusion
     VLOG(L1, "check fusion parameter = %d\n", PARAM_I32(2));
@@ -1800,11 +2164,10 @@ bool PreparedModel::operationConCat(const Operation& operation) {
     auto n = operation.inputs.size() - 1;
     std::vector<OutputPort> inputs;
     if (getPort(operation.inputs[0])->getLayout() == InferenceEngine::NCHW) {
-	std::vector<uint32_t> axisMap = {0, 2, 3, 1};
-	axis = axisMap[PARAM_I32(n)];
-    }
-    else
-	axis = PARAM_I32(n);
+        std::vector<uint32_t> axisMap = {0, 2, 3, 1};
+        axis = axisMap[PARAM_I32(n)];
+    } else
+        axis = PARAM_I32(n);
 
     for (int i = 0; i < n; i++) inputs.push_back(getPort(operation.inputs[i]));
     auto out = Concat(inputs, axis);
@@ -1815,7 +2178,7 @@ bool PreparedModel::operationConCat(const Operation& operation) {
 
 bool PreparedModel::operationConv2D(const Operation& operation) {
     VLOG(L1, "OperationType::CONV_2D");
-
+    dumpOperationParam(operation);
     /**
      * Performs an 2-D convolution operation.
      *
@@ -1904,10 +2267,19 @@ bool PreparedModel::operationConv2D(const Operation& operation) {
 
     ***/
 
-    auto input = getPort(operation.inputs[0]);
-    auto filter = GetConstOperandAsTensor(operation.inputs[1]);  // OIHW
-    // auto filter = GetConstWeightsOperandAsTensor(operation.inputs[1]);
-    auto bias = GetConstOperandAsTensor(operation.inputs[2]);
+    auto input = getPort(operation.inputs[OP_INPUT_IDX_CONV]);
+    auto filter = GetConstOperandAsTensor(operation.inputs[OP_FILTER_IDX_CONV],
+                                          OP_FILTER_IDX_CONV);  // OIHW
+    auto bias = GetConstOperandAsTensor(operation.inputs[OP_BIAS_IDX_CONV], OP_BIAS_IDX_CONV);
+    if (bias == nullptr) {
+        VLOG(L1, "NNERR:bias blob is NULL");
+        return false;
+    }
+
+    if (operation.outputs.size() > 1) {
+        VLOG(L1, "NNERR:More than one output for Conv2d,Aborting!!");
+        return false;
+    }
 
     const auto inputDims = input->getTensorDesc().getDims();
     const auto filterDims = filter->getTensorDesc().getDims();
@@ -1927,22 +2299,28 @@ bool PreparedModel::operationConv2D(const Operation& operation) {
     int filter_height = (int)filterDims[2];
     int filter_width = (int)filterDims[3];
 
-    // int32_t padding_left, padding_right;
-    // int32_t padding_top, padding_bottom;
-    // int32_t stride_width, stride_height;
-
-    uint32_t fusion_index = -1;
+    int32_t fusion_index = -1;
 
-    if (operation.inputs.size() == 10) {
+    if (operation.inputs.size() == EXPL_PAD_PARAMS_CONV) {
+        VLOG(L1, "Explicit padding requested");
+        mPadreq = EXPL_PAD;
         prms.padType = "explicit";
-        prms.pad_start = {PARAM_I32(3), PARAM_I32(5)};
-        prms.pad_end = {PARAM_I32(4), PARAM_I32(6)};
-        prms.stride = {PARAM_I32(7), PARAM_I32(8)};
+        prms.pad_start.x = PARAM_I32(OP_PADL_IDX_CONV);
+        prms.pad_start.y = PARAM_I32(OP_PADH_IDX_CONV);
+        CHECK_OPERAND_2D(prms.pad_start, OP_PADL_IDX_CONV, OP_PADH_IDX_CONV);
+        prms.pad_end.x = PARAM_I32(OP_PADR_IDX_CONV);
+        prms.pad_end.y = PARAM_I32(OP_PADW_IDX_CONV);
+        CHECK_OPERAND_2D(prms.pad_end, OP_PADR_IDX_CONV, OP_PADW_IDX_CONV);
+        prms.stride.x = PARAM_I32(OP_STRD_WD_IDX_EXPL_CONV);
+        prms.stride.y = PARAM_I32(OP_STRD_HT_IDX_EXPL_CONV);
+        CHECK_OPERAND_2D(prms.stride, OP_STRD_WD_IDX_EXPL_CONV, OP_STRD_HT_IDX_EXPL_CONV);
         prms.kernel = {filter_width, filter_height};
         prms.num_output_planes = filter_out;  // depth out
-        fusion_index = 9;
-    } else if (operation.inputs.size() == 7) {  // PAD SAME
-        const auto pad_type = PARAM_I32(3);     // padding_implicit
+        fusion_index = OP_ACTV_FUNC_IDX_EXPL_CONV;
+    } else if (operation.inputs.size() == IMPL_PAD_PARAMS_CONV) {  // PAD SAME
+        VLOG(L1, "Implicit padding requested");
+        mPadreq = IMPL_PAD;
+        const auto pad_type = PARAM_I32(3);  // padding_implicit
         int stride_width = PARAM_I32(4);
         int stride_height = PARAM_I32(5);
         int padding_left, padding_right;
@@ -1995,18 +2373,22 @@ bool PreparedModel::operationConv2D(const Operation& operation) {
         prms.stride = {stride_width, stride_height};
         prms.kernel = {filter_width, filter_height};
         prms.num_output_planes = filter_out;  // depth out
-        fusion_index = 6;
+        fusion_index = OP_ACTV_FUNC_IDX_IMPL_CONV;
     }
 
-    if (bias->size() != prms.num_output_planes) {
-        VLOG(L1, "biases size mismatch filer's depth");
-        nnAssert(false);
+    if (bias && bias->size() != prms.num_output_planes) {
+        VLOG(L1, "NNERR:biases size (%d)mismatch output planes (%d),warning", bias->size(),
+             prms.num_output_planes);
+        // return false;
+        // nnAssert(false);
     }
 
     // input_size (validate)
     if (filter_in != in_channels) {
-        VLOG(L1, "filter depth_in size mismatch input depth");
-        nnAssert(false);
+        VLOG(L1, "NNERR:filter depth_in size (%d) mismatch input depth (%d),warning!!", filter_in,
+             in_channels);
+        // return false;
+        // nnAssert(false);
     }
 
     // Reshape CONV_2D or use GetConstWeightsOperandAsTensor()
@@ -2065,7 +2447,14 @@ bool PreparedModel::operationConv2D(const Operation& operation) {
         VLOG(L1, "invalid fusion index");
         nnAssert(false);
     }
-    mPorts[operation.outputs[0]] = handleFusion(out, PARAM_I32(fusion_index));
+    auto acv_func = PARAM_I32(fusion_index);
+    if (acv_func < 0) {
+        VLOG(L1, "Invalid Activation function passed,aborting!!");
+        return false;
+    }
+    // now here the out is next layer's input , and next layer is an activation
+    // layer..relu/sigmoid etc...
+    mPorts[operation.outputs[0]] = handleFusion(out, acv_func);
 
     VLOG(L1, "----------------------------------------------");
     VLOGDIMS(L1, inputDims, "inputs dims");
@@ -2078,6 +2467,7 @@ bool PreparedModel::operationConv2D(const Operation& operation) {
 
 bool PreparedModel::operationDepthwiseConv2D(const Operation& operation) {
     VLOG(L1, "OperationType::DEPTHWISE_CONV_2D");
+    dumpOperationParam(operation);
     /**
      * Performs a depthwise 2-D convolution operation.
      *
@@ -2168,12 +2558,11 @@ bool PreparedModel::operationDepthwiseConv2D(const Operation& operation) {
      *      must be satisfied: output_scale > input_scale * filter_scale.
      */
 
-    auto input = getPort(operation.inputs[0]);
-    // auto filter = GetConstOperandAsTensor(operation.inputs[1]); //NCHW [1, depth_out,
-    // filter_height, filter_width]
+    auto input = getPort(operation.inputs[OP_INPUT_IDX_CONV]);
     auto filter = GetConstWeightsOperandAsTensor(
-        operation.inputs[1]);  //[depth_out, 1, filter_height, filter_width] OIHW
-    auto bias = GetConstOperandAsTensor(operation.inputs[2]);
+        operation.inputs[OP_FILTER_IDX_CONV]);  //[depth_out, 1, filter_height,
+                                                // filter_width] OIHW
+    auto bias = GetConstOperandAsTensor(operation.inputs[OP_BIAS_IDX_CONV], OP_BIAS_IDX_CONV);
 
     const auto inputDims = input->getTensorDesc().getDims();
     const auto filterDims = filter->getTensorDesc().getDims();
@@ -2198,6 +2587,8 @@ bool PreparedModel::operationDepthwiseConv2D(const Operation& operation) {
     int depth_multiplier = 0;
 
     if (operation.inputs.size() == 11) {
+        VLOG(L1, "Explicit padding requested");
+        mPadreq = EXPL_PAD;
         prms.padType = "explicit";
         prms.pad_start = {PARAM_I32(3), PARAM_I32(5)};
         prms.pad_end = {PARAM_I32(4), PARAM_I32(6)};
@@ -2209,6 +2600,8 @@ bool PreparedModel::operationDepthwiseConv2D(const Operation& operation) {
         prms.num_output_planes =
             in_channels * depth_multiplier;     // same as filter_out; //dims[0]; //depth out
     } else if (operation.inputs.size() == 8) {  // implicit padding
+        VLOG(L1, "Implicit padding requested");
+        mPadreq = IMPL_PAD;
         const auto pad_type = PARAM_I32(3);
         int stride_width = PARAM_I32(4);
         int stride_height = PARAM_I32(5);
@@ -2351,6 +2744,7 @@ bool PreparedModel::operationDepthwiseConv2D(const Operation& operation) {
 
 bool PreparedModel::operationFullyConnected(const Operation& operation) {
     VLOG(L1, "OperationType::FULLY_CONNECTED");
+    dumpOperationParam(operation);
     /**
      * Denotes a fully (densely) connected layer, which connects all elements
      * in the input tensor with each element in the output tensor.
@@ -2395,9 +2789,10 @@ bool PreparedModel::operationFullyConnected(const Operation& operation) {
     FULLY_CONNECTED = 9,
      */
 
-    auto input = getPort(operation.inputs[0]);
-    auto weights = GetConstOperandAsTensor(operation.inputs[1]);
-    auto bias = GetConstOperandAsTensor(operation.inputs[2]);
+    auto input = getPort(operation.inputs[OP_INPUT_IDX_CONV]);
+    auto weights =
+        GetConstOperandAsTensor(operation.inputs[OP_FILTER_IDX_CONV], OP_FILTER_IDX_CONV);
+    auto bias = GetConstOperandAsTensor(operation.inputs[OP_BIAS_IDX_CONV], OP_BIAS_IDX_CONV);
 
     auto inputDims = input->getTensorDesc().getDims();
     for (auto i = 0; i < inputDims.size(); i++) VLOG(L1, "input dims[%d] = %d ", i, inputDims[i]);
@@ -2517,6 +2912,7 @@ bool PreparedModel::operationFullyConnected(const Operation& operation) {
 
 bool PreparedModel::operationL2Normalization(const Operation& operation) {
     VLOG(L1, "OperationType::L2_NORMALIZATION");
+    dumpOperationParam(operation);
     /*
      * Inputs:
      * 0: A 4-D tensor, of shape [batches, height, width, depth], specifying the input.
@@ -2829,7 +3225,7 @@ void PreparedModel::initializeInput() {
     }
 }
 
-void PreparedModel::finalizeOutput(/*RunTimeOperandInfo* output */) {
+bool PreparedModel::finalizeOutput(/*RunTimeOperandInfo* output */) {
     VLOG(L1, "finalize Output");
     for (auto i : mModel.outputIndexes) {
         int dims_size = mOperands[i].dimensions.size();
@@ -2878,6 +3274,7 @@ void PreparedModel::finalizeOutput(/*RunTimeOperandInfo* output */) {
             */
         }
     }
+    return true;
 }
 
 IRBlob::Ptr VpuPreparedModel::GetConstWeightsOperandAsTensor(uint32_t index) {
@@ -3009,8 +3406,7 @@ IRBlob::Ptr VpuPreparedModel::GetConstWeightsOperandAsTensor(uint32_t index) {
     return nullptr;
 }
 
-IRBlob::Ptr VpuPreparedModel::GetConstOperandAsTensor(uint32_t index) {
-    // const auto op = model.operands.at(index);
+IRBlob::Ptr VpuPreparedModel::GetConstOperandAsTensor(int index, int operation_idx) {
     const auto op = mModel.operands[index];
     uint32_t len;
     const uint8_t* buf = GetOperandMemory(mModel, index, len);
@@ -3441,12 +3837,15 @@ IRBlob::Ptr CpuPreparedModel::GetConstWeightsOperandAsTensor(uint32_t index) {
     return nullptr;
 }
 
-IRBlob::Ptr CpuPreparedModel::GetConstOperandAsTensor(uint32_t index) {
-    dumpOperand(index);
-    const auto op = mModel.operands[index];
+IRBlob::Ptr CpuPreparedModel::GetConstOperandAsTensor(int operand_idx, int operation_idx) {
+    dumpOperand(operand_idx);
+    const auto op = mModel.operands[operand_idx];
     uint32_t len;
-    const uint8_t* buf = GetOperandMemory(mModel, index, len);
-    VLOG(L1, "CpuPreparedModel:: Operand: index: %d, len: %d, buf: %p", index, len, buf);
+
+    const uint8_t* buf = GetOperandMemory(mModel, operand_idx, len);
+    VLOG(L1, "CpuPreparedModel:: operand_index: %d, operation_index :%d,len: %d, buf: %p",
+         operand_idx, operation_idx, len, buf);
+
     if (op.type == OperandType::TENSOR_FLOAT32 || op.type == OperandType::FLOAT32) {
         vec<unsigned int> order;
         Layout layout;
@@ -3646,8 +4045,7 @@ Blob::Ptr CpuPreparedModel::GetInOutOperandAsBlob(RunTimeOperandInfo& op, const
     return nullptr;
 }
 
-}  // namespace driver
-}  // namespace V1_0
+}  // namespace nnhal
 }  // namespace neuralnetworks
 }  // namespace hardware
 }  // namespace android
diff --git a/intel_nn_hal/PreparedModel.h b/intel_nn_hal/PreparedModel.h
index f6b815f..24f3562 100644
--- a/intel_nn_hal/PreparedModel.h
+++ b/intel_nn_hal/PreparedModel.h
@@ -18,27 +18,28 @@
 #define ANDROID_ML_NN_PREPAREDMODEL_H
 
 #include <android/hardware/neuralnetworks/1.0/IPreparedModel.h>
+#include <android/hardware/neuralnetworks/1.1/types.h>
 #include <android/hidl/memory/1.0/IMemory.h>
-#include <hidlmemory/mapping.h>
 #include <hardware/hardware.h>
+#include <hidlmemory/mapping.h>
 #include <sys/mman.h>
-#include <string>
 #include <fstream>
+#include <string>
 
+#include "Driver.h"
 #include "IENetwork.h"
 
 using ::android::hidl::memory::V1_0::IMemory;
-using namespace IRBuilder;
 using namespace InferenceEngine;
 
 namespace android {
 namespace hardware {
 namespace neuralnetworks {
-namespace V1_0 {
-namespace driver {
+namespace nnhal {
 
-template <class T> using  vec = std::vector<T>;
-typedef uint8_t * memory;
+template <class T>
+using vec = std::vector<T>;
+typedef uint8_t* memory;
 
 // The type and dimensions of an operand.
 struct Shape {
@@ -51,9 +52,9 @@ struct Shape {
 // Information we maintain about each operand during execution that
 // may change during execution.
 struct RunTimeOperandInfo {
-    //std::string name;
-    //uint32_t opIdx;
-    //void * opIdx;
+    // std::string name;
+    // uint32_t opIdx;
+    // void * opIdx;
 
     // TODO Storing the type here is redundant, as it won't change during execution.
     OperandType type;
@@ -84,7 +85,6 @@ struct RunTimeOperandInfo {
     }
 };
 
-
 // Used to keep a pointer to each of the memory pools.
 struct RunTimePoolInfo {
     sp<IMemory> memory;
@@ -95,12 +95,9 @@ struct RunTimePoolInfo {
     bool update();
 };
 
-
 bool setRunTimePoolInfosFromHidlMemories(std::vector<RunTimePoolInfo>* poolInfos,
                                          const hidl_vec<hidl_memory>& pools);
 
-
-
 // Base class used to create vpu drivers for the NN HAL.  This class
 // provides some implementation of the more common functions.
 //
@@ -109,22 +106,21 @@ bool setRunTimePoolInfosFromHidlMemories(std::vector<RunTimePoolInfo>* poolInfos
 class PreparedModel : public IPreparedModel {
 public:
     PreparedModel(const Model& model)
-          :mTargetDevice(TargetDevice::eMYRIAD), mModel(model), mNet("nnNet"), enginePtr(nullptr) {
-        IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
+        : mTargetDevice(TargetDevice::eMYRIAD), mModel(model), mNet("nnNet"), enginePtr(nullptr) {
+        g_layer_precision = InferenceEngine::Precision::FP16;
     }
 
     PreparedModel(const TargetDevice device, const Model& model)
-          :mTargetDevice(device), mModel(model), mNet("nnNet"), enginePtr(nullptr) {
+        : mTargetDevice(device), mModel(model), mNet("nnNet"), enginePtr(nullptr) {
         if (mTargetDevice == TargetDevice::eCPU)
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::FP32;
-           //using type = typename InferenceEngine::PrecisionTrait<IRBuilder::g_layer_precision>::value_type;
+            g_layer_precision = InferenceEngine::Precision::FP32;
         else if (mTargetDevice == TargetDevice::eMYRIAD)
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
+            g_layer_precision = InferenceEngine::Precision::FP16;
         else
-           IRBuilder::g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
+            g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
     }
 
-    ~PreparedModel() override {deinitialize();}
+    ~PreparedModel() override { deinitialize(); }
     bool initialize();
     Return<ErrorStatus> execute(const Request& request,
                                 const sp<IExecutionCallback>& callback) override;
@@ -145,7 +141,7 @@ protected:
     bool operationLRN(const Operation& operation);
     bool operationMaxPool2D(const Operation& operation);
     bool operationLogisticSigmoid(const Operation& operation);
-    //bool operationLSTM(const Operation& operation);
+    // bool operationLSTM(const Operation& operation);
     bool operationMUL(const Operation& operation);
     bool operationRELU(const Operation& operation);
     bool operationRELU1(const Operation& operation);
@@ -155,25 +151,27 @@ protected:
     bool operationTANH(const Operation& operation);
 
     void initializeInput();
-    void finalizeOutput(/*RunTimeOperandInfo* output*/);
-
-    OutputPort handleFusion(const OutputPort &out, int32_t fusedOp);
-    template<typename T>
-    T GetConstFromBuffer(const uint8_t *buf, uint32_t len);
-    template<typename T>
-    std::vector<T> GetConstVecFromBuffer(const uint8_t *buf, uint32_t len);
-    const uint8_t *GetOperandMemory(const Model &model, uint32_t index, uint32_t &len_out);
+    bool finalizeOutput(/*RunTimeOperandInfo* output*/);
+
+    OutputPort handleFusion(const OutputPort& out, int32_t fusedOp);
+    template <typename T>
+    T GetConstFromBuffer(const uint8_t* buf, uint32_t len);
+    template <typename T>
+    std::vector<T> GetConstVecFromBuffer(const uint8_t* buf, uint32_t len);
+    const uint8_t* GetOperandMemory(const Model& model, uint32_t index, uint32_t& len_out);
     template <typename T>
-    T ParseOperationInput(const Model &model, const Operation& operation, uint32_t index);
+    T ParseOperationInput(const Model& model, const Operation& operation, uint32_t index);
     template <typename T>
-    T GetConstOperand(const Model &model, uint32_t index);
+    T GetConstOperand(const Model& model, uint32_t index);
     template <typename T>
-    std::vector<T> GetConstVecOperand(const Model &model, uint32_t index);
-    virtual Blob::Ptr GetConstOperandAsTensor(uint32_t index);
-    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf, uint32_t& len);
+    std::vector<T> GetConstVecOperand(const Model& model, uint32_t index);
+    virtual Blob::Ptr GetConstOperandAsTensor(int operand_index, int operation_idx);
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
+                                            uint32_t& len);
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index);
-    void SetOperandMemory(const Model &model, uint32_t index, uint32_t &len_out, const uint8_t *buf);
-    void SetOperandFromTensor(uint8_t* buf, uint32_t &length, Blob::Ptr infOutput);
+    void SetOperandMemory(const Model& model, uint32_t index, uint32_t& len_out,
+                          const uint8_t* buf);
+    void SetOperandFromTensor(uint8_t* buf, uint32_t& length, Blob::Ptr infOutput);
     bool isConst(int index);
     OutputPort getPort(int index);
 
@@ -182,37 +180,34 @@ protected:
     std::vector<RunTimeOperandInfo> mOperands;
     std::vector<RunTimePoolInfo> mPoolInfos;
     IRDocument mNet;
-    std::vector<OutputPort> mPorts;  //typedef std::shared_ptr<Data> DataPtr;
+    std::vector<OutputPort> mPorts;  // typedef std::shared_ptr<Data> DataPtr;
     ExecuteNetwork* enginePtr;
-
+    bool mPadreq;
 };
 
 class VpuPreparedModel : public PreparedModel {
 public:
-    VpuPreparedModel(const Model& model)
-          :PreparedModel(TargetDevice::eMYRIAD, model) {
-    }
+    VpuPreparedModel(const Model& model) : PreparedModel(TargetDevice::eMYRIAD, model) {}
 
-    virtual Blob::Ptr GetConstOperandAsTensor(uint32_t index) override;
-    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf, uint32_t& len) override;
+    virtual Blob::Ptr GetConstOperandAsTensor(int operand_index, int operation_idx) override;
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
+                                            uint32_t& len) override;
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index) override;
 };
 
 class CpuPreparedModel : public PreparedModel {
 public:
-    CpuPreparedModel(const Model& model)
-          :PreparedModel(TargetDevice::eCPU, model) {
-    }
+    CpuPreparedModel(const Model& model) : PreparedModel(TargetDevice::eCPU, model) {}
 
-    virtual Blob::Ptr GetConstOperandAsTensor(uint32_t index) override;
-    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t *buf, uint32_t& len) override;
+    virtual Blob::Ptr GetConstOperandAsTensor(int operand_index, int operation_idx) override;
+    virtual Blob::Ptr GetInOutOperandAsBlob(RunTimeOperandInfo& op, const uint8_t* buf,
+                                            uint32_t& len) override;
     virtual Blob::Ptr GetConstWeightsOperandAsTensor(uint32_t index) override;
 };
 
-}  // namespace driver
-}  // namespace V1_0
+}  // namespace nnhal
 }  // namespace neuralnetworks
 }  // namespace hardware
 }  // namespace android
 
-#endif // ANDROID_ML_NN_PREPAREDMODEL_H
+#endif  // ANDROID_ML_NN_PREPAREDMODEL_H
diff --git a/intel_nn_hal/android.hardware.neuralnetworks@1.1-generic.rc b/intel_nn_hal/android.hardware.neuralnetworks@1.1-generic.rc
new file mode 100644
index 0000000..2b6caef
--- /dev/null
+++ b/intel_nn_hal/android.hardware.neuralnetworks@1.1-generic.rc
@@ -0,0 +1,4 @@
+service neuralnetworks-hal-1-1-cpu /vendor/bin/hw/android.hardware.neuralnetworks@1.1-generic-service -D CPU
+    class hal
+    user system
+    group system
diff --git a/intel_nn_hal/coding_style.txt b/intel_nn_hal/coding_style.txt
index bb64522..82d08bc 100644
--- a/intel_nn_hal/coding_style.txt
+++ b/intel_nn_hal/coding_style.txt
@@ -1,2 +1,2 @@
 # Before commiting any changes, run the following command to make sure proper coding style is followed
-find . -regex '.*\.\(cpp\|hpp\|cc\|cxx\)' -exec clang-format -style=file -i {} \;
+find . -regex '.*\.\(cpp\|hpp\|cc\|cxx\|h\)' -exec clang-format -style=file -i {} \;
diff --git a/intel_nn_hal/dl/helpers.hpp b/intel_nn_hal/dl/helpers.hpp
index 30fbd87..26f643f 100644
--- a/intel_nn_hal/dl/helpers.hpp
+++ b/intel_nn_hal/dl/helpers.hpp
@@ -92,7 +92,7 @@ class ExecuteNetwork {
     InferRequest inferRequest;
     ResponseDesc resp;
 
-   public:
+public:
     ExecuteNetwork() {}
     ExecuteNetwork(IRDocument &doc, TargetDevice target = TargetDevice::eCPU) {
         InferenceEngine::PluginDispatcher dispatcher(
diff --git a/intel_nn_hal/graphAPI/IENetwork.h b/intel_nn_hal/graphAPI/IENetwork.h
index 67b0767..6cda2ae 100644
--- a/intel_nn_hal/graphAPI/IENetwork.h
+++ b/intel_nn_hal/graphAPI/IENetwork.h
@@ -21,19 +21,19 @@
 
 #pragma once
 
-#include "IRDocument.h"
-#include "IRLayers.h"
 #include <ie_plugin_config.hpp>
 #include <ie_plugin_dispatcher.hpp>
 #include <ie_plugin_ptr.hpp>
 #include <inference_engine.hpp>
+#include "IRDocument.h"
+#include "IRLayers.h"
 
+#include <fstream>
+#include "debug.h"
+#include "ie_exception_conversion.hpp"
 #include "ie_iinfer_request.hpp"
 #include "ie_infer_request.hpp"
 #include "ie_plugin_cpp.hpp"
-#include "ie_exception_conversion.hpp"
-#include "debug.h"
-#include <fstream>
 
 #include <android/log.h>
 #include <log/log.h>
@@ -42,20 +42,23 @@
 #include "vpu_plugin_config.hpp"
 #endif
 using namespace InferenceEngine::details;
-using namespace IRBuilder;
 using namespace InferenceEngine;
 
+namespace android {
+namespace hardware {
+namespace neuralnetworks {
+namespace nnhal {
 template <typename T>
-inline std::ostream & operator << (std::ostream &out, const std::vector<T> &vec) {
+inline std::ostream &operator<<(std::ostream &out, const std::vector<T> &vec) {
     if (vec.empty()) return std::operator<<(out, "[]");
     out << "[" << vec[0];
-    for (unsigned i=1; i < vec.size(); i++) {
+    for (unsigned i = 1; i < vec.size(); i++) {
         out << ", " << vec[i];
     }
     return out << "]";
 }
 
-//aks
+// aks
 /*
 void dumpBlob(const std::string &prefix, size_t len, TBlob<short>::Ptr blob)
 {
@@ -85,28 +88,31 @@ void dumpBlob(const std::string &prefix, size_t len, TBlob<short>::Ptr blob)
 
 */
 static void setConfig(std::map<std::string, std::string> &config) {
-    //config[VPUConfigParams::FIRST_SHAVE] = "0";
-    //config[VPUConfigParams::LAST_SHAVE] = "11";
-    //config[VPUConfigParams::MEMORY_OPTIMIZATION] = CONFIG_VALUE(NO);//InferenceEngine::PluginConfigParams::YES;
-    //config[VPUConfigParams::COPY_OPTIMIZATION] = CONFIG_VALUE(NO);//InferenceEngine::PluginConfigParams::YES;
-/* //enable below for VPU logs
-    config[CONFIG_KEY(LOG_LEVEL)] = CONFIG_VALUE(LOG_INFO);
-    config[VPUConfigParams::KEY_VPU_LOG_LEVEL] = CONFIG_VALUE(LOG_DEBUG);
-    config[CONFIG_KEY(LOG_LEVEL)] = CONFIG_VALUE(LOG_DEBUG);
-*/
-    //config[VPU_CONFIG_KEY(LOG_LEVEL)] = CONFIG_VALUE(LOG_DEBUG);
-    //config[InferenceEngine::PluginConfigParams::CONFIG_KEY(LOG_LEVEL)] = InferenceEngine::PluginConfigParams::LOG_DEBUG;
-    //config[VPUConfigParams::VPU_LOG_LEVEL] = CONFIG_VALUE(LOG_DEBUG);
-    //config[InferenceEngine::PluginConfigParams::KEY_LOG_LEVEL] = InferenceEngine::PluginConfigParams::LOG_DEBUG /*LOG_WARNING*/;
-    //config[InferenceEngine::VPUConfigParams::IGNORE_UNKNOWN_LAYERS] = InferenceEngine::PluginConfigParams::NO;
-    //config[VPU_CONFIG_KEY(COMPUTE_LAYOUT)] = VPU_CONFIG_VALUE(NHWC);
+    // config[VPUConfigParams::FIRST_SHAVE] = "0";
+    // config[VPUConfigParams::LAST_SHAVE] = "11";
+    // config[VPUConfigParams::MEMORY_OPTIMIZATION] =
+    // CONFIG_VALUE(NO);//InferenceEngine::PluginConfigParams::YES;
+    // config[VPUConfigParams::COPY_OPTIMIZATION] =
+    // CONFIG_VALUE(NO);//InferenceEngine::PluginConfigParams::YES;
+    /* //enable below for VPU logs
+        config[CONFIG_KEY(LOG_LEVEL)] = CONFIG_VALUE(LOG_INFO);
+        config[VPUConfigParams::KEY_VPU_LOG_LEVEL] = CONFIG_VALUE(LOG_DEBUG);
+        config[CONFIG_KEY(LOG_LEVEL)] = CONFIG_VALUE(LOG_DEBUG);
+    */
+    // config[VPU_CONFIG_KEY(LOG_LEVEL)] = CONFIG_VALUE(LOG_DEBUG);
+    // config[InferenceEngine::PluginConfigParams::CONFIG_KEY(LOG_LEVEL)] =
+    // InferenceEngine::PluginConfigParams::LOG_DEBUG; config[VPUConfigParams::VPU_LOG_LEVEL] =
+    // CONFIG_VALUE(LOG_DEBUG); config[InferenceEngine::PluginConfigParams::KEY_LOG_LEVEL] =
+    // InferenceEngine::PluginConfigParams::LOG_DEBUG /*LOG_WARNING*/;
+    // config[InferenceEngine::VPUConfigParams::IGNORE_UNKNOWN_LAYERS] =
+    // InferenceEngine::PluginConfigParams::NO; config[VPU_CONFIG_KEY(COMPUTE_LAYOUT)] =
+    // VPU_CONFIG_VALUE(NHWC);
 }
 
-class ExecuteNetwork
-{
+class ExecuteNetwork {
     InferenceEnginePluginPtr enginePtr;
     ICNNNetwork *network;
-    //IExecutableNetwork::Ptr pExeNet;
+    // IExecutableNetwork::Ptr pExeNet;
     ExecutableNetwork executable_network;
     InputsDataMap inputInfo = {};
     OutputsDataMap outputInfo = {};
@@ -115,155 +121,152 @@ class ExecuteNetwork
     ResponseDesc resp;
 
 public:
-    ExecuteNetwork() : network(nullptr){}
-    ExecuteNetwork(IRDocument &doc, TargetDevice target = TargetDevice::eCPU) : network(nullptr)
-    {
-        InferenceEngine::PluginDispatcher dispatcher({"/vendor/lib64","/vendor/lib","/system/lib64","/system/lib","","./"});
+    ExecuteNetwork() : network(nullptr) {}
+    ExecuteNetwork(IRDocument &doc, TargetDevice target = TargetDevice::eCPU) : network(nullptr) {
+        InferenceEngine::PluginDispatcher dispatcher(
+            {"/vendor/lib64", "/vendor/lib", "/system/lib64", "/system/lib", "", "./"});
         enginePtr = dispatcher.getSuitablePlugin(target);
 
         network = doc.getNetwork();
         network->getInputsInfo(inputInfo);
         network->getOutputsInfo(outputInfo);
 
-        //size_t batch = 1;
-        //network->setBatchSize(batch);
+        // size_t batch = 1;
+        // network->setBatchSize(batch);
 
-        #ifdef NNLOG
-        ALOGI("%s Plugin loaded",InferenceEngine::TargetDeviceInfo::name(target));
-        #endif
+#ifdef NNLOG
+        ALOGI("%s Plugin loaded", InferenceEngine::TargetDeviceInfo::name(target));
+#endif
     }
 
-    ExecuteNetwork(ExecutableNetwork& exeNet) : ExecuteNetwork(){
-    executable_network = exeNet;
-    inferRequest = executable_network.CreateInferRequest();
-    ALOGI("infer request created");
-
+    ExecuteNetwork(ExecutableNetwork &exeNet) : ExecuteNetwork() {
+        executable_network = exeNet;
+        inferRequest = executable_network.CreateInferRequest();
+        ALOGI("infer request created");
     }
 
     //~ExecuteNetwork(){ }
-    void loadNetwork()
-    {
-
+    void loadNetwork() {
         std::map<std::string, std::string> networkConfig;
         setConfig(networkConfig);
 
         InferencePlugin plugin(enginePtr);
         executable_network = plugin.LoadNetwork(*network, networkConfig);
-        //std::cout << "Network loaded" << std::endl;
-	 ALOGI("Network loaded");
+        // std::cout << "Network loaded" << std::endl;
+        ALOGI("Network loaded");
 
         inferRequest = executable_network.CreateInferRequest();
-        //std::cout << "infer request created" << std::endl;
-      }
-
-    void prepareInput()
-    {
-	  #ifdef NNLOG
-      ALOGI("Prepare input blob");
-	  #endif
-      Precision inputPrecision = Precision::FP32;
-      inputInfo.begin()->second->setPrecision(inputPrecision);
-      //inputInfo.begin()->second->setPrecision(Precision::U8);
-
-      auto inputDims = inputInfo.begin()->second->getTensorDesc().getDims();
-      if (inputDims.size() == 4)
-      inputInfo.begin()->second->setLayout(Layout::NCHW);
-      else if (inputDims.size() == 2)
-      inputInfo.begin()->second->setLayout(Layout::NC);
-      else
-      inputInfo.begin()->second->setLayout(Layout::C);
-
-
-      //inputInfo.begin()->second->setPrecision(Precision::U8);
-      //inputInfo.begin()->second->setLayout(Layout::NCHW);
+        // std::cout << "infer request created" << std::endl;
+    }
 
+    void prepareInput() {
+#ifdef NNLOG
+        ALOGI("Prepare input blob");
+#endif
+        Precision inputPrecision = Precision::FP32;
+        inputInfo.begin()->second->setPrecision(inputPrecision);
+        // inputInfo.begin()->second->setPrecision(Precision::U8);
+
+        auto inputDims = inputInfo.begin()->second->getTensorDesc().getDims();
+        if (inputDims.size() == 4)
+            inputInfo.begin()->second->setLayout(Layout::NCHW);
+        else if (inputDims.size() == 2)
+            inputInfo.begin()->second->setLayout(Layout::NC);
+        else
+            inputInfo.begin()->second->setLayout(Layout::C);
+
+        // inputInfo.begin()->second->setPrecision(Precision::U8);
+        // inputInfo.begin()->second->setLayout(Layout::NCHW);
     }
 
-    void prepareOutput()
-    {
-	  #ifdef NNLOG
-      ALOGI("Prepare output blob");
-	  #endif
-      Precision outputPrecision = Precision::FP32;
-      outputInfo.begin()->second->setPrecision(outputPrecision);
-
-      auto outputDims = outputInfo.begin()->second->getDims();
-      if (outputDims.size() == 4)
-      outputInfo.begin()->second->setLayout(Layout::NHWC);
-      else if (outputDims.size() == 2)
-      outputInfo.begin()->second->setLayout(Layout::NC);
-      else
-      outputInfo.begin()->second->setLayout(Layout::C);
-
-      #ifdef NNLOG
-      //auto dims = inputInfo.begin()->second->getDims();
-      //ALOGI("inputInfo dims size = %d\n", dims.size());
-      //ALOGI("outputInfo dims size = %d\n", outputDims.size());
-      #endif
+    void prepareOutput() {
+#ifdef NNLOG
+        ALOGI("Prepare output blob");
+#endif
+        Precision outputPrecision = Precision::FP32;
+        outputInfo.begin()->second->setPrecision(outputPrecision);
+
+        auto outputDims = outputInfo.begin()->second->getDims();
+        if (outputDims.size() == 4)
+            outputInfo.begin()->second->setLayout(Layout::NHWC);
+        else if (outputDims.size() == 2)
+            outputInfo.begin()->second->setLayout(Layout::NC);
+        else
+            outputInfo.begin()->second->setLayout(Layout::C);
+
+#ifdef NNLOG
+// auto dims = inputInfo.begin()->second->getDims();
+// ALOGI("inputInfo dims size = %d\n", dims.size());
+// ALOGI("outputInfo dims size = %d\n", outputDims.size());
+#endif
     }
 
-    //setBlob input/output blob for infer request
-    void setBlob(const std::string& inName, const Blob::Ptr& inputBlob)
-    {
-        #ifdef NNLOG
+    // setBlob input/output blob for infer request
+    void setBlob(const std::string &inName, const Blob::Ptr &inputBlob) {
+#ifdef NNLOG
         ALOGI("setBlob input or output blob name : %s", inName.c_str());
-        ALOGI("Blob size %d and size in bytes %d bytes element size %d bytes", inputBlob->size(), inputBlob->byteSize(), inputBlob->element_size());
-        #endif
+        ALOGI("Blob size %d and size in bytes %d bytes element size %d bytes", inputBlob->size(),
+              inputBlob->byteSize(), inputBlob->element_size());
+#endif
 
-        //inferRequest.SetBlob(inName.c_str(), inputBlob);
+        // inferRequest.SetBlob(inName.c_str(), inputBlob);
         inferRequest.SetBlob(inName, inputBlob);
 
-        //std::cout << "setBlob input or output name : " << inName << std::endl;
-
+        // std::cout << "setBlob input or output name : " << inName << std::endl;
     }
 
-     //for non aync infer request
-    TBlob<float>::Ptr getBlob(const std::string& outName) {
-       Blob::Ptr outputBlob;
-       outputBlob = inferRequest.GetBlob(outName);
-       //std::cout << "GetBlob input or output name : " << outName << std::endl;
-       #ifdef NNLOG
-       ALOGI("Get input/output blob, name : ", outName.c_str());
-       #endif
-       return As<TBlob<float>>(outputBlob);
-       //return outputBlob;
+    // for non aync infer request
+    TBlob<float>::Ptr getBlob(const std::string &outName) {
+        Blob::Ptr outputBlob;
+        outputBlob = inferRequest.GetBlob(outName);
+// std::cout << "GetBlob input or output name : " << outName << std::endl;
+#ifdef NNLOG
+        ALOGI("Get input/output blob, name : ", outName.c_str());
+#endif
+        return As<TBlob<float>>(outputBlob);
+        // return outputBlob;
     }
 
     void Infer() {
-        #ifdef NNLOG
+#ifdef NNLOG
         ALOGI("Infer Network\n");
-        #endif
-//        inferRequest = executable_network.CreateInferRequest();
-/*
-        auto inName = inputInfo.begin()->first;
-        ALOGI("set input blob\n");
-        inferRequest.SetBlob(inName, in);
-
-        ALOGI("aks prepare output blob\n");
-        const std::string firstOutName = outputInfo.begin()->first;
-        InferenceEngine::TBlob<PrecisionTrait<Precision::FP32>::value_type>::Ptr outputBlob;
-        outputBlob = InferenceEngine::make_shared_blob<PrecisionTrait<Precision::FP32>::value_type,
-                InferenceEngine::SizeVector>(Precision::FP32, outputInfo.begin()->second->getDims());
-        outputBlob->allocate();
-
-        ALOGI("set output blob\n");
-        inferRequest.SetBlob(firstOutName, outputBlob);
-
-*/
-        #ifdef NNLOG
+#endif
+        //        inferRequest = executable_network.CreateInferRequest();
+        /*
+                auto inName = inputInfo.begin()->first;
+                ALOGI("set input blob\n");
+                inferRequest.SetBlob(inName, in);
+
+                ALOGI("aks prepare output blob\n");
+                const std::string firstOutName = outputInfo.begin()->first;
+                InferenceEngine::TBlob<PrecisionTrait<Precision::FP32>::value_type>::Ptr outputBlob;
+                outputBlob =
+           InferenceEngine::make_shared_blob<PrecisionTrait<Precision::FP32>::value_type,
+                        InferenceEngine::SizeVector>(Precision::FP32,
+           outputInfo.begin()->second->getDims()); outputBlob->allocate();
+
+                ALOGI("set output blob\n");
+                inferRequest.SetBlob(firstOutName, outputBlob);
+
+        */
+#ifdef NNLOG
         ALOGI("StartAsync scheduled");
-        #endif
-        inferRequest.StartAsync();  //for async infer
-        //ALOGI("async wait");
-        //inferRequest.Wait(1000);
-        inferRequest.Wait(10000); //check right value to infer
-        //inferRequest.Wait(IInferRequest::WaitMode::RESULT_READY);
-
-        //std::cout << "output name : " << firstOutName << std::endl;
-        #ifdef NNLOG
+#endif
+        inferRequest.StartAsync();  // for async infer
+        // ALOGI("async wait");
+        // inferRequest.Wait(1000);
+        inferRequest.Wait(10000);  // check right value to infer
+// inferRequest.Wait(IInferRequest::WaitMode::RESULT_READY);
+
+// std::cout << "output name : " << firstOutName << std::endl;
+#ifdef NNLOG
         ALOGI("infer request completed");
-        #endif
+#endif
 
         return;
     }
 };
+}  // namespace nnhal
+}  // namespace neuralnetworks
+}  // namespace hardware
+}  // namespace android
diff --git a/intel_nn_hal/graphAPI/IRDocument.cpp b/intel_nn_hal/graphAPI/IRDocument.cpp
index 4aa5ab8..80e531e 100644
--- a/intel_nn_hal/graphAPI/IRDocument.cpp
+++ b/intel_nn_hal/graphAPI/IRDocument.cpp
@@ -43,15 +43,18 @@
 #include <log/log.h>
 #endif
 
-using namespace IRBuilder;
 using namespace std;
 using namespace InferenceEngine;
 
+namespace android {
+namespace hardware {
+namespace neuralnetworks {
+namespace nnhal {
 class InternalNetworkImpl : public InferenceEngine::details::CNNNetworkImpl {
-   public:
+public:
     InternalNetworkImpl() {}
     InternalNetworkImpl(const std::string netName) : InternalNetworkImpl() {
-        setPrecision(IRBuilder::g_layer_precision);
+        setPrecision(g_layer_precision);
         setName(netName);
     }
 
@@ -130,8 +133,7 @@ void IRDocument::optimize() {
 
             it = _layers.erase(it);
             network->remove(l->name);
-        }
-        else {
+        } else {
             ++it;
         }
     }
@@ -139,7 +141,7 @@ void IRDocument::optimize() {
 
 void IRDocument::build() {
     if (_processed) return;
-    network->setPrecision(IRBuilder::g_layer_precision);
+    network->setPrecision(g_layer_precision);
     InputsDataMap inputs;
     network->getInputsInfo(inputs);
     for (auto i : inputs) {
@@ -273,14 +275,15 @@ void IRDocument::crateDotFile(std::ostream &dot) const {
 InferenceEngine::InputInfo::Ptr IRDocument::createInput(const std::string &name,
                                                         const TensorDims &dims) const {
     Layout layout;
-    if (dims.size() == 4) layout = NCHW;
+    if (dims.size() == 4)
+        layout = NCHW;
     else if (dims.size() == 2)
         layout = InferenceEngine::Layout::NC;
     else
         layout = InferenceEngine::Layout::C;
 
     std::cout << "createInput input data dims[0] " << dims[0] << "dims[1]" << dims[1] << std::endl;
-    TensorDesc td(IRBuilder::g_layer_precision, dims, layout);
+    TensorDesc td(g_layer_precision, dims, layout);
 
     auto inputData = std::make_shared<InferenceEngine::Data>(name, td);
     InferenceEngine::InputInfo::Ptr info(new InferenceEngine::InputInfo());
@@ -365,7 +368,7 @@ void IRDocument::saveToIR(std::ostream &binFile, pugi::xml_node &parent, const I
     layer.append_attribute("type").set_value(irLayer->type.c_str());
     layer.append_attribute("id").set_value(irLayer->userValue.v_int);
     layer.append_attribute("precision")
-        .set_value(IRBuilder::g_layer_precision == Precision::FP16 ? "FP16" : "FP32");
+        .set_value(g_layer_precision == Precision::FP16 ? "FP16" : "FP32");
 
     if (!irLayer->params.empty()) {
         auto attr = layer.append_child("data");  // todo: need to check for type and overide it
@@ -421,3 +424,7 @@ void IRDocument::setName(const char *name) {
     _name = name;
     network->setName(_name);
 }
+}  // namespace nnhal
+}  // namespace neuralnetworks
+}  // namespace hardware
+}  // namespace android
diff --git a/intel_nn_hal/graphAPI/IRDocument.h b/intel_nn_hal/graphAPI/IRDocument.h
index 0b91657..f9f60e3 100644
--- a/intel_nn_hal/graphAPI/IRDocument.h
+++ b/intel_nn_hal/graphAPI/IRDocument.h
@@ -32,36 +32,32 @@
 
 #pragma once
 #include "IRLayer.h"
-#include "ie_icnn_network.hpp"
 #include "ie_common.h"
+#include "ie_icnn_network.hpp"
 
-
+namespace android {
+namespace hardware {
+namespace neuralnetworks {
+namespace nnhal {
 class InternalNetworkImpl;
-
-namespace IRBuilder
-{
-
-class IRDocument
-{
+class IRDocument {
 private:
-    struct Edge
-    {
-        struct port
-        {
+    struct Edge {
+        struct port {
             int lid, pid;
         } from, to;
     };
 
     InternalNetworkImpl *network;
-    std::vector<IRLayer> _layers; // ordered by input propagation
+    std::vector<IRLayer> _layers;  // ordered by input propagation
     std::vector<Edge> _edges;
     std::string _name;
     size_t _layer_id_cnt = 1;
     bool _processed = false;
 
-    std::map<const float *, size_t> _segmentsMap={}; //org
+    std::map<const float *, size_t> _segmentsMap = {};  // org
 
-    //std::map<const short*, size_t> _segmentsMap;
+    // std::map<const short*, size_t> _segmentsMap;
 
     static bool shouldRemove(const IRLayer &l);
     void process(const IRLayer &value);
@@ -70,19 +66,20 @@ private:
 
     // saving functions
     static void saveOutputToIR(pugi::xml_node &parent, const InferenceEngine::DataPtr &port);
-    static void saveInputToIR(pugi::xml_node &parent, int index, const InferenceEngine::DataPtr &port);
+    static void saveInputToIR(pugi::xml_node &parent, int index,
+                              const InferenceEngine::DataPtr &port);
 
     // save a layer
     void saveToIR(std::ostream &binFile, pugi::xml_node &parent, const IRLayer &irLayer);
     // svae a blob
-    void saveBlobToIR(std::ostream &binFile, const InferenceEngine::Blob::Ptr &blob, pugi::xml_node &layer, const std::string &name);
+    void saveBlobToIR(std::ostream &binFile, const InferenceEngine::Blob::Ptr &blob,
+                      pugi::xml_node &layer, const std::string &name);
 
     void saveLayerToDot(std::ostream &dot, const IRLayer &irLayer) const;
     IRDocument(IRDocument &) = delete;
     IRDocument operator=(IRDocument &) = delete;
 
 public:
-
     explicit IRDocument(const std::string &cs);
     ~IRDocument();
 
@@ -95,7 +92,8 @@ public:
 
     DelayObj createDelay(const std::string &id, const TensorDims &dims);
 
-    InferenceEngine::InputInfo::Ptr createInput(const std::string &name, const TensorDims &dims) const;
+    InferenceEngine::InputInfo::Ptr createInput(const std::string &name,
+                                                const TensorDims &dims) const;
     InferenceEngine::ICNNNetwork *buildNetwork();
 
     void addOutput(const IRLayer &src, int outIndx = 0);
@@ -104,4 +102,7 @@ public:
     InferenceEngine::ICNNNetwork *getNetwork();
 };
 
-}  // namespace IRBuilder
+}  // namespace nnhal
+}  // namespace neuralnetworks
+}  // namespace hardware
+}  // namespace android
diff --git a/intel_nn_hal/graphAPI/IRLayer.cpp b/intel_nn_hal/graphAPI/IRLayer.cpp
index 8c50d15..d5bcb89 100644
--- a/intel_nn_hal/graphAPI/IRLayer.cpp
+++ b/intel_nn_hal/graphAPI/IRLayer.cpp
@@ -32,11 +32,14 @@
 
 #include "IRLayers.h"
 
-using namespace IRBuilder;
+namespace android {
+namespace hardware {
+namespace neuralnetworks {
+namespace nnhal {
 
-int IRBuilder::layer_name_count = 0;
+int layer_name_count = 0;
 
-InferenceEngine::Precision IRBuilder::g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
+InferenceEngine::Precision g_layer_precision = InferenceEngine::Precision::UNSPECIFIED;
 
 const std::string ActivationLayer::Sigmoid("sigmoid");
 
@@ -44,8 +47,11 @@ const std::string ActivationLayer::Tanh("tanh");
 
 const std::string ActivationLayer::ReLU("ReLU");
 
-void IRBuilder::operator>>(const InferenceEngine::DataPtr &lhs,
-                           const InferenceEngine::CNNLayerPtr &rhs) {
+void operator>>(const InferenceEngine::DataPtr &lhs, const InferenceEngine::CNNLayerPtr &rhs) {
     lhs->inputTo[rhs->name] = rhs;
     rhs->insData.push_back(lhs);
 }
+}  // namespace nnhal
+}  // namespace neuralnetworks
+}  // namespace hardware
+}  // namespace android
diff --git a/intel_nn_hal/graphAPI/IRLayer.h b/intel_nn_hal/graphAPI/IRLayer.h
index eb4d196..cd871bd 100644
--- a/intel_nn_hal/graphAPI/IRLayer.h
+++ b/intel_nn_hal/graphAPI/IRLayer.h
@@ -31,20 +31,20 @@
  */
 
 #pragma once
+#include <memory>
+#include <sstream>
 #include <string>
 #include <vector>
-#include <sstream>
-#include <memory>
 #include "file_utils.h"
-#include "w_unistd.h"
-#include "ie_common.h"
 #include "ie_api.h"
-#include "ie_layouts.h"
-#include "ie_input_info.hpp"
 #include "ie_blob.h"
+#include "ie_common.h"
+#include "ie_input_info.hpp"
 #include "ie_layers.h"
-#include "pugixml.hpp"
 #include "ie_layers_property.hpp"
+#include "ie_layouts.h"
+#include "pugixml.hpp"
+#include "w_unistd.h"
 
 #ifdef _WIN32
 #define strncasecmp _strnicmp
@@ -52,28 +52,26 @@
 
 namespace FileUtils {
 // Hack: should be in file utils, this is to avoid chaning master
-inline std::string GetCWD()
-{
-	char cwd[4096];
-	return getcwd(cwd, 4095);
+inline std::string GetCWD() {
+    char cwd[4096];
+    return getcwd(cwd, 4095);
 }
-};
-
+};  // namespace FileUtils
 
-namespace IRBuilder
-{
+namespace android {
+namespace hardware {
+namespace neuralnetworks {
+namespace nnhal {
 
-template<typename T>
-std::string &operator<<(std::string &&src, T i)
-{
+template <typename T>
+std::string &operator<<(std::string &&src, T i) {
     std::stringstream oss;
     oss << i;
     return src.append(oss.str());
 }
 
-template<typename T>
-std::string operator<<(const std::string &src, T i)
-{
+template <typename T>
+std::string operator<<(const std::string &src, T i) {
     std::stringstream oss;
     oss << src;
     oss << i;
@@ -81,61 +79,55 @@ std::string operator<<(const std::string &src, T i)
 }
 
 #define THROW(x) THROW_IE_EXCEPTION << x
-#define IR_ASSERT(x) if (!(x)) THROW_IE_EXCEPTION << "Assert failed for " #x << ": "
+#define IR_ASSERT(x) \
+    if (!(x)) THROW_IE_EXCEPTION << "Assert failed for " #x << ": "
 
-typedef InferenceEngine::CNNLayer::Ptr 	IRLayer;
-typedef InferenceEngine::DataPtr 	OutputPort;
-typedef InferenceEngine::Blob 		IRBlob;
+typedef InferenceEngine::CNNLayer::Ptr IRLayer;
+typedef InferenceEngine::DataPtr OutputPort;
+typedef InferenceEngine::Blob IRBlob;
 
-struct Vector
-{
+struct Vector {
     uint32_t length;
     IRBlob::Ptr data;
 };
 
-struct DelayObj
-{
-    IRLayer in_t; // x(t)
-    IRLayer out_t_1; // x(t-1)
+struct DelayObj {
+    IRLayer in_t;     // x(t)
+    IRLayer out_t_1;  // x(t-1)
 };
 
 typedef InferenceEngine::SizeVector TensorDims;
 
-inline size_t sizeOf(const TensorDims &dims)
-{
-    size_t ret = dims[0];
-    for(int i = 1; i < dims.size(); ++i) ret *= dims[i];
-    return ret;
-}
-
 void operator>>(const InferenceEngine::DataPtr &lhs, const InferenceEngine::CNNLayerPtr &rhs);
 
-inline void operator>>(const InferenceEngine::CNNLayerPtr &lhs, const InferenceEngine::CNNLayerPtr &rhs)
-{
+inline void operator>>(const InferenceEngine::CNNLayerPtr &lhs,
+                       const InferenceEngine::CNNLayerPtr &rhs) {
     lhs->outData[0] >> rhs;
 }
 
 template <typename T>
-IRBlob::Ptr readBlobFromFile(const std::string &file)
-{
+IRBlob::Ptr readBlobFromFile(const std::string &file) {
     auto fs = FileUtils::fileSize(file);
     if (fs <= 0) THROW("blob file ") << file << " not found or empty";
     InferenceEngine::Precision precision = sizeof(T) == sizeof(short)
-        ? InferenceEngine::Precision::FP16 : InferenceEngine::Precision::FP32;
-    auto ret = typename InferenceEngine::TBlob<T>::Ptr(
-        new InferenceEngine::TBlob<T>(precision, InferenceEngine::C,
-        { static_cast<size_t>(fs / sizeof(T)) }));
+                                               ? InferenceEngine::Precision::FP16
+                                               : InferenceEngine::Precision::FP32;
+    auto ret = typename InferenceEngine::TBlob<T>::Ptr(new InferenceEngine::TBlob<T>(
+        precision, InferenceEngine::C, {static_cast<size_t>(fs / sizeof(T))}));
     ret->allocate();
     FileUtils::readAllFile(file, ret->data(), fs);
     return ret;
 }
 
 template <typename T>
-IRBlob::Ptr readBlobFromFile(const std::string &file, const TensorDims &dims, InferenceEngine::Layout l)
-{
+IRBlob::Ptr readBlobFromFile(const std::string &file, const TensorDims &dims,
+                             InferenceEngine::Layout l) {
     auto data = readBlobFromFile<T>(file);
     data->Reshape(dims, l);
     return data;
 }
 
-}  // namespace IRBuilder
+}  // namespace nnhal
+}  // namespace neuralnetworks
+}  // namespace hardware
+}  // namespace android
diff --git a/intel_nn_hal/graphAPI/IRLayers.h b/intel_nn_hal/graphAPI/IRLayers.h
index f75cc9e..3ce7c94 100644
--- a/intel_nn_hal/graphAPI/IRLayers.h
+++ b/intel_nn_hal/graphAPI/IRLayers.h
@@ -31,11 +31,11 @@
  */
 
 #pragma once
-#include "IRLayer.h"
+#include <cassert>
 #include "IRDocument.h"
+#include "IRLayer.h"
 #include "file_utils.h"
 #include "ie_common.h"
-#include <cassert>
 #include "ie_layers_property.hpp"
 
 //#define LOG_TAG "graphAPI"
@@ -45,41 +45,38 @@
 #include <log/log.h>
 #endif
 
-namespace IRBuilder
-{
+namespace android {
+namespace hardware {
+namespace neuralnetworks {
+namespace nnhal {
 
 extern int layer_name_count;
 extern InferenceEngine::Precision g_layer_precision;
+inline size_t sizeOf(const TensorDims &dims);
 
-inline OutputPort addOutput(const IRLayer &layer, const InferenceEngine::SizeVector &dims)
-{
+inline OutputPort addOutput(const IRLayer &layer, const InferenceEngine::SizeVector &dims) {
     std::string d_name = layer->name;
-    if(!layer->outData.empty())
-    {
+    if (!layer->outData.empty()) {
         std::stringstream oss;
         oss << d_name << ":" << layer->outData.size();
         d_name = oss.str();
     }
     OutputPort data;
-    if(dims.size() == 2)
-    {
-        std::cout << "addOutput dims size 2"<< std::endl;
+    if (dims.size() == 2) {
+        std::cout << "addOutput dims size 2" << std::endl;
         InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::NC);
         data = std::make_shared<InferenceEngine::Data>(d_name, td);
 
-    }
-    else if(dims.size() == 4)
-    {
-        std::cout << "addOutput dims size "<< dims.size()<<std::endl;
-        //InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::ANY);
+    } else if (dims.size() == 4) {
+        std::cout << "addOutput dims size " << dims.size() << std::endl;
+        // InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::ANY);
         InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::NCHW);
-        //InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::NHWC);
+        // InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::NHWC);
         data = std::make_shared<InferenceEngine::Data>(d_name, td);
 
-    }
-    else {
-        std::cout << "addOutput dims size "<< dims.size()<<std::endl;
-        //InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::ANY);
+    } else {
+        std::cout << "addOutput dims size " << dims.size() << std::endl;
+        // InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::ANY);
         InferenceEngine::TensorDesc td(g_layer_precision, dims, InferenceEngine::Layout::C);
         data = std::make_shared<InferenceEngine::Data>(d_name, td);
     }
@@ -87,32 +84,30 @@ inline OutputPort addOutput(const IRLayer &layer, const InferenceEngine::SizeVec
     layer->outData.push_back(data);
     data->creatorLayer = layer;
 
-    #ifdef NNLOG
+#ifdef NNLOG
     std::vector<size_t> outdims = data->getTensorDesc().getDims();
-    for (int i=0; i< outdims.size(); i++) {
-      ALOGI("addOutput data dims[%d] = %lu ", i, outdims[i]);
+    for (int i = 0; i < outdims.size(); i++) {
+        ALOGI("addOutput data dims[%d] = %lu ", i, outdims[i]);
     }
-    #endif
+#endif
 
     return data;
 }
 
-
-template<typename T>
-void addAttr(IRLayer layer, const std::string &a_name, T val)
-{
+template <typename T>
+void addAttr(IRLayer layer, const std::string &a_name, T val) {
     std::stringstream oss;
     oss << val;
     layer->params[a_name] = oss.str();
 };
-template<typename T, typename S>
-std::shared_ptr<T> As(const std::shared_ptr<S> &src)
-{ return /*std::dynamic_pointer_cast<T>(src)*/std::static_pointer_cast<T>(src); }  //aks
-
+template <typename T, typename S>
+std::shared_ptr<T> As(const std::shared_ptr<S> &src) {
+    return /*std::dynamic_pointer_cast<T>(src)*/ std::static_pointer_cast<T>(src);
+}  // aks
 
 /*
-* @brief Creates a generic layer with one input and one output
-*/
+ * @brief Creates a generic layer with one input and one output
+ */
 
 inline IRLayer Generic(const std::string &type) {
     std::string name = type + "-";  // todo: make it unique
@@ -124,9 +119,8 @@ inline IRLayer Generic(const std::string &type) {
     return std::make_shared<InferenceEngine::CNNLayer>(prms);
 }
 
-inline IRLayer Generic(const std::string &type, const OutputPort &src)
-{
-    std::string name = type + "-"; // todo: make it unique
+inline IRLayer Generic(const std::string &type, const OutputPort &src) {
+    std::string name = type + "-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prms;
     prms.precision = g_layer_precision;
@@ -138,105 +132,93 @@ inline IRLayer Generic(const std::string &type, const OutputPort &src)
     return layer;
 }
 
+inline OutputPort output(const IRLayer &src, int index = 0) { return src->outData[index]; }
 
-inline OutputPort output(const IRLayer &src, int index = 0)
-{
-    return src->outData[index];
-}
-
-inline IRLayer LayerOf(const OutputPort &src)
-{
-  return src->creatorLayer.lock();
-}
+inline IRLayer LayerOf(const OutputPort &src) { return src->creatorLayer.lock(); }
 
-inline IRLayer Generic(const std::string &type, const IRLayer &src)
-{
+inline IRLayer Generic(const std::string &type, const IRLayer &src) {
     return Generic(type, output(src));
 }
 
-template<typename T, typename A>
-std::string dumpVec(std::vector<T, A> const &vec)
-{
-    if(vec.empty()) return "[]";
+template <typename T, typename A>
+std::string dumpVec(std::vector<T, A> const &vec) {
+    if (vec.empty()) return "[]";
     std::stringstream oss;
     oss << "[" << vec[0];
-    for(size_t i = 1; i < vec.size(); i++) oss << "," << vec[i];
+    for (size_t i = 1; i < vec.size(); i++) oss << "," << vec[i];
     oss << "]";
     return oss.str();
 }
 
-namespace FCLayer
-{
-static IRLayer create(const IRBlob::Ptr &weights, const OutputPort &src)
-{
-    #ifdef NNLOG
+namespace FCLayer {
+static IRLayer create(const IRBlob::Ptr &weights, const OutputPort &src) {
+#ifdef NNLOG
     ALOGI("Create FC layer");
-    #endif
-    std::string name = "FC-"; // todo: make it unique
+#endif
+    std::string name = "FC-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
     prm.name = name;
 
-    //auto inDims = src->getDims(); // (batch, IFM)
-    auto inDims = src->getTensorDesc().getDims(); // (batch, IFM)
-    //std::cout << "inDims size "<<inDims.size()<< "inDims[0] "<<inDims[0]<< "inDims[1] "<<inDims[1]<< std::endl;
-
+    // auto inDims = src->getDims(); // (batch, IFM)
+    auto inDims = src->getTensorDesc().getDims();  // (batch, IFM)
+    // std::cout << "inDims size "<<inDims.size()<< "inDims[0] "<<inDims[0]<< "inDims[1]
+    // "<<inDims[1]<< std::endl;
 
     auto wDim = weights->getTensorDesc().getDims();
-    //std::cout << "wDim size "<<wDim.size()<<"wDim[0] "<<wDim[0]<< "wDim[1] "<<wDim[1]<< std::endl;
+    // std::cout << "wDim size "<<wDim.size()<<"wDim[0] "<<wDim[0]<< "wDim[1] "<<wDim[1]<<
+    // std::endl;
 
     IR_ASSERT(inDims.size() == 2);
 
     unsigned int ofm = 0;
-    if (wDim.size() == 2)
-    {
-        //std::cout << "inDims[1]"<<inDims[1]<< "wDim[1]" <<wDim[1]<< std::endl;
+    if (wDim.size() == 2) {
+        // std::cout << "inDims[1]"<<inDims[1]<< "wDim[1]" <<wDim[1]<< std::endl;
 
-        #ifdef NNLOG
+#ifdef NNLOG
         ALOGI("inDims[0] = %d inDims[1] = %d", inDims[0], inDims[1]);
         ALOGI("wDim[0] = %d wDim[1] = %d", wDim[0], wDim[1]);
-        #endif
+#endif
 
-        IR_ASSERT(inDims[1] == wDim[1]); // Weights: (Out,In)
-        ofm = static_cast<unsigned int>(wDim[0]); // Out
-    } else if (wDim.size()==1) // linear, just a blob, line in IR
+        IR_ASSERT(inDims[1] == wDim[1]);           // Weights: (Out,In)
+        ofm = static_cast<unsigned int>(wDim[0]);  // Out
+    } else if (wDim.size() == 1)                   // linear, just a blob, line in IR
     {
         ofm = static_cast<unsigned int>(weights->size() / inDims[1]);
-        IR_ASSERT(inDims[1]*ofm == weights->size()); // should be divided properly
+        IR_ASSERT(inDims[1] * ofm == weights->size());  // should be divided properly
     } else
         THROW_IE_EXCEPTION << "expecting weights for FC only as 1 dim (blob) or 2 dim (Matrix)";
 
-
     auto fc = std::make_shared<InferenceEngine::FullyConnectedLayer>(prm);
     fc->type = "FullyConnected";
 
     fc->_out_num = ofm;
-	  addAttr(fc, "out-size ", ofm);  //aks added
+    addAttr(fc, "out-size ", ofm);  // aks added
     // todo: assert that input should be cols
     addOutput(fc, {inDims[0], static_cast<uint32_t>(fc->_out_num)});
     src >> fc;
     fc->_weights = weights;
-    fc->blobs["weights"] = weights; // todo: have setter for those layers...
+    fc->blobs["weights"] = weights;  // todo: have setter for those layers...
     return fc;
 }
-};
-
+};  // namespace FCLayer
 
-inline InferenceEngine::CNNLayer::Ptr operator*(const IRBlob::Ptr &weights, const IRLayer &b)
-{
-    std::cout << "FCLayer::create operator*(const IRBlob::Ptr &weights, const IRLayer &b)"<< std::endl;
+inline InferenceEngine::CNNLayer::Ptr operator*(const IRBlob::Ptr &weights, const IRLayer &b) {
+    std::cout << "FCLayer::create operator*(const IRBlob::Ptr &weights, const IRLayer &b)"
+              << std::endl;
     return FCLayer::create(weights, output(b));
 }
 
-inline OutputPort operator*(const IRBlob::Ptr &weights, const OutputPort &op)
-{
-    std::cout << "FCLayer::create operator*(const IRBlob::Ptr &weights, const OutputPort &op)"<< std::endl;
+inline OutputPort operator*(const IRBlob::Ptr &weights, const OutputPort &op) {
+    std::cout << "FCLayer::create operator*(const IRBlob::Ptr &weights, const OutputPort &op)"
+              << std::endl;
     return output(FCLayer::create(weights, op));
 }
 
-static OutputPort ScaleShiftNode(const OutputPort &src, const IRBlob::Ptr &scale, const IRBlob::Ptr &bias) {
-    std::cout << "ScaleShiftNode"<< std::endl;
+static OutputPort ScaleShiftNode(const OutputPort &src, const IRBlob::Ptr &scale,
+                                 const IRBlob::Ptr &bias) {
+    std::cout << "ScaleShiftNode" << std::endl;
     std::string name = "ConstMul-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
@@ -253,7 +235,6 @@ static OutputPort ScaleShiftNode(const OutputPort &src, const IRBlob::Ptr &scale
     return addOutput(l, src->getTensorDesc().getDims());
 }
 
-
 /*
 inline IRLayer AddConst(const IRLayer &lhs, const IRBlob::Ptr &biases)
 {
@@ -264,8 +245,8 @@ inline IRLayer AddConst(const IRLayer &lhs, const IRBlob::Ptr &biases)
       fc->blobs["biases"] = biases;
       return lhs; // it was fused with prev layer
     } else {
-	// need to create an add with Const here using ScaleShift with no weights...
-	    THROW_IE_EXCEPTION << "not implemented yet" ;
+        // need to create an add with Const here using ScaleShift with no weights...
+            THROW_IE_EXCEPTION << "not implemented yet" ;
     }
 
 }
@@ -275,10 +256,10 @@ inline OutputPort AddTryConst(const OutputPort &src, const IRBlob::Ptr &biases)
     auto fc = As<InferenceEngine::WeightableLayer>(LayerOf(src));
     if (fc) {
         // todo: check if biases was not lready being set
-        std::cout << "AddTryConst"<< std::endl;
-        #ifdef NNLOG
+        std::cout << "AddTryConst" << std::endl;
+#ifdef NNLOG
         ALOGI("AddTryConst for biases");
-        #endif
+#endif
 
         fc->_biases = biases;
         fc->blobs["biases"] = biases;
@@ -290,7 +271,6 @@ inline OutputPort AddTryConst(const OutputPort &src, const IRBlob::Ptr &biases)
     }
 }
 
-
 inline OutputPort operator+(const OutputPort &src, const IRBlob::Ptr &biases) {
     return AddTryConst(src, biases);
 }
@@ -302,11 +282,9 @@ inline OutputPort operator+(const OutputPort &src, const IRBlob::Ptr &biases)
 }
 */
 
-namespace ConvLayer
-{
-static IRLayer create(const OutputPort &src)
-{
-    std::string name = "Conv-"; // todo: make it unique
+namespace ConvLayer {
+static IRLayer create(const OutputPort &src) {
+    std::string name = "Conv-";  // todo: make it unique
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
     name = name << layer_name_count++;
@@ -316,39 +294,21 @@ static IRLayer create(const OutputPort &src)
     src >> conv_layer;
     return conv_layer;
 }
-};
+};  // namespace ConvLayer
 
-struct Point2D
-{
+struct Point2D {
     int x, y;
 
-    inline int size() const
-    { return x * y; }
+    inline int size() const { return x * y; }
 };
-inline Point2D operator+(const Point2D &a, const Point2D &b)
-{
-    return {a.x + b.x, a.y + b.y};
-}
-inline Point2D operator-(const Point2D &a, const Point2D &b)
-{
-    return {a.x - b.x, a.y - b.y};
-}
-inline Point2D operator*(const Point2D &a, const Point2D &b)
-{
-    return {a.x * b.x, a.y * b.y};
-}
-inline Point2D operator/(const Point2D &a, const Point2D &b)
-{
-    return {a.x / b.x, a.y / b.y};
-}
-inline Point2D operator+(const Point2D &a, const int &rhs)
-{
-    return {a.x + rhs, a.y + rhs};
-}
-
-struct ConvolutionParams
-{
-    int groups=1;
+inline Point2D operator+(const Point2D &a, const Point2D &b) { return {a.x + b.x, a.y + b.y}; }
+inline Point2D operator-(const Point2D &a, const Point2D &b) { return {a.x - b.x, a.y - b.y}; }
+inline Point2D operator*(const Point2D &a, const Point2D &b) { return {a.x * b.x, a.y * b.y}; }
+inline Point2D operator/(const Point2D &a, const Point2D &b) { return {a.x / b.x, a.y / b.y}; }
+inline Point2D operator+(const Point2D &a, const int &rhs) { return {a.x + rhs, a.y + rhs}; }
+
+struct ConvolutionParams {
+    int groups = 1;
     Point2D kernel, stride = {1}, pad_start = {0}, pad_end = {0};
     int num_output_planes;
     IRBlob::Ptr weights;
@@ -356,18 +316,16 @@ struct ConvolutionParams
     std::string padType;
 };
 
-inline size_t in_ch(const OutputPort &src)
-{
+inline size_t in_ch(const OutputPort &src) {
     auto dims = src->getTensorDesc().getDims();
     return dims.size() == 4 ? dims[1] : dims[2];
 }
-inline OutputPort Convolution(const OutputPort &src, const ConvolutionParams &prms)
-{
+inline OutputPort Convolution(const OutputPort &src, const ConvolutionParams &prms) {
     auto ret = As<InferenceEngine::ConvolutionLayer>(ConvLayer::create(src));
     auto inDims = src->getTensorDesc().getDims();
     IR_ASSERT(inDims.size() == 4);
-    //IR_ASSERT(prms.kernel.size() * n(src) * prms.num_output_planes == prms.weights->size());
-	  IR_ASSERT((prms.kernel.size() * in_ch(src) * prms.num_output_planes)/prms.groups == prms.weights->size());
+    IR_ASSERT((prms.kernel.size() * in_ch(src) * prms.num_output_planes) / prms.groups ==
+              prms.weights->size());
 
     ret->_weights = prms.weights;
     ret->blobs["weights"] = prms.weights;
@@ -395,7 +353,8 @@ inline OutputPort Convolution(const OutputPort &src, const ConvolutionParams &pr
     ret->_group = prms.groups;
     ret->_out_depth = prms.num_output_planes;
 
-    //<data dilation-x="1" dilation-y="1" group="1" kernel-x="3" kernel-y="3" output="8" pad-x="0" pad-y="0" stride="1,1,2,2" stride-x="2" stride-y="2"/>
+    //<data dilation-x="1" dilation-y="1" group="1" kernel-x="3" kernel-y="3" output="8" pad-x="0"
+    // pad-y="0" stride="1,1,2,2" stride-x="2" stride-y="2"/>
 
     ret->params["auto_pad"] = prms.padType;
     ret->params["dilation-x"] = std::to_string(ret->_dilation.at(InferenceEngine::X_AXIS));
@@ -412,77 +371,82 @@ inline OutputPort Convolution(const OutputPort &src, const ConvolutionParams &pr
     ret->params["stride-x"] = std::to_string(ret->_stride.at(InferenceEngine::X_AXIS));
     ret->params["stride-y"] = std::to_string(ret->_stride.at(InferenceEngine::Y_AXIS));
 
-
-    #ifdef NNLOG
-        ALOGI("Convolution  prms.groups = %d kernel.x= %d kernel.y= %d stride.x= %d stride.y= %d pad_start.x= %d pad_start.y= %d \
-        pad_end.x= %d pad_end.y= %d ", prms.groups, \
-        ret->_kernel.at(InferenceEngine::X_AXIS), ret->_kernel.at(InferenceEngine::Y_AXIS), \
-        ret->_stride.at(InferenceEngine::X_AXIS), ret->_stride.at(InferenceEngine::Y_AXIS), \
-        ret->_padding.at(InferenceEngine::X_AXIS), ret->_padding.at(InferenceEngine::Y_AXIS), \
-        ret->_pads_end.at(InferenceEngine::X_AXIS), ret->_pads_end.at(InferenceEngine::Y_AXIS));
-    #endif
+#ifdef NNLOG
+    ALOGI(
+        "Convolution  prms.groups = %d kernel.x= %d kernel.y= %d stride.x= %d stride.y= %d pad_start.x= %d pad_start.y= %d \
+        pad_end.x= %d pad_end.y= %d ",
+        prms.groups, ret->_kernel.at(InferenceEngine::X_AXIS),
+        ret->_kernel.at(InferenceEngine::Y_AXIS), ret->_stride.at(InferenceEngine::X_AXIS),
+        ret->_stride.at(InferenceEngine::Y_AXIS), ret->_padding.at(InferenceEngine::X_AXIS),
+        ret->_padding.at(InferenceEngine::Y_AXIS), ret->_pads_end.at(InferenceEngine::X_AXIS),
+        ret->_pads_end.at(InferenceEngine::Y_AXIS));
+#endif
 
     if (prms.padType == "explicit") {
-          Point2D in_size = {static_cast<int>(inDims[3]), static_cast<int>(inDims[2])};
-          //Point2D out_size = (in_size + prms.pad_start + prms.pad_end - prms.kernel + prms.stride) / prms.stride + 1;
-          Point2D out_size = (in_size - prms.kernel + prms.stride + prms.pad_start + prms.pad_end ) / prms.stride;
-
-          addOutput(ret, {inDims[0], (size_t) prms.num_output_planes, (size_t) out_size.y, (size_t) out_size.x}); //nchw
-          //addOutput(ret, {inDims[0], (size_t) out_size.y, (size_t) out_size.x, (size_t) prms.num_output_planes}); //nhwc
-    }
-    else {
-
-          //Calculate output height and width for uneven padding
-          size_t inputN = inDims[0];
-          size_t IH = inDims[2];
-          size_t IW = inDims[3];
-          size_t KH = 0, KW = 0;
-          float OH_temp, OW_temp;
-
-          if (ret->_dilation[InferenceEngine::Y_AXIS])
-              KH = (ret->_kernel[InferenceEngine::Y_AXIS] - 1) * ret->_dilation[InferenceEngine::Y_AXIS] + 1;
-          else
-              KH = ret->_kernel[InferenceEngine::Y_AXIS];
-          if (ret->_dilation[InferenceEngine::X_AXIS])
-              KW = (ret->_kernel[InferenceEngine::X_AXIS] - 1) * ret->_dilation[InferenceEngine::X_AXIS] + 1;
-          else
-              KW = ret->_kernel[InferenceEngine::X_AXIS];
-
-          size_t SH = ret->_stride[InferenceEngine::Y_AXIS];
-          size_t SW = ret->_stride[InferenceEngine::X_AXIS];
-          size_t PH = ret->_padding[InferenceEngine::Y_AXIS];
-          size_t PW = ret->_padding[InferenceEngine::X_AXIS];
-          size_t OC = ret->_out_depth;
-
-          if (prms.padType == "valid") {
-              OH_temp = std::ceil((IH - KH + 1.f) / SH);
-              OW_temp = std::ceil((IW - KW + 1.f) / SW);
-          } else if (prms.padType == "same_upper") {
-              OH_temp = std::ceil(1.f * IH / SH);
-              OW_temp = std::ceil(1.f * IW / SW);
-          } else if (prms.padType == "same_lower") {
-              OH_temp = std::floor(1.f * IH / SH);
-              OW_temp = std::floor(1.f * IW / SW);
-          }
-
-          size_t OH = static_cast<size_t>(OH_temp);
-          size_t OW = static_cast<size_t>(OW_temp);
-          addOutput(ret, {inputN, OC, OH, OW});
+        Point2D in_size = {static_cast<int>(inDims[3]), static_cast<int>(inDims[2])};
+        // Point2D out_size = (in_size + prms.pad_start + prms.pad_end - prms.kernel + prms.stride)
+        // / prms.stride + 1;
+        Point2D out_size =
+            (in_size - prms.kernel + prms.stride + prms.pad_start + prms.pad_end) / prms.stride;
+
+        addOutput(ret, {inDims[0], (size_t)prms.num_output_planes, (size_t)out_size.y,
+                        (size_t)out_size.x});  // nchw
+        // addOutput(ret, {inDims[0], (size_t) out_size.y, (size_t) out_size.x, (size_t)
+        // prms.num_output_planes}); //nhwc
+    } else {
+        // Calculate output height and width for uneven padding
+        size_t inputN = inDims[0];
+        size_t IH = inDims[2];
+        size_t IW = inDims[3];
+        size_t KH = 0, KW = 0;
+        float OH_temp, OW_temp;
+
+        if (ret->_dilation[InferenceEngine::Y_AXIS])
+            KH = (ret->_kernel[InferenceEngine::Y_AXIS] - 1) *
+                     ret->_dilation[InferenceEngine::Y_AXIS] +
+                 1;
+        else
+            KH = ret->_kernel[InferenceEngine::Y_AXIS];
+        if (ret->_dilation[InferenceEngine::X_AXIS])
+            KW = (ret->_kernel[InferenceEngine::X_AXIS] - 1) *
+                     ret->_dilation[InferenceEngine::X_AXIS] +
+                 1;
+        else
+            KW = ret->_kernel[InferenceEngine::X_AXIS];
+
+        size_t SH = ret->_stride[InferenceEngine::Y_AXIS];
+        size_t SW = ret->_stride[InferenceEngine::X_AXIS];
+        size_t PH = ret->_padding[InferenceEngine::Y_AXIS];
+        size_t PW = ret->_padding[InferenceEngine::X_AXIS];
+        size_t OC = ret->_out_depth;
+
+        if (prms.padType == "valid") {
+            OH_temp = std::ceil((IH - KH + 1.f) / SH);
+            OW_temp = std::ceil((IW - KW + 1.f) / SW);
+        } else if (prms.padType == "same_upper") {
+            OH_temp = std::ceil(1.f * IH / SH);
+            OW_temp = std::ceil(1.f * IW / SW);
+        } else if (prms.padType == "same_lower") {
+            OH_temp = std::floor(1.f * IH / SH);
+            OW_temp = std::floor(1.f * IW / SW);
+        }
+
+        size_t OH = static_cast<size_t>(OH_temp);
+        size_t OW = static_cast<size_t>(OW_temp);
+        addOutput(ret, {inputN, OC, OH, OW});
     }
 
     return output(ret);
 }
-struct BatchNormParams
-{
+struct BatchNormParams {
     float epsilon;
     IRBlob::Ptr weights;
     IRBlob::Ptr bias;
 };
 
-inline IRLayer BatchNormalization(const OutputPort &src, BatchNormParams &prms)
-{
+inline IRLayer BatchNormalization(const OutputPort &src, BatchNormParams &prms) {
     auto inp = src;
-    std::string name = "BatchNormalization-"; // todo: make it unique
+    std::string name = "BatchNormalization-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -497,10 +461,10 @@ inline IRLayer BatchNormalization(const OutputPort &src, BatchNormParams &prms)
     return l;
 }
 
-inline OutputPort LRN(const OutputPort &src, float alpha, float beta, int local_size, bool isAcross=true, float k=1)
-{
+inline OutputPort LRN(const OutputPort &src, float alpha, float beta, int local_size,
+                      bool isAcross = true, float k = 1) {
     auto inp = src;
-    std::string name = "Norm-"; // todo: make it unique
+    std::string name = "Norm-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -517,13 +481,10 @@ inline OutputPort LRN(const OutputPort &src, float alpha, float beta, int local_
     return addOutput(l, inp->getTensorDesc().getDims());
 }
 
-inline OutputPort Crop(const OutputPort &src,
-                         const std::vector<int> &axis,
-                         const std::vector<int> &dim,
-                         const std::vector<int> &offset)
-{
+inline OutputPort Crop(const OutputPort &src, const std::vector<int> &axis,
+                       const std::vector<int> &dim, const std::vector<int> &offset) {
     auto inp = src;
-    std::string name = "Crop-"; // todo: make it unique
+    std::string name = "Crop-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -538,14 +499,10 @@ inline OutputPort Crop(const OutputPort &src,
     return addOutput(l, sv);
 }
 
-inline OutputPort Pooling(const OutputPort &inp,
-                            const Point2D &kernel,
-                            const Point2D &stride,
-                            const Point2D &pad,
-                            InferenceEngine::PoolingLayer::PoolType type)
-{
+inline OutputPort Pooling(const OutputPort &inp, const Point2D &kernel, const Point2D &stride,
+                          const Point2D &pad, InferenceEngine::PoolingLayer::PoolType type) {
     auto src = inp;
-    std::string name = "Pooling-"; // todo: make it unique
+    std::string name = "Pooling-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -553,14 +510,14 @@ inline OutputPort Pooling(const OutputPort &inp,
     auto ret = std::make_shared<InferenceEngine::PoolingLayer>(prm);
     ret->type = "Pooling";
 
-/*
-    ret->_kernel_x = kernel.x;
-    ret->_kernel_y = kernel.y;
-    ret->_stride_x = stride.x;
-    ret->_stride_y = stride.y;
-    ret->_padding_x = pad.x;
-    ret->_padding_y = pad.y;
-*/
+    /*
+        ret->_kernel_x = kernel.x;
+        ret->_kernel_y = kernel.y;
+        ret->_stride_x = stride.x;
+        ret->_stride_y = stride.y;
+        ret->_padding_x = pad.x;
+        ret->_padding_y = pad.y;
+    */
     ret->_kernel.clear();
     ret->_kernel.insert(InferenceEngine::X_AXIS, kernel.x);
     ret->_kernel.insert(InferenceEngine::Y_AXIS, kernel.y);
@@ -580,128 +537,122 @@ inline OutputPort Pooling(const OutputPort &inp,
     // todo: handle uneven padding
     Point2D out_size = (in_size + pad + pad - kernel + stride) / stride;
     src >> ret;
-    addOutput(ret, {inDims[0], inDims[1], (size_t) out_size.y, (size_t) out_size.x});
+    addOutput(ret, {inDims[0], inDims[1], (size_t)out_size.y, (size_t)out_size.x});
     return output(ret);
 }
 
-inline OutputPort Pooling(const OutputPort &inp,
-                            const Point2D &kernel,
-                            const Point2D &stride,
-                            const Point2D &pad_start,
-                            const Point2D &pad_end,
-                            std::string padType,
-                            InferenceEngine::PoolingLayer::PoolType type)
-{
-      auto src = inp;
-      std::string name = "Pooling-"; // todo: make it unique
-      name = name << layer_name_count++;
-      InferenceEngine::LayerParams prm;
-      prm.precision = g_layer_precision;
-      prm.name = name;
-      auto ret = std::make_shared<InferenceEngine::PoolingLayer>(prm);
-      ret->type = "Pooling";
-
-      ret->_kernel.clear();
-      ret->_kernel.insert(InferenceEngine::X_AXIS, kernel.x);
-      ret->_kernel.insert(InferenceEngine::Y_AXIS, kernel.y);
-      ret->_stride.clear();
-      ret->_stride.insert(InferenceEngine::X_AXIS, stride.x);
-      ret->_stride.insert(InferenceEngine::Y_AXIS, stride.y);
-      ret->_padding.clear();
-      ret->_padding.insert(InferenceEngine::X_AXIS, pad_start.x);
-      ret->_padding.insert(InferenceEngine::Y_AXIS,  pad_start.y);
-      ret->_pads_end.clear();
-      ret->_pads_end.insert(InferenceEngine::X_AXIS, pad_end.x);
-      ret->_pads_end.insert(InferenceEngine::Y_AXIS, pad_end.y);
-      ret->_type = type;
-      ret->_exclude_pad = true;
-
-      #ifdef NNLOG
-//        ALOGI("Pooling  kernel.x= %d kernel.y= %d stride.x= %d stride.y= %d pad_start.x= %d pad_start.y= %d \
+inline OutputPort Pooling(const OutputPort &inp, const Point2D &kernel, const Point2D &stride,
+                          const Point2D &pad_start, const Point2D &pad_end, std::string padType,
+                          InferenceEngine::PoolingLayer::PoolType type) {
+    auto src = inp;
+    std::string name = "Pooling-";  // todo: make it unique
+    name = name << layer_name_count++;
+    InferenceEngine::LayerParams prm;
+    prm.precision = g_layer_precision;
+    prm.name = name;
+    auto ret = std::make_shared<InferenceEngine::PoolingLayer>(prm);
+    ret->type = "Pooling";
+
+    ret->_kernel.clear();
+    ret->_kernel.insert(InferenceEngine::X_AXIS, kernel.x);
+    ret->_kernel.insert(InferenceEngine::Y_AXIS, kernel.y);
+    ret->_stride.clear();
+    ret->_stride.insert(InferenceEngine::X_AXIS, stride.x);
+    ret->_stride.insert(InferenceEngine::Y_AXIS, stride.y);
+    ret->_padding.clear();
+    ret->_padding.insert(InferenceEngine::X_AXIS, pad_start.x);
+    ret->_padding.insert(InferenceEngine::Y_AXIS, pad_start.y);
+    ret->_pads_end.clear();
+    ret->_pads_end.insert(InferenceEngine::X_AXIS, pad_end.x);
+    ret->_pads_end.insert(InferenceEngine::Y_AXIS, pad_end.y);
+    ret->_type = type;
+    ret->_exclude_pad = true;
+
+#ifdef NNLOG
+    //        ALOGI("Pooling  kernel.x= %d kernel.y= %d stride.x= %d stride.y= %d pad_start.x= %d pad_start.y= %d \
 //        pad_end.x= %d pad_end.y= %d ", kernel.x, kernel.y, stride.x, stride.y, pad_start.x, pad_start.y, pad_end.x, pad_end.y);
-      #endif
-      //<data exclude-pad="true" kernel-x="4" kernel-y="4" pad-x="0" pad-y="0" pool-method="avg" stride="1,1,2,2" stride-x="2" stride-y="2"/>
-      ret->params["auto_pad"] = padType;
-      ret->params["_exclude_pad"] = std::to_string(ret->_exclude_pad);
-      ret->params["kernel-x"] = std::to_string(ret->_kernel.at(InferenceEngine::X_AXIS));
-      ret->params["kernel-y"] = std::to_string(ret->_kernel.at(InferenceEngine::Y_AXIS));
-      ret->params["pad-begin-x"] = std::to_string(ret->_padding.at(InferenceEngine::X_AXIS));
-      ret->params["pad-begin-y"] = std::to_string(ret->_padding.at(InferenceEngine::Y_AXIS));
-      ret->params["pad-end-x"] = std::to_string(ret->_pads_end.at(InferenceEngine::X_AXIS));
-      ret->params["pad-end-y"] = std::to_string(ret->_pads_end.at(InferenceEngine::Y_AXIS));
-      std::string poolingType = ret->_type == InferenceEngine::PoolingLayer::PoolType::AVG ? "avg" : "max";
-      ret->params["pool-method"] = poolingType; //std::to_string(poolingType);
-      ret->params["stride-x"] = std::to_string(ret->_stride.at(InferenceEngine::X_AXIS));
-      ret->params["stride-y"] = std::to_string(ret->_stride.at(InferenceEngine::Y_AXIS));
-
-      src >> ret;
-
-      auto inDims = src->getTensorDesc().getDims();
-
-      if (padType == "explicit") {
-          Point2D in_size = {static_cast<int>(inDims[3]), static_cast<int>(inDims[2])};
-          // todo: handle uneven padding
-          Point2D out_size = (in_size - kernel + pad_start + pad_end + stride) / stride; // add stride-1 to round ceiling
-
-          addOutput(ret, {inDims[0], inDims[1], (size_t) out_size.y, (size_t) out_size.x});
-      } else {
-      //Calculate output height and width for uneven padding
-          float OHTemp = 1.f, OWTemp = 1.f;
-          size_t inputN = inDims[0];
-          size_t IC = inDims[1];
-          size_t IH = inDims[2];
-          size_t IW = inDims[3];
-          size_t KH = ret->_kernel[InferenceEngine::Y_AXIS];
-          size_t KW = ret->_kernel[InferenceEngine::X_AXIS];
-          size_t SH = ret->_stride[InferenceEngine::Y_AXIS];
-          size_t SW = ret->_stride[InferenceEngine::X_AXIS];
-          size_t PH = ret->_padding[InferenceEngine::Y_AXIS];
-          size_t PW = ret->_padding[InferenceEngine::X_AXIS];
-
-          if (padType == "valid") {
-              OHTemp = std::ceil((IH - KH + 1.f) / SH);
-              OWTemp = std::ceil((IW - KW + 1.f) / SW);
-          } else if (padType == "same_upper") {
-              OHTemp = std::ceil(1.f * IH / SH);
-              OWTemp = std::ceil(1.f * IW / SW);
-          } else if (padType == "same_lower") {
-              OHTemp = std::floor(1.f * IH / SH);
-              OWTemp = std::floor(1.f * IW / SW);
-          }
-
-          size_t OH = static_cast<size_t>(OHTemp);
-          size_t OW = static_cast<size_t>(OWTemp);
-          addOutput(ret, {inputN, IC, OH, OW});
-      }
-
-      return output(ret);
-}
+#endif
+    //<data exclude-pad="true" kernel-x="4" kernel-y="4" pad-x="0" pad-y="0" pool-method="avg"
+    // stride="1,1,2,2" stride-x="2" stride-y="2"/>
+    ret->params["auto_pad"] = padType;
+    ret->params["_exclude_pad"] = std::to_string(ret->_exclude_pad);
+    ret->params["kernel-x"] = std::to_string(ret->_kernel.at(InferenceEngine::X_AXIS));
+    ret->params["kernel-y"] = std::to_string(ret->_kernel.at(InferenceEngine::Y_AXIS));
+    ret->params["pad-begin-x"] = std::to_string(ret->_padding.at(InferenceEngine::X_AXIS));
+    ret->params["pad-begin-y"] = std::to_string(ret->_padding.at(InferenceEngine::Y_AXIS));
+    ret->params["pad-end-x"] = std::to_string(ret->_pads_end.at(InferenceEngine::X_AXIS));
+    ret->params["pad-end-y"] = std::to_string(ret->_pads_end.at(InferenceEngine::Y_AXIS));
+    std::string poolingType =
+        ret->_type == InferenceEngine::PoolingLayer::PoolType::AVG ? "avg" : "max";
+    ret->params["pool-method"] = poolingType;  // std::to_string(poolingType);
+    ret->params["stride-x"] = std::to_string(ret->_stride.at(InferenceEngine::X_AXIS));
+    ret->params["stride-y"] = std::to_string(ret->_stride.at(InferenceEngine::Y_AXIS));
 
+    src >> ret;
 
-namespace SumLayer
-{
-   static IRLayer create(const OutputPort &src1, const OutputPort &src2)
-   {
-       std::string name = "Sum-"; // todo: make it unique
-       name = name << layer_name_count++;
-       InferenceEngine::LayerParams prm;
-       prm.precision = g_layer_precision;
-       prm.name = name;
-       auto sum = std::make_shared<InferenceEngine::EltwiseLayer>(prm);
-       sum->type = "Eltwise";
-       src1 >> sum;
-       src2 >> sum;
-       if(src1->getTensorDesc().getDims() != src2->getTensorDesc().getDims()) THROW_IE_EXCEPTION << "input sizes for Element wise Sum do not match";
-       addOutput(sum, src1->getTensorDesc().getDims());
-       return sum;
-   }
-};
+    auto inDims = src->getTensorDesc().getDims();
 
-namespace MulLayer
-{
-static IRLayer create(const OutputPort &src1, const OutputPort &src2)
-{
-    std::string name = "Mul-"; // todo: make it unique
+    if (padType == "explicit") {
+        Point2D in_size = {static_cast<int>(inDims[3]), static_cast<int>(inDims[2])};
+        // todo: handle uneven padding
+        Point2D out_size = (in_size - kernel + pad_start + pad_end + stride) /
+                           stride;  // add stride-1 to round ceiling
+
+        addOutput(ret, {inDims[0], inDims[1], (size_t)out_size.y, (size_t)out_size.x});
+    } else {
+        // Calculate output height and width for uneven padding
+        float OHTemp = 1.f, OWTemp = 1.f;
+        size_t inputN = inDims[0];
+        size_t IC = inDims[1];
+        size_t IH = inDims[2];
+        size_t IW = inDims[3];
+        size_t KH = ret->_kernel[InferenceEngine::Y_AXIS];
+        size_t KW = ret->_kernel[InferenceEngine::X_AXIS];
+        size_t SH = ret->_stride[InferenceEngine::Y_AXIS];
+        size_t SW = ret->_stride[InferenceEngine::X_AXIS];
+        size_t PH = ret->_padding[InferenceEngine::Y_AXIS];
+        size_t PW = ret->_padding[InferenceEngine::X_AXIS];
+
+        if (padType == "valid") {
+            OHTemp = std::ceil((IH - KH + 1.f) / SH);
+            OWTemp = std::ceil((IW - KW + 1.f) / SW);
+        } else if (padType == "same_upper") {
+            OHTemp = std::ceil(1.f * IH / SH);
+            OWTemp = std::ceil(1.f * IW / SW);
+        } else if (padType == "same_lower") {
+            OHTemp = std::floor(1.f * IH / SH);
+            OWTemp = std::floor(1.f * IW / SW);
+        }
+
+        size_t OH = static_cast<size_t>(OHTemp);
+        size_t OW = static_cast<size_t>(OWTemp);
+        addOutput(ret, {inputN, IC, OH, OW});
+    }
+
+    return output(ret);
+}
+
+namespace SumLayer {
+static IRLayer create(const OutputPort &src1, const OutputPort &src2) {
+    std::string name = "Sum-";  // todo: make it unique
+    name = name << layer_name_count++;
+    InferenceEngine::LayerParams prm;
+    prm.precision = g_layer_precision;
+    prm.name = name;
+    auto sum = std::make_shared<InferenceEngine::EltwiseLayer>(prm);
+    sum->type = "Eltwise";
+    src1 >> sum;
+    src2 >> sum;
+    if (src1->getTensorDesc().getDims() != src2->getTensorDesc().getDims())
+        THROW_IE_EXCEPTION << "input sizes for Element wise Sum do not match";
+    addOutput(sum, src1->getTensorDesc().getDims());
+    return sum;
+}
+};  // namespace SumLayer
+
+namespace MulLayer {
+static IRLayer create(const OutputPort &src1, const OutputPort &src2) {
+    std::string name = "Mul-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -711,23 +662,21 @@ static IRLayer create(const OutputPort &src1, const OutputPort &src2)
     mul->_operation = InferenceEngine::EltwiseLayer::Prod;
     src1 >> mul;
     src2 >> mul;
-    if(src1->getTensorDesc().getDims() != src2->getTensorDesc().getDims()) THROW_IE_EXCEPTION << "input sizes for Element wise Mul do not match";
+    if (src1->getTensorDesc().getDims() != src2->getTensorDesc().getDims())
+        THROW_IE_EXCEPTION << "input sizes for Element wise Mul do not match";
     addOutput(mul, src1->getTensorDesc().getDims());
     return mul;
 }
-};
+};  // namespace MulLayer
 
-inline OutputPort operator*(const OutputPort &a, const OutputPort &b)
-{
+inline OutputPort operator*(const OutputPort &a, const OutputPort &b) {
     return output(MulLayer::create(a, b));
 }
 
-namespace ScaleShift
-{
+namespace ScaleShift {
 
-static OutputPort Diagnoal(const Vector &weights, const OutputPort &src)
-{
-    std::string name = "ConstMul-"; // todo: make it unique
+static OutputPort Diagnoal(const Vector &weights, const OutputPort &src) {
+    std::string name = "ConstMul-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -737,17 +686,16 @@ static OutputPort Diagnoal(const Vector &weights, const OutputPort &src)
     src >> l;
     addOutput(l, src->getTensorDesc().getDims());
     l->_weights = weights.data;
-    if(weights.length == 1) l->_broadcast = 0;
-    else if(weights.length == src->getTensorDesc().getDims()[1])
-    { l->_broadcast = 1; }
+    if (weights.length == 1)
+        l->_broadcast = 0;
+    else if (weights.length == src->getTensorDesc().getDims()[1]) {
+        l->_broadcast = 1;
+    }
 
     return output(l);
 }
-static InferenceEngine::CNNLayer::Ptr create(OutputPort src,
-                                             IRBlob::Ptr scale,
-                                             IRBlob::Ptr bias)
-{
-    std::string name = "ConstMul-"; // todo: make it unique
+static InferenceEngine::CNNLayer::Ptr create(OutputPort src, IRBlob::Ptr scale, IRBlob::Ptr bias) {
+    std::string name = "ConstMul-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -758,60 +706,49 @@ static InferenceEngine::CNNLayer::Ptr create(OutputPort src,
     l->_weights = scale;
     l->_broadcast = false;
     addOutput(l, src->getTensorDesc().getDims());
-    //AddConst(l, bias);
+    // AddConst(l, bias);
     return l;
 }
-};
+};  // namespace ScaleShift
 
-inline OutputPort operator*(const Vector &weights, const IRLayer &b)
-{
+inline OutputPort operator*(const Vector &weights, const IRLayer &b) {
     return (ScaleShift::Diagnoal(weights, output(b)));
 }
 
-inline OutputPort operator*(const Vector &weights, const OutputPort &op)
-{
+inline OutputPort operator*(const Vector &weights, const OutputPort &op) {
     return (ScaleShift::Diagnoal(weights, op));
 }
 
-namespace ActivationLayer
-{
+namespace ActivationLayer {
 extern const std::string Sigmoid;
 
 extern const std::string Tanh;
 
 extern const std::string ReLU;
 
-static IRLayer create(const OutputPort &src, const std::string &type)
-{
-    std::string name = type + "-"; // todo: make it unique
+static IRLayer create(const OutputPort &src, const std::string &type) {
+    std::string name = type + "-";  // todo: make it unique
     name = name << layer_name_count++;
     IRLayer layer;
-    if((strncasecmp(type.c_str(), "relu", type.size()) == 0))
-    {
+    if ((strncasecmp(type.c_str(), "relu", type.size()) == 0)) {
         InferenceEngine::LayerParams prm;
         prm.precision = g_layer_precision;
         prm.name = name;
         layer = std::make_shared<InferenceEngine::ReLULayer>(prm);
         layer->type = "ReLU";
-    }
-    else if((strncasecmp(type.c_str(), "tanh", type.size()) == 0))
-    {
+    } else if ((strncasecmp(type.c_str(), "tanh", type.size()) == 0)) {
         InferenceEngine::LayerParams prm;
         prm.precision = g_layer_precision;
         prm.name = name;
         layer = std::make_shared<InferenceEngine::TanHLayer>(prm);
         layer->type = "TanH";
-    }
-    else if((strncasecmp(type.c_str(), "sigmoid", type.size()) == 0))
-    {
+    } else if ((strncasecmp(type.c_str(), "sigmoid", type.size()) == 0)) {
         InferenceEngine::LayerParams prm;
         prm.precision = g_layer_precision;
         prm.name = name;
         layer = std::make_shared<InferenceEngine::SigmoidLayer>(prm);
         layer->type = "Sigmoid";
-    }
-    else
-    {
+    } else {
         InferenceEngine::LayerParams prm;
         prm.precision = g_layer_precision;
         prm.name = name;
@@ -823,47 +760,41 @@ static IRLayer create(const OutputPort &src, const std::string &type)
     src >> layer;
 
     std::vector<size_t> dims = src->getTensorDesc().getDims();
-    #ifdef NNLOG
-    for (int i=0; i< dims.size(); i++) {
-      ALOGI("Activation function output dims[%d] = %lu ", i, dims[i]);
+#ifdef NNLOG
+    for (int i = 0; i < dims.size(); i++) {
+        ALOGI("Activation function output dims[%d] = %lu ", i, dims[i]);
     }
-    #endif
+#endif
 
     addOutput(layer, src->getTensorDesc().getDims());
     return layer;
 }
 
-static IRLayer create(const IRLayer &src, const std::string &type)
-{
+static IRLayer create(const IRLayer &src, const std::string &type) {
     return create(output(src), type);
 }
 
-};
+};  // namespace ActivationLayer
 
-template<typename T>
-OutputPort ReLU(const T &src)
-{
+template <typename T>
+OutputPort ReLU(const T &src) {
     return output(ActivationLayer::create(src, ActivationLayer::ReLU));
 }
 
-template<typename T>
-OutputPort Sigmoid(const T &src)
-{
+template <typename T>
+OutputPort Sigmoid(const T &src) {
     return output(ActivationLayer::create(src, ActivationLayer::Sigmoid));
 }
 
-template<typename T>
-OutputPort Tanh(const T &src)
-{
+template <typename T>
+OutputPort Tanh(const T &src) {
     return output(ActivationLayer::create(src, ActivationLayer::Tanh));
 }
 
-namespace SplitUtil
-{
+namespace SplitUtil {
 
-static IRLayer create(int size, const OutputPort &src, int axis = 1)
-{
-    std::string name = "Split-"; // todo: make it unique
+static IRLayer create(int size, const OutputPort &src, int axis = 1) {
+    std::string name = "Split-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -875,29 +806,25 @@ static IRLayer create(int size, const OutputPort &src, int axis = 1)
     auto out_dim = src->getTensorDesc().getDims();
     // axis = static_cast<int>(out_dim.size()) - axis - 1; // todo: we are all in reverse here :-(
     out_dim[axis] = out_dim[axis] / size;
-    IR_ASSERT(out_dim[axis]*size == src->getTensorDesc().getDims()[axis]);
+    IR_ASSERT(out_dim[axis] * size == src->getTensorDesc().getDims()[axis]);
 
-    for(int i = 0; i < size; i++)
-    {
+    for (int i = 0; i < size; i++) {
         addOutput(me, out_dim);
     }
     return me;
 }
-};
+};  // namespace SplitUtil
 
-inline std::vector<OutputPort> Split(const OutputPort &src, int splitElements, int axis = 1)
-{
+inline std::vector<OutputPort> Split(const OutputPort &src, int splitElements, int axis = 1) {
     return SplitUtil::create(splitElements, src, axis)->outData;
 }
 
-inline std::vector<OutputPort> Split(const IRLayer &src, int splitElements, int axis = 1)
-{
+inline std::vector<OutputPort> Split(const IRLayer &src, int splitElements, int axis = 1) {
     return Split(output(src), splitElements, axis);
 }
 
-inline OutputPort Concat(const std::vector<OutputPort> inputs, int axis = 1)
-{
-    std::string name = "Concat-"; // todo: make it unique
+inline OutputPort Concat(const std::vector<OutputPort> inputs, int axis = 1) {
+    std::string name = "Concat-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -910,8 +837,7 @@ inline OutputPort Concat(const std::vector<OutputPort> inputs, int axis = 1)
     // it was fixed, should be backward compatiobale though...
     // axis = static_cast<int>(outDim.size()) - axis - 1; // todo: we are all in reverse here :-(
     auto axisSize = outDim[axis];
-    for(int i = 1; i < inputs.size(); ++i)
-    {
+    for (int i = 1; i < inputs.size(); ++i) {
         inputs[i] >> ret;
         axisSize += inputs[i]->getTensorDesc().getDims()[axis];
     }
@@ -919,10 +845,9 @@ inline OutputPort Concat(const std::vector<OutputPort> inputs, int axis = 1)
     return addOutput(ret, outDim);
 }
 
-//template<typename T>
-inline OutputPort Clamp(const OutputPort &src, float min, float max)
-{
-    std::string name = "Clamp-"; // todo: make it unique
+// template<typename T>
+inline OutputPort Clamp(const OutputPort &src, float min, float max) {
+    std::string name = "Clamp-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prms;
     prms.precision = g_layer_precision;
@@ -938,44 +863,43 @@ inline OutputPort Clamp(const OutputPort &src, float min, float max)
     return output(layer);
 }
 
-inline OutputPort L2Normalization(const OutputPort &src, bool isAcross, bool isShareChannel)
-{
+inline OutputPort L2Normalization(const OutputPort &src, bool isAcross, bool isShareChannel) {
     auto layer = Generic("Normalize", src);
     addAttr(layer, "across_spatial", isAcross ? 1 : 0);
     addAttr(layer, "channel_shared", isShareChannel ? 1 : 0);
     return output(layer);
 }
 
-inline OutputPort Reshape(const TensorDims &newDims, const OutputPort &src)
-{
-    if(sizeOf(src->getTensorDesc().getDims()) != sizeOf(newDims)) THROW("Cannot reorder different volumes");
-
-/*//first implementation
-    if(src->creatorLayer.lock()->type == "Reshape") // fuse reshapes
-    {
-        src->setDims(newDims);
-        return src;
-    }
-
-    auto op = output(Generic("Reshape", src));
-    op->setDims(newDims);
-
-    return op;
-*/
-//end of first implementation
-
-/*
- //FIX ME fuse reshape
-    //if(src->creatorLayer.lock()->type == "Reshape") // fuse reshapes
-    if(src->getCreatorLayer().lock()->type == "Reshape") // fuse reshapes
-    {
-        src->setDims(newDims);
-        return src;
-    }
-*/
- //latest implementation
-
-    std::string name = "Reshape-"; // todo: make it unique
+inline OutputPort Reshape(const TensorDims &newDims, const OutputPort &src) {
+    if (sizeOf(src->getTensorDesc().getDims()) != sizeOf(newDims))
+        THROW("Cannot reorder different volumes");
+
+    /*//first implementation
+        if(src->creatorLayer.lock()->type == "Reshape") // fuse reshapes
+        {
+            src->setDims(newDims);
+            return src;
+        }
+
+        auto op = output(Generic("Reshape", src));
+        op->setDims(newDims);
+
+        return op;
+    */
+    // end of first implementation
+
+    /*
+     //FIX ME fuse reshape
+        //if(src->creatorLayer.lock()->type == "Reshape") // fuse reshapes
+        if(src->getCreatorLayer().lock()->type == "Reshape") // fuse reshapes
+        {
+            src->setDims(newDims);
+            return src;
+        }
+    */
+    // latest implementation
+
+    std::string name = "Reshape-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prms;
     prms.precision = g_layer_precision;
@@ -983,76 +907,74 @@ inline OutputPort Reshape(const TensorDims &newDims, const OutputPort &src)
     auto layer = std::make_shared<InferenceEngine::ReshapeLayer>(prms);
     layer->type = "Reshape";
     src >> layer;
-    //addOutput(layer, src->getTensorDesc().getDims());
+    // addOutput(layer, src->getTensorDesc().getDims());
 
-   /*
-   brief A vector of sizes of the shape
-   std::vector<int> shape;
-   */
+    /*
+    brief A vector of sizes of the shape
+    std::vector<int> shape;
+    */
 
     layer->params["axis"] = std::to_string(layer->axis);
     layer->params["num_axes"] = std::to_string(layer->num_axes);
 
-/*  //check if mandatory to provide shape
-    for (int i = 0; i < newDims.size(); i++)
-    layer->shape[i] = static_cast<int>(newDims[i]);
-    // VectorToStringI(layer->shape)
-    std::string result;
-    const char sep = ',';
-    for (auto it : layer->shape) {
-        result += std::to_string(it) + sep;
-    }
-    if (!result.empty()) {
-        result = result.substr(0, result.size() - 2);
-    }
-
-   layer->params["dim"] = result;
-*/
-   addOutput(layer, newDims);
-   auto op = output(layer);
-    //op->setDims(newDims);
-
-/*
-    //FIX ME : HACK for [VPU] Unsupported 1D dimensions
-    if (op->getTensorDesc().getDims().size() == 1) {
-    TensorDims dims = {1, newDims[0]};
-    op->setDims(dims);
-    #ifdef NNLOG
-    ALOGI("Reshape oputput data set dims size = %lu ", op->getTensorDesc().getDims().size());
-    #endif
-    }
-*/
+    /*  //check if mandatory to provide shape
+        for (int i = 0; i < newDims.size(); i++)
+        layer->shape[i] = static_cast<int>(newDims[i]);
+        // VectorToStringI(layer->shape)
+        std::string result;
+        const char sep = ',';
+        for (auto it : layer->shape) {
+            result += std::to_string(it) + sep;
+        }
+        if (!result.empty()) {
+            result = result.substr(0, result.size() - 2);
+        }
+
+       layer->params["dim"] = result;
+    */
+    addOutput(layer, newDims);
+    auto op = output(layer);
+    // op->setDims(newDims);
+
+    /*
+        //FIX ME : HACK for [VPU] Unsupported 1D dimensions
+        if (op->getTensorDesc().getDims().size() == 1) {
+        TensorDims dims = {1, newDims[0]};
+        op->setDims(dims);
+        #ifdef NNLOG
+        ALOGI("Reshape oputput data set dims size = %lu ", op->getTensorDesc().getDims().size());
+        #endif
+        }
+    */
     return op;
-
 }
 
-static OutputPort Softmax(const OutputPort &src)
-{
-
+static OutputPort Softmax(const OutputPort &src) {
     auto inputDims = src->getTensorDesc().getDims();
-/*
-    //handle 2D and 4D tensors
-    TensorDims newDims;
-    if (inputDims.size() == 2) {
-        uint32_t batch_size = inputDims[0];//getSizeOfDimension(inputShape, 0);
-        uint32_t input_size = sizeOf(inputDims) / batch_size; //getNumberOfElements(inputShape) / batch_size;
-
-        newDims = {batch_size, input_size, 1, 1};
-        inputDims = newDims;
-
-
-    } else if (inputDims.size() == 4) {
-        //dim = convertShapeToDims(inputShape);
-        //newDims = inputDims;
-    } else {
-        #ifdef NNLOG
-        ALOGI("Softmax only 2D and 4D tensors supported");
-        #endif
-        //return false;
-    }
-
-*/
-    std::string name = "Softmax-"; // todo: make it unique
+    /*
+        //handle 2D and 4D tensors
+        TensorDims newDims;
+        if (inputDims.size() == 2) {
+            uint32_t batch_size = inputDims[0];//getSizeOfDimension(inputShape, 0);
+            uint32_t input_size = sizeOf(inputDims) / batch_size; //getNumberOfElements(inputShape)
+       / batch_size;
+
+            newDims = {batch_size, input_size, 1, 1};
+            inputDims = newDims;
+
+
+        } else if (inputDims.size() == 4) {
+            //dim = convertShapeToDims(inputShape);
+            //newDims = inputDims;
+        } else {
+            #ifdef NNLOG
+            ALOGI("Softmax only 2D and 4D tensors supported");
+            #endif
+            //return false;
+        }
+
+    */
+    std::string name = "Softmax-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -1060,20 +982,19 @@ static OutputPort Softmax(const OutputPort &src)
     auto l = std::make_shared<InferenceEngine::SoftMaxLayer>(prm);
     l->type = "SoftMax";
     src >> l;
-    //addOutput(l, src->getTensorDesc().getDims());
+    // addOutput(l, src->getTensorDesc().getDims());
     addOutput(l, inputDims);
 
     return output(l);
-/*
-    auto op = output(l);
-    op->setDims(newDims);
-    return op;
-*/
+    /*
+        auto op = output(l);
+        op->setDims(newDims);
+        return op;
+    */
 }
 
-inline OutputPort Gather(const std::vector<OutputPort> inputs, int axis = 1)
-{
-    std::string name = "Gather-"; // todo: make it unique
+inline OutputPort Gather(const std::vector<OutputPort> inputs, int axis = 1) {
+    std::string name = "Gather-";  // todo: make it unique
     name = name << layer_name_count++;
     InferenceEngine::LayerParams prm;
     prm.precision = g_layer_precision;
@@ -1084,15 +1005,13 @@ inline OutputPort Gather(const std::vector<OutputPort> inputs, int axis = 1)
     inputs[0] >> ret;
     inputs[1] >> ret;
     auto outDim = inputs[0]->getTensorDesc().getDims();
-    //axis = static_cast<int>(outDim.size()) - axis - 1; // todo: we are all in reverse here :-(
+    // axis = static_cast<int>(outDim.size()) - axis - 1; // todo: we are all in reverse here :-(
     outDim[0] = inputs[1]->getTensorDesc().getDims()[1];
     addOutput(ret, outDim);
     return output(ret);
 }
 
-
-inline OutputPort operator+(const OutputPort &a, const OutputPort &b)
-{
+inline OutputPort operator+(const OutputPort &a, const OutputPort &b) {
     return output(SumLayer::create(a, b));
 }
 
@@ -1111,4 +1030,7 @@ inline OutputPort AddConst(IRDocument &doc, const OutputPort &src, const IRBlob:
     return src + constOut;
 }
 
-}  // namespace IRBuilder
+}  // namespace nnhal
+}  // namespace neuralnetworks
+}  // namespace hardware
+}  // namespace android
diff --git a/intel_nn_hal/graphTests/helpers-test.hpp b/intel_nn_hal/graphTests/helpers-test.hpp
index 6f310d1..4ed3a38 100644
--- a/intel_nn_hal/graphTests/helpers-test.hpp
+++ b/intel_nn_hal/graphTests/helpers-test.hpp
@@ -12,7 +12,7 @@
 #include <android/log.h>
 #include <log/log.h>
 
-using namespace IRBuilder;
+using namespace ::android::hardware::neuralnetworks::nnhal;
 using namespace InferenceEngine;
 
 template <class T>
@@ -246,7 +246,7 @@ class ExecuteNetwork {
     InferRequest inferRequest;
     ResponseDesc resp;
 
-   public:
+public:
     ExecuteNetwork() : network(nullptr) {}
     ExecuteNetwork(IRDocument &doc, TargetDevice target = TargetDevice::eCPU) : network(nullptr) {
         InferenceEngine::PluginDispatcher dispatcher(
diff --git a/intel_nn_hal/graphTests/main.cpp b/intel_nn_hal/graphTests/main.cpp
index 0f303b6..7c542af 100644
--- a/intel_nn_hal/graphTests/main.cpp
+++ b/intel_nn_hal/graphTests/main.cpp
@@ -4,7 +4,7 @@
 #include <android/log.h>
 #include <log/log.h>
 
-using namespace IRBuilder;
+using namespace ::android::hardware::neuralnetworks::nnhal;
 
 template <typename T>
 void createAlexNet(IRDocument &doc) {
@@ -20,7 +20,7 @@ void createAlexNet(IRDocument &doc) {
 
     ConvolutionParams prms;
     //<convolution_data stride-x="4" stride-y="4" pad-x="0" pad-y="0" kernel-x="11" kernel-y="11"
-    //output="96" group="1"/>
+    // output="96" group="1"/>
     prms.weights = readBlobFromFile<T>("AlexNet-bins/conv1_weights.bin");
     prms.kernel = {11, 11};
     prms.stride = {4, 4};
@@ -32,13 +32,13 @@ void createAlexNet(IRDocument &doc) {
     //<norm_data alpha = "9.9999997e-05" beta = "0.75" local-size = "5" region = "across" / >
     auto n1 = LRN(r1, 0.0001f, 0.75f, 5, true);
     //<pooling_data kernel-x="3" kernel-y="3" pad-x="0" pad-y="0" stride-x="2" stride-y="2"
-    //rounding-type="ceil" pool-method="max"/>
+    // rounding-type="ceil" pool-method="max"/>
     auto p1 = Pooling(n1, {3, 3}, {2, 2}, {0, 0}, PoolingLayer::MAX);
 
     /// 2nd part
     auto s1 = Split(p1, 2);
     //<convolution_data stride-x="1" stride-y="1" pad-x="2" pad-y="2" kernel-x="5" kernel-y="5"
-    //output="128" group="1"/>
+    // output="128" group="1"/>
     prms.kernel = {5, 5};
     prms.stride = {1, 1};
     prms.pad_start = {2, 2};
@@ -53,12 +53,12 @@ void createAlexNet(IRDocument &doc) {
     auto relu2 = ReLU(c2);
     auto n2 = LRN(relu2, 0.0001f, 0.75f, 5, true);
     //<pooling_data kernel-x="3" kernel-y="3" pad-x="0" pad-y="0" stride-x="2" stride-y="2"
-    //rounding-type="ceil" pool-method="max"/>
+    // rounding-type="ceil" pool-method="max"/>
     auto p2 = Pooling(n2, {3, 3}, {2, 2}, {0, 0}, PoolingLayer::MAX);
 
     // 3rd part
     //<convolution_data stride-x="1" stride-y="1" pad-x="1" pad-y="1" kernel-x="3" kernel-y="3"
-    //output="384" group="1"/>
+    // output="384" group="1"/>
     prms.kernel = {3, 3};
     prms.stride = {1, 1};
     prms.pad_start = {1, 1};
@@ -70,7 +70,7 @@ void createAlexNet(IRDocument &doc) {
     // 4th part
     auto s4 = Split(c3, 2);
     //<convolution_data stride-x="1" stride-y="1" pad-x="1" pad-y="1" kernel-x="3" kernel-y="3"
-    //output="192" group="1"/>
+    // output="192" group="1"/>
     prms.num_output_planes = 192;
     prms.weights = readBlobFromFile<T>("AlexNet-bins/conv4_0_weights.bin");
     auto c4_0 =
@@ -81,7 +81,7 @@ void createAlexNet(IRDocument &doc) {
 
     // 5th part
     //<convolution_data stride-x="1" stride-y="1" pad-x="1" pad-y="1" kernel-x="3" kernel-y="3"
-    //output="128" group="1"/>
+    // output="128" group="1"/>
     prms.num_output_planes = 128;
     prms.weights = readBlobFromFile<T>("AlexNet-bins/conv5_0_weights.bin");
     auto c5_0 =
@@ -92,7 +92,7 @@ void createAlexNet(IRDocument &doc) {
     auto c5 = Concat({c5_0, c5_1});
 
     //<pooling_data kernel-x="3" kernel-y="3" pad-x="0" pad-y="0" stride-x="2" stride-y="2"
-    //rounding-type="ceil" pool-method="max"/>
+    // rounding-type="ceil" pool-method="max"/>
     auto p5 = Pooling(c5, {3, 3}, {2, 2}, {0, 0}, PoolingLayer::MAX);
 
     // fc6
@@ -382,7 +382,7 @@ bool testMKLBug() {
 
         ConvolutionParams prms;
         //<convolution_data stride-x="4" stride-y="4" pad-x="0" pad-y="0" kernel-x="11"
-        //kernel-y="11" output="96" group="1"/>
+        // kernel-y="11" output="96" group="1"/>
         prms.weights = readBlobFromFile<T>("AlexNet-bins/conv1_weights.bin");
         prms.kernel = {11, 11};
         prms.stride = {4, 4};
@@ -393,13 +393,13 @@ bool testMKLBug() {
         //<norm_data alpha = "9.9999997e-05" beta = "0.75" local-size = "5" region = "across" / >
         auto n1 = LRN(r1, 0.0001f, 0.75f, 5, true);
         //<pooling_data kernel-x="3" kernel-y="3" pad-x="0" pad-y="0" stride-x="2" stride-y="2"
-        //rounding-type="ceil" pool-method="max"/>
+        // rounding-type="ceil" pool-method="max"/>
         auto p1 = Pooling(n1, {3, 3}, {2, 2}, {0, 0}, PoolingLayer::MAX);
 
         /// 2nd part
         auto s1 = Split(p1, 2);
         //<convolution_data stride-x="1" stride-y="1" pad-x="2" pad-y="2" kernel-x="5" kernel-y="5"
-        //output="128" group="1"/>
+        // output="128" group="1"/>
         prms.kernel = {5, 5};
         prms.stride = {1, 1};
         prms.pad_end = prms.pad_start = {2, 2};
@@ -413,7 +413,7 @@ bool testMKLBug() {
         auto c2 = ReLU(conv2);
         auto n2 = LRN(c2, 0.0001f, 0.75f, 5, true);
         //<pooling_data kernel-x="3" kernel-y="3" pad-x="0" pad-y="0" stride-x="2" stride-y="2"
-        //rounding-type="ceil" pool-method="max"/>
+        // rounding-type="ceil" pool-method="max"/>
         auto p2 = Pooling(n2, {3, 3}, {2, 2}, {0, 0}, PoolingLayer::MAX);
 
         doc.addOutput(c2);
@@ -443,11 +443,11 @@ int main(int argc, const char *argv[]) {
     std::string inp;
 
 #ifdef ENABLE_MYRIAD
-    IRBuilder::g_layer_precision = InferenceEngine::Precision::FP16;
+    g_layer_precision = InferenceEngine::Precision::FP16;
     // testAlexNet();
     // testMKLBug<short>();
 #elif ENABLE_MKLDNN
-    IRBuilder::g_layer_precision = InferenceEngine::Precision::FP32;
+    g_layer_precision = InferenceEngine::Precision::FP32;
     // testAlexNet();
     // testMKLBug<float>();
 #endif
diff --git a/intel_nn_hal/service.cpp b/intel_nn_hal/service.cpp
index 2cde670..3013222 100644
--- a/intel_nn_hal/service.cpp
+++ b/intel_nn_hal/service.cpp
@@ -23,14 +23,14 @@
 
 using android::hardware::configureRpcThreadpool;
 using android::hardware::joinRpcThreadpool;
-using android::hardware::neuralnetworks::V1_0::driver::Driver;
+using android::hardware::neuralnetworks::nnhal::Driver;
 
 int main(int argc, char* argv[]) {
     if (argc > 2 && strlen(argv[2]) > 0) {
         if (strcmp(argv[1], "-D") != 0) return 0;
         const char* deviceType = argv[2];
         android::sp<Driver> device = new Driver(deviceType);
-        ALOGD("NN-HAL(%s) is ready.", deviceType);
+        ALOGD("NN-HAL-1.1(%s) is ready.", deviceType);
         configureRpcThreadpool(4, true);
         android::status_t status = device->registerAsService(deviceType);
         LOG_ALWAYS_FATAL_IF(status != android::OK, "Error while registering as service for %s: %d",
-- 
2.17.1

