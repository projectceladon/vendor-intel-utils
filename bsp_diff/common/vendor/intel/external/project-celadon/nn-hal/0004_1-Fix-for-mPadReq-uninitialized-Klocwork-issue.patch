From 080230febb7577e628f91f08d05355d4e11c1080 Mon Sep 17 00:00:00 2001
From: Jeevaka Prabu Badrappan <jeevaka.badrappan@intel.com>
Date: Tue, 24 Sep 2019 10:20:08 +0530
Subject: [PATCH] Fix for mPadReq uninitialized Klocwork issue

Tracked-On: OAM-86661
Signed-off-by: Jeevaka Prabu Badrappan <jeevaka.badrappan@intel.com>
---
 intel_nn_hal/PreparedModel.cpp | 2 --
 intel_nn_hal/PreparedModel.h   | 9 ++++++---
 2 files changed, 6 insertions(+), 5 deletions(-)

diff --git a/intel_nn_hal/PreparedModel.cpp b/intel_nn_hal/PreparedModel.cpp
index 2a53663..c747522 100644
--- a/intel_nn_hal/PreparedModel.cpp
+++ b/intel_nn_hal/PreparedModel.cpp
@@ -106,8 +106,6 @@ unsigned int debugMask = ((1 << (L1 + 1)) - 1);
 #define EXPL_PAD_PARAMS_DW_CONV 11
 #define IMPL_PAD_PARAMS_DW_CONV 8
 #define SOFTMAX_INPUT_PARAMS 2
-#define EXPL_PAD 1
-#define IMPL_PAD 2
 #define NHWC_DIM_NUM 4
 #define NHWC_CH_IDX 3
 #define NHWC_HT_IDX 1
diff --git a/intel_nn_hal/PreparedModel.h b/intel_nn_hal/PreparedModel.h
index cbb17de..b033c51 100644
--- a/intel_nn_hal/PreparedModel.h
+++ b/intel_nn_hal/PreparedModel.h
@@ -29,6 +29,9 @@
 #include "Driver.h"
 #include "IENetwork.h"
 
+#define EXPL_PAD 1
+#define IMPL_PAD 2
+
 using ::android::hidl::memory::V1_0::IMemory;
 using namespace InferenceEngine;
 
@@ -106,12 +109,12 @@ bool setRunTimePoolInfosFromHidlMemories(std::vector<RunTimePoolInfo>* poolInfos
 class PreparedModel : public IPreparedModel {
 public:
     PreparedModel(const Model& model)
-        : mTargetDevice(TargetDevice::eMYRIAD), mModel(model), mNet("nnNet"), enginePtr(nullptr), mPadreq(false) {
+        : mTargetDevice(TargetDevice::eMYRIAD), mModel(model), mNet("nnNet"), enginePtr(nullptr), mPadreq(EXPL_PAD) {
         g_layer_precision = InferenceEngine::Precision::FP16;
     }
 
     PreparedModel(const TargetDevice device, const Model& model)
-        : mTargetDevice(device), mModel(model), mNet("nnNet"), enginePtr(nullptr) {
+        : mTargetDevice(device), mModel(model), mNet("nnNet"), enginePtr(nullptr), mPadreq(EXPL_PAD) {
         if (mTargetDevice == TargetDevice::eCPU)
             g_layer_precision = InferenceEngine::Precision::FP32;
         else if (mTargetDevice == TargetDevice::eMYRIAD)
@@ -182,7 +185,7 @@ protected:
     IRDocument mNet;
     std::vector<OutputPort> mPorts;  // typedef std::shared_ptr<Data> DataPtr;
     ExecuteNetwork* enginePtr;
-    bool mPadreq;
+    uint32_t mPadreq;
 };
 
 class VpuPreparedModel : public PreparedModel {
-- 
2.17.1

