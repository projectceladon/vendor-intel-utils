From fb92b2922283e86d73685c2de7d3c96fa1542cc1 Mon Sep 17 00:00:00 2001
From: dengliqiang <liqiangx.deng@intel.com>
Date: Mon, 14 Oct 2019 23:57:02 +0800
Subject: [PATCH] Enable cldnn and cldnn_plugin compiling in Celadon

Tracked-On: OAM-86770
Signed-off-by: dengliqiang <liqiangx.deng@intel.com>
---
 .../include/cldnn/device/ks_primitive_db.inc  | 62160 ++++++++++++++++
 .../src/cldnn_engine/cldnn_engine.cpp         |     5 +
 .../src/cldnn_engine/cldnn_infer_request.cpp  |     6 +-
 3 files changed, 62170 insertions(+), 1 deletion(-)
 create mode 100644 inference-engine/include/cldnn/device/ks_primitive_db.inc

diff --git a/inference-engine/include/cldnn/device/ks_primitive_db.inc b/inference-engine/include/cldnn/device/ks_primitive_db.inc
new file mode 100644
index 00000000..81312a41
--- /dev/null
+++ b/inference-engine/include/cldnn/device/ks_primitive_db.inc
@@ -0,0 +1,62160 @@
+// This file is autogenerated by primitive_db_gen.py, all changes to this file will be undone
+
+{"convolution_grad_weights_yxfb",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_grad_weights_gpu_ref)(
+    const __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output,
+    __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    __global UNIT_TYPE* bias,
+#endif
+#if MOMENTUM
+    __global UNIT_TYPE* prev_grad_w,
+#if BIAS_TERM
+    __global UNIT_TYPE* prev_grad_b,
+#endif
+#endif
+    const __global UNIT_TYPE* input,
+    uint split_idx,
+    float lr)
+{
+    const uint local_id = get_local_id(0);
+    const uint ofm_ifm  = get_global_id(1);
+    const uint id_x_y   = get_global_id(2);
+
+    const uint id_x     = id_x_y % FILTER_SIZE_X;
+    const uint id_y     = id_x_y / FILTER_SIZE_X;
+    const uint ifm      = ofm_ifm % INPUT1_FEATURE_NUM;
+    const uint ofm      = ofm_ifm / INPUT1_FEATURE_NUM;
+
+    const int in_x      = id_x - PADDING_SIZE_X;
+    const int in_y      = id_y - PADDING_SIZE_Y;
+
+    ACCUMULATOR_TYPE grad_w = 0;
+#if BIAS_TERM
+    ACCUMULATOR_TYPE grad_b = 0;
+#endif
+
+    const uint grad_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_OFM_NUM;
+    const uint in_split_offset = split_idx * INPUT1_FEATURE_PITCH * FILTER_IFM_NUM;
+
+    uint weights_idx = ofm * FILTER_OFM_PITCH + ifm * FILTER_IFM_PITCH + id_y * FILTER_Y_PITCH + id_x * FILTER_X_PITCH;
+
+    for(int y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+)__krnl"
+R"__krnl(        const int input_offset_y = in_y + y * STRIDE_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT1_SIZE_Y || input_offset_y < 0;
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            const int input_offset_x = in_x + x * STRIDE_SIZE_X;
+            const bool zero_x = input_offset_x >= INPUT1_SIZE_X || input_offset_x < 0;
+            for (uint b = 0; b < INPUT0_BATCH_NUM / 16; b++)
+            {
+#if BIAS_TERM
+                uint input_grad_idx = grad_split_offset + b*16*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + x*INPUT0_X_PITCH + y*INPUT0_Y_PITCH;
+                UNIT_TYPE grad = as_float(intel_sub_group_block_read((const __global uint*)(input_grad + input_grad_idx)));
+                grad_b += grad;
+#endif
+                if(!zero_x && !zero_y)
+                {
+                uint input_idx = in_split_offset + b*16*INPUT1_BATCH_PITCH + ifm*INPUT1_FEATURE_PITCH + (uint)input_offset_x*INPUT1_X_PITCH + (uint)input_offset_y*INPUT1_Y_PITCH;
+#if BIAS_TERM
+                grad_w = fma(as_float(intel_sub_group_block_read((const __global uint*)(input + input_idx))), grad, grad_w);
+#else
+                uint input_grad_idx = grad_split_offset + b*16*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + x*INPUT0_X_PITCH + y*INPUT0_Y_PITCH;
+                grad_w = fma(as_float(intel_sub_group_block_read((const __global uint*)(input + input_idx))), as_float(intel_sub_group_block_read((const __global uint*)(input_grad + input_grad_idx))), grad_w);
+#endif
+                }
+            }
+        }
+    }
+
+    grad_w = sub_group_reduce_add(grad_w);
+#if BIAS_TERM
+    grad_b = sub_group_reduce_add(grad_b);
+#endif
+
+    if (local_id == 0)
+    {
+#if MOMENTUM
+        UNIT_TYPE update_gradient_w = lr * (grad_w + DECAY_RATE * filter[weights_idx]) + prev_grad_w[weights_idx] * MOMENTUM_FACTOR;
+        filter[weights_idx] -= update_gradient_w;
+        prev_grad_w[weights_idx] = update_gradient_w;
+#else
+        filter[weights_idx] -= lr * (grad_w + DECAY_RATE * filter[weights_idx]);
+#endif
+
+#if BIAS_TERM
+        if(ifm == 0 && id_x == 0 && id_y == 0)
+        {
+#if MOMENTUM
+            UNIT_TYPE update_gradient_b = lr * grad_b + prev_grad_b[ofm] * MOMENTUM_FACTOR;
+            bias[ofm] -= update_gradient_b;
+            prev_grad_b[ofm] = update_gradient_b;
+#else
+            bias[ofm] -= lr * grad_b;
+#endif
+        }
+#endif
+    }
+}
+
+)__krnl"},
+
+{"pooling_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if MAX_POOLING || MAX_WITH_ARGMAX_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_MIN
+#elif AVG_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_ZERO
+#else
+#error
+#endif
+
+
+inline UNIT_TYPE FUNC(apply_pooling)(UNIT_TYPE tmp, UNIT_TYPE in)
+{
+#if MAX_POOLING || MAX_WITH_ARGMAX_POOLING
+    return max(tmp, in);
+#elif AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+KERNEL(pooling_gpu)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output
+#if MAX_WITH_ARGMAX_POOLING
+, __global float* arg_max
+#endif
+)
+{
+#if OUTPUT_LAYOUT_BFYX  || OUTPUT_LAYOUT_BYXF
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf % INPUT0_FEATURE_NUM;
+    const uint b    = bf / INPUT0_FEATURE_NUM;
+
+    if (x >= OUTPUT_SIZE_X)
+    {
+        return;
+    }
+#elif OUTPUT_LAYOUT_YXFB
+    const uint x    = (uint)get_global_id(1);
+    const uint y    = (uint)get_global_id(2);
+    const uint bf   = (uint)get_global_id(0);
+    const uint f    = bf / INPUT0_BATCH_NUM;
+    const uint b    = bf % INPUT0_BATCH_NUM;
+)__krnl"
+R"__krnl(#endif
+
+    const int offset_x = (int)x*STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int offset_y = (int)y*STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    UNIT_TYPE result = UNIT_INIT_VAL;
+
+#if MAX_WITH_ARGMAX_POOLING
+    uint arg_max_idx = 0;
+#endif
+
+#ifdef CHECK_BOUNDRY
+    if (offset_x + POOL_SIZE_X < 0 || offset_x >= INPUT0_SIZE_X ||
+        offset_y + POOL_SIZE_Y < 0 || offset_y >= INPUT0_SIZE_Y)
+    {
+        return;
+    }
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    uint num_elementes = 0;
+#endif
+
+    const uint batch_and_feature_offset = GET_DATA_INDEX(INPUT0, b, f, 0, 0);
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        int input_offset_y = offset_y + j;
+        bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+        if(!zero_y)
+        {
+            for(uint i = 0; i < POOL_SIZE_X; i++)
+            {
+                int input_offset_x = offset_x + i;
+                bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+                if(!zero)
+                {
+                    const uint input_idx = batch_and_feature_offset + input_offset_y*INPUT0_Y_PITCH + input_offset_x*INPUT0_X_PITCH;
+
+#if MAX_WITH_ARGMAX_POOLING
+                    if(input[input_idx] > result)
+                    {
+                        const uint input_idx_bfyx_no_padding = input_offset_x + INPUT0_SIZE_X * (input_offset_y + INPUT0_SIZE_Y * (f + INPUT0_FEATURE_NUM * b));
+                        arg_max_idx = input_idx_bfyx_no_padding;
+                    }
+#endif
+                    result = FUNC_CALL(apply_pooling)(result, input[input_idx]);
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+                    num_elementes++;
+#endif
+                }
+            }
+        }
+    }
+#ifdef DYNAMIC_WITH_PADDING_KERNEL_DIVIDER
+    const int hend = min(offset_y + POOL_SIZE_Y, INPUT0_SIZE_Y + PADDING_SIZE_Y);
+    const int wend = min(offset_x + POOL_SIZE_X, INPUT0_SIZE_X + PADDING_SIZE_X);
+    const uint num_elementes = (hend - offset_y) * (wend - offset_x);
+#endif
+#else
+    uint input_idx = GET_DATA_INDEX(INPUT0, b, f, offset_y, offset_x);
+
+#if MAX_WITH_ARGMAX_POOLING
+    uint input_idx_bfyx_no_padding = offset_x + INPUT0_SIZE_X * (offset_y + INPUT0_SIZE_Y * (f + INPUT0_FEATURE_NUM * b));
+#endif
+
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        for(uint i = 0; i < POOL_SIZE_X; i++)
+        {
+
+#if MAX_WITH_ARGMAX_POOLING
+            if(input[input_idx] > result)
+                arg_max_idx = input_idx_bfyx_no_padding;
+#endif
+
+            result = FUNC_CALL(apply_pooling)(result, input[input_idx]);
+            input_idx += INPUT0_X_PITCH;
+#if MAX_WITH_ARGMAX_POOLING
+            input_idx_bfyx_no_padding++;
+#endif
+        }
+        input_idx += (INPUT0_Y_PITCH - POOL_SIZE_X*INPUT0_X_PITCH);
+#if MAX_WITH_ARGMAX_POOLING
+        input_idx_bfyx_no_padding += (INPUT0_SIZE_X - POOL_SIZE_X);
+#endif
+    }
+
+#if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+    const uint num_elementes = POOL_SIZE_X*POOL_SIZE_Y;
+#endif
+#endif
+
+#if defined AVG_POOLING
+    #if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+        result /= (UNIT_TYPE)max(num_elementes, (uint)1);
+    #else
+        result /= (UNIT_TYPE)(POOL_SIZE_Y * POOL_SIZE_X);
+    #endif
+#endif
+
+    const uint output_pos = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+    output[output_pos] = ACTIVATION(result, NL_M ,NL_N);
+
+#if MAX_WITH_ARGMAX_POOLING
+    //INPUT1 macro stands for Argmax
+    const uint arg_max_pos = GET_DATA_INDEX(INPUT1, b, f, y, x);
+    arg_max[arg_max_pos] = convert_float(arg_max_idx);
+#endif
+
+}
+
+#undef UNIT_INIT_VAL
+
+)__krnl"},
+
+{"activation_tutorial",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifdef ADVANCED_TUTORIAL
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(activation)(
+    __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output
+#ifdef PARAMETERIZED
+    , __global ADDITIONAL_PARAMS_TYPE* params
+#endif
+    )
+{
+#if defined OUTPUT_LAYOUT_YXFB                  // in Case of YXFB we need a different processing order than BFYX (from performance aspect)
+    const uint x = get_global_id(1);
+    const uint y = get_global_id(2);
+#if OUTPUT_BATCH_NUM == 1
+    const uint feature = get_global_id(0);
+    const uint batch = 0;
+#else
+    const uint feature = get_global_id(0) % OUTPUT_FEATURE_NUM;
+    const uint batch = get_global_id(0) / OUTPUT_FEATURE_NUM;
+#endif
+#else
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const uint feature = get_global_id(2);
+    const uint batch = 0;
+#else
+    const uint feature = get_global_id(2) % OUTPUT_FEATURE_NUM;
+    const uint batch = get_global_id(2) / OUTPUT_FEATURE_NUM;
+#endif
+#endif
+
+    const uint src_index = GET_DATA_INDEX(INPUT0, batch, feature, y, x);    // helper macro to deduce the buffer index.
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, batch, feature, y, x);
+
+#if defined PARAMETERIZED                                                   // in case that the input additional params is located on a bufffer
+    #if   PARAMS_NUM == 2
+        const float nl_m = (float)params[2*feature + 0];
+        const float nl_n = (float)params[2*feature + 1];
+    #elif PARAMS_NUM == 1
+        const float nl_m = (float)params[feature];
+        const float nl_n = (float)NL_N;
+)__krnl"
+R"__krnl(    #else
+        const float nl_m = (float)NL_M;
+        const float nl_n = (float)NL_N;
+    #endif
+#else
+    const float nl_m = (float)NL_M;
+    const float nl_n = (float)NL_N;
+#endif
+    output[dst_index] = ACTIVATION(input[src_index], nl_m, nl_n);           // Do the activation
+}
+
+#else
+
+//#include "put here your include files"
+
+__kernel void activation_tutorial(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output)
+{
+	// fill here your kernel
+}
+
+#endif
+
+)__krnl"},
+
+{"generic_eltwise_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if ELTWISE_LAYOUT_BASED || QUANTIZATION_TERM
+
+#define GET_INDEX(prefix, num)                                                          \
+    CAT(CAT(prefix, num), _OFFSET) +                                                    \
+    (d1 % CAT(CAT(prefix, num), _SIZE_X     ))*CAT(CAT(prefix, num), _X_PITCH) +         \
+    (d2 % CAT(CAT(prefix, num), _SIZE_Y     ))*CAT(CAT(prefix, num), _Y_PITCH) +         \
+    (d3 % CAT(CAT(prefix, num), _FEATURE_NUM))*CAT(CAT(prefix, num), _FEATURE_PITCH) +   \
+    (d4 % CAT(CAT(prefix, num), _BATCH_NUM  ))*CAT(CAT(prefix, num), _BATCH_PITCH)
+
+#elif ELTWISE_NO_PITCH_SAME_DIMS
+#define GET_INDEX(prefix, num)                                                      \
+    CAT(CAT(prefix, num), _OFFSET) + d1
+
+#else
+
+#define GET_INDEX(prefix, num)                                                      \
+    CAT(CAT(prefix, num), _OFFSET) +                                                \
+    (d1 % CAT(CAT(prefix, num), _SIZES)[0])*CAT(CAT(prefix, num), _PITCHES)[0] +    \
+    (d2 % CAT(CAT(prefix, num), _SIZES)[1])*CAT(CAT(prefix, num), _PITCHES)[1] +    \
+    (d3 % CAT(CAT(prefix, num), _SIZES)[2])*CAT(CAT(prefix, num), _PITCHES)[2] +    \
+    (d4 % CAT(CAT(prefix, num), _SIZES)[3])*CAT(CAT(prefix, num), _PITCHES)[3]
+
+#endif
+
+KERNEL(eltwise)(
+    INPUTS_DECLS
+    __global UNIT_TYPE* output
+#if CALIBRATION_TERM
+    , const __global float* calibrations
+#endif
+    )
+{
+#if ELTWISE_LAYOUT_BASED || QUANTIZATION_TERM
+    const uint d1 = get_global_id(GWS_YX) % INPUT0_SIZE_X;   // X
+    const uint d2 = get_global_id(GWS_YX) / INPUT0_SIZE_X;   // Y
+    const uint d3 = get_global_id(GWS_FEATURE);             // Feature
+    const uint d4 = get_global_id(GWS_BATCH);               // Batch
+
+    uint output_offset = OUTPUT_OFFSET +
+                         d1*OUTPUT_X_PITCH +
+)__krnl"
+R"__krnl(                         d2*OUTPUT_Y_PITCH +
+                         d3*OUTPUT_FEATURE_PITCH +
+                         d4*OUTPUT_BATCH_PITCH;
+#elif ELTWISE_NO_PITCH_SAME_DIMS
+    const uint d1 = get_global_id(0);
+    uint output_offset = OUTPUT_OFFSET + d1;
+#else
+    const uint d1 = get_global_id(0);
+    const uint d2 = get_global_id(1);
+    const uint d3 = get_global_id(2) % OUTPUT_SIZES[2];
+    const uint d4 = get_global_id(2) / OUTPUT_SIZES[2];
+
+    uint output_offset = OUTPUT_OFFSET +
+                         d1*OUTPUT_PITCHES[0] +
+                         d2*OUTPUT_PITCHES[1] +
+                         d3*OUTPUT_PITCHES[2] +
+                         d4*OUTPUT_PITCHES[3];
+#endif
+
+#if QUANTIZATION_TERM
+    int res;
+#else
+    UNIT_TYPE res;
+#endif
+
+    DO_ELTWISE;
+
+#if QUANTIZATION_TERM
+#if CALIBRATION_TERM
+    res = (int)round(((float)res) * calibrations[d3]);
+#else  // CALIBRATION_TERM
+    res = (int)round(((float)res) * O_QF);
+#endif // CALIBRATION_TERM
+#endif // QUANTIZATION_TERM
+
+#if QUANTIZATION_TERM
+    output[output_offset] = ACTIVATION(convert_char(res), NL_M, NL_N);
+#else
+    output[output_offset] = ACTIVATION(res, NL_M, NL_N);
+#endif
+}
+
+)__krnl"},
+
+{"activation_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+// TODO: move it from layout based to memory based
+KERNEL(activation)(
+#if GRADIENT
+    __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output_grad,
+    __global UNIT_TYPE* input
+#else
+    __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output
+#endif
+#ifdef PARAMETERIZED
+    , __global ADDITIONAL_PARAMS_TYPE* params
+#endif
+    )
+{
+#if defined OUTPUT_LAYOUT_YXFB
+    const unsigned x = get_global_id(1);
+    const unsigned y = get_global_id(2);
+#if OUTPUT_BATCH_NUM == 1
+    const unsigned feature = get_global_id(0);
+    const unsigned batch = 0;
+#else
+    const unsigned feature = get_global_id(0) % OUTPUT_FEATURE_NUM;
+    const unsigned batch = get_global_id(0) / OUTPUT_FEATURE_NUM;
+#endif
+#else
+    const unsigned x = get_global_id(0);
+    const unsigned y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const unsigned feature = get_global_id(2);
+    const unsigned batch = 0;
+#else
+    const unsigned feature = get_global_id(2) % OUTPUT_FEATURE_NUM;
+    const unsigned batch = get_global_id(2) / OUTPUT_FEATURE_NUM;
+#endif
+#endif
+
+#if GRADIENT
+    const unsigned src_grad_index = batch*INPUT0_BATCH_PITCH + feature*INPUT0_FEATURE_PITCH + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH + INPUT0_OFFSET;
+    const unsigned src_index = batch*INPUT1_BATCH_PITCH + feature*INPUT1_FEATURE_PITCH + y*INPUT1_Y_PITCH + x*INPUT1_X_PITCH + INPUT1_OFFSET;
+#else
+    const unsigned src_index = batch*INPUT0_BATCH_PITCH + feature*INPUT0_FEATURE_PITCH + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH + INPUT0_OFFSET;
+#endif
+    const unsigned dst_index = batch*OUTPUT_BATCH_PITCH + feature*OUTPUT_FEATURE_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH + OUTPUT_OFFSET;
+
+#if defined PARAMETERIZED
+    #if   PARAMS_NUM == 2
+        const float nl_m = (float)params[2*feature + 0];
+        const float nl_n = (float)params[2*feature + 1];
+    #elif PARAMS_NUM == 1
+        const float nl_m = (float)params[feature];
+        const float nl_n = (float)NL_N;
+    #else
+        const float nl_m = (float)NL_M;
+        const float nl_n = (float)NL_N;
+    #endif
+#else
+    const float nl_m = (float)NL_M;
+    const float nl_n = (float)NL_N;
+#endif
+
+#if GRADIENT
+    output_grad[dst_index] = ACTIVATION(input_grad[src_grad_index], input[src_index], nl_m, nl_n);
+#else
+    output[dst_index] = ACTIVATION(input[src_index], nl_m, nl_n);
+#endif
+}
+
+)__krnl"},
+
+{"upsampling_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+KERNEL (upsampling_gpu_ref)(__global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+#if defined OUTPUT_LAYOUT_YXFB
+    const uint x = get_global_id(1);
+    const uint y = get_global_id(2);
+#if OUTPUT_BATCH_NUM == 1
+    const uint feature = get_global_id(0);
+    const uint batch = 0;
+#else
+    const uint feature = get_global_id(0) % OUTPUT_FEATURE_NUM;
+    const uint batch = get_global_id(0) / OUTPUT_FEATURE_NUM;
+#endif
+#else
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const uint feature = get_global_id(2);
+    const uint batch = 0;
+#else
+    const uint feature = get_global_id(2) % OUTPUT_FEATURE_NUM;
+    const uint batch = get_global_id(2) / OUTPUT_FEATURE_NUM;
+#endif
+#endif
+
+    const uint dst_index = batch*OUTPUT_BATCH_PITCH + feature*OUTPUT_FEATURE_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH + OUTPUT_OFFSET;
+
+    const uint src_x = floor(x * X_RATIO);
+    const uint src_y = floor(y * Y_RATIO);
+    const uint src_index = batch*INPUT0_BATCH_PITCH + feature*INPUT0_FEATURE_PITCH + src_y*INPUT0_Y_PITCH + src_x*INPUT0_X_PITCH + INPUT0_OFFSET;
+    output[dst_index] = input[src_index];
+
+}
+
+)__krnl"},
+
+{"arg_max_min_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+#ifndef SG_SIZE
+    #define SG_SIZE   16
+    #define SG_SIZE_NEEDSUNDEF_
+#endif
+#ifndef INB_ARRAY_SIZE
+    #define INB_ARRAY_SIZE   8
+    #define INB_ARRAY_SIZE_NEEDSUNDEF_
+#endif
+#ifndef UNIT_FILL_VAL
+    #ifdef MAX_OUT
+        #define UNIT_FILL_VAL   UNIT_VAL_MIN
+    #else
+        #define UNIT_FILL_VAL   UNIT_VAL_MAX
+    #endif
+    #define UNIT_FILL_VAL_NEEDSUNDEF_
+#endif
+#if MAX_OUT
+    #define OP_ARG_REL   >
+#else
+    #define OP_ARG_REL   <
+#endif
+
+
+#if SG_SIZE != 8 && SG_SIZE != 16
+    #error This kernel does not support specified sub-group size.
+#endif
+#if TOP_K > INB_ARRAY_SIZE * SG_SIZE || TOP_K <= 0
+    #error This kernel does not support specified "TOP_K" JIT parameter.
+#endif
+
+
+__attribute__((intel_reqd_sub_group_size(SG_SIZE)))
+__attribute__((reqd_work_group_size(SG_SIZE, 1, 1)))
+KERNEL(arg_max_min_opt)(const __global UNIT_TYPE* input, __global uint* output)
+{
+    const uint input_size = INPUT0_FEATURE_NUM * INPUT0_SIZE_X * INPUT0_SIZE_Y;
+    const uint gid = get_group_id(0);
+    const uint lid = get_sub_group_local_id();
+
+    UNIT_TYPE input_blocks[INB_ARRAY_SIZE];
+    uint indices[INB_ARRAY_SIZE];
+
+    // Read INB_ARRAY_SIZE * SG_SIZE elements (cache them in registers, fill unaligned/unpadded data with
+    //                                         UNIT_FILL_VAL).
+    //
+    // gid * INB_ARRAY_SIZE * SG_SIZE + (INB_ARRAY_SIZE - 1) * SG_SIZE + SG_SIZE - 1 < input_size
+    // gid * INB_ARRAY_SIZE * SG_SIZE + INB_ARRAY_SIZE * SG_SIZE <= input_size
+    // (gid + 1) * INB_ARRAY_SIZE * SG_SIZE <= input_size
+    // (gid + 1) <= input_size / (INB_ARRAY_SIZE * SG_SIZE)   ->   as gid is integral, the floor is not an issue
+    if (gid + 1 <= input_size / (INB_ARRAY_SIZE * SG_SIZE))
+    {
+        __attribute__((opencl_unroll_hint))
+        for (uint ai = 0; ai < INB_ARRAY_SIZE; ++ai)
+        {
+            // Can be exchanged with sub-group block read to INB_ARRAY_SIZE-component vector.
+            input_blocks[ai] = input[gid * INB_ARRAY_SIZE * SG_SIZE + ai * SG_SIZE + lid];
+            indices[ai] = gid * INB_ARRAY_SIZE * SG_SIZE + ai * SG_SIZE + lid;
+        }
+    }
+    else
+    {
+        const uint last_gid = input_size / (INB_ARRAY_SIZE * SG_SIZE);
+
+        uint ai = 0;
+        __attribute__((opencl_unroll_hint))
+        for (uint last_base_off = last_gid * INB_ARRAY_SIZE * SG_SIZE; last_base_off + SG_SIZE <= input_size; last_base_off += SG_SIZE)
+        {
+            // Can be exchanged with sub-group block read to scalar.
+            input_blocks[ai] = input[last_base_off + lid];
+            indices[ai++] = last_base_off + lid;
+        }
+
+        const uint remainder_off = input_size / SG_SIZE * SG_SIZE;
+
+        if (remainder_off < input_size)
+        {
+            input_blocks[ai] = lid < input_size - remainder_off ? input[remainder_off + lid] : UNIT_FILL_VAL;
+            indices[ai++] = lid < input_size - remainder_off ? remainder_off + lid : 0;
+        }
+
+        __attribute__((opencl_unroll_hint))
+        for (; ai < INB_ARRAY_SIZE; ++ai)
+        {
+            input_blocks[ai] = UNIT_FILL_VAL;
+        }
+    }
+
+
+    // Sort TOP_K elements (by linear scan and insert).
+    const uint minmax_acc_array_size = (TOP_K + SG_SIZE - 1) / SG_SIZE;
+    UNIT_TYPE acc[minmax_acc_array_size];
+    uint result[minmax_acc_array_size];
+
+    __attribute__((opencl_unroll_hint))
+    for (uint ai = 0; ai < minmax_acc_array_size; ++ai)
+    {
+        acc[ai] = UNIT_FILL_VAL;
+        result[ai] = 0;
+    }
+
+    //__attribute__((opencl_unroll_hint))
+    __attribute__((opencl_unroll_hint(1)))
+    for (uint ii = 0; ii < INB_ARRAY_SIZE * SG_SIZE; ++ii)
+    {
+        UNIT_TYPE in_val = intel_sub_group_shuffle(input_blocks[ii / SG_SIZE], ii % SG_SIZE);
+        uint in_index = intel_sub_group_shuffle(input_blocks[ii / SG_SIZE], ii % SG_SIZE);
+        __attribute__((opencl_unroll_hint))
+        for (uint ai = 0; ai < minmax_acc_array_size; ++ai)
+        {
+            bool insert_flag = (in_val OP_ARG_REL acc[ai]);
+            if (sub_group_any(insert_flag))
+            {
+                __attribute__((opencl_unroll_hint))
+                for (uint aj = minmax_acc_array_size; aj > ai + 1; --aj)
+                {
+                    acc[aj - 1] = intel_sub_group_shuffle_up(acc[aj - 2], acc[aj - 1], 1);
+                    result[aj - 1] = intel_sub_group_shuffle_up(result[aj - 2], acc[aj - 1], 1);
+                }
+                UNIT_TYPE in_val_acc_mask = select(in_val, acc[ai], insert_flag);
+                uint in_index_mask = select(in_index, result[ai], insert_flag);
+                acc[ai] = select(acc[ai], intel_sub_group_shuffle_up(in_val, in_val_acc_mask, 1), insert_flag);
+                result[ai] = select(result[ai], intel_sub_group_shuffle_up(in_index, in_index_mask, 1), insert_flag);
+                break;
+            }
+        }
+    }
+
+)__krnl"
+R"__krnl(
+    // Write TOP_K sorted results.
+    uint ai = 0;
+    __attribute__((opencl_unroll_hint))
+    for (uint k_base_off = 0; k_base_off + SG_SIZE <= TOP_K; k_base_off += SG_SIZE)
+    {
+        output[k_base_off + lid] = result[ai++] % input_size;
+    }
+
+    const uint k_remainder_off = TOP_K / SG_SIZE * SG_SIZE;
+    if (k_remainder_off < TOP_K && lid < TOP_K - k_remainder_off)
+    {
+        output[k_remainder_off + lid] = result[ai] % input_size;
+    }
+}
+
+
+#ifdef SG_SIZE_NEEDSUNDEF_
+    #undef SG_SIZE
+    #undef SG_SIZE_NEEDSUNDEF_
+#endif
+#ifdef INB_ARRAY_SIZE_NEEDSUNDEF_
+    #undef INB_ARRAY_SIZE
+    #undef INB_ARRAY_SIZE_NEEDSUNDEF_
+#endif
+#ifdef UNIT_FILL_VAL_NEEDSUNDEF_
+    #undef UNIT_FILL_VAL
+    #undef UNIT_FILL_VAL_NEEDSUNDEF_
+#endif
+#undef OP_ARG_REL
+
+)__krnl"},
+
+{"mvn_gpu_bfyx_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+__attribute__((reqd_work_group_size(LWS, 1, 1)))
+KERNEL (mvn_gpu_bfyx_opt)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    const uint data_set_idx = get_global_id(1);     //in processing of which data set this WI participates?
+    const uint workers_per_data_set = LWS;          //how many WI participates in processing of one data set
+    const uint in_data_set_idx = get_global_id(0);  //this WI's id in group of items processing single data set
+    const uint data_set_size = DATA_SET_SIZE;       //how many elements are in one data set
+    const uint data_sets_count = DATA_SETS_COUNT;   //how many data sets are in the processing payload
+
+    const uint data_set_offset = data_set_idx * data_set_size;
+    const uint my_data_offset = data_set_offset + in_data_set_idx;
+
+    float my_sum = 0.f;
+    float tmp;
+
+    __local float lg_storage[LWS];
+
+    //each WI reads ITEMS_NUM consecutive items from batch*feature
+    for (uint i=0; i<ITEMS_NUM; ++i)
+    {
+        my_sum += (float)input[my_data_offset + i * workers_per_data_set];
+    }
+
+    if (in_data_set_idx < LEFTOVERS)
+    {
+        my_sum += (float)input[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx];
+    }
+
+    lg_storage[in_data_set_idx] = my_sum;
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+    if (in_data_set_idx == 0)
+    {
+        for (uint i=1; i<LWS; ++i)
+            my_sum += lg_storage[i];
+
+        lg_storage[0] = my_sum / data_set_size;
+    }
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    my_sum = lg_storage[0];
+
+#if NORMALIZE_VARIANCE == 0
+    for (uint i=0; i<ITEMS_NUM; ++i)
+        output[my_data_offset + i * workers_per_data_set] = ACTIVATION(UNIT_CVT_FUNC(input[my_data_offset + i * workers_per_data_set]) - UNIT_CVT_FUNC(my_sum), NL_M ,NL_N);
+    if (in_data_set_idx < LEFTOVERS)
+        output[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx] = ACTIVATION(UNIT_CVT_FUNC(input[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx]) - UNIT_CVT_FUNC(my_sum), NL_M ,NL_N);
+#else
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    float my_variance = 0.f;
+    //each WI reads ITEMS_NUM consecutive items from batch*feature
+    for (uint i=0; i<ITEMS_NUM; ++i)
+    {
+        tmp = (float)input[my_data_offset + i * workers_per_data_set];
+        tmp -= my_sum;
+        my_variance = fma(tmp, tmp, my_variance);
+    }
+
+    if (in_data_set_idx < LEFTOVERS)
+    {
+        tmp = (float)input[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx];
+        tmp -= my_sum;
+        my_variance = fma(tmp, tmp, my_variance);
+    }
+
+    lg_storage[in_data_set_idx] = my_variance;
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+    if (in_data_set_idx == 0)
+    {
+        for (uint i=1; i<LWS; ++i)
+            my_variance += lg_storage[i];
+
+        my_variance /= data_set_size;
+        lg_storage[0] = native_powr(my_variance + (float)EPSILON, -0.5f);
+    }
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    my_variance = lg_storage[0];
+
+    for (uint i=0; i<ITEMS_NUM; ++i)
+        output[my_data_offset + i * workers_per_data_set] = ACTIVATION((UNIT_CVT_FUNC(input[my_data_offset + i * workers_per_data_set]) - UNIT_CVT_FUNC(my_sum)) * UNIT_CVT_FUNC(my_variance), NL_M ,NL_N);
+    if (in_data_set_idx < LEFTOVERS)
+        output[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx] = ACTIVATION((UNIT_CVT_FUNC(input[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx]) - UNIT_CVT_FUNC(my_sum)) * UNIT_CVT_FUNC(my_variance), NL_M ,NL_N);
+#endif
+}
+
+#undef UNIT_CVT_FUNC
+
+)__krnl"},
+
+{"fully_connected_gpu_bf_io_gemm",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+)__krnl"
+R"__krnl(
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+)__krnl"
+R"__krnl(
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if defined(__fc_f16)
+
+#define WORK_GROUP_X 64
+#define VEC_SIZE 4
+__attribute__ ((reqd_work_group_size(WORK_GROUP_X, 1, 1)))
+KERNEL(fc_f16)(
+    const __global const half  *src_vector,
+    __global half        *dst_vector,
+    const __global const half  *matrix
+#if BIAS_TERM
+    , const __global const half  *biases
+#endif
+    )
+{
+    local half slm[WORK_GROUP_X];
+    const unsigned x = get_local_id(0);
+    const unsigned y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const unsigned oidx = (y / OUTPUT_SIZE_X) * OUTPUT_Y_PITCH + y % OUTPUT_SIZE_X + OUTPUT_OFFSET;
+    const unsigned batch_id = 0;
+#else
+    const unsigned batch_id = get_global_id(2);
+
+    const unsigned out_z = y / (OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+    const unsigned out_yx = y % (OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+    const unsigned out_y = out_yx / (OUTPUT_SIZE_X);
+    const unsigned out_x = out_yx % (OUTPUT_SIZE_X);
+
+    const unsigned oidx = batch_id*OUTPUT_BATCH_PITCH + out_z*OUTPUT_FEATURE_PITCH + out_y*OUTPUT_Y_PITCH + out_x + OUTPUT_OFFSET;
+#endif
+
+    // TODO: we need to support multi dims. currently it doesn't
+    // TODO: check cases we have padding in y/z dimensions
+    unsigned w = INPUT0_BATCH_PITCH;
+
+    #if (LAST_INPUT_SIZE_DIV_4 == 0)
+    w /= VEC_SIZE;
+    __global const half4 *mat_read    = (__global const half4 *) (matrix);
+    const int start_offset = w*y;
+)__krnl"
+R"__krnl(    const int end_offset = start_offset + w;
+    #else
+    __global const half4 *mat_read    = (__global const half4 *) (matrix + w * y);
+    const int start_offset = 0;
+    const int end_offset = start_offset + (w + VEC_SIZE - 1) / VEC_SIZE;
+    #endif
+
+    __global const half4 *src_read    = (__global const half4 *) (src_vector + batch_id*INPUT0_BATCH_PITCH + INPUT0_OFFSET);
+    int m_offset = start_offset + x;
+    int v_offset = x;
+    half4 sum = (half4)(0);
+    #if (LAST_INPUT_SIZE_REMAINDER == 0)
+    for (; m_offset < end_offset; m_offset += WORK_GROUP_X, v_offset += WORK_GROUP_X) {
+        const half4 m = mat_read[m_offset];
+        const half4 v = src_read[v_offset];
+        sum = mad(m, v, sum);
+    }
+    #else
+
+        #if (LAST_INPUT_SIZE_DIV_4 == 0)
+        for (; m_offset < end_offset; m_offset += WORK_GROUP_X, v_offset += WORK_GROUP_X) {
+            const half4 m = mat_read[m_offset];
+            const half4 v = src_read[v_offset];
+
+            sum = mad(m, v, sum);
+        }
+        #else
+        for (; m_offset < end_offset - WORK_GROUP_X; m_offset += WORK_GROUP_X, v_offset += WORK_GROUP_X) {
+            const half4 m = vload4(m_offset, (__global const half*)mat_read);
+            const half4 v = vload4(v_offset, (__global const half*)src_read);
+
+            sum = mad(m, v, sum);
+        }
+
+        if (m_offset < end_offset)
+        {
+            const half4 m = vload4(m_offset, (__global const half*)mat_read);
+            const half4 v = vload4(v_offset, (__global const half*)src_read);
+            if ((x + 1) == ((LAST_INPUT_SIZE_REMAINDER + VEC_SIZE - 1) / VEC_SIZE))
+            {
+                #if (LAST_INPUT_SIZE_DIV_4 == 3)
+                    sum.xyz += m.xyz * v.xyz;
+                #elif (LAST_INPUT_SIZE_DIV_4 == 2)
+                    sum.xy += m.xy * v.xy;
+                #else
+                    sum.x += m.x * v.x;
+                #endif
+            }
+            else
+            {
+                sum = mad(m, v, sum);
+            }
+        }
+        #endif
+    #endif
+
+    slm[x] = sum.x + sum.y + sum.z + sum.w;
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    // Reduction now
+    for (int max_offset = WORK_GROUP_X / 2; max_offset > 0; max_offset >>= 1) {
+        if (x < max_offset) slm[x] += slm[x + max_offset];
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    #if BIAS_TERM
+    const half bias = biases[y];
+    if (x == 0) dst_vector[oidx] = ACTIVATION(slm[0] + bias, NL_M, NL_N);
+    #else
+    if (x == 0) dst_vector[oidx] = ACTIVATION(slm[0], NL_M, NL_N);
+    #endif
+}
+#endif
+
+
+#if defined(__fc_f32)
+
+#define WORK_GROUP_X 64
+#define VEC_SIZE 4
+__attribute__ ((reqd_work_group_size(WORK_GROUP_X, 1, 1)))
+KERNEL(fc_f32)(
+    const __global const float  *src_vector,
+    __global float        *dst_vector,
+    const __global const float  *matrix
+#if BIAS_TERM
+    , const __global const float  *biases
+#endif
+    )
+{
+    local float slm[WORK_GROUP_X];
+    const unsigned x = get_local_id(0);
+    const unsigned y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const unsigned oidx = (y / OUTPUT_SIZE_X) * OUTPUT_Y_PITCH + y % OUTPUT_SIZE_X + OUTPUT_OFFSET;
+    const unsigned batch_id = 0;
+#else
+    const unsigned batch_id = get_global_id(2);
+
+    const unsigned out_z = y / (OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+    const unsigned out_yx = y % (OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+    const unsigned out_y = out_yx / (OUTPUT_SIZE_X);
+    const unsigned out_x = out_yx % (OUTPUT_SIZE_X);
+
+    const unsigned oidx = batch_id*OUTPUT_BATCH_PITCH + out_z*OUTPUT_FEATURE_PITCH + out_y*OUTPUT_Y_PITCH + out_x + OUTPUT_OFFSET;
+#endif
+    // TODO: we need to support multi dims. currently it doesn't
+    // TODO: check cases we have padding in y/z dimensions
+    unsigned w = INPUT0_BATCH_PITCH;
+
+    #if BIAS_TERM
+    const float bias = biases[y];
+    #else
+    const float bias = 0;
+    #endif
+
+    #if (LAST_INPUT_SIZE_DIV_4 == 0)
+    w /= VEC_SIZE;
+    __global const float4 *mat_read    = (__global const float4 *) (matrix);
+    const int start_offset = w*y;
+    const int end_offset = start_offset + w;
+    #else
+    __global const float4 *mat_read    = (__global const float4 *) (matrix + w * y);
+    const int start_offset = 0;
+    const int end_offset = start_offset + (w + VEC_SIZE - 1) / VEC_SIZE;
+    #endif
+
+    __global const float4 *src_read    = (__global const float4 *) (src_vector + batch_id*INPUT0_BATCH_PITCH + INPUT0_OFFSET);
+    int m_offset = start_offset + x;
+    int v_offset = x;
+    float4 sum = (float4)(0);
+    #if (LAST_INPUT_SIZE_REMAINDER == 0)
+    for (; m_offset < end_offset; m_offset += WORK_GROUP_X, v_offset += WORK_GROUP_X) {
+        const float4 m = mat_read[m_offset];
+        const float4 v = src_read[v_offset];
+        sum = mad(m, v, sum);
+    }
+    #else
+
+        #if (LAST_INPUT_SIZE_DIV_4 == 0)
+        for (; m_offset < end_offset; m_offset += WORK_GROUP_X, v_offset += WORK_GROUP_X) {
+            const float4 m = mat_read[m_offset];
+            const float4 v = src_read[v_offset];
+
+            sum = mad(m, v, sum);
+        }
+        #else
+        for (; m_offset < end_offset - WORK_GROUP_X; m_offset += WORK_GROUP_X, v_offset += WORK_GROUP_X) {
+            const float4 m = mat_read[m_offset];
+            const float4 v = src_read[v_offset];
+
+            sum = mad(m, v, sum);
+        }
+
+        if (m_offset < end_offset)
+        {
+            const float4 m = mat_read[m_offset];
+            const float4 v = src_read[v_offset];
+            if ((x + 1) == ((LAST_INPUT_SIZE_REMAINDER + VEC_SIZE - 1) / VEC_SIZE))
+            {
+                #if (LAST_INPUT_SIZE_DIV_4 == 3)
+                    sum.xyz += m.xyz * v.xyz;
+                #elif (LAST_INPUT_SIZE_DIV_4 == 2)
+                    sum.xy += m.xy * v.xy;
+                #else
+                    sum.x += m.x * v.x;
+                #endif
+            }
+            else
+            {
+                sum = mad(m, v, sum);
+            }
+        }
+        #endif
+    #endif
+
+    slm[x] = sum.x + sum.y + sum.z + sum.w;
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    // Reduction now
+    for (int max_offset = WORK_GROUP_X / 2; max_offset > 0; max_offset >>= 1) {
+        if (x < max_offset) slm[x] += slm[x + max_offset];
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    if (x == 0) dst_vector[oidx] = ACTIVATION(slm[0] + bias, NL_M, NL_N);
+}
+#endif
+
+
+)__krnl"},
+
+{"lrn_gpu_across_channel_yxfb_b8_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+KERNEL (lrn_gpu_yxfb_b8)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+
+    const uint batch_num_group  = (INPUT0_BATCH_NUM/SUB_GROUP_SIZE);
+    const uint b_f              = get_global_id(0);
+    const uint x                = (uint)get_global_id(1);
+    const uint y                = (uint)get_global_id(2);
+    const uint feature_id       = b_f / batch_num_group;
+    const uint batch_id_group   = b_f % batch_num_group;
+    const uint batch_id         = batch_id_group * SUB_GROUP_SIZE;
+
+    const uint input_id = INPUT0_OFFSET + batch_id*INPUT0_BATCH_PITCH + feature_id*INPUT0_FEATURE_PITCH + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH;
+    const uint input_id_group = input_id / SUB_GROUP_SIZE;
+
+    int input_offset_f = feature_id - PADDING;
+
+    const uint input_feature_pitch_group  = (INPUT0_FEATURE_PITCH/SUB_GROUP_SIZE);
+    int input_idx_group = (int)input_id_group - PADDING*input_feature_pitch_group;
+
+    float8 acc = 0;
+
+    for (int i = 0; i < LOCAL_SIZE; i++)
+    {
+        bool zero = input_offset_f < 0 || input_offset_f >= INPUT0_FEATURE_NUM;
+
+        if(!zero)
+        {
+            float8 value = vload8(input_idx_group, input);
+            acc = mad(value, value, acc);
+        }
+
+        input_offset_f++;
+        input_idx_group += input_feature_pitch_group;
+    }
+    acc = mad(acc, UNIT_CVT_FUNC(ALPHA_DIV_BY_SIZE), UNIT_CVT_FUNC(K));
+    acc = native_powr(acc, -UNIT_CVT_FUNC(BETA));
+
+    const uint output_idx = OUTPUT_OFFSET + batch_id*OUTPUT_BATCH_PITCH + feature_id*OUTPUT_FEATURE_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH;
+    const uint output_idx_group = output_idx / SUB_GROUP_SIZE;
+    float8 _in = vload8(input_id_group, input);
+    float8 res = ACTIVATION(acc * _in, NL_M ,NL_N);
+    vstore8(res, output_idx_group, output);
+}
+
+)__krnl"},
+
+{"convolution_grad_weights_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+KERNEL(convolution_grad_weights_gpu_ref)(
+    const __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output,
+    __global float* filter,
+#if BIAS_TERM
+    __global float* bias,
+#endif
+#if MOMENTUM
+    __global float* prev_grad_w,
+#if BIAS_TERM
+    __global float* prev_grad_b,
+#endif
+#endif
+    const __global UNIT_TYPE* input,
+    uint split_idx,
+    float lr)
+{
+    const uint ofm_ifm       = get_global_id(0);
+    const uint id_x          = (uint)get_global_id(1);
+    const uint id_y          = (uint)get_global_id(2);
+    const uint ifm           = ofm_ifm % INPUT1_FEATURE_NUM;
+    const uint ofm           = ofm_ifm / INPUT1_FEATURE_NUM;
+
+    const int in_x    = id_x - PADDING_SIZE_X;
+    const int in_y    = id_y - PADDING_SIZE_Y;
+
+    ACCUMULATOR_TYPE grad_w = 0;
+
+#if BIAS_TERM
+    ACCUMULATOR_TYPE grad_b = 0;
+#endif
+
+    uint weights_idx = ofm * FILTER_OFM_PITCH + ifm * FILTER_IFM_PITCH + id_y * FILTER_Y_PITCH + id_x * FILTER_X_PITCH;
+
+    for(int b = 0; b < INPUT0_BATCH_NUM; b++)
+    {
+        ACCUMULATOR_TYPE result = ACCUMULATOR_TYPE_ZERO;
+
+#if BIAS_TERM
+        ACCUMULATOR_TYPE result_bias = ACCUMULATOR_TYPE_ZERO;
+#endif
+
+        const uint grad_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_OFM_NUM;
+)__krnl"
+R"__krnl(        const uint in_split_offset = split_idx * INPUT1_FEATURE_PITCH * FILTER_IFM_NUM;
+
+        for (uint i = 0; i < INPUT0_SIZE_Y; i++)
+        {
+            for (uint j = 0; j < INPUT0_SIZE_X; j++)
+            {
+                const int input_offset_y = in_y + i * STRIDE_SIZE_Y;
+                const bool zero_y = input_offset_y >= INPUT1_SIZE_Y || input_offset_y < 0;
+                const int input_offset_x = in_x + j * STRIDE_SIZE_X;
+                const bool zero_x = input_offset_x >= INPUT1_SIZE_X || input_offset_x < 0;
+#if BIAS_TERM
+                uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                ACCUMULATOR_TYPE grad = TO_ACCUMULATOR_TYPE(input_grad[input_grad_idx]);
+#endif
+                if(!zero_x && !zero_y)
+                {
+                    uint input_idx = in_split_offset + b*INPUT1_BATCH_PITCH + ifm*INPUT1_FEATURE_PITCH + (uint)input_offset_x*INPUT1_X_PITCH + (uint)input_offset_y*INPUT1_Y_PITCH;
+#if BIAS_TERM
+                    result = fma(TO_ACCUMULATOR_TYPE(input[input_idx]), grad, result);
+#else
+                    uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                    result = fma(TO_ACCUMULATOR_TYPE(input[input_idx]), TO_ACCUMULATOR_TYPE(input_grad[input_grad_idx]), result);
+#endif
+                }
+#if BIAS_TERM
+                result_bias += grad;
+#endif
+            }
+        }
+
+        grad_w += result;
+
+#if BIAS_TERM
+        grad_b += result_bias;
+#endif
+    }
+
+#if MOMENTUM
+    float update_gradient_w = lr * (grad_w + DECAY_RATE * filter[weights_idx]) + prev_grad_w[weights_idx] * MOMENTUM_FACTOR;
+    filter[weights_idx] -= update_gradient_w;
+    prev_grad_w[weights_idx] = update_gradient_w;
+#else
+    filter[weights_idx] -= lr * grad_w + DECAY_RATE * lr * filter[weights_idx];
+#endif
+
+#if BIAS_TERM
+        if(ifm == 0 && id_x == 0 && id_y == 0)
+        {
+#if MOMENTUM
+        float update_gradient_b = lr * grad_b + prev_grad_b[ofm] * MOMENTUM_FACTOR;
+        bias[ofm] -= update_gradient_b;
+        prev_grad_b[ofm] = update_gradient_b;
+#else
+        bias[ofm] -= lr * grad_b;
+#endif
+        }
+#endif
+
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_MMAD",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+)__krnl"
+R"__krnl(        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+KERNEL(fully_connected_gpu_MMAD)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    const __global FILTER_TYPE* weights
+#if BIAS_TERM
+    , const __global BIAS_TYPE* biases
+#endif
+#if QUANTIZATION_TERM
+    ,const __global float* quantizations
+#endif
+#if CALIBRATION_TERM
+    ,const __global float* calibrations
+#endif
+    )
+{
+    const uint x = 0;
+    const uint y = 0;
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(2);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(2) % FILTER_OFM_ALIGNED;
+    const uint b = get_global_id(2) / FILTER_OFM_ALIGNED;
+#endif
+
+    int dotProd = 0;
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+
+    const uint filter_offset = (get_group_id(2) % FILTER_OFM_MMAD_NUM) * FILTER_OFM_BLOCK_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    for (uint k = 0; k < FILTER_IFM_MMAD_NUM; ++k)
+    {
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+            const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+            if(!zero_y)
+            {
+                for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+                {
+                    const int input_offset_x = input_x + i * DILATION_SIZE_X;
+                    const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                    if(!zero_x)
+                    {
+                        uint input_idx = input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH + k*32;
+                        uint filter_idx = filter_offset + k*FILTER_Y_PITCH * FILTER_SIZE_Y + j*FILTER_Y_PITCH + i*FILTER_X_PITCH;
+
+						int input_data = as_int(intel_sub_group_block_read((const __global uint*)(input + input_idx)));
+						int8 activations;  //activations of all lanes
+						activations.s0 = sub_group_broadcast(input_data, 0);
+                        activations.s1 = sub_group_broadcast(input_data, 1);
+                        activations.s2 = sub_group_broadcast(input_data, 2);
+                        activations.s3 = sub_group_broadcast(input_data, 3);
+                        activations.s4 = sub_group_broadcast(input_data, 4);
+                        activations.s5 = sub_group_broadcast(input_data, 5);
+                        activations.s6 = sub_group_broadcast(input_data, 6);
+                        activations.s7 = sub_group_broadcast(input_data, 7);
+
+						int8 weights_data = as_int8(intel_sub_group_block_read8((const __global uint*)(weights + filter_idx)));
+
+						dotProd = MMAD_8(activations, weights_data, dotProd);
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+#if CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+#endif // BIAS_TERM
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y, x) + out_split_offset;
+    output[dst_index] = ACTIVATION(convert_char(dotProd), NL_M, NL_N);
+}
+
+#undef FILTER_IFM_MMAD_NUM
+#undef FILTER_OFM_MMAD_NUM
+#undef FILTER_IFM_ALIGNED
+#undef FILTER_OFM_ALIGNED
+
+)__krnl"},
+
+{"deconvolution_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(deconvolution_gpu_yxfb_ref)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    const __global UNIT_TYPE* bias,
+#endif
+    uint split_idx
+#if FUSED_ELTWISE
+	, const __global UNIT_TYPE* fuse_input
+#endif
+	)
+{
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+
+#if DIM_ORDER_XYBF == 1
+    const uint out_x        = get_global_id(0);
+    const uint out_y        = get_global_id(1);
+    const uint b_f          = get_global_id(2);
+    const uint batch_offset = b_f / OUTPUT_FEATURE_NUM;
+    const uint ofm_offset   = b_f % OUTPUT_FEATURE_NUM;
+
+    if (out_x >= OUTPUT_SIZE_X)
+        return;
+#else
+    const uint b_f           = get_global_id(0);
+    const uint out_x         = (uint)get_global_id(1);
+    const uint out_y         = (uint)get_global_id(2);
+    const uint ofm_offset    = b_f / INPUT0_BATCH_NUM;
+    const uint batch_offset  = b_f % INPUT0_BATCH_NUM;
+#endif
+
+    const int x = (int)out_x + PADDING_SIZE_X - (FILTER_SIZE_X - 1);
+    const int y = (int)out_y + PADDING_SIZE_Y - (FILTER_SIZE_Y - 1);
+
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (ofm_offset / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint input_offset = INPUT0_OFFSET + batch_offset*INPUT0_BATCH_PITCH + in_split_offset;
+
+)__krnl"
+R"__krnl(    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i;
+        const bool zero_y = (input_offset_y >= INPUT0_SIZE_Y * STRIDE_SIZE_Y) || (input_offset_y < 0) || ((input_offset_y % STRIDE_SIZE_Y) != 0);
+
+        if(!zero_y)
+        {
+            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j;
+                const bool zero_x = (input_offset_x >= INPUT0_SIZE_X * STRIDE_SIZE_X) || (input_offset_x < 0) || ((input_offset_x % STRIDE_SIZE_X) != 0);
+
+                if(!zero_x)
+                {
+                    uint fixed_input_offset_x = (uint)input_offset_x / STRIDE_SIZE_X;
+                    uint fixed_input_offset_y = (uint)input_offset_y / STRIDE_SIZE_Y;
+                    uint input_idx = input_offset + (uint)fixed_input_offset_x*INPUT0_X_PITCH + (uint)fixed_input_offset_y*INPUT0_Y_PITCH;
+#if GRADIENT
+                    uint filter_idx = ofm_offset*FILTER_IFM_PITCH + (FILTER_SIZE_Y - i - 1)*FILTER_Y_PITCH + (FILTER_SIZE_X - j - 1)*FILTER_X_PITCH;
+                    for (uint h = 0; h < FILTER_OFM_NUM; h++)
+                    {
+                        result = fma(input[input_idx], filter[filter_idx], result);
+                        filter_idx += FILTER_OFM_PITCH;
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+#else
+                    uint filter_idx = ofm_offset*FILTER_OFM_PITCH + (FILTER_SIZE_Y - i - 1)*FILTER_Y_PITCH + (FILTER_SIZE_X - j - 1)*FILTER_X_PITCH;
+                    for (uint h = 0; h < FILTER_IFM_NUM; h++)
+                    {
+                        result = fma(input[input_idx], filter[filter_idx], result);
+                        filter_idx += FILTER_IFM_PITCH;
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+#endif
+                }
+            }
+        }
+    }
+#if BIAS_TERM
+    result += bias[ofm_offset];
+#endif
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * FILTER_OFM_NUM;
+    const uint dst_index = OUTPUT_OFFSET + out_split_offset + batch_offset*OUTPUT_BATCH_PITCH + ofm_offset*OUTPUT_FEATURE_PITCH + out_y*OUTPUT_Y_PITCH + out_x*OUTPUT_X_PITCH;
+#if FUSED_ELTWISE
+    const uint fused_index = INPUT1_OFFSET + split_idx * INPUT1_FEATURE_PITCH * FILTER_OFM_NUM + batch_offset*INPUT1_BATCH_PITCH + ofm_offset*INPUT1_FEATURE_PITCH + out_y*INPUT1_Y_PITCH + out_x*INPUT1_X_PITCH;
+#if !GRADIENT
+	output[dst_index] = ACTIVATION(result + fuse_input[fused_index], NL_M, NL_N);
+#else
+	output[dst_index] = result + fuse_input[fused_index];
+#endif
+
+#else
+    output[dst_index] = ACTIVATION(result, NL_M, NL_N);
+#endif
+}
+
+#undef ACTIVATION
+
+)__krnl"},
+
+{"convolution_gpu_winograd_2x3_s1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// --------------------------------------------------------------------------------------------------------------------------------
+// L3_SIMD_4x8
+// Input matrices dimensions: M x K x N
+// Output matrix dimensions: M x N
+// --------------------------------------------------------------------------------------------------------------------------------
+#define VEC_SIZE        4   // dx
+#define TILE_M          8   // dy
+#define TILE_K          32
+#define TILE_N          32
+
+#define WINOGRAD_TILE_WIDTH 4
+#define WINOGRAD_FILTER_HEIGHT 3
+#define WINOGRAD_OUTPUT_TILE_WIDTH 2 //width of the winograd tile when transformed back to standard domain, do not confuse with outpout of this kernel (which is still in winograd domain)
+
+#define _CAT(a,b) a##b
+#define CAT(a,b) _CAT(a,b)
+#define UNIT_TYPE_4 CAT(UNIT_TYPE, 4)
+
+#define INPUT0_PITCH_SIZE_Y INPUT0_FEATURE_NUM
+#define WEIGHTS_PITCH_FEATURE OUTPUT_FEATURE_NUM
+#define INPUT0_PITCH_FEATURE 1
+
+__attribute__((reqd_work_group_size(8, 1, 1)))
+KERNEL(convolution_gpu_winograd_2x3_s1)
+(
+    const __global UNIT_TYPE *signalw,
+          __global UNIT_TYPE *outputw,
+    const __global UNIT_TYPE *filterw,
+    uint split_idx)
+{
+    const int INPUT0_SIZE_Y_PITCH_UNIT_4 = INPUT0_PITCH_SIZE_Y / VEC_SIZE; //for bxyf -> INPUT0_PITCH_SIZE_Y is equal to input features count, since ifm % 32 == 0, division by VEC_SIZE is ok
+    const int OUTPUT_SIZE_Y_PITCH_UNIT_4 = OUTPUT_Y_PITCH / VEC_SIZE; //for bxyf -> OUTPUT_Y_PITCH is equal to output features count, since ofm % 32 == 0, division by VEC_SIZE is ok
+	  const int WEIGHTS_FEATURE_PITCH_UNIT_4 = WEIGHTS_PITCH_FEATURE / VEC_SIZE; //for xyio -> WEIGHTS_PITCH_FEATURE is equal to the output features count
+
+    const int group_x = get_group_id(0);
+    const int group_y = get_group_id(1);
+    const int group_z = get_group_id(2);
+    const int local_x = get_local_id(0);
+    const int local_y = get_local_id(1);
+    const int local_z = get_local_id(2);
+
+    const int no_of_tiles_x = INPUT0_SIZE_WINOGRAD_X / WINOGRAD_TILE_WIDTH;
+    const int no_of_tiles_y = INPUT0_SIZE_WINOGRAD_Y - WINOGRAD_FILTER_HEIGHT + 1;
+
+    const int x_offset_from_z_id = group_z % WINOGRAD_TILE_WIDTH;
+    const int batch_idx = group_z / WINOGRAD_TILE_WIDTH;
+
+    //y-dim size is equal to a flattened number of tiles in x-y dims,
+    //since one work group processes TILE_M tiles, flattened tile idx is group_y * TILE_M,
+    //this idx is then deflattened to idx in x and y dim by dividing by no_of_tiles_y,
+    //note: we do not add local id because group size in y-dim is 1
+    const int linear_x = (group_y * TILE_M) / no_of_tiles_y;
+    const int tile_idx_y = (group_y * TILE_M) % no_of_tiles_y;
+    const int x_idx = linear_x + x_offset_from_z_id * no_of_tiles_x;
+    const int y_idx = tile_idx_y; //winograd tile height == 1
+    const int f_idx = group_x * TILE_N + local_x * VEC_SIZE;
+    const int b_idx = batch_idx;
+
+	  const int in_tile_idx = (x_idx % WINOGRAD_TILE_WIDTH);
+	  const int tile_idx_x = (x_idx / WINOGRAD_TILE_WIDTH);
+
+    // Result ctile is M rows x N columns
+    // M = 8, we have 1 rows of work-items, so we need 8/1 = 8 results down
+    // N = 32, we have 8 columns of work-items, so we need 32/8 = 4 results across = 1 float4s across
+
+    UNIT_TYPE_4 c0 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c1 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c2 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c3 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c4 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c5 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c6 = (UNIT_TYPE_4)(0.f);
+    UNIT_TYPE_4 c7 = (UNIT_TYPE_4)(0.f);
+
+    //optimal format is bxyf
+    const int output_idx = b_idx * OUTPUT_BATCH_PITCH +
+                           f_idx * OUTPUT_FEATURE_PITCH +
+                           x_idx * OUTPUT_X_PITCH +
+                           y_idx * OUTPUT_Y_PITCH;
+
+    __global UNIT_TYPE_4 *dst = (__global UNIT_TYPE_4 *)(outputw + output_idx);
+
+    // Src0 is used directly as atile.
+    // It starts at the left side of signalw and walks across.
+    // atile is M rows x K columns.
+    // M = 8, we have 1 rows of work-items, so we need 8/1 = 8 rows.
+    // K = 32, we have 8 columns of work-items, so we need 32/8 = 4 floats across = 1 float4s across
+    const int src0_idx = local_x * VEC_SIZE * INPUT0_PITCH_FEATURE
+                         + y_idx * INPUT0_FEATURE_NUM
+                         + x_idx * INPUT0_SIZE_WINOGRAD_Y * INPUT0_FEATURE_NUM
+                         + batch_idx * INPUT0_SIZE_WINOGRAD_X * INPUT0_SIZE_WINOGRAD_Y * INPUT0_FEATURE_NUM;
+
+    const __global UNIT_TYPE_4 *src0 = (__global UNIT_TYPE_4 *)(signalw + src0_idx);
+
+    // Src1 is directly used as btile.
+    // It starts at the top of filterw and walks down.
+    // btile is K rows x N columns.
+    // K = 32, we'll process four rows at a time
+    // N = 32, we have 8 columns of work-items, so we need 32/8 = 4 floats across = 1 float4s across
+    const int src1_idx = local_x * VEC_SIZE
+                         + (group_x * TILE_N)
+                         + in_tile_idx * WINOGRAD_FILTER_HEIGHT * INPUT0_FEATURE_NUM * OUTPUT_FEATURE_NUM;
+
+    const __global UNIT_TYPE_4 *src1 = (__global UNIT_TYPE_4 *)(filterw + src1_idx);
+
+    UNIT_TYPE_4 a;
+
+    // Walk ACROSS signalw and DOWN filterw:
+    for (int w = 0; w < K; w += TILE_K)
+    {
+		//in one iteration load tile 1-width, 8-height, 4-depth (REQ: in_y % 8 == 0),
+		//SIMD reads are chained along f-axis, resulting in a 1-width, 8-height, 4*8=32-depth input block (REQ: ifm % 32 == 0)
+		//consecutive blocks are also chained along f-axis and overflows to y-axis, reading in total 3*f values (i.e., read all in-depth values from 3 consecutive y values and constant x)
+        const UNIT_TYPE_4 a0 = src0[0 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a1 = src0[1 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a2 = src0[2 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a3 = src0[3 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a4 = src0[4 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a5 = src0[5 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a6 = src0[6 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+        const UNIT_TYPE_4 a7 = src0[7 * INPUT0_SIZE_Y_PITCH_UNIT_4];
+
+#define DOT_PRODUCT( _i, _j ) { a = intel_sub_group_shuffle(a ## _i, _j); c ## _i = mad(a.x, b0, mad(a.y, b1, mad(a.z, b2, mad(a.w, b3, c ## _i)))); }
+
+		//in one iteration load weights tile 1-width, 1-height, 4-depth from 4 different filters (ofms)
+		//SIMD reads are chained along b-axis (different ofms), resulting in 1-width, 1-height, 4-depth blocks from 4*8=32 different filters
+		//consecutive reads are chained along f-dim and overflows to y-dim, reading in total
+#define ITERATION( _j ) \
+        {   \
+            const UNIT_TYPE_4 b0 = src1[0]; src1 += WEIGHTS_FEATURE_PITCH_UNIT_4; \
+            const UNIT_TYPE_4 b1 = src1[0]; src1 += WEIGHTS_FEATURE_PITCH_UNIT_4; \
+            const UNIT_TYPE_4 b2 = src1[0]; src1 += WEIGHTS_FEATURE_PITCH_UNIT_4; \
+            const UNIT_TYPE_4 b3 = src1[0]; src1 += WEIGHTS_FEATURE_PITCH_UNIT_4; \
+            \
+            DOT_PRODUCT(0, _j) \
+            DOT_PRODUCT(1, _j) \
+            DOT_PRODUCT(2, _j) \
+            DOT_PRODUCT(3, _j) \
+            DOT_PRODUCT(4, _j) \
+            DOT_PRODUCT(5, _j) \
+            DOT_PRODUCT(6, _j) \
+            DOT_PRODUCT(7, _j) \
+        }
+
+        // If I had #pragma unroll I wouldn't need to do this manually...
+
+        // We need K/VEC_SIZE iterations.
+        // K = 32, VEC_SIZE = 4
+        // So, 32/4 = 8 iterations.
+        ITERATION(0);
+        ITERATION(1);
+        ITERATION(2);
+        ITERATION(3);
+        ITERATION(4);
+        ITERATION(5);
+        ITERATION(6);
+        ITERATION(7);
+
+#undef ITERATION
+#undef DOT_PRODUCT
+
+        src0 += TILE_K / VEC_SIZE;
+    }
+
+    dst[0] = c0; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c1; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c2; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c3; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c4; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c5; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c6; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+    dst[0] = c7; dst += OUTPUT_SIZE_Y_PITCH_UNIT_4;
+};
+
+)__krnl"},
+
+{"fully_connected_gpu_fb_io_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// Required JIT constants:
+//  - FP16_SUPPORTED       - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED       - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE            - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO        - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT_BATCH_NUM      - [int] Number of elements from single spatial and single feature that are grouped in single batch in input.
+//  - INPUT_ELEMENTS_COUNT - [int] Cumulative number of elements from input that are processed in single batch.
+//  - FILTER_OFM_NUM       - [int] Cumulative number of elements that are outputted in single batch.
+//  - RELU                 - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE       - [float] Factor for negative output values (required when ReLU is specified).
+
+
+KERNEL (fully_connected_gpu_xb_xb)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint x = get_global_id(0);
+    const uint batch_id = x % INPUT0_BATCH_NUM;
+
+    const uint outXIdx = x / INPUT0_BATCH_NUM;
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+
+    uint input_idx = MULTIPLY_OFFSET(UNIT_TYPE, batch_id);
+    uint weight_idx = MULTIPLY_OFFSET(UNIT_TYPE, outXIdx);
+
+    for (uint i = 0; i < INPUT0_ELEMENTS_COUNT; i++)
+    {
+        UNIT_TYPE _in = *OFFSET_GLOBAL_PTR(UNIT_TYPE, input, input_idx);
+        UNIT_TYPE _w =  *OFFSET_GLOBAL_PTR(UNIT_TYPE, weight, weight_idx);
+        result += _in * _w;
+        input_idx  += MULTIPLY_OFFSET(UNIT_TYPE, INPUT0_BATCH_NUM);
+        weight_idx += MULTIPLY_OFFSET(UNIT_TYPE, FILTER_OFM_NUM);
+    }
+
+#if BIAS_TERM
+)__krnl"
+R"__krnl(    result += bias[outXIdx];
+#endif
+
+    output[x] = ACTIVATION(result, NL_M, NL_N);
+}
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_3x3_dw_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if FP16_UNIT_USED == 0
+    #define ALIGNED_BLOCK_READ(ptr, offset) as_float(intel_sub_group_block_read((const __global uint*)(ptr) + (offset)))
+#endif
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+KERNEL(convolution_gpu_bfyx_3x3_dw_opt)(
+    __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    __global UNIT_TYPE* weights,
+#if BIAS_TERM
+    __global UNIT_TYPE* biases,
+#endif
+    uint split_idx)
+{
+    const uint local_id = get_local_id(0);
+    const uint tile_x = get_global_id(0);
+    const uint tile_y = get_global_id(1);
+    const uint bf = get_global_id(2);
+    const uint f = bf % INPUT0_FEATURE_NUM;
+    const uint b = bf / INPUT0_FEATURE_NUM;
+
+    const uint start_x = tile_x / SUB_GROUP_SIZE * TILE_WIDTH;
+    const uint offset_x = start_x + (tile_x - tile_x / SUB_GROUP_SIZE * SUB_GROUP_SIZE) % TILE_WIDTH;
+    const uint offset = b * INPUT0_BATCH_PITCH + INPUT0_FEATURE_PITCH * f;
+    const uint out_offset = b * OUTPUT_BATCH_PITCH + OUTPUT_FEATURE_PITCH * f;
+
+    const int start_y = tile_y * TILE_HEIGHT;
+    const int end_y = min(INPUT0_SIZE_Y - 1, start_y + TILE_HEIGHT - 1);
+    const uint weight_offset = f * FILTER_IFM_PITCH + local_id;
+
+    // Read 3 lines of SUB_GROUP_SIZE floats.
+    // The 3 lines start one float before the current (to the left) and one line up:
+    // SUB_GROUP_SIZE=16
+    // 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
+    // 0 X 2 3 4 5 6 7 8 9 10 11 12 13 14 15
+    // 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
+    // In the diagram above X represents the current work item.
+
+    const int input_offset_const = INPUT0_OFFSET + offset + (start_y * INPUT0_Y_PITCH + start_x) - 1;
+
+)__krnl"
+R"__krnl(    const uint base_addr_offset = INPUT0_Y_PITCH;
+
+    UNIT_TYPE input_buffer[3] = { UNIT_VAL_ZERO };
+    const int base_offset = -base_addr_offset * UNIT_BYTE_SIZE;
+
+#if FP16_UNIT_USED
+    const uint lid = get_sub_group_local_id();
+    if(input_offset_const - base_addr_offset >= 0)
+        input_buffer[0] = input[input_offset_const - base_addr_offset + lid];
+    if(input_offset_const >= 0)
+        input_buffer[1] = input[input_offset_const + lid];
+#else
+    input_buffer[0] = ALIGNED_BLOCK_READ(input, input_offset_const - base_addr_offset);
+    input_buffer[1] = ALIGNED_BLOCK_READ(input, input_offset_const);
+#endif
+
+    UNIT_TYPE w = weights[weight_offset];
+
+    int first = 0;
+    int second = 1;
+    int third = 2;
+    int input_offset = input_offset_const;
+
+    for (int y = start_y; y <= end_y; y++)
+    {
+        UNIT_TYPE res = UNIT_VAL_ZERO;
+        input_offset += base_addr_offset;
+
+#if FP16_UNIT_USED
+        if(input_offset >= 0)
+            input_buffer[third] = input[input_offset + lid];
+#else
+        input_buffer[third] = ALIGNED_BLOCK_READ(input, input_offset);
+#endif
+
+        uint kc = 0;
+        LOOP(FILTER_SIZE_X, kc,
+        {
+            res = mad(intel_sub_group_shuffle( w, FILTER_SIZE_Y + kc),intel_sub_group_shuffle( input_buffer[second], local_id + kc),res);
+
+            if (y == 0)
+            {
+            res = mad(intel_sub_group_shuffle( w, 2*FILTER_SIZE_Y + kc),intel_sub_group_shuffle( input_buffer[third], local_id + kc),res);
+            }
+            else if (y == INPUT0_SIZE_Y - 1)
+            {
+            res = mad(intel_sub_group_shuffle( w, kc),intel_sub_group_shuffle( input_buffer[first], local_id + kc),res);
+            }
+            else
+            {
+            res = mad(intel_sub_group_shuffle( w, kc),intel_sub_group_shuffle( input_buffer[first], local_id + kc),res);
+            res = mad(intel_sub_group_shuffle( w, 2*FILTER_SIZE_Y + kc),intel_sub_group_shuffle( input_buffer[third], local_id + kc),res);
+            }
+        });
+
+#if BIAS_TERM
+        res += biases[f];
+#endif
+
+        if ((local_id < TILE_WIDTH) && (offset_x < INPUT0_SIZE_X))
+        {
+            output[OUTPUT_OFFSET + out_offset + y * INPUT0_SIZE_X + offset_x] = ACTIVATION(res, NL_M, NL_N);
+        }
+
+        first = (first + 1) % 3;
+        second = (second + 1) % 3;
+        third = (third + 1) % 3;
+    }
+
+}
+
+#undef ALIGNED_BLOCK_READ
+
+)__krnl"},
+
+{"lrn_gpu_within_channel",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+KERNEL (lrn_gpu_within_channel)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    for (uint index = get_global_id(0) ; index < INPUT0_LENGTH ; index += get_global_size(0))
+    {
+#if   defined OUTPUT_LAYOUT_YXFB
+        const uint batch_id   = index % INPUT0_BATCH_NUM;
+        const uint yxf        = index / INPUT0_BATCH_NUM;
+        const uint feature_id = yxf   % INPUT0_FEATURE_NUM;
+        const uint yx         = yxf   / INPUT0_FEATURE_NUM;
+        const uint x          = yx    % INPUT0_SIZE_X;
+        const uint y          = yx    / INPUT0_SIZE_X;
+#elif defined OUTPUT_LAYOUT_BFYX
+        const uint x          = index % INPUT0_SIZE_X;
+        const uint bfy        = index / INPUT0_SIZE_X;
+        const uint y          = bfy   % INPUT0_SIZE_Y;
+        const uint bf         = bfy   / INPUT0_SIZE_Y;
+        const uint feature_id = bf    % INPUT0_FEATURE_NUM;
+        const uint batch_id   = bf    / INPUT0_FEATURE_NUM;
+#endif
+
+        const uint first_index_in_feature = INPUT0_OFFSET + batch_id*INPUT0_BATCH_PITCH + feature_id*INPUT0_FEATURE_PITCH;
+        const uint input_id = first_index_in_feature + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH;
+
+        int wstart = x - PADDING;
+        int hstart = y - PADDING;
+        int hend = min(hstart + LOCAL_SIZE, INPUT0_SIZE_Y + PADDING);
+        int wend = min(wstart + LOCAL_SIZE, INPUT0_SIZE_X + PADDING);
+        const int pool_size = (hend - hstart) * (wend - wstart);
+        hstart = max(hstart, (int)0);
+        wstart = max(wstart, (int)0);
+        hend = min(hend, INPUT0_SIZE_Y);
+        wend = min(wend, INPUT0_SIZE_X);
+        UNIT_TYPE aveval = 0;
+
+        __global const UNIT_TYPE* bottom_slice = input + first_index_in_feature;
+        for (int h = hstart; h < hend; ++h)
+        {
+            for (int w = wstart; w < wend; ++w)
+            {
+                UNIT_TYPE tmp_val = bottom_slice[h*INPUT0_Y_PITCH + w*INPUT0_X_PITCH] * TO_UNIT_TYPE(ALPHA_VAL_FACTOR);
+                aveval += (tmp_val * tmp_val);
+            }
+        }
+
+        UNIT_TYPE acc = aveval / pool_size;
+        acc = mad(acc, TO_UNIT_TYPE(ALPHA_AFTER_FACTORED), TO_UNIT_TYPE(K));
+        acc = native_powr(acc, -TO_UNIT_TYPE(BETA));
+
+        const uint output_idx = OUTPUT_OFFSET + batch_id*OUTPUT_BATCH_PITCH + feature_id*OUTPUT_FEATURE_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH;
+        output[output_idx] = ACTIVATION(acc * input[input_id], NL_M ,NL_N);
+    }
+}
+
+)__krnl"},
+
+{"batch_norm_grad_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define LOCAL_SIZE INPUT0_BATCH_NUM
+
+__attribute__((reqd_work_group_size(LOCAL_SIZE, 1, 1)))
+KERNEL(batch_norm_grad_gpu)(const __global UNIT_TYPE* input_grad, __global UNIT_TYPE* input, __global UNIT_TYPE* inv_var,  __global UNIT_TYPE* output)
+{
+    __local ACCUMULATOR_TYPE grad_sum[LOCAL_SIZE];
+    __local ACCUMULATOR_TYPE grad_sum_in[LOCAL_SIZE];
+
+    const uint local_idx = (uint)get_local_id(0);
+    const uint f = (uint)get_global_id(1);
+
+    grad_sum[local_idx] = 0;
+    grad_sum_in[local_idx] = 0;
+
+    uint grad_idx = GET_DATA_INDEX(INPUT0, local_idx, f, 0, 0);
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            UNIT_TYPE in_g = input_grad[grad_idx];
+            grad_sum[local_idx] += in_g;
+            grad_sum_in[local_idx] += in_g * input[grad_idx];
+            grad_idx += INPUT0_X_PITCH;
+        }
+        grad_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X * INPUT0_X_PITCH;
+    }
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    for(uint offset = LOCAL_SIZE / 2; offset > 0; offset /= 2)
+    {
+        if (local_idx < offset)
+        {
+            grad_sum[local_idx] += grad_sum[local_idx + offset];
+            grad_sum_in[local_idx] += grad_sum_in[local_idx + offset];
+        }
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    UNIT_TYPE grad_mean = grad_sum[0] / (OUTPUT_BATCH_NUM * OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+    UNIT_TYPE grad_mean_in = grad_sum_in[0] / (OUTPUT_BATCH_NUM * OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+)__krnl"
+R"__krnl(
+    uint out_idx = GET_DATA_INDEX(OUTPUT, local_idx, f, 0, 0);
+    for (uint y = 0; y < OUTPUT_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < OUTPUT_SIZE_X; x++)
+        {
+            UNIT_TYPE grad_out = inv_var[f] * (input_grad[out_idx] - grad_mean - grad_mean_in * input[out_idx]);
+
+            if (grad_out > 5.0f)
+                grad_out = 5.0f;
+            else if (grad_out < -5.0f)
+                grad_out = -5.0f;
+
+            output[out_idx] = grad_out;
+            out_idx += OUTPUT_X_PITCH;
+        }
+        out_idx += OUTPUT_Y_PITCH - OUTPUT_SIZE_X * OUTPUT_X_PITCH;
+    }
+
+}
+
+#undef LOCAL_SIZE
+
+)__krnl"},
+
+{"lstm_gemm_gpu_bfyx_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#ifndef DIRECTION
+#define DIRECTION 0
+#endif
+
+// input     = [    batch,  sequence,               1,      input_size ]
+// weights   = [        1, direction, 4 * hidden_size,      input_size ]
+// recurrent = [        1, direction, 4 * hidden_size,     hidden_size ]
+// biases    = [        1,         1,       direction, 4 * hidden_size ] optional
+// hidden    = [    batch, direction,               1,     hidden_size ] optional
+// tempGEMM  = [    batch, direction,               1, 4 * hidden_size ] output
+KERNEL(lstm_gemm)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    const __global WEIGHTS_TYPE* weights
+#if HIDDEN_TERM
+    , const __global OUTPUT_TYPE* hidden,
+    const __global RECURRENT_TYPE* recurrent
+#endif
+#if BIAS_TERM
+    , const __global BIAS_TYPE* biases
+#endif
+    )
+{
+    const uint y = get_global_id(0);
+    const uint b = get_global_id(1);
+
+    ACCUMULATOR_TYPE dotProd = 0;
+    for(uint x = 0; x < INPUT0_SIZE_X; ++x ) {
+      const uint input_idx     = GET_DATA_INDEX(INPUT0, b, 0, 0, x);
+      const uint weights_idx   = GET_DATA_INDEX(WEIGHTS, 0, DIRECTION, y, x);
+      dotProd += (ACCUMULATOR_TYPE)(input[input_idx] * weights[weights_idx]);
+    }
+
+#if HIDDEN_TERM
+    for(uint x = 0; x < HIDDEN_SIZE_X; ++x ) {
+      const uint hidden_idx    = GET_DATA_INDEX(HIDDEN, b, 0, 0, x);
+      const uint recurrent_idx = GET_DATA_INDEX(RECURRENT, 0, DIRECTION, y, x);
+      dotProd += (ACCUMULATOR_TYPE)(hidden[hidden_idx] * recurrent[recurrent_idx]);
+    }
+#endif
+
+)__krnl"
+R"__krnl(#if BIAS_TERM
+    const uint bias_idx = GET_DATA_INDEX(BIAS, 0, 0, DIRECTION, y);
+    dotProd += (ACCUMULATOR_TYPE)biases[bias_idx];
+#endif
+    const uint output_idx = GET_DATA_INDEX(OUTPUT, b, 0, 0, y);
+    output[output_idx] = (OUTPUT_TYPE)dotProd;
+}
+
+)__krnl"},
+
+{"convolution_gpu_1x1_gemm_mmad",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+)__krnl"
+R"__krnl(        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+#define FILTER_IFM_MMAD_NUM ((FILTER_IFM_NUM + 31) / 32)
+#define FILTER_OFM_MMAD_NUM ((FILTER_OFM_NUM + 7) / 8)
+#define FILTER_IFM_ALIGNED (FILTER_IFM_MMAD_NUM * 32)
+#define FILTER_OFM_ALIGNED (FILTER_OFM_MMAD_NUM * 8)
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+KERNEL(convolution_1x1_gemm_MMAD)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+#if QUANTIZATION_TERM
+    __global float* quantizations,
+#endif
+#if CALIBRATION_TERM
+    __global float* calibrations,
+#endif
+    uint split_idx)
+{
+    const uint sg_channel = get_sub_group_local_id();
+
+    const uint x = (get_group_id(0) * 8) % INPUT0_SIZE_X;
+    const uint y = (get_group_id(0) * 8) / INPUT0_SIZE_X;
+    const uint f = get_global_id(1) % FILTER_OFM_ALIGNED;
+    const uint b = get_global_id(1) / FILTER_OFM_ALIGNED;
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+    uint in_addr = input_offset + input_x * INPUT0_X_PITCH + input_y * INPUT0_Y_PITCH;
+
+    const uint filter_offset = (get_group_id(1) % FILTER_OFM_MMAD_NUM) * FILTER_OFM_BLOCK_PITCH;
+    uint filter_idx = filter_offset;
+
+    int8 tileA;
+    int8 tileB;
+    int8 tileC;
+    for(uint i = 0; i < 8; i++)
+    {
+        tileC[i] = 0;
+    }
+
+    for (uint k = 0; k < FILTER_IFM_MMAD_NUM; ++k)
+    {
+        // load A tile ( input )
+        for(uint i = 0; i < 8; i++)
+        {
+            uint tmp_addr = in_addr + i * INPUT0_X_PITCH;
+            tileA[i] = as_int(intel_sub_group_block_read((const __global uint*)(input + tmp_addr)));
+        }
+
+        // load B tile ( weights )
+        tileB = as_int8(intel_sub_group_block_read8((const __global uint*)(weights + filter_idx)));
+
+        // compute C tile ( output )
+        tileC = MMAD_8x8(tileA, tileB, tileC);
+
+        in_addr += 32; // 4 features per channel * 8 SIMD channels
+        filter_idx += 32*8; // 32 features per channel * 8 output features per SIMD channel
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+    for(uint i = 0; i < 8; i++)
+    {
+#if CALIBRATION_TERM
+    tileC[i] = (UNIT_TYPE)round(((float)tileC[i] * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    tileC[i] = (UNIT_TYPE)round(((float)tileC[i] * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+    }
+#endif // BIAS_TERM
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    // save to output
+    for(uint i = 0; i < 8; i++)
+    {
+        const uint curr_x = (x + i) % INPUT0_SIZE_X;
+        const uint curr_y = y + (x + i) / INPUT0_SIZE_X;
+        if(curr_x < INPUT0_SIZE_X && curr_y < INPUT0_SIZE_Y)
+        {
+            const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, curr_y, curr_x) + out_split_offset;
+            output[dst_index] = ACTIVATION(convert_char(tileC[i]), NL_M, NL_N);
+        }
+    }
+}
+
+#undef FILTER_IFM_MMAD_NUM
+#undef FILTER_OFM_MMAD_NUM
+#undef FILTER_IFM_ALIGNED
+#undef FILTER_OFM_ALIGNED
+
+)__krnl"},
+
+{"convolution_gpu_yxfb_yxio_b1_block_multiple_x_fp32",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((reqd_work_group_size(LOCAL_WORK_GROUP_SIZE, 1, 1)))
+KERNEL(convolution_gpu_yxfb_yxio_b1_block_multiple_x)(
+    const __global float* input,
+    __global float* output,
+    const __global float* filter,
+#if BIAS_TERM
+    const __global float* bias,
+#endif
+    uint split_idx)
+{
+#if USE_VECTOR == 8
+    #define VECTOR_FLOAT float8
+    #define BLOCK_READ(IN) as_float8(intel_sub_group_block_read8((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write8((__global uint*)OUT, as_uint8(DATA));
+#endif
+#if USE_VECTOR == 4
+    #define VECTOR_FLOAT float4
+    #define BLOCK_READ(IN) as_float4(intel_sub_group_block_read4((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write4((__global uint*)OUT, as_uint4(DATA));
+#endif
+#if USE_VECTOR == 2
+    #define VECTOR_FLOAT float2
+    #define BLOCK_READ(IN) as_float2(intel_sub_group_block_read2((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write2((__global uint*)OUT, as_uint2(DATA));
+#endif
+#if USE_VECTOR == 1
+    #define VECTOR_FLOAT float
+    #define BLOCK_READ(IN) as_float(intel_sub_group_block_read((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write((__global uint*)OUT, as_uint(DATA));
+#endif
+
+    const uint batch_num = INPUT0_BATCH_NUM;
+    const uint linear_id_xy = (uint)get_group_id(1) * X_PER_WORK_ITEM + OUTPUT_SIZE_X * (uint)get_group_id(2);
+    uint global_id = (((uint)get_group_id(0) * LOCAL_WORK_GROUP_SIZE) / batch_num) * batch_num + ( linear_id_xy * FILTER_ARRAY_NUM + split_idx) * (FILTER_OFM_NUM / OFM_PER_WORK_ITEM) * batch_num;
+
+    const uint out_batch_id = (uint)get_local_id(0) % INPUT0_BATCH_NUM;
+    const uint out_x = (uint)get_group_id(1) * X_PER_WORK_ITEM;
+    const uint out_y = get_group_id(2);
+
+    uint out_id[X_PER_WORK_ITEM];
+    for(uint i = 0; i < X_PER_WORK_ITEM; i++)
+    {
+)__krnl"
+R"__krnl(        out_id[i] = OUTPUT_OFFSET + ( (global_id + i * FILTER_ARRAY_NUM * (FILTER_OFM_NUM / OFM_PER_WORK_ITEM) * INPUT0_BATCH_NUM) / batch_num) * OFM_PER_WORK_ITEM * batch_num + out_batch_id;
+    }
+
+    const uint ofm_offset = (global_id * (OFM_PER_WORK_ITEM / batch_num)) % FILTER_OFM_NUM;
+
+    const uint sub_group_id = (uint)get_local_id(0) % INPUT0_BATCH_NUM;
+
+    VECTOR_FLOAT _data[X_PER_WORK_ITEM];
+    for(uint i = 0; i < X_PER_WORK_ITEM; i++)
+    {
+        _data[i] = 0.0f;
+    }
+
+    const int x = (int)out_x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int y = (int)out_y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i * DILATION_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j * DILATION_SIZE_X;
+
+                bool zero_x[X_PER_WORK_ITEM];
+                for(int z = 0; z < X_PER_WORK_ITEM; z++)
+                {
+                    zero_x[z] = (input_offset_x + z * STRIDE_SIZE_X) >= INPUT0_SIZE_X || (input_offset_x + z * STRIDE_SIZE_X) < 0;
+                }
+
+                VECTOR_FLOAT _tmp[X_PER_WORK_ITEM];
+                for(uint t = 0; t < X_PER_WORK_ITEM; t++)
+                {
+                    _tmp[t] = 0.f;
+                }
+
+                uint input_idx = input_offset_x*INPUT0_X_PITCH + input_offset_y*INPUT0_Y_PITCH;
+                input_idx += INPUT0_OFFSET + split_idx * FILTER_IFM_NUM * INPUT0_FEATURE_PITCH;
+                input_idx += out_batch_id;
+
+                uint filter_idx = ofm_offset + sub_group_id + i*FILTER_Y_PITCH + j*FILTER_X_PITCH;
+
+#if FILTER_IFM_NUM >= 8
+                for(uint h = 0; h < FILTER_IFM_NUM / 8; h++)
+                {
+                    float _in[X_PER_WORK_ITEM];
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _in[a] = as_float(intel_sub_group_block_read((const __global uint*)input + (input_idx + a * INPUT0_FEATURE_NUM * STRIDE_SIZE_X)));
+                    }
+                    float8 _input[X_PER_WORK_ITEM];
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _input[a] = TRANSPOSE_BLOCK_8(_in[a]);
+                    }
+
+                    VECTOR_FLOAT _filter;
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s0, _filter, _tmp[a]);
+                    }
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s1, _filter, _tmp[a]);
+                    }
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s2, _filter, _tmp[a]);
+                    }
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s3, _filter, _tmp[a]);
+                    }
+
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s4, _filter, _tmp[a]);
+                    }
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s5, _filter, _tmp[a]);
+                    }
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s6, _filter, _tmp[a]);
+                    }
+
+                    _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(_input[a].s7, _filter, _tmp[a]);
+                    }
+
+                    input_idx += 8 * INPUT0_FEATURE_PITCH;
+                }
+                for (uint h = FILTER_IFM_NUM - (FILTER_IFM_NUM % 8); h < FILTER_IFM_NUM; h++)
+#else
+                for (uint h = 0; h < FILTER_IFM_NUM; h++)
+#endif
+                {
+                    VECTOR_FLOAT _filter = BLOCK_READ(filter + filter_idx);
+                    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                    {
+                        _tmp[a] = mad(input[input_idx + a * INPUT0_FEATURE_NUM * STRIDE_SIZE_X], _filter, _tmp[a]);
+                    }
+                    filter_idx += FILTER_IFM_PITCH;
+                    input_idx += INPUT0_FEATURE_PITCH;
+                }
+                for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+                {
+                    if(!zero_x[a])
+                        _data[a] += _tmp[a];
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+    {
+        _data[a] += BLOCK_READ(bias + ofm_offset);
+    }
+#endif
+    for(uint a = 0; a < X_PER_WORK_ITEM; a++)
+    {
+        _data[a] = ACTIVATION(_data[a], NL_M, NL_N);
+    }
+
+    BLOCK_WRITE(output + out_id[0], _data[0]);
+    for(uint a = 1; a < X_PER_WORK_ITEM; a++)
+    {
+        if(out_x + a < OUTPUT_SIZE_X)
+        {
+            BLOCK_WRITE(output + out_id[a], _data[a]);
+        }
+    }
+
+#if defined(USE_VECTOR)
+    #undef VECTOR_FLOAT
+    #undef BLOCK_READ
+    #undef BLOCK_WRITE
+#endif
+}
+
+)__krnl"},
+
+{"index_select_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+
+KERNEL(index_select_gpu_ref)(
+    const __global UNIT_TYPE* input,
+    const __global int* indices,
+    __global UNIT_TYPE* output)
+{
+    // [CONSTEXPR]:
+    const uint input_sx  = INPUT0_SIZE_X;
+    const uint input_sy  = INPUT0_SIZE_Y;
+    const uint input_sf  = INPUT0_FEATURE_NUM;
+    const uint input_sb  = INPUT0_BATCH_NUM;
+
+    const uint out_b         = (uint) get_global_id(0);
+    const uint indices_idx   = (uint) get_global_id(1);
+    const uint feature_idx   = (uint) get_global_id(2);
+    const uint indices_value = indices[indices_idx];
+
+    // [LOGIC]:
+#ifdef INDEX_SELECT_AXIS_BATCH
+    for(uint x = 0; x < input_sx; x++)
+    {
+        for(uint y = 0; y < input_sy; y++)
+        {
+            output[GET_DATA_INDEX(OUTPUT, indices_idx, feature_idx, y, x)] = input[GET_DATA_INDEX(INPUT0, indices_value, feature_idx, y, x)];
+        }
+    }
+#elif defined INDEX_SELECT_AXIS_FEATURE
+    for(uint x = 0; x < input_sx; x++)
+    {
+        output[GET_DATA_INDEX(OUTPUT, out_b, indices_idx, feature_idx, x)] = input[GET_DATA_INDEX(INPUT0, out_b, indices_value, feature_idx, x)];
+    }
+#elif defined INDEX_SELECT_AXIS_X
+    for(uint i = 0; i < input_sx; i++)
+    {
+        output[GET_DATA_INDEX(OUTPUT, out_b, feature_idx, i, indices_idx)] = input[GET_DATA_INDEX(INPUT0, out_b, feature_idx, i, indices_value)];
+    }
+#elif defined INDEX_SELECT_AXIS_Y
+
+    for(uint i = 0; i < input_sx; i++)
+    {
+        output[GET_DATA_INDEX(OUTPUT, out_b, feature_idx, indices_idx, i)] = input[GET_DATA_INDEX(INPUT0, out_b, feature_idx, indices_value, i)];
+    }
+)__krnl"
+R"__krnl(#endif
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_bf_io_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// Required JIT constants:
+//  - FP16_SUPPORTED        - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED        - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE             - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO         - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT0_BATCH_NUM      - [int] Number of elements from single spatial and single feature that are grouped in single batch in input.
+//  - INPUT0_ELEMENTS_COUNT - [int] Cumulative number of elements from input that are processed in single batch.
+//  - FILTER_OFM_NUM        - [int] Cumulative number of elements that are outputted in single batch.
+//  - RELU                  - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE        - [float] Factor for negative output values (required when ReLU is specified).
+
+
+KERNEL (fully_connected_gpu_bx_xb_from_fyxb)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint x = get_global_id(0);
+    const uint batch_id = x / FILTER_OFM_NUM;
+
+    const uint outXIdx = x % FILTER_OFM_NUM;
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+
+    uint input_idx = batch_id * INPUT0_ELEMENTS_COUNT;
+    input_idx = MULTIPLY_OFFSET(UNIT_TYPE, input_idx);
+    uint weight_idx = MULTIPLY_OFFSET(UNIT_TYPE, outXIdx);
+    for (uint i = 0; i < INPUT0_ELEMENTS_COUNT; i++)
+    {
+        UNIT_TYPE _in = *OFFSET_GLOBAL_PTR(UNIT_TYPE, input, input_idx);
+        UNIT_TYPE _w =  *OFFSET_GLOBAL_PTR(UNIT_TYPE, weight, weight_idx);
+        result += _in * _w;
+        input_idx  += MULTIPLY_OFFSET(UNIT_TYPE, 1);
+        weight_idx += MULTIPLY_OFFSET(UNIT_TYPE, FILTER_OFM_NUM);
+    }
+#if BIAS_TERM
+    result += bias[outXIdx];
+)__krnl"
+R"__krnl(#endif
+    output[x] = ACTIVATION(result, NL_M, NL_N);
+}
+
+
+)__krnl"},
+
+{"lrn_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+)__krnl"
+R"__krnl(    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+KERNEL(normalization)(__global const INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+    const uint b = get_global_id(GWS_BATCH);
+    const uint f = get_global_id(GWS_FEATURE);
+    const uint y = get_global_id(GWS_YX) / INPUT0_SIZE_X;
+    const uint x = get_global_id(GWS_YX) % INPUT0_SIZE_X;
+
+    const uint input_index  = GET_DATA_INDEX(INPUT0, b, f, y, x);
+    const uint output_index = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+
+    ACCUMULATOR_TYPE sum = 0.0f;
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    uint num_elementes = 0;
+#endif
+
+#ifdef ACROSS_CHANNEL
+
+    uint j_offset = input_index - PADDING*INPUT0_FEATURE_PITCH;
+
+    for(int j = 0 ; j < LOCAL_SIZE ; j++)
+    {
+        const int z_idx = (j + f - PADDING);
+        bool zero = (z_idx < 0 || z_idx >= INPUT0_FEATURE_NUM);
+        UNIT_TYPE val = zero ? 0.0f : input[j_offset];
+        sum += val*val;
+        j_offset += INPUT0_FEATURE_PITCH;
+#ifdef DYNAMIC_KERNEL_DIVIDER
+        num_elementes += zero ? 0 : 1;
+#endif
+    }
+
+#else
+
+    const int x_start = ((int)x - PADDING);
+    const int y_start = ((int)y - PADDING);
+    int input_offset = GET_DATA_INDEX(INPUT0, b, f, y_start, x_start);
+
+    for (int j = 0; j < LOCAL_SIZE ; ++j)
+    {
+        for (int i = 0; i < LOCAL_SIZE ; ++i)
+        {
+            int input_offset_x = x_start + i;
+            int input_offset_y = y_start + j;
+            bool zero = false;
+            zero = input_offset_x < 0 ? true : zero;
+            zero = input_offset_y < 0 ? true : zero;
+            zero = input_offset_x >= INPUT0_SIZE_X ? true : zero;
+)__krnl"
+R"__krnl(            zero = input_offset_y >= INPUT0_SIZE_Y ? true : zero;
+
+            UNIT_TYPE val = zero ? UNIT_VAL_ZERO : input[input_offset];
+
+            sum += val*val;
+            input_offset += INPUT0_X_PITCH;
+#ifdef DYNAMIC_KERNEL_DIVIDER
+            num_elementes += zero ? 0 : 1;
+#endif
+        }
+        input_offset += INPUT0_Y_PITCH - LOCAL_SIZE*INPUT0_X_PITCH;
+    }
+#endif
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    const UNIT_TYPE num_elementes_div = UNIT_VAL_ONE / TO_UNIT_TYPE(num_elementes);
+#else
+    const UNIT_TYPE num_elementes_div = NUM_ELEMENTS_DIV;
+#endif
+
+    const UNIT_TYPE base = TO_UNIT_TYPE(K) + TO_UNIT_TYPE((ACCUMULATOR_TYPE)ALPHA*sum * num_elementes_div);
+    const UNIT_TYPE normalization_factor = native_powr(base, TO_UNIT_TYPE(-BETA));
+
+    const UNIT_TYPE val = input[input_index];
+    const UNIT_TYPE normres =  val*normalization_factor;
+    output[output_index] = ACTIVATION(normres, NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"reshape_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+KERNEL (reshape_ref)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    const uint d1 = get_global_id(0);
+    const uint d2 = get_global_id(1);
+    const uint d3 = get_global_id(2) % INPUT0_SIZES[2];
+    const uint d4 = get_global_id(2) / INPUT0_SIZES[2];
+
+    uint linear = d1 + d2*INPUT0_SIZES[0] + d3*INPUT0_SIZES[0]*INPUT0_SIZES[1] + d4*INPUT0_SIZES[0]*INPUT0_SIZES[1]*INPUT0_SIZES[2];
+
+    const uint od1 = linear % OUTPUT_SIZES[0]; linear /= OUTPUT_SIZES[0];
+    const uint od2 = linear % OUTPUT_SIZES[1]; linear /= OUTPUT_SIZES[1];
+    const uint od3 = linear % OUTPUT_SIZES[2]; linear /= OUTPUT_SIZES[2];
+    const uint od4 = linear;
+
+    uint input_offset =  INPUT0_OFFSET +
+                         d1*INPUT0_PITCHES[0] +
+                         d2*INPUT0_PITCHES[1] +
+                         d3*INPUT0_PITCHES[2] +
+                         d4*INPUT0_PITCHES[3];
+    uint output_offset = OUTPUT_OFFSET +
+                         od1*OUTPUT_PITCHES[0] +
+                         od2*OUTPUT_PITCHES[1] +
+                         od3*OUTPUT_PITCHES[2] +
+                         od4*OUTPUT_PITCHES[3];
+
+    output[output_offset] = ACTIVATION(input[input_offset], NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"convolution_grad_weights_3x3",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(convolution_grad_weights_gpu_3x3)(
+    const __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output,
+    __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    __global UNIT_TYPE* bias,
+#endif
+#if MOMENTUM
+    __global UNIT_TYPE* prev_grad_w,
+#if BIAS_TERM
+    __global UNIT_TYPE* prev_grad_b,
+#endif
+#endif
+    const __global UNIT_TYPE* input,
+    uint split_idx,
+    float lr)
+{
+    const uint ofm = get_global_id(0);
+    const uint ifm = get_global_id(1);
+
+    if (ofm >= INPUT0_FEATURE_NUM || ifm >= INPUT1_FEATURE_NUM)
+        return;
+
+    const int in_x = -PADDING_SIZE_X;
+    const int in_y = -PADDING_SIZE_Y;
+
+    ACCUMULATOR_TYPE grad_w[9] = {};
+#if BIAS_TERM
+    ACCUMULATOR_TYPE grad_b = 0;
+#endif
+
+    uint weights_idx = ofm * FILTER_OFM_PITCH + ifm * FILTER_IFM_PITCH;
+
+    for(int b = 0; b < INPUT0_BATCH_NUM; b++)
+    {
+        const uint grad_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_OFM_NUM;
+        const uint in_split_offset = split_idx * INPUT1_FEATURE_PITCH * FILTER_IFM_NUM;
+
+        for (uint i = 0; i < INPUT0_SIZE_Y; i++)
+        {
+
+            for (uint j = 0; j < INPUT0_SIZE_X; j+=2)
+)__krnl"
+R"__krnl(            {
+                float2 grad;
+                if (j + 1 >= INPUT0_SIZE_X)
+                {
+                    uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                    grad.s0 = input_grad[input_grad_idx];
+                    grad.s1 = 0;
+                }
+                else
+                {
+                    uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                    grad = vload2(0, &input_grad[input_grad_idx]);
+                }
+                for (uint y = 0; y < 3; y++)
+                {
+                    const int input_offset_y = in_y + y + i;
+                    const bool zero_y = input_offset_y >= INPUT1_SIZE_Y || input_offset_y < 0;
+                    const int input_offset_x = in_x + j;
+                    const bool zero_x = input_offset_x < 0 || input_offset_x + 3 >= INPUT1_SIZE_X;
+                    uint input_idx = in_split_offset + b*INPUT1_BATCH_PITCH + ifm*INPUT1_FEATURE_PITCH + (uint)input_offset_x*INPUT1_X_PITCH + (uint)input_offset_y*INPUT1_Y_PITCH;
+                    union v4 {
+                        float s[4];
+                        float4 v;
+                    };
+                    union v4 inp;
+                    if (zero_y)
+                        continue;
+                    if (zero_x)
+                    {
+                        for (uint k = 0; k < 4; k++)
+                        {
+                            if (input_offset_x + k >= INPUT1_SIZE_X || input_offset_x + k < 0)
+                                inp.s[k] = 0;
+                            else
+                                inp.s[k] = input[input_idx + k];
+                        }
+                    }
+                    else
+                    {
+                        inp.v = vload4(0, &input[input_idx]);
+                    }
+                    for (uint x = 0; x < 3; x++)
+                    {
+                        grad_w[y * 3 + x] = mad(inp.s[x] * lr, grad.s0, grad_w[y * 3 + x]);
+                        grad_w[y * 3 + x] = mad(inp.s[x + 1] * lr, grad.s1, grad_w[y * 3 + x]);
+                    }
+                }
+#if BIAS_TERM
+                grad_b += grad.s0;
+                grad_b += grad.s1;
+#endif
+            }
+        }
+    }
+
+    union {
+        float  s[8];
+        float8 v;
+    } uweights_0_7;
+    float uweights8;
+
+#if MOMENTUM
+    float dwa[9];
+    uweights_0_7.v = vload8(0, &prev_grad_w[weights_idx]);
+    dwa[0 * 3 + 0] = uweights_0_7.v.s0;
+    dwa[0 * 3 + 1] = uweights_0_7.v.s1;
+    dwa[0 * 3 + 2] = uweights_0_7.v.s2;
+    dwa[1 * 3 + 0] = uweights_0_7.v.s3;
+    dwa[1 * 3 + 1] = uweights_0_7.v.s4;
+    dwa[1 * 3 + 2] = uweights_0_7.v.s5;
+    dwa[2 * 3 + 0] = uweights_0_7.v.s6;
+    dwa[2 * 3 + 1] = uweights_0_7.v.s7;
+    dwa[2 * 3 + 2] = prev_grad_w[weights_idx + 8];
+#endif
+
+    uweights_0_7.v = vload8(0, &filter[weights_idx]);
+    uweights8 = filter[weights_idx + 8];
+
+#if MOMENTUM
+    float8 newDelta_0_7 = (float8)(
+                                    grad_w[0 * 3 + 0] + (MOMENTUM_FACTOR * dwa[0 * 3 + 0]),
+                                    grad_w[0 * 3 + 1] + (MOMENTUM_FACTOR * dwa[0 * 3 + 1]),
+                                    grad_w[0 * 3 + 2] + (MOMENTUM_FACTOR * dwa[0 * 3 + 2]),
+                                    grad_w[1 * 3 + 0] + (MOMENTUM_FACTOR * dwa[1 * 3 + 0]),
+                                    grad_w[1 * 3 + 1] + (MOMENTUM_FACTOR * dwa[1 * 3 + 1]),
+                                    grad_w[1 * 3 + 2] + (MOMENTUM_FACTOR * dwa[1 * 3 + 2]),
+                                    grad_w[2 * 3 + 0] + (MOMENTUM_FACTOR * dwa[2 * 3 + 0]),
+                                    grad_w[2 * 3 + 1] + (MOMENTUM_FACTOR * dwa[2 * 3 + 1]));
+    float newDelta8 =               grad_w[2 * 3 + 2] + (MOMENTUM_FACTOR * dwa[2 * 3 + 2]);
+#else
+    float8 newDelta_0_7 = (float8)(
+                                    grad_w[0 * 3 + 0],
+                                    grad_w[0 * 3 + 1],
+                                    grad_w[0 * 3 + 2],
+                                    grad_w[1 * 3 + 0],
+                                    grad_w[1 * 3 + 1],
+                                    grad_w[1 * 3 + 2],
+                                    grad_w[2 * 3 + 0],
+                                    grad_w[2 * 3 + 1]);
+    float newDelta8 =               grad_w[2 * 3 + 2];
+#endif
+    uweights8      -= newDelta8;
+    uweights_0_7.v -= newDelta_0_7;
+
+    vstore8(uweights_0_7.v, 0, &filter[weights_idx]);
+    filter[weights_idx + 8] = uweights8;
+#if MOMENTUM
+    vstore8(newDelta_0_7, 0, &prev_grad_w[weights_idx]);
+    prev_grad_w[weights_idx + 8] = newDelta8;
+#endif
+
+#if BIAS_TERM
+    if(ifm == 0)
+    {
+#if MOMENTUM
+        UNIT_TYPE update_gradient_b = lr * grad_b + prev_grad_b[ofm] * MOMENTUM_FACTOR;
+        bias[ofm] -= update_gradient_b;
+        prev_grad_b[ofm] = update_gradient_b;
+#else
+        bias[ofm] -= lr * grad_b;
+#endif
+    }
+#endif
+}
+
+)__krnl"},
+
+{"scale_grad_weights_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define LOCAL_SIZE INPUT0_BATCH_NUM
+
+KERNEL(scale_grad_weights_gpu_ref)(
+    const __global UNIT_TYPE* input,
+    const __global UNIT_TYPE* input_grad,
+    __global OUTPUT_TYPE* output,
+	__global float* scale,
+#if BIAS_TERM
+    __global float* bias,
+#endif
+#if MOMENTUM
+    __global float* prev_grad_w,
+#if BIAS_TERM
+    __global float* prev_grad_b,
+#endif
+#endif
+    const float lr
+    )
+{
+    __local ACCUMULATOR_TYPE grad_sum[LOCAL_SIZE];
+    __local ACCUMULATOR_TYPE grad_sum_in[LOCAL_SIZE];
+
+    const uint local_idx = (uint)get_local_id(0);
+    const uint f = (uint)get_global_id(1);
+
+    grad_sum[local_idx] = 0;
+    grad_sum_in[local_idx] = 0;
+
+    uint grad_idx = GET_DATA_INDEX(INPUT0, local_idx, f, 0, 0);
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            ACCUMULATOR_TYPE in_g = TO_ACCUMULATOR_TYPE(input_grad[grad_idx]);
+            grad_sum[local_idx] += in_g * lr;
+            grad_sum_in[local_idx] += in_g * TO_ACCUMULATOR_TYPE(input[grad_idx]) * lr;
+            grad_idx += INPUT0_X_PITCH;
+        }
+        grad_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X * INPUT0_X_PITCH;
+    }
+
+)__krnl"
+R"__krnl(    barrier(CLK_LOCAL_MEM_FENCE);
+
+    for(uint offset = LOCAL_SIZE / 2; offset > 0; offset /= 2)
+    {
+        if (local_idx < offset)
+        {
+            grad_sum[local_idx] += grad_sum[local_idx + offset];
+            grad_sum_in[local_idx] += grad_sum_in[local_idx + offset];
+        }
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    if (local_idx == 0)
+    {
+#if MOMENTUM
+    ACCUMULATOR_TYPE update_gradient_w = grad_sum_in[0] + prev_grad_w[f] * MOMENTUM_FACTOR + DECAY_RATE * lr * scale[f];
+    scale[f] -= update_gradient_w;
+    prev_grad_w[f] = update_gradient_w;
+#else
+    scale[f] -= grad_sum_in[0] + DECAY_RATE * lr * scale[f];
+#endif
+
+#if BIAS_TERM
+#if MOMENTUM
+    ACCUMULATOR_TYPE update_gradient_b = prev_grad_b[f] * MOMENTUM_FACTOR + grad_sum[0];
+    bias[f] -= update_gradient_b;
+    prev_grad_b[f] = update_gradient_b;
+#else
+    bias[f] -= grad_sum[0];
+#endif
+#endif
+    }
+}
+
+#undef LOCAL_SIZE
+
+)__krnl"},
+
+{"reorder_weights_image_2d_c4_fyx_b",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL (reorder_weights_image_2d_c4_fyx_b)(const __global INPUT0_TYPE* input, write_only image2d_t output)
+{
+    const unsigned o = get_global_id(0);
+    const unsigned iyx = get_global_id(1);
+    const unsigned x = iyx % INPUT0_SIZE_X;
+    const unsigned y = (iyx / INPUT0_SIZE_X) % INPUT0_SIZE_Y;
+    const unsigned i = y / INPUT0_SIZE_Y;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 4) input_val = (MAKE_VECTOR_TYPE(UNIT_TYPE, 4))(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+
+    const int2 coord = (int2)(iyx, o);
+    uint input_idx = o * INPUT0_OFM_PITCH + iyx*4;
+
+    input_val.s0 = TO_OUTPUT_TYPE(input[input_idx]);
+    if(iyx*4 + 1 < INPUT0_OFM_PITCH)
+        input_val.s1 = TO_OUTPUT_TYPE(input[input_idx+1]);
+    if(iyx*4 + 2 < INPUT0_OFM_PITCH)
+        input_val.s2 = TO_OUTPUT_TYPE(input[input_idx+2]);
+    if(iyx*4 + 3 < INPUT0_OFM_PITCH)
+        input_val.s3 = TO_OUTPUT_TYPE(input[input_idx+3]);
+    IMAGE_WRITE(output, coord, input_val);
+}
+
+)__krnl"},
+
+{"batch_norm_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define LOCAL_SIZE INPUT0_BATCH_NUM
+
+__attribute__((reqd_work_group_size(LOCAL_SIZE, 1, 1)))
+KERNEL(batch_norm_gpu)(
+    const __global UNIT_TYPE* input,
+#ifdef FORWARD
+     __global UNIT_TYPE* inv_var,
+#endif
+       __global UNIT_TYPE* output)
+{
+    __local ACCUMULATOR_TYPE sum[LOCAL_SIZE];
+
+    const uint local_idx = (uint)get_global_id(0);
+    const uint f = (uint)get_global_id(1);
+
+    sum[local_idx] = 0;
+
+    uint input_idx = GET_DATA_INDEX(INPUT0, local_idx, f, 0, 0);
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            UNIT_TYPE in = input[input_idx];
+            sum[local_idx] += in;
+            input_idx += INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X * INPUT0_X_PITCH;
+    }
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    for(uint offset = LOCAL_SIZE / 2; offset > 0; offset /= 2)
+    {
+        if (local_idx < offset)
+        {
+            sum[local_idx] += sum[local_idx + offset];
+        }
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    UNIT_TYPE mean = sum[0] / (OUTPUT_BATCH_NUM * OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+)__krnl"
+R"__krnl(
+    sum[local_idx] = 0;
+
+    input_idx = GET_DATA_INDEX(INPUT0, local_idx, f, 0, 0);
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            UNIT_TYPE in = input[input_idx] - mean;
+            sum[local_idx] += in * in;
+            input_idx += INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X * INPUT0_X_PITCH;
+    }
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    for(uint offset = LOCAL_SIZE / 2; offset > 0; offset /= 2)
+    {
+        if (local_idx < offset)
+        {
+            sum[local_idx] += sum[local_idx + offset];
+        }
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    float variance = sum[0] / (OUTPUT_BATCH_NUM * OUTPUT_SIZE_X * OUTPUT_SIZE_Y);
+
+    float inv_variance = (float)(1.0 / sqrt(variance + EPSILON));
+#ifdef FORWARD
+    if (local_idx == 0)
+        inv_var[f] = inv_variance;
+#endif
+
+    uint out_idx = GET_DATA_INDEX(OUTPUT, local_idx, f, 0, 0);
+    for (uint y = 0; y < OUTPUT_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < OUTPUT_SIZE_X; x++)
+        {
+            output[out_idx] = inv_variance * (input[out_idx] - mean);
+            out_idx += OUTPUT_X_PITCH;
+        }
+        out_idx += OUTPUT_Y_PITCH - OUTPUT_SIZE_X * OUTPUT_X_PITCH;
+    }
+}
+
+)__krnl"},
+
+{"roi_pooling_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+/****************************************************************************
+ *                                                                          *
+ *                               Utility Defines                            *
+ *                                                                          *
+ ***************************************************************************/
+
+// Each RoI is described by 5 elements, the first one being unused. This is
+// required for the kernel to have the same API as other implmentations.
+#define ROI_NUM_ELEMENTS 5
+
+#define SRC_W INPUT0_SIZE_X
+#define SRC_H INPUT0_SIZE_Y
+#define DST_W POOLED_WIDTH
+#define DST_H POOLED_HEIGHT
+#define PITCH_ROI_R INPUT1_BATCH_PITCH
+
+#if GROUP_SIZE == 0
+#define DST_C INPUT0_FEATURE_NUM
+#else
+#define DST_C (GROUP_SIZE ? (INPUT0_FEATURE_NUM / GROUP_SIZE / GROUP_SIZE) : INPUT0_FEATURE_NUM)
+#endif
+
+// Note: In the non-ROI_OLD case we keep the coordinates in float instead
+//       of using UNIT_TYPE, since with FP16 we might actually lose some
+//       precision in the coordinates, given a sufficiently large W or H.
+#define COORD_T float
+#define ACCUM_T float
+
+#define MIN(a,b) ((a) < (b) ? (a) : (b))
+#define MAX(a,b) ((a) > (b) ? (a) : (b))
+#define CLAMP(v,l,u) MAX((l),MIN((v),(u)))
+
+#if INPUT1_FEATURE_NUM != ROI_NUM_ELEMENTS
+#error - unknown ROI_POOLING kernel type
+#endif
+
+/****************************************************************************
+ *                                                                          *
+ *                                RoI Pooling                               *
+ *                                                                          *
+ ***************************************************************************/
+
+KERNEL(roi_pooling_gpu)
+(
+    const __global INPUT0_TYPE * src_data,
+    __global OUTPUT_TYPE * dst_data,
+    const __global INPUT1_TYPE * src_rois
+)
+{
+    const size_t i = get_global_id(0);
+
+    const uint x = i % DST_W;
+    const uint y = i / DST_W % DST_H;
+    const uint c = i / DST_W / DST_H % DST_C;
+    const uint r = i / DST_W / DST_H / DST_C % OUTPUT_ROI_NUM;
+    // const uint b = i / DST_W / DST_H / DST_C / OUTPUT_ROI_NUM; - TODO: support batching correctly
+    // Note: The rounding of the coordinates is done prior to the mul
+    //       with SPATIAL_SCALE: It makes sense since the resolution of
+    //       the pooled data is limited by its dimensions. (Is this clear?)
+
+    const __global INPUT1_TYPE * roi_ptr = &src_rois[PITCH_ROI_R * r];
+
+#if BILINEAR_POOLING
+    const uint output_offset = OUTPUT_OFFSET + x*OUTPUT_X_PITCH + y*OUTPUT_Y_PITCH + c*OUTPUT_FEATURE_PITCH + r*OUTPUT_ROI_PITCH;
+
+    COORD_T roi_start_w = roi_ptr[1];
+    COORD_T roi_start_h = roi_ptr[2];
+    COORD_T roi_end_w   = roi_ptr[3];
+    COORD_T roi_end_h   = roi_ptr[4];
+
+    COORD_T height_scale = (roi_end_h - roi_start_h) * (SRC_H - 1) / (COORD_T)(POOLED_HEIGHT - 1);
+    COORD_T width_scale  = (roi_end_w - roi_start_w) * (SRC_W - 1) / (COORD_T)(POOLED_WIDTH  - 1);
+
+    COORD_T in_y = y*height_scale + roi_start_h*(COORD_T)(SRC_H - 1);
+    COORD_T in_x = x*width_scale  + roi_start_w*(COORD_T)(SRC_W - 1);
+
+    if (in_y < 0 || in_y > (COORD_T)(SRC_H - 1) || in_x < 0 || in_x > (COORD_T)(SRC_W - 1) || roi_ptr[0] == -1) {
+        dst_data[output_offset] = ACTIVATION((OUTPUT_TYPE)0, NL_M, NL_N);
+        return;
+    }
+
+    int top_y_index    = (int)(floor(in_y));
+    int bottom_y_index = (int)(min(ceil(in_y), (COORD_T)SRC_H - 1));
+    int left_x_index   = (int)(floor(in_x));
+    int right_x_index  = (int)(min(ceil(in_x), (COORD_T)SRC_W - 1));
+
+    const __global INPUT0_TYPE* data = src_data + INPUT0_OFFSET + INPUT0_FEATURE_PITCH*c;
+
+    ACCUM_T top_left     = (ACCUM_T)data[top_y_index*INPUT0_Y_PITCH + left_x_index*INPUT0_X_PITCH];
+    ACCUM_T top_right    = (ACCUM_T)data[top_y_index*INPUT0_Y_PITCH + right_x_index*INPUT0_X_PITCH];
+    ACCUM_T bottom_left  = (ACCUM_T)data[bottom_y_index*INPUT0_Y_PITCH + left_x_index*INPUT0_X_PITCH];
+    ACCUM_T bottom_right = (ACCUM_T)data[bottom_y_index*INPUT0_Y_PITCH + right_x_index*INPUT0_X_PITCH];
+
+    ACCUM_T top    = top_left + (top_right - top_left) * (in_x - left_x_index);
+    ACCUM_T bottom = bottom_left + (bottom_right - bottom_left) * (in_x - left_x_index);
+
+    ACCUM_T res = top + (bottom - top) * (in_y - top_y_index);
+
+    dst_data[output_offset] = ACTIVATION((OUTPUT_TYPE)res, NL_M, NL_N);
+#else
+
+#if USE_OLD_SCALE_AND_ROUNDING
+    const int roi_x  = round(roi_ptr[1] * SPATIAL_SCALE);
+    const int roi_y  = round(roi_ptr[2] * SPATIAL_SCALE);
+    const int roi_x1 = round(roi_ptr[3] * SPATIAL_SCALE);
+    const int roi_y1 = round(roi_ptr[4] * SPATIAL_SCALE);
+
+    // The final coordinate is within the ROI and malformed dimensions are treated as 1
+    const uint roi_w = max(roi_x1 - roi_x, 0) + 1;
+    const uint roi_h = max(roi_y1 - roi_y, 0) + 1;
+#else
+    const COORD_T roi_x  = (COORD_T)(round(roi_ptr[1]) + 0.f) * SPATIAL_SCALE;
+    const COORD_T roi_y  = (COORD_T)(round(roi_ptr[2]) + 0.f) * SPATIAL_SCALE;
+    const COORD_T roi_x1 = (COORD_T)(round(roi_ptr[3]) + 1.f) * SPATIAL_SCALE;
+    const COORD_T roi_y1 = (COORD_T)(round(roi_ptr[4]) + 1.f) * SPATIAL_SCALE;
+
+    // The final coordinate is within the ROI and malformed dimensions are treated as 1
+    const COORD_T roi_w = max(roi_x1 - roi_x, .1f);
+    const COORD_T roi_h = max(roi_y1 - roi_y, .1f);
+#endif
+
+    // Note that when the "after" is rounded rounded up else we get the last cell,
+    // instead of the cell beyond (For "symmetry").
+    //
+    // For ex. with src being a 6 cell row and dest being a 4 cell one:
+    // >>> [((x + 0) * 6) // 4 for x in [0, 1, 2, 3]]   # "begin" values
+)__krnl"
+R"__krnl(    // [0, 1, 3, 4]                                     # as expected
+    // >>> [((x + 1) * 6) // 4 for x in [0, 1, 2, 3]]   # "after" values
+    // [1, 3, 4 ,6]                                     # [2, 3, 5, 6] expected!
+#if USE_OLD_SCALE_AND_ROUNDING
+    const int dx_begin = ((x + 0) * roi_w) / DST_W;
+    const int dy_begin = ((y + 0) * roi_h) / DST_H;
+    const int dx_after = ((x + 1) * roi_w + (DST_W - 1)) / DST_W;
+    const int dy_after = ((y + 1) * roi_h + (DST_H - 1)) / DST_H;
+
+    // clamp in case roi_x or roi_y were unreasonable
+    const int x_begin = clamp(roi_x + dx_begin, 0, SRC_W);
+    const int y_begin = clamp(roi_y + dy_begin, 0, SRC_H);
+    const int x_after = clamp(roi_x + dx_after, 0, SRC_W);
+    const int y_after = clamp(roi_y + dy_after, 0, SRC_H);
+#else
+    const COORD_T dx_begin = (x + 0) * (COORD_T)(roi_w / DST_W);
+    const COORD_T dy_begin = (y + 0) * (COORD_T)(roi_h / DST_H);
+    const COORD_T dx_after = (x + 1) * (COORD_T)(roi_w / DST_W);
+    const COORD_T dy_after = (y + 1) * (COORD_T)(roi_h / DST_H);
+
+    // clamp in case roi_x or roi_y were unreasonable
+    const int x_begin = CLAMP(floor(roi_x + dx_begin), 0, SRC_W);
+    const int y_begin = CLAMP(floor(roi_y + dy_begin), 0, SRC_H);
+    const int x_after = CLAMP(ceil(roi_x + dx_after), 0, SRC_W);
+    const int y_after = CLAMP(ceil(roi_y + dy_after), 0, SRC_H);
+#endif
+
+#if GROUP_SIZE == 0
+    const uint work_c = c;
+#else
+
+#if 0
+    const COORD_T group_bin_w = (COORD_T)roi_w / DST_W;
+    const COORD_T group_bin_h = (COORD_T)roi_h / DST_H;
+
+    const uint group_x = CLAMP(x * group_bin_w, 0, GROUP_SIZE - 1);
+    const uint group_y = CLAMP(y * group_bin_h, 0, GROUP_SIZE - 1);
+#else
+    const uint group_x = x;
+    const uint group_y = y;
+#endif
+
+    const uint work_c = group_x + GROUP_SIZE * (group_y + GROUP_SIZE * c);
+#endif
+
+    const __global INPUT0_TYPE* data = src_data + INPUT0_OFFSET + INPUT0_FEATURE_PITCH*work_c;
+
+#if MAX_POOLING
+    ACCUM_T res = x_begin < x_after && y_begin < y_after ? -FLT_MAX : 0;
+#else
+    ACCUM_T res = 0;
+#endif
+
+    for (int yy = y_begin; yy < y_after; ++yy)
+    for (int xx = x_begin; xx < x_after; ++xx)
+    {
+        INPUT0_TYPE val = data[xx*INPUT0_X_PITCH + yy*INPUT0_Y_PITCH];
+#if MAX_POOLING
+        res = MAX(res, (ACCUM_T)val);
+#else
+        res = res + (ACCUM_T)val;
+#endif
+    }
+
+#if (!MAX_POOLING)
+    {
+        //TODO(ruv): again, differs from the standard fixed size area (?)
+        const COORD_T area = (y_after - y_begin) * (x_after - x_begin);
+        if (area) res /= area;
+    }
+#endif
+
+    const uint output_offset = OUTPUT_OFFSET + x*OUTPUT_X_PITCH + y*OUTPUT_Y_PITCH + c*OUTPUT_FEATURE_PITCH + r*OUTPUT_ROI_PITCH;
+    dst_data[output_offset] = ACTIVATION((OUTPUT_TYPE)res, NL_M, NL_N);
+#endif
+}
+
+)__krnl"},
+
+{"convolution_grad_weights_1x1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_grad_weights_gpu_1x1)(
+    const __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output,
+    __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    __global UNIT_TYPE* bias,
+#endif
+#if MOMENTUM
+    __global UNIT_TYPE* prev_grad_w,
+#if BIAS_TERM
+    __global UNIT_TYPE* prev_grad_b,
+#endif
+#endif
+    const __global UNIT_TYPE* input,
+    uint split_idx,
+    float lr)
+{
+    const uint local_id = get_local_id(0);
+    const uint ifm      = get_global_id(1);
+    const uint ofm      = get_global_id(2);
+
+    const int in_x = -PADDING_SIZE_X;
+    const int in_y = -PADDING_SIZE_Y;
+
+    ACCUMULATOR_TYPE grad_w = 0;
+
+#if BIAS_TERM
+    ACCUMULATOR_TYPE grad_b = 0;
+#endif
+
+    uint weights_idx = ofm * FILTER_OFM_PITCH + ifm * FILTER_IFM_PITCH;
+
+    for(int b = 0; b < INPUT0_BATCH_NUM; b++)
+    {
+        UNIT_TYPE result = UNIT_VAL_ZERO;
+
+#if BIAS_TERM
+        UNIT_TYPE result_bias = UNIT_VAL_ZERO;
+#endif
+
+        const uint grad_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_OFM_NUM;
+)__krnl"
+R"__krnl(        const uint in_split_offset = split_idx * INPUT1_FEATURE_PITCH * FILTER_IFM_NUM;
+
+        for (uint i = 0; i < INPUT0_SIZE_Y; i++)
+        {
+		    const int input_offset_y = in_y + i * STRIDE_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT1_SIZE_Y || input_offset_y < 0;
+            for (uint j = 0; j < (INPUT0_SIZE_X + 15)/16; j++)
+            {
+                const int input_offset_x = in_x + j * STRIDE_SIZE_X * 16 + local_id * STRIDE_SIZE_X;
+                const bool zero_x = input_offset_x >= INPUT1_SIZE_X || input_offset_x < 0;
+                const bool grad_zero = j*16 + local_id >= INPUT0_SIZE_X;
+#if BIAS_TERM
+                UNIT_TYPE grad;
+                if(grad_zero)
+                {
+                    grad = 0;
+                }
+                else
+                {
+                    uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH*16 + local_id*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                    grad = input_grad[input_grad_idx];
+                }
+#endif
+                if(!zero_x && !zero_y)
+                {
+                    uint input_idx = in_split_offset + b*INPUT1_BATCH_PITCH + ifm*INPUT1_FEATURE_PITCH + (uint)input_offset_x*INPUT1_X_PITCH + (uint)input_offset_y*INPUT1_Y_PITCH;
+#if BIAS_TERM
+                    result = fma(input[input_idx], grad, result);
+#else
+                    if(!grad_zero)
+                    {
+                        uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH*16 + local_id*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                        result = fma(input[input_idx], input_grad[input_grad_idx], result);
+                    }
+#endif
+                }
+#if BIAS_TERM
+                result_bias += grad;
+#endif
+            }
+        }
+
+        grad_w += result;
+
+#if BIAS_TERM
+        grad_b += result_bias;
+#endif
+    }
+
+    grad_w = sub_group_reduce_add(grad_w);
+#if BIAS_TERM
+    grad_b = sub_group_reduce_add(grad_b);
+#endif
+
+    if (local_id == 0)
+    {
+#if MOMENTUM
+        UNIT_TYPE update_gradient_w = lr * (prev_grad_w[weights_idx] * MOMENTUM_FACTOR + grad_w + DECAY_RATE * filter[weights_idx]);
+        filter[weights_idx] -= update_gradient_w;
+        prev_grad_w[weights_idx] = update_gradient_w;
+#else
+        filter[weights_idx] -= lr * grad_w + DECAY_RATE * lr * filter[weights_idx];
+#endif
+
+#if BIAS_TERM
+        if(ifm == 0)
+        {
+#if MOMENTUM
+            UNIT_TYPE update_gradient_b = lr * grad_b + prev_grad_b[ofm] * MOMENTUM_FACTOR;
+            bias[ofm] -= update_gradient_b;
+            prev_grad_b[ofm] = update_gradient_b;
+#else
+            bias[ofm] -= lr * grad_b;
+#endif
+        }
+#endif
+    }
+}
+
+)__krnl"},
+
+{"lrn_gpu_within_channel_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+KERNEL (lrn_gpu_within_channel_opt)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    uint index = get_global_id(0);
+#if   defined OUTPUT_LAYOUT_YXFB
+    const uint yxf        = index / INPUT0_BATCH_NUM;
+    const uint batch_id   = index - yxf * INPUT0_BATCH_NUM;
+    const uint yx         = yxf / INPUT0_FEATURE_NUM;
+    const uint feature_id = yxf - yx * INPUT0_FEATURE_NUM;
+    const uint y          = yx / INPUT0_SIZE_X;
+    const uint x          = yx - y * INPUT0_SIZE_X;
+
+#elif defined OUTPUT_LAYOUT_BFYX
+    const uint bfy        = index / INPUT0_SIZE_X;
+    const uint x          = index - bfy * INPUT0_SIZE_X;
+    const uint bf         = bfy / INPUT0_SIZE_Y;
+    const uint y          = bfy - bf * INPUT0_SIZE_Y;
+    const uint batch_id   = bf / INPUT0_FEATURE_NUM;
+    const uint feature_id = bf - batch_id * INPUT0_FEATURE_NUM;
+#endif
+
+    const uint first_index_in_feature = INPUT0_OFFSET + batch_id * INPUT0_BATCH_PITCH + feature_id * INPUT0_FEATURE_PITCH;
+    const uint input_id = first_index_in_feature + y * INPUT0_Y_PITCH + x * INPUT0_X_PITCH;
+
+    UNIT_TYPE aveval = 0;
+    uint pool_size = 0;
+    int wstart = x - PADDING;
+    int hstart = y - PADDING;
+
+
+    if (((hstart + LOCAL_SIZE) < INPUT0_SIZE_Y) &&
+        ((wstart + LOCAL_SIZE) < INPUT0_SIZE_X) &&
+        (x > PADDING) &&
+        (y > PADDING))
+    {
+        pool_size = LOCAL_SIZE * LOCAL_SIZE;
+
+        __global const UNIT_TYPE* bottom_slice = input + first_index_in_feature + hstart * INPUT0_Y_PITCH + wstart * INPUT0_X_PITCH;
+        for (int h = 0; h < LOCAL_SIZE; ++h)
+        {
+            uint hPitch = h * INPUT0_Y_PITCH;
+            for (int w = 0; w < LOCAL_SIZE; ++w)
+            {
+                UNIT_TYPE tmp_val = bottom_slice[hPitch + w * INPUT0_X_PITCH] * TO_UNIT_TYPE(ALPHA_VAL_FACTOR);
+                aveval = mad(tmp_val, tmp_val, aveval);
+            }
+        }
+    }
+    else
+    {
+        int hend = min(hstart + LOCAL_SIZE, INPUT0_SIZE_Y + PADDING);
+        int wend = min(wstart + LOCAL_SIZE, INPUT0_SIZE_X + PADDING);
+        pool_size = (hend - hstart) * (wend - wstart);
+        hstart = max(hstart, (int)0);
+        wstart = max(wstart, (int)0);
+        hend = min(hend, INPUT0_SIZE_Y);
+        wend = min(wend, INPUT0_SIZE_X);
+
+        __global const UNIT_TYPE* bottom_slice = input + first_index_in_feature;
+        for (uint h = hstart; h < hend; ++h)
+        {
+            uint hPitch = h * INPUT0_Y_PITCH;
+            for (uint w = wstart; w < wend; ++w)
+            {
+                UNIT_TYPE tmp_val = bottom_slice[hPitch + w * INPUT0_X_PITCH] * TO_UNIT_TYPE(ALPHA_VAL_FACTOR);
+                aveval = mad(tmp_val, tmp_val, aveval);
+            }
+        }
+    }
+
+    UNIT_TYPE acc = aveval / pool_size;
+    acc = mad(acc, TO_UNIT_TYPE(ALPHA_AFTER_FACTORED), TO_UNIT_TYPE(K));
+    acc = native_powr(acc, -TO_UNIT_TYPE(BETA));
+
+    const uint output_idx = OUTPUT_OFFSET + batch_id * OUTPUT_BATCH_PITCH + feature_id * OUTPUT_FEATURE_PITCH + y * OUTPUT_Y_PITCH + x * OUTPUT_X_PITCH;
+    output[output_idx] = ACTIVATION(acc * input[input_id], NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"convolution_grad_weights_7x7",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(convolution_grad_weights_gpu_7x7)(
+    const __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output,
+    __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    __global UNIT_TYPE* bias,
+#endif
+#if MOMENTUM
+    __global UNIT_TYPE* prev_grad_w,
+#if BIAS_TERM
+    __global UNIT_TYPE* prev_grad_b,
+#endif
+#endif
+    const __global UNIT_TYPE* input,
+    uint split_idx,
+    float lr)
+{
+    const uint x_filter = get_global_id(0);
+    const uint ofm = get_global_id(1);
+    const uint ifm = get_global_id(2);
+
+    if (x_filter >= 7 || ofm >= INPUT0_FEATURE_NUM || ifm >= INPUT1_FEATURE_NUM)
+        return;
+
+    const int in_x = -PADDING_SIZE_X;
+    const int in_y = -PADDING_SIZE_Y;
+
+    ACCUMULATOR_TYPE grad_w[7] = { 0, 0, 0, 0, 0, 0, 0 };
+#if BIAS_TERM
+    ACCUMULATOR_TYPE grad_b = UNIT_VAL_ZERO;
+#endif
+
+    uint weights_idx = ofm * FILTER_OFM_PITCH + ifm * FILTER_IFM_PITCH;
+
+    for(int b = 0; b < INPUT0_BATCH_NUM; b++)
+    {
+        const uint grad_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_OFM_NUM;
+        const uint in_split_offset = split_idx * INPUT1_FEATURE_PITCH * FILTER_IFM_NUM;
+
+        for(int i = 0; i < INPUT0_SIZE_Y; i++)
+        {
+            for(int j = 0; j < INPUT0_SIZE_X; j++)
+)__krnl"
+R"__krnl(            {
+                float grad;
+                uint input_grad_idx = grad_split_offset + b*INPUT0_BATCH_PITCH + ofm*INPUT0_FEATURE_PITCH + j*INPUT0_X_PITCH + i*INPUT0_Y_PITCH;
+                grad = input_grad[input_grad_idx];
+                for(uint y_filter = 0; y_filter < 7; y_filter++)
+                {
+                    const int input_offset_y = in_y + y_filter + i * STRIDE_SIZE_Y;
+                    const bool zero_y = input_offset_y >= INPUT1_SIZE_Y || input_offset_y < 0;
+                    const int input_offset_x = in_x + x_filter + j * STRIDE_SIZE_X;
+                    const bool zero_x = input_offset_x < 0 || input_offset_x >= INPUT1_SIZE_X;
+                    uint input_idx = in_split_offset + b*INPUT1_BATCH_PITCH + ifm*INPUT1_FEATURE_PITCH + (uint)input_offset_x*INPUT1_X_PITCH + (uint)input_offset_y*INPUT1_Y_PITCH;
+                    if(!zero_x && !zero_y)
+                    {
+                        const float delta_f = input[input_idx] * lr * grad;
+                        grad_w[y_filter] += delta_f;
+                    }
+                }
+#if BIAS_TERM
+                grad_b += grad;
+#endif
+            }
+        }
+    }
+    for(uint y_filter = 0; y_filter < 7; y_filter++)
+    {
+        uint address = weights_idx + 48 - (7 * (6 - y_filter) + (6 - x_filter));
+#if MOMENTUM
+        float dw = prev_grad_w[address];
+        const float delta_f_m = MOMENTUM_FACTOR * dw;
+        grad_w[y_filter] += delta_f_m;
+        prev_grad_w[address] = grad_w[y_filter];
+#endif
+        filter[address] -= grad_w[y_filter];
+    }
+#if BIAS_TERM
+    if(ifm == 0 && x_filter == 0)
+    {
+#if MOMENTUM
+        UNIT_TYPE update_gradient_b = lr * grad_b + prev_grad_b[ofm] * MOMENTUM_FACTOR;
+        bias[ofm] -= update_gradient_b;
+        prev_grad_b[ofm] = update_gradient_b;
+#else
+        bias[ofm] -= lr * grad_b;
+#endif
+    }
+#endif
+}
+
+)__krnl"},
+
+{"softmax_gpu_bf",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+__attribute__((reqd_work_group_size(LWS, 1, 1)))
+KERNEL (softmax_gpu_continoues_bfyx)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    const uint data_set_idx = get_global_id(1);     //in processing of which data set this WI participates?
+    const uint workers_per_data_set = LWS;          //how many WI participates in processing of one data set
+    const uint in_data_set_idx = get_global_id(0);  //this WI's id in group of items processing single data set
+    const uint data_set_size = DATA_SET_SIZE;       //how many elements are in one data set
+    const uint data_sets_count = DATA_SETS_COUNT;   //how many data sets are in the processing payload
+
+    const uint data_set_offset = data_set_idx * data_set_size;
+    const uint my_data_offset = data_set_offset + in_data_set_idx;
+
+    UNIT_TYPE my_chunk[ITEMS_NUM + 1];
+    UNIT_TYPE my_maximum = -UNIT_VAL_MAX;
+    UNIT_TYPE my_sum = UNIT_VAL_ZERO;
+    UNIT_TYPE tmp;
+
+    __local UNIT_TYPE lg_storage[LWS];
+
+    //each WI reads ITEMS_NUM consecutive items from batch
+    for (uint i=0; i<ITEMS_NUM; ++i)
+    {
+        tmp = input[my_data_offset + i * workers_per_data_set];
+        my_maximum = max(my_maximum, tmp);
+        my_chunk[i] = tmp;
+    }
+
+    if (in_data_set_idx < LEFTOVERS)
+    {
+        tmp = input[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx];
+        my_maximum = max(my_maximum, tmp);
+        my_chunk[ITEMS_NUM] = tmp;
+    }
+
+    lg_storage[in_data_set_idx] = my_maximum;
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+    if (in_data_set_idx == 0)
+    {
+        for (uint i=1; i<LWS; ++i)
+            my_maximum = max(my_maximum, lg_storage[i]);
+
+        lg_storage[0] = my_maximum;
+    }
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    //my_maximum from this point is in fact global maximum
+    my_maximum = lg_storage[0];
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    for (uint i=0; i<ITEMS_NUM; ++i)
+    {
+        tmp = native_exp(my_chunk[i] - my_maximum);
+        my_sum += tmp;
+        my_chunk[i] = tmp;
+    }
+
+    if (in_data_set_idx < LEFTOVERS)
+    {
+        tmp = native_exp(my_chunk[ITEMS_NUM] - my_maximum);
+        my_sum += tmp;
+        my_chunk[ITEMS_NUM] = tmp;
+    }
+
+    lg_storage[in_data_set_idx] = my_sum;
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+    if (in_data_set_idx == 0)
+    {
+        for (uint i=1; i<LWS; ++i)
+            my_sum += lg_storage[i];
+
+        lg_storage[0] = my_sum;
+    }
+    barrier(CLK_LOCAL_MEM_FENCE);
+
+    my_sum = lg_storage[0];
+
+    for (uint i=0; i<ITEMS_NUM; ++i)
+        output[my_data_offset + i * workers_per_data_set] = ACTIVATION(my_chunk[i] / my_sum, NL_M ,NL_N);
+    if (in_data_set_idx < LEFTOVERS)
+        output[data_set_offset + workers_per_data_set * ITEMS_NUM + in_data_set_idx] = ACTIVATION(my_chunk[ITEMS_NUM] / my_sum, NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_mmad_batched",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+)__krnl"
+R"__krnl(        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+#define FILTER_IFM_MMAD_NUM ((FILTER_IFM_NUM + 31) / 32)
+#define FILTER_OFM_MMAD_NUM ((FILTER_OFM_NUM + 7) / 8)
+#define FILTER_IFM_ALIGNED (FILTER_IFM_MMAD_NUM * 32)
+#define FILTER_OFM_ALIGNED (FILTER_OFM_MMAD_NUM * 8)
+
+__attribute__((intel_reqd_sub_group_size(8)))
+KERNEL(fully_connected_kernel_mmad_batched)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    const __global FILTER_TYPE* weights
+#if BIAS_TERM
+    , const __global BIAS_TYPE* biases
+#endif
+#if QUANTIZATION_TERM
+    ,const __global float* quantizations
+#endif
+#if CALIBRATION_TERM
+    ,const __global float* calibrations
+#endif
+    )
+{
+    const uint sg_channel = get_sub_group_local_id();
+
+    const uint batch_id = (uint)get_group_id(0) * 8;
+    const uint b_block = batch_id / 4;
+    const uint f = get_global_id(1) % FILTER_OFM_ALIGNED;
+
+    uint in_addr = IN_OFFSET + b_block * IN_B_BLOCK_PITCH;
+
+    const uint filter_offset = (get_group_id(1) % FILTER_OFM_MMAD_NUM) * FILTER_OFM_BLOCK_PITCH;
+    uint filter_idx = filter_offset;
+
+    int8 tileA;
+    int8 tileB;
+    int8 tileC = 0;
+
+    for(uint z = 0; z < FILTER_IFM_MMAD_NUM; z++ )
+    {
+        for (uint k = 0; k < FILTER_SIZE_X * FILTER_SIZE_Y; ++k)
+        {
+            // load A tile ( input )
+            // load 8 batches 4 channels per WI, so we'll have 8x32 block
+
+            tileA.lo = as_int4(intel_sub_group_block_read4((const __global uint*)(input + in_addr)));
+            tileA.hi = as_int4(intel_sub_group_block_read4((const __global uint*)(input + in_addr + IN_B_BLOCK_PITCH)));
+
+            // load B tile ( weights )
+            tileB = as_int8(intel_sub_group_block_read8((const __global uint*)(weights + filter_idx)));
+
+            // compute C tile ( output )
+            tileC = MMAD_8x8(tileA, tileB, tileC); // here we output 8 batches per workitem, and each workitem gets different output feature
+
+            in_addr += 32 * 4; // 4 batches * 4 features per channel * 8 SIMD channels
+            filter_idx += 32*8; // 32 features per channel * 8 output features per SIMD channel
+        }
+        in_addr += IN_F_BLOCK_PITCH;
+        in_addr -= (FILTER_SIZE_X * FILTER_SIZE_Y * 32 * 4);
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, batch_id, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+    for(uint i = 0; i < 8; i++)
+    {
+#if CALIBRATION_TERM
+    tileC[i] = (UNIT_TYPE)round(((float)tileC[i] * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    tileC[i] = (UNIT_TYPE)round(((float)tileC[i] * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+    }
+#endif // BIAS_TERM
+
+    // save to output
+    if(f < FILTER_OFM_NUM)
+    {
+        for(uint i = 0; i < 8; i++)
+        {
+            const uint curr_b = batch_id + i;
+#if defined OUTPUT_LAYOUT_FS_BS_YX_BSV4_FSV32
+            const uint dst_index = GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(OUTPUT, curr_b, f, 0, 0);
+#else
+            const uint dst_index = GET_DATA_INDEX(OUTPUT, curr_b, f, 0, 0);
+#endif
+            output[dst_index] = ACTIVATION(convert_char(tileC[i]), NL_M, NL_N);
+        }
+    }
+}
+
+#undef FILTER_IFM_MMAD_NUM
+#undef FILTER_OFM_MMAD_NUM
+#undef FILTER_IFM_ALIGNED
+#undef FILTER_OFM_ALIGNED
+
+)__krnl"},
+
+{"pooling_gpu_average_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+KERNEL(pooling_gpu_average_opt)(const __global float* input, __global float* output)
+{
+    int local_id = get_local_id(0);
+    int tile_x = get_global_id(0);
+    int tile_y = get_global_id(1);
+    int channel = get_global_id(2);
+
+    int start_x = tile_x / SUB_GROUP_SIZE * TILE_WIDTH;
+    int offset_x = start_x + (tile_x - tile_x / SUB_GROUP_SIZE * SUB_GROUP_SIZE) % TILE_WIDTH;
+    int offset = INPUT0_SIZE_Y * INPUT0_SIZE_X * channel;
+
+    int start_y = tile_y * TILE_HEIGHT;
+    int end_y = min(INPUT0_SIZE_Y - 1, start_y + TILE_HEIGHT - 1);
+
+    // Read 3 lines of SUB_GROUP_SIZE floats.
+    // The 3 lines start one float before the current (to the left) and one line up:
+    // For example: SUB_GROUP_SIZE=16
+    // 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
+    // 0 X 2 3 4 5 6 7 8 9 10 11 12 13 14 15
+    // 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
+    // In the diagram above X represents the current work item.
+
+    const __global float* base_addr = input + offset + (start_y * INPUT0_SIZE_X + start_x) - 1;
+
+    float input_buffer[3];
+    input_buffer[0] = as_float(intel_sub_group_block_read((const __global uint*)(base_addr - INPUT0_SIZE_X)));
+    input_buffer[1] = as_float(intel_sub_group_block_read((const __global uint*)(base_addr)));
+
+    int first = 0;
+    int second = 1;
+    int third = 2;
+    float res, sum, sum_1, sum_2;
+
+    for (int y = start_y; y <= end_y; y++)
+    {
+        base_addr += INPUT0_SIZE_X;
+
+        input_buffer[third] = as_float(intel_sub_group_block_read((const __global uint*)(base_addr)));
+
+)__krnl"
+R"__krnl(#if INPUT0_SIZE_Y == 1
+        sum = input_buffer[second];
+#else
+        if (y == 0)
+        {
+            sum = input_buffer[second] + input_buffer[third];
+        }
+        else if (y == INPUT0_SIZE_Y - 1)
+        {
+            sum = input_buffer[first] + input_buffer[second];
+        }
+        else
+        {
+            sum = input_buffer[first] + input_buffer[second] + input_buffer[third];
+        }
+#endif
+
+        sum_1 = intel_sub_group_shuffle_down(sum, 0.f, 1);
+        sum_2 = intel_sub_group_shuffle_down(sum, 0.f, 2);
+
+#if INPUT0_SIZE_X == 1
+        res = sum_1 * ONE_OVER_POOL_SIZE;
+#else
+        if (offset_x == 0)
+        {
+            res = (sum_1 + sum_2) * ONE_OVER_POOL_SIZE;
+        }
+        else if (offset_x == INPUT0_SIZE_X - 1)
+        {
+            res = (sum + sum_1) * ONE_OVER_POOL_SIZE;
+        }
+        else
+        {
+            res = (sum + sum_1 + sum_2) * ONE_OVER_POOL_SIZE;
+        }
+#endif
+
+        if ((local_id < TILE_WIDTH) && (offset_x < INPUT0_SIZE_X))
+        {
+            output[offset + y * INPUT0_SIZE_X + offset_x] = ACTIVATION(res, NL_M ,NL_N);
+        }
+
+        first = (first + 1) % 3;
+        second = (second + 1) % 3;
+        third = (third + 1) % 3;
+    }
+
+}
+
+)__krnl"},
+
+{"fully_connected_grad_weights_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(fully_connected_grad_weights_gpu_ref)(
+    const __global INPUT0_TYPE* input_grad,
+    __global OUTPUT_TYPE* output,
+    __global float* weights,
+#if BIAS_TERM
+    __global float* bias,
+#endif
+#if MOMENTUM
+    __global float* prev_grad_w,
+#if BIAS_TERM
+    __global float* prev_grad_b,
+#endif
+#endif
+    const __global INPUT1_TYPE* input,
+    const float lr
+    )
+{
+    const uint ofm_ifm       = get_global_id(0);
+    const uint id_x          = (uint)get_global_id(1);
+    const uint id_y          = (uint)get_global_id(2);
+    const uint ifm           = ofm_ifm % FILTER_IFM_NUM;
+    const uint ofm           = ofm_ifm / FILTER_IFM_NUM;
+
+    ACCUMULATOR_TYPE grad_w = 0;
+#if BIAS_TERM
+    ACCUMULATOR_TYPE grad_b = 0;
+#endif
+
+    const uint filter_idx = GET_FILTER_INDEX(FILTER, ofm, ifm, id_y, id_x);
+    for (uint b = 0; b < INPUT0_BATCH_NUM; b++)
+    {
+        const uint input_grad_idx = GET_DATA_INDEX(INPUT0, b, 0, 0, ofm);
+        const uint input_idx = GET_DATA_INDEX(INPUT1, b, ifm, id_y, id_x);
+        ACCUMULATOR_TYPE grad = TO_ACCUMULATOR_TYPE(input_grad[input_grad_idx]);
+        grad_w += TO_ACCUMULATOR_TYPE(input[input_idx] * grad);
+#if BIAS_TERM
+        grad_b += TO_ACCUMULATOR_TYPE(grad);
+#endif
+    }
+
+#if MOMENTUM
+)__krnl"
+R"__krnl(    float update_gradient_w = lr * (grad_w + DECAY_RATE * weights[filter_idx]) + prev_grad_w[filter_idx] * MOMENTUM_FACTOR;
+    weights[filter_idx] -= update_gradient_w;
+    prev_grad_w[filter_idx] = update_gradient_w;
+#else
+    weights[filter_idx] -= lr * grad_w + DECAY_RATE * lr * weights[filter_idx];
+#endif
+
+#if BIAS_TERM
+    if(ifm == 0 && id_x == 0 && id_y == 0)
+    {
+#if MOMENTUM
+        float update_gradient_b = lr * grad_b + prev_grad_b[ofm] * MOMENTUM_FACTOR;
+        bias[ofm] -= update_gradient_b;
+        prev_grad_b[ofm] = update_gradient_b;
+#else
+        bias[ofm] -= lr * grad_b;
+#endif
+    }
+#endif
+
+
+}
+
+)__krnl"},
+
+{"lookup_table_axis",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifdef BATCH_AXIS
+    #define GAP_SIZE (INPUT0_FEATURE_NUM * INPUT0_SIZE_X * INPUT0_SIZE_Y)
+    #define VALUES_NUM INPUT0_BATCH_NUM
+    #define FIRST_DIM_SIZE INPUT0_SIZE_X
+    #define SECOND_DIM_SIZE INPUT0_SIZE_Y
+    #define FIRST_DIM_MUL 1
+    #define SECOND_DIM_MUL INPUT0_SIZE_X
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y)
+#endif
+#ifdef FEATURE_AXIS
+    #define GAP_SIZE (INPUT0_SIZE_X * INPUT0_SIZE_Y)
+    #define VALUES_NUM INPUT0_FEATURE_NUM
+    #define FIRST_DIM_SIZE INPUT0_SIZE_X
+    #define SECOND_DIM_SIZE INPUT0_SIZE_Y
+    #define FIRST_DIM_MUL 1
+    #define SECOND_DIM_MUL INPUT0_SIZE_X
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM)
+#endif
+#ifdef Y_AXIS
+    #define GAP_SIZE INPUT0_SIZE_X
+    #define VALUES_NUM INPUT0_SIZE_Y
+    #define FIRST_DIM_SIZE INPUT0_SIZE_X
+    #define SECOND_DIM_SIZE INPUT0_FEATURE_NUM
+    #define FIRST_DIM_MUL 1
+    #define SECOND_DIM_MUL (INPUT0_SIZE_Y * INPUT0_SIZE_X)
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM)
+#endif
+#ifdef X_AXIS
+    #define GAP_SIZE 1
+    #define VALUES_NUM INPUT0_SIZE_X
+    #define FIRST_DIM_SIZE INPUT0_SIZE_Y
+    #define SECOND_DIM_SIZE INPUT0_FEATURE_NUM
+    #define FIRST_DIM_MUL INPUT0_SIZE_X
+    #define SECOND_DIM_MUL (INPUT0_SIZE_Y * INPUT0_SIZE_X)
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM)
+#endif
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+KERNEL(lookup_table_axis)(const __global UNIT_TYPE* input0, const __global float* indices, __global UNIT_TYPE* output)
+{
+    const uint first_dim_id = (uint)get_global_id(0);
+    const uint second_dim_id = (uint)get_global_id(1);
+    const uint third_dim_id = (uint)get_global_id(2);
+	const uint offset = first_dim_id * FIRST_DIM_MUL + second_dim_id * SECOND_DIM_MUL + third_dim_id * THIRD_DIM_MUL;
+    const uint val_index = (first_dim_id + second_dim_id * FIRST_DIM_SIZE + third_dim_id * FIRST_DIM_SIZE * SECOND_DIM_SIZE) * VAL_NUM;
+	for (uint i = 0; i < VAL_NUM; i++)
+    {
+        uint global_index = offset + (int)indices[val_index + i] * GAP_SIZE;
+        output[val_index + i] = input0[global_index];
+    }
+}
+
+
+#undef GAP_SIZE
+#undef VALUES_NUM
+#undef FIRST_DIM_SIZE
+#undef SECOND_DIM_SIZE
+#undef FIRST_DIM_MUL
+#undef SECOND_DIM_MUL
+#undef THIRD_DIM_MUL
+
+)__krnl"},
+
+{"activation_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+KERNEL(activation)(
+#if GRADIENT
+    __global UNIT_TYPE* input_grad,
+    __global UNIT_TYPE* output,
+    __global UNIT_TYPE* input
+#else
+    __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output
+#endif
+    )
+{
+    const unsigned int x = get_global_id(0) * NUM_COLS_WI;
+
+    unsigned int input_offset  = x + INPUT0_OFFSET;
+    unsigned int output_offset = x + OUTPUT_OFFSET;
+
+    typedef CAT(UNIT_TYPE, 4) type_t;
+#if GRADIENT
+    type_t g = ((__global type_t*) (input_grad + input_offset))[0];
+#endif
+    type_t v = ((__global type_t*) (input + input_offset))[0];
+
+#if GRADIENT
+    v = ACTIVATION(g, v, NL_M, NL_N);
+#else
+    v = ACTIVATION(v, NL_M, NL_N);
+#endif
+
+    *((__global type_t*)(output + output_offset)) = v;
+}
+
+)__krnl"},
+
+{"convolution_gpu_winograd_6x3_s1_fused",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// --------------------------------------------------------------------------------------------------------------------------------
+// L3_SIMD_4x8
+// Input matrices dimensions: M x K x N
+// Output matrix dimensions: M x N
+// --------------------------------------------------------------------------------------------------------------------------------
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#define DOT8i_0( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s0, sub_group_broadcast( _B.s0, i), _result);	\
+    }
+#define DOT8i_1( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s1, sub_group_broadcast( _B.s1, i), _result);	\
+    }
+#define DOT8i_2( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s2, sub_group_broadcast( _B.s2, i), _result);	\
+    }
+#define DOT8i_3( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s3, sub_group_broadcast( _B.s3, i), _result);	\
+    }
+#define DOT8i_4( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s4, sub_group_broadcast( _B.s4, i), _result);	\
+    }
+#define DOT8i_5( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s5, sub_group_broadcast( _B.s5, i), _result);	\
+    }
+#define DOT8i_6( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s6, sub_group_broadcast( _B.s6, i), _result);	\
+    }
+#define DOT8i_7( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s7, sub_group_broadcast( _B.s7, i), _result);	\
+    }
+
+#define DOT8i_( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s0, sub_group_broadcast( _B.s0, i), _result);	\
+	_result = mad(_A.s1, sub_group_broadcast( _B.s1, i), _result);	\
+	_result = mad(_A.s2, sub_group_broadcast( _B.s2, i), _result);	\
+	_result = mad(_A.s3, sub_group_broadcast( _B.s3, i), _result);	\
+	_result = mad(_A.s4, sub_group_broadcast( _B.s4, i), _result);	\
+	_result = mad(_A.s5, sub_group_broadcast( _B.s5, i), _result);	\
+	_result = mad(_A.s6, sub_group_broadcast( _B.s6, i), _result);	\
+	_result = mad(_A.s7, sub_group_broadcast( _B.s7, i), _result);	\
+    }
+
+#define UNIT_TYPE_2 MAKE_VECTOR_TYPE(UNIT_TYPE, 2)
+#define UNIT_TYPE_4 MAKE_VECTOR_TYPE(UNIT_TYPE, 4)
+#define UNIT_TYPE_8 MAKE_VECTOR_TYPE(UNIT_TYPE, 8)
+
+
+__attribute__((reqd_work_group_size(16, 1, 8)))
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_gpu_winograd_6x3_s1_fused)
+(
+	__global INPUT0_TYPE* I,
+	__global OUTPUT_TYPE* O,
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB || FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+    __read_only image2d_t  U,
+#else
+	__global FILTER_TYPE* U,
+#endif
+#if BIAS_TERM
+	const __global UNIT_TYPE * bias,
+#endif
+	uint split_idx)
+{
+	//               (DxC2)x(UxWx8c)
+	const uint slmSize = (2 * 8)*(16 * 4);
+	__local UNIT_TYPE_4 V[slmSize]; // 8 KB
+
+	/* These constants are defined as precompiler macros during compilation. */
+	const uint WC = W*INPUT0_FEATURE_NUM;
+	const uint HW = H*W;
+	const uint HWC = H*WC;
+	const uint WC4 = WC >> 2;
+	const uint K16 = FILTER_OFM_NUM >> 4;
+	const uint C4 = INPUT0_FEATURE_NUM >> 2;
+	const uint K2 = FILTER_OFM_NUM >> 1;
+	const uint QK2 = Q*K2;
+	const uint QK = Q*FILTER_OFM_NUM;
+	const uint PQK = P*QK;
+
+    const UNIT_TYPE sc = 0.1h;
+    const UNIT_TYPE scl = 1.0h/sc;
+    const UNIT_TYPE_4 scl_vec = (UNIT_TYPE_4)(sc, sc, sc, sc);
+
+	uint gx = get_group_id(0);
+	uint gy = get_group_id(1);
+	uint gz = get_group_id(2);
+	uint gk = gz % K16;
+	uint gn = gz / K16;
+
+#define lx get_local_id(0)
+#define lz get_local_id(2)
+
+	uint lxd8 = lx >> 3;
+	uint lxm8 = lx % 8;
+
+	// Load 16x8 input tile, with 2 pixel overlap in X and y.
+	// Compute 14x6 output tile.
+	// Load 32 filters.
+	// 8 threads total
+	int x = gx * 14 + lz * 2 + lxd8 - px;
+	int y = gy * 6 - py;
+	uint k = gk * 16;
+	uint c0 = lxm8 * 4;
+
+	// #                                  x->
+	// #     M0    M1    M2    M3    M4    M5    M6
+	// #   +------------------------------------------
+	// # u | 0 1 | 0 1 | 0 1 | 0 1 | 0 1 | 0 1 | 0 1 |
+	// # | | 2 3 | 2 3 | 2 3 | 2 3 | 2 3 | 2 3 | 2 3 |
+	// # v
+	// #
+
+	UNIT_TYPE_2 M0 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+	UNIT_TYPE_2 M1 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+	UNIT_TYPE_2 M2 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+	UNIT_TYPE_2 M3 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+	UNIT_TYPE_2 M4 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+)__krnl"
+R"__krnl(	UNIT_TYPE_2 M5 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+	UNIT_TYPE_2 M6 = (UNIT_TYPE_2)(UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+
+	/*if (gy == 0) {
+		y = 0;
+	}*/
+
+	uint lxm4 = lx % 4;
+	uint lxb2 = (lx & 4) / 4;
+
+#if INPUT0_LAYOUT_BYXF
+	uint adr = gn*HWC + ((uint)y)*WC + ((uint)x)*INPUT0_FEATURE_NUM + c0;
+	const __global UNIT_TYPE_4 *I_load = ((const __global UNIT_TYPE_4*)&(I[adr]));
+#else
+	uint adr = gn*HWC + c0*HW + ((uint)y)*W + ((uint)x);
+	const __global UNIT_TYPE *I_load = (const __global UNIT_TYPE*)&I[adr];
+#endif
+
+	// c, Kdsk
+	uint2 coordU0;
+
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB
+	coordU0.x = (lz * 48 + k * 24);
+	coordU0.y = 0;
+#else // FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+	coordU0.x = (k * 3);
+	coordU0.y = lz*C_;
+	int last_coord_y = lz*C_ + C_;
+#endif
+
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB || FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+	coordU0.x *= sizeof(UNIT_TYPE);
+#endif
+
+	__attribute__((opencl_unroll_hint(1)))
+		for (uint c = 0; c < C4; c += 8) {
+
+			__local UNIT_TYPE_4 *V_write = &V[lxb2 * 512 + lz * 8 + lxd8 * 4 + lxm4];
+			__local const UNIT_TYPE_8 *V_read = (__local const UNIT_TYPE_8 *)&V[lz * 64 + lx * 2];
+
+			// 2*14 * 3 * 16 = 1344 MADs
+
+			// Transform HxW x C        -> DxUxW x C
+			//           6x16x16 inputs -> 4x2x16x16 winograd components.
+			{
+				bool x_in = 0 <= x && x < W;
+				bool y0_in = 0 <= (y + 0) && (y + 0) < H && x_in;
+				bool y1_in = 0 <= (y + 1) && (y + 1) < H && x_in;
+				bool y2_in = 0 <= (y + 2) && (y + 2) < H && x_in;
+				bool y3_in = 0 <= (y + 3) && (y + 3) < H && x_in;
+				bool y4_in = 0 <= (y + 4) && (y + 4) < H && x_in;
+				bool y5_in = 0 <= (y + 5) && (y + 5) < H && x_in;
+				bool y6_in = 0 <= (y + 6) && (y + 6) < H && x_in;
+				bool y7_in = 0 <= (y + 7) && (y + 7) < H && x_in;
+
+#if INPUT0_LAYOUT_BYXF
+
+				UNIT_TYPE_4 I0 = y0_in ? *((const __global UNIT_TYPE_4*)(I + adr + (0 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I1 = y1_in ? *((const __global UNIT_TYPE_4*)(I + adr + (1 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I2 = y2_in ? *((const __global UNIT_TYPE_4*)(I + adr + (2 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I3 = y3_in ? *((const __global UNIT_TYPE_4*)(I + adr + (3 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I4 = y4_in ? *((const __global UNIT_TYPE_4*)(I + adr + (4 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I5 = y5_in ? *((const __global UNIT_TYPE_4*)(I + adr + (5 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I6 = y6_in ? *((const __global UNIT_TYPE_4*)(I + adr + (6 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I7 = y7_in ? *((const __global UNIT_TYPE_4*)(I + adr + (7 * WC4 + c) * 4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+
+#else
+				const __global UNIT_TYPE *I_load_0 = &I_load[0 * W];
+				const __global UNIT_TYPE *I_load_1 = &I_load[1 * W];
+				const __global UNIT_TYPE *I_load_2 = &I_load[2 * W];
+				const __global UNIT_TYPE *I_load_3 = &I_load[3 * W];
+				const __global UNIT_TYPE *I_load_4 = &I_load[4 * W];
+				const __global UNIT_TYPE *I_load_5 = &I_load[5 * W];
+				const __global UNIT_TYPE *I_load_6 = &I_load[6 * W];
+				const __global UNIT_TYPE *I_load_7 = &I_load[7 * W];
+
+				UNIT_TYPE_4 I0 = y0_in ? (UNIT_TYPE_4)(I_load_0[c*HW * 4], I_load_0[c*HW * 4 + HW], I_load_0[c*HW * 4 + HW * 2], I_load_0[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I1 = y1_in ? (UNIT_TYPE_4)(I_load_1[c*HW * 4], I_load_1[c*HW * 4 + HW], I_load_1[c*HW * 4 + HW * 2], I_load_1[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I2 = y2_in ? (UNIT_TYPE_4)(I_load_2[c*HW * 4], I_load_2[c*HW * 4 + HW], I_load_2[c*HW * 4 + HW * 2], I_load_2[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I3 = y3_in ? (UNIT_TYPE_4)(I_load_3[c*HW * 4], I_load_3[c*HW * 4 + HW], I_load_3[c*HW * 4 + HW * 2], I_load_3[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I4 = y4_in ? (UNIT_TYPE_4)(I_load_4[c*HW * 4], I_load_4[c*HW * 4 + HW], I_load_4[c*HW * 4 + HW * 2], I_load_4[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I5 = y5_in ? (UNIT_TYPE_4)(I_load_5[c*HW * 4], I_load_5[c*HW * 4 + HW], I_load_5[c*HW * 4 + HW * 2], I_load_5[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I6 = y6_in ? (UNIT_TYPE_4)(I_load_6[c*HW * 4], I_load_6[c*HW * 4 + HW], I_load_6[c*HW * 4 + HW * 2], I_load_6[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+				UNIT_TYPE_4 I7 = y7_in ? (UNIT_TYPE_4)(I_load_7[c*HW * 4], I_load_7[c*HW * 4 + HW], I_load_7[c*HW * 4 + HW * 2], I_load_7[c*HW * 4 + HW * 3]) : (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+
+#endif
+
+
+
+				//For winograd 6x3 the WA to scale input needed to be added, as the intermediate computations overflow in some cases
+				//Later on the output is adjusted with the same scale factor before adding bias and ACTIVATION
+				I0 = I0*scl_vec;
+				I1 = I1*scl_vec;
+				I2 = I2*scl_vec;
+				I3 = I3*scl_vec;
+				I4 = I4*scl_vec;
+				I5 = I5*scl_vec;
+				I6 = I6*scl_vec;
+				I7 = I7*scl_vec;
+
+
+				// Compute Winograd f6x3 data transform and store components in SLM.
+				V_write[0 * 64] = I0 - 5.25h*I2 + 5.25h*I4 - I6;
+
+				UNIT_TYPE_4 x0 = I1 - 4.25h*I3 + I5;
+				UNIT_TYPE_4 x1 = I2 - 4.25h*I4 + I6;
+
+				V_write[1 * 64] = x1 + x0;
+				V_write[2 * 64] = x1 - x0;
+
+				UNIT_TYPE_4 x2 = -5.h*I3 + I1;
+				UNIT_TYPE_4 x3 = 4.h*I5 + x2;
+				UNIT_TYPE_4 x4 = 0.25h*I2 + I6;
+				UNIT_TYPE_4 x5 = -1.25h*I4 + x4;
+
+				V_write[3 * 64] = +0.5h * x3 + x5;
+				V_write[4 * 64] = -0.5h * x3 + x5;
+
+				UNIT_TYPE_4 x6 = 4.h*I1 + I5;
+				UNIT_TYPE_4 x7 = -5.h*I3 + x6;
+				UNIT_TYPE_4 x8 = 4.h*I2 + I6;
+				UNIT_TYPE_4 x9 = -5.h*I4 + x8;
+
+				V_write[5 * 64] = +0.5h*x7 + x9;
+				V_write[6 * 64] = -0.5h*x7 + x9;
+
+				V_write[7 * 64] = -I1 + 5.25h*I3 - 5.25h*I5 + I7;
+			}
+
+			barrier(CLK_LOCAL_MEM_FENCE);
+
+			__local const UNIT_TYPE_8 *V_read_c16 = V_read;
+
+			__attribute__((opencl_unroll_hint(1)))
+            for (uint c16 = 0; c16 < 2
+#ifndef FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB
+				&& coordU0.y < last_coord_y
+#endif
+				; ++c16) {
+
+					// 2*14 * 3 * 8 = 672 MADs
+
+					// Fetch 16 channels of Winograd input components, spread across subgroup.
+					UNIT_TYPE_8 V0 = V_read_c16[0 * 16 + c16 * 256];
+					UNIT_TYPE_8 V8 = V_read_c16[1 * 16 + c16 * 256];
+
+					__attribute__((opencl_unroll_hint(2)))
+                    for (int c8 = 0; c8 < 2 ; ++c8) {
+
+
+							// filter 0
+
+							// 2*14 * 3 * 4 = 336 MADs
+                            const uint2 coordU = coordU0;
+
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB
+							const uint WEIGHTWIDTH = FILTER_OFM_NUM*KCOLSW*KROWSW;
+#else
+							const uint WEIGHTWIDTH = FILTER_OFM_NUM*KROWSW;
+#endif
+
+							const uint flatA = coordU0.y*WEIGHTWIDTH + coordU0.x;
+
+							// Fetch 8 channels of Winograd components from f(k,s)
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB || FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+							const UNIT_TYPE_8 f00 = as_half8(intel_sub_group_block_read_us8(U, (int2)(coordU0.x, coordU0.y)));
+#else
+							const UNIT_TYPE_8 f00 = (UNIT_TYPE_8)(
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 0 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 1 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 2 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 3 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 4 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 5 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 6 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 7 * WEIGHTWIDTH])));
+#endif
+
+
+							// f0 x v[0 .. 14]
+							DOT8i_0(M0.s0, f00, V0, 0 + c8);
+							DOT8i_0(M0.s1, f00, V0, 2 + c8);
+							DOT8i_0(M1.s0, f00, V0, 4 + c8);
+							DOT8i_0(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_0(M2.s0, f00, V0, 8 + c8);
+							DOT8i_0(M2.s1, f00, V0, 10 + c8);
+							DOT8i_0(M3.s0, f00, V0, 12 + c8);
+							DOT8i_0(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_0(M4.s0, f00, V8, 0 + c8);
+							DOT8i_0(M4.s1, f00, V8, 2 + c8);
+							DOT8i_0(M5.s0, f00, V8, 4 + c8);
+							DOT8i_0(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_0(M6.s0, f00, V8, 8 + c8);
+							DOT8i_0(M6.s1, f00, V8, 10 + c8);
+
+							// f0 x v[0 .. 14]
+							DOT8i_1(M0.s0, f00, V0, 0 + c8);
+)__krnl"
+R"__krnl(							DOT8i_1(M0.s1, f00, V0, 2 + c8);
+							DOT8i_1(M1.s0, f00, V0, 4 + c8);
+							DOT8i_1(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_1(M2.s0, f00, V0, 8 + c8);
+							DOT8i_1(M2.s1, f00, V0, 10 + c8);
+							DOT8i_1(M3.s0, f00, V0, 12 + c8);
+							DOT8i_1(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_1(M4.s0, f00, V8, 0 + c8);
+							DOT8i_1(M4.s1, f00, V8, 2 + c8);
+							DOT8i_1(M5.s0, f00, V8, 4 + c8);
+							DOT8i_1(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_1(M6.s0, f00, V8, 8 + c8);
+							DOT8i_1(M6.s1, f00, V8, 10 + c8);
+
+							// f0 x v[0 .. 14]
+							DOT8i_2(M0.s0, f00, V0, 0 + c8);
+							DOT8i_2(M0.s1, f00, V0, 2 + c8);
+							DOT8i_2(M1.s0, f00, V0, 4 + c8);
+							DOT8i_2(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_2(M2.s0, f00, V0, 8 + c8);
+							DOT8i_2(M2.s1, f00, V0, 10 + c8);
+							DOT8i_2(M3.s0, f00, V0, 12 + c8);
+							DOT8i_2(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_2(M4.s0, f00, V8, 0 + c8);
+							DOT8i_2(M4.s1, f00, V8, 2 + c8);
+							DOT8i_2(M5.s0, f00, V8, 4 + c8);
+							DOT8i_2(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_2(M6.s0, f00, V8, 8 + c8);
+							DOT8i_2(M6.s1, f00, V8, 10 + c8);
+
+
+							// f0 x v[0 .. 14]
+							DOT8i_3(M0.s0, f00, V0, 0 + c8);
+							DOT8i_3(M0.s1, f00, V0, 2 + c8);
+							DOT8i_3(M1.s0, f00, V0, 4 + c8);
+							DOT8i_3(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_3(M2.s0, f00, V0, 8 + c8);
+							DOT8i_3(M2.s1, f00, V0, 10 + c8);
+							DOT8i_3(M3.s0, f00, V0, 12 + c8);
+							DOT8i_3(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_3(M4.s0, f00, V8, 0 + c8);
+							DOT8i_3(M4.s1, f00, V8, 2 + c8);
+							DOT8i_3(M5.s0, f00, V8, 4 + c8);
+							DOT8i_3(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_3(M6.s0, f00, V8, 8 + c8);
+							DOT8i_3(M6.s1, f00, V8, 10 + c8);
+
+
+							// f0 x v[0 .. 14]
+							DOT8i_4(M0.s0, f00, V0, 0 + c8);
+							DOT8i_4(M0.s1, f00, V0, 2 + c8);
+							DOT8i_4(M1.s0, f00, V0, 4 + c8);
+							DOT8i_4(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_4(M2.s0, f00, V0, 8 + c8);
+							DOT8i_4(M2.s1, f00, V0, 10 + c8);
+							DOT8i_4(M3.s0, f00, V0, 12 + c8);
+							DOT8i_4(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_4(M4.s0, f00, V8, 0 + c8);
+							DOT8i_4(M4.s1, f00, V8, 2 + c8);
+							DOT8i_4(M5.s0, f00, V8, 4 + c8);
+							DOT8i_4(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_4(M6.s0, f00, V8, 8 + c8);
+							DOT8i_4(M6.s1, f00, V8, 10 + c8);
+
+							// f0 x v[0 .. 14]
+							DOT8i_5(M0.s0, f00, V0, 0 + c8);
+							DOT8i_5(M0.s1, f00, V0, 2 + c8);
+							DOT8i_5(M1.s0, f00, V0, 4 + c8);
+							DOT8i_5(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_5(M2.s0, f00, V0, 8 + c8);
+							DOT8i_5(M2.s1, f00, V0, 10 + c8);
+							DOT8i_5(M3.s0, f00, V0, 12 + c8);
+							DOT8i_5(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_5(M4.s0, f00, V8, 0 + c8);
+							DOT8i_5(M4.s1, f00, V8, 2 + c8);
+							DOT8i_5(M5.s0, f00, V8, 4 + c8);
+							DOT8i_5(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_5(M6.s0, f00, V8, 8 + c8);
+							DOT8i_5(M6.s1, f00, V8, 10 + c8);
+
+							// f0 x v[0 .. 14]
+							DOT8i_6(M0.s0, f00, V0, 0 + c8);
+							DOT8i_6(M0.s1, f00, V0, 2 + c8);
+							DOT8i_6(M1.s0, f00, V0, 4 + c8);
+							DOT8i_6(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_6(M2.s0, f00, V0, 8 + c8);
+							DOT8i_6(M2.s1, f00, V0, 10 + c8);
+							DOT8i_6(M3.s0, f00, V0, 12 + c8);
+							DOT8i_6(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_6(M4.s0, f00, V8, 0 + c8);
+							DOT8i_6(M4.s1, f00, V8, 2 + c8);
+							DOT8i_6(M5.s0, f00, V8, 4 + c8);
+							DOT8i_6(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_6(M6.s0, f00, V8, 8 + c8);
+							DOT8i_6(M6.s1, f00, V8, 10 + c8);
+
+
+							// f0 x v[0 .. 14]
+							DOT8i_7(M0.s0, f00, V0, 0 + c8);
+							DOT8i_7(M0.s1, f00, V0, 2 + c8);
+							DOT8i_7(M1.s0, f00, V0, 4 + c8);
+							DOT8i_7(M1.s1, f00, V0, 6 + c8);
+
+							DOT8i_7(M2.s0, f00, V0, 8 + c8);
+							DOT8i_7(M2.s1, f00, V0, 10 + c8);
+							DOT8i_7(M3.s0, f00, V0, 12 + c8);
+							DOT8i_7(M3.s1, f00, V0, 14 + c8);
+
+							DOT8i_7(M4.s0, f00, V8, 0 + c8);
+							DOT8i_7(M4.s1, f00, V8, 2 + c8);
+							DOT8i_7(M5.s0, f00, V8, 4 + c8);
+							DOT8i_7(M5.s1, f00, V8, 6 + c8);
+
+							DOT8i_7(M6.s0, f00, V8, 8 + c8);
+							DOT8i_7(M6.s1, f00, V8, 10 + c8);
+
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB || FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+							const UNIT_TYPE_8 f01 = as_half8(intel_sub_group_block_read_us8(U, (int2)(coordU0.x + 16 * sizeof(UNIT_TYPE), coordU0.y)));
+#else
+							const UNIT_TYPE_8 f01 = (UNIT_TYPE_8)(
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 0 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 1 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 2 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 3 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 4 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 5 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 6 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 16 + 7 * WEIGHTWIDTH])));
+#endif
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_0(M0.s0, f01, V0, 2 + c8);
+							DOT8i_0(M0.s1, f01, V0, 4 + c8);
+							DOT8i_0(M1.s0, f01, V0, 6 + c8);
+							DOT8i_0(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_0(M2.s0, f01, V0, 10 + c8);
+							DOT8i_0(M2.s1, f01, V0, 12 + c8);
+							DOT8i_0(M3.s0, f01, V0, 14 + c8);
+							DOT8i_0(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_0(M4.s0, f01, V8, 2 + c8);
+							DOT8i_0(M4.s1, f01, V8, 4 + c8);
+							DOT8i_0(M5.s0, f01, V8, 6 + c8);
+							DOT8i_0(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_0(M6.s0, f01, V8, 10 + c8);
+							DOT8i_0(M6.s1, f01, V8, 12 + c8);
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_1(M0.s0, f01, V0, 2 + c8);
+							DOT8i_1(M0.s1, f01, V0, 4 + c8);
+							DOT8i_1(M1.s0, f01, V0, 6 + c8);
+							DOT8i_1(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_1(M2.s0, f01, V0, 10 + c8);
+							DOT8i_1(M2.s1, f01, V0, 12 + c8);
+							DOT8i_1(M3.s0, f01, V0, 14 + c8);
+							DOT8i_1(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_1(M4.s0, f01, V8, 2 + c8);
+							DOT8i_1(M4.s1, f01, V8, 4 + c8);
+							DOT8i_1(M5.s0, f01, V8, 6 + c8);
+							DOT8i_1(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_1(M6.s0, f01, V8, 10 + c8);
+							DOT8i_1(M6.s1, f01, V8, 12 + c8);
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_2(M0.s0, f01, V0, 2 + c8);
+							DOT8i_2(M0.s1, f01, V0, 4 + c8);
+							DOT8i_2(M1.s0, f01, V0, 6 + c8);
+							DOT8i_2(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_2(M2.s0, f01, V0, 10 + c8);
+							DOT8i_2(M2.s1, f01, V0, 12 + c8);
+							DOT8i_2(M3.s0, f01, V0, 14 + c8);
+							DOT8i_2(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_2(M4.s0, f01, V8, 2 + c8);
+							DOT8i_2(M4.s1, f01, V8, 4 + c8);
+							DOT8i_2(M5.s0, f01, V8, 6 + c8);
+)__krnl"
+R"__krnl(							DOT8i_2(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_2(M6.s0, f01, V8, 10 + c8);
+							DOT8i_2(M6.s1, f01, V8, 12 + c8);
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_3(M0.s0, f01, V0, 2 + c8);
+							DOT8i_3(M0.s1, f01, V0, 4 + c8);
+							DOT8i_3(M1.s0, f01, V0, 6 + c8);
+							DOT8i_3(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_3(M2.s0, f01, V0, 10 + c8);
+							DOT8i_3(M2.s1, f01, V0, 12 + c8);
+							DOT8i_3(M3.s0, f01, V0, 14 + c8);
+							DOT8i_3(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_3(M4.s0, f01, V8, 2 + c8);
+							DOT8i_3(M4.s1, f01, V8, 4 + c8);
+							DOT8i_3(M5.s0, f01, V8, 6 + c8);
+							DOT8i_3(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_3(M6.s0, f01, V8, 10 + c8);
+							DOT8i_3(M6.s1, f01, V8, 12 + c8);
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_4(M0.s0, f01, V0, 2 + c8);
+							DOT8i_4(M0.s1, f01, V0, 4 + c8);
+							DOT8i_4(M1.s0, f01, V0, 6 + c8);
+							DOT8i_4(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_4(M2.s0, f01, V0, 10 + c8);
+							DOT8i_4(M2.s1, f01, V0, 12 + c8);
+							DOT8i_4(M3.s0, f01, V0, 14 + c8);
+							DOT8i_4(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_4(M4.s0, f01, V8, 2 + c8);
+							DOT8i_4(M4.s1, f01, V8, 4 + c8);
+							DOT8i_4(M5.s0, f01, V8, 6 + c8);
+							DOT8i_4(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_4(M6.s0, f01, V8, 10 + c8);
+							DOT8i_4(M6.s1, f01, V8, 12 + c8);
+
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_5(M0.s0, f01, V0, 2 + c8);
+							DOT8i_5(M0.s1, f01, V0, 4 + c8);
+							DOT8i_5(M1.s0, f01, V0, 6 + c8);
+							DOT8i_5(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_5(M2.s0, f01, V0, 10 + c8);
+							DOT8i_5(M2.s1, f01, V0, 12 + c8);
+							DOT8i_5(M3.s0, f01, V0, 14 + c8);
+							DOT8i_5(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_5(M4.s0, f01, V8, 2 + c8);
+							DOT8i_5(M4.s1, f01, V8, 4 + c8);
+							DOT8i_5(M5.s0, f01, V8, 6 + c8);
+							DOT8i_5(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_5(M6.s0, f01, V8, 10 + c8);
+							DOT8i_5(M6.s1, f01, V8, 12 + c8);
+
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_6(M0.s0, f01, V0, 2 + c8);
+							DOT8i_6(M0.s1, f01, V0, 4 + c8);
+							DOT8i_6(M1.s0, f01, V0, 6 + c8);
+							DOT8i_6(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_6(M2.s0, f01, V0, 10 + c8);
+							DOT8i_6(M2.s1, f01, V0, 12 + c8);
+							DOT8i_6(M3.s0, f01, V0, 14 + c8);
+							DOT8i_6(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_6(M4.s0, f01, V8, 2 + c8);
+							DOT8i_6(M4.s1, f01, V8, 4 + c8);
+							DOT8i_6(M5.s0, f01, V8, 6 + c8);
+							DOT8i_6(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_6(M6.s0, f01, V8, 10 + c8);
+							DOT8i_6(M6.s1, f01, V8, 12 + c8);
+
+
+
+							// f1[c8] x v[1 .. 15]
+							DOT8i_7(M0.s0, f01, V0, 2 + c8);
+							DOT8i_7(M0.s1, f01, V0, 4 + c8);
+							DOT8i_7(M1.s0, f01, V0, 6 + c8);
+							DOT8i_7(M1.s1, f01, V0, 8 + c8);
+
+							DOT8i_7(M2.s0, f01, V0, 10 + c8);
+							DOT8i_7(M2.s1, f01, V0, 12 + c8);
+							DOT8i_7(M3.s0, f01, V0, 14 + c8);
+							DOT8i_7(M3.s1, f01, V8, 0 + c8);
+
+							DOT8i_7(M4.s0, f01, V8, 2 + c8);
+							DOT8i_7(M4.s1, f01, V8, 4 + c8);
+							DOT8i_7(M5.s0, f01, V8, 6 + c8);
+							DOT8i_7(M5.s1, f01, V8, 8 + c8);
+
+							DOT8i_7(M6.s0, f01, V8, 10 + c8);
+							DOT8i_7(M6.s1, f01, V8, 12 + c8);
+
+#if FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB || FILTER_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+							const UNIT_TYPE_8 f02 = as_half8(intel_sub_group_block_read_us8(U, (int2)(coordU0.x + 32 * sizeof(UNIT_TYPE), coordU0.y)));
+#else
+							const UNIT_TYPE_8 f02 = (UNIT_TYPE_8)(
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 0 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 1 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 2 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 3 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 4 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 5 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 6 * WEIGHTWIDTH])),
+								as_half(intel_sub_group_block_read_us((__global unsigned short *)&U[flatA + 32 + 7 * WEIGHTWIDTH])));
+#endif
+							coordU0.y += 8;
+
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_0(M0.s0, f02, V0, 4 + c8);
+							DOT8i_0(M0.s1, f02, V0, 6 + c8);
+							DOT8i_0(M1.s0, f02, V0, 8 + c8);
+							DOT8i_0(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_0(M2.s0, f02, V0, 12 + c8);
+							DOT8i_0(M2.s1, f02, V0, 14 + c8);
+							DOT8i_0(M3.s0, f02, V8, 0 + c8);
+							DOT8i_0(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_0(M4.s0, f02, V8, 4 + c8);
+							DOT8i_0(M4.s1, f02, V8, 6 + c8);
+							DOT8i_0(M5.s0, f02, V8, 8 + c8);
+							DOT8i_0(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_0(M6.s0, f02, V8, 12 + c8);
+							DOT8i_0(M6.s1, f02, V8, 14 + c8);
+
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_1(M0.s0, f02, V0, 4 + c8);
+							DOT8i_1(M0.s1, f02, V0, 6 + c8);
+							DOT8i_1(M1.s0, f02, V0, 8 + c8);
+							DOT8i_1(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_1(M2.s0, f02, V0, 12 + c8);
+							DOT8i_1(M2.s1, f02, V0, 14 + c8);
+							DOT8i_1(M3.s0, f02, V8, 0 + c8);
+							DOT8i_1(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_1(M4.s0, f02, V8, 4 + c8);
+							DOT8i_1(M4.s1, f02, V8, 6 + c8);
+							DOT8i_1(M5.s0, f02, V8, 8 + c8);
+							DOT8i_1(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_1(M6.s0, f02, V8, 12 + c8);
+							DOT8i_1(M6.s1, f02, V8, 14 + c8);
+
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_2(M0.s0, f02, V0, 4 + c8);
+							DOT8i_2(M0.s1, f02, V0, 6 + c8);
+							DOT8i_2(M1.s0, f02, V0, 8 + c8);
+							DOT8i_2(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_2(M2.s0, f02, V0, 12 + c8);
+							DOT8i_2(M2.s1, f02, V0, 14 + c8);
+							DOT8i_2(M3.s0, f02, V8, 0 + c8);
+							DOT8i_2(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_2(M4.s0, f02, V8, 4 + c8);
+							DOT8i_2(M4.s1, f02, V8, 6 + c8);
+							DOT8i_2(M5.s0, f02, V8, 8 + c8);
+							DOT8i_2(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_2(M6.s0, f02, V8, 12 + c8);
+							DOT8i_2(M6.s1, f02, V8, 14 + c8);
+
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_3(M0.s0, f02, V0, 4 + c8);
+							DOT8i_3(M0.s1, f02, V0, 6 + c8);
+							DOT8i_3(M1.s0, f02, V0, 8 + c8);
+							DOT8i_3(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_3(M2.s0, f02, V0, 12 + c8);
+							DOT8i_3(M2.s1, f02, V0, 14 + c8);
+							DOT8i_3(M3.s0, f02, V8, 0 + c8);
+							DOT8i_3(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_3(M4.s0, f02, V8, 4 + c8);
+							DOT8i_3(M4.s1, f02, V8, 6 + c8);
+							DOT8i_3(M5.s0, f02, V8, 8 + c8);
+							DOT8i_3(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_3(M6.s0, f02, V8, 12 + c8);
+							DOT8i_3(M6.s1, f02, V8, 14 + c8);
+
+
+)__krnl"
+R"__krnl(							// f2[c8] x v[2 .. 16]
+							DOT8i_4(M0.s0, f02, V0, 4 + c8);
+							DOT8i_4(M0.s1, f02, V0, 6 + c8);
+							DOT8i_4(M1.s0, f02, V0, 8 + c8);
+							DOT8i_4(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_4(M2.s0, f02, V0, 12 + c8);
+							DOT8i_4(M2.s1, f02, V0, 14 + c8);
+							DOT8i_4(M3.s0, f02, V8, 0 + c8);
+							DOT8i_4(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_4(M4.s0, f02, V8, 4 + c8);
+							DOT8i_4(M4.s1, f02, V8, 6 + c8);
+							DOT8i_4(M5.s0, f02, V8, 8 + c8);
+							DOT8i_4(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_4(M6.s0, f02, V8, 12 + c8);
+							DOT8i_4(M6.s1, f02, V8, 14 + c8);
+
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_5(M0.s0, f02, V0, 4 + c8);
+							DOT8i_5(M0.s1, f02, V0, 6 + c8);
+							DOT8i_5(M1.s0, f02, V0, 8 + c8);
+							DOT8i_5(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_5(M2.s0, f02, V0, 12 + c8);
+							DOT8i_5(M2.s1, f02, V0, 14 + c8);
+							DOT8i_5(M3.s0, f02, V8, 0 + c8);
+							DOT8i_5(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_5(M4.s0, f02, V8, 4 + c8);
+							DOT8i_5(M4.s1, f02, V8, 6 + c8);
+							DOT8i_5(M5.s0, f02, V8, 8 + c8);
+							DOT8i_5(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_5(M6.s0, f02, V8, 12 + c8);
+							DOT8i_5(M6.s1, f02, V8, 14 + c8);
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_6(M0.s0, f02, V0, 4 + c8);
+							DOT8i_6(M0.s1, f02, V0, 6 + c8);
+							DOT8i_6(M1.s0, f02, V0, 8 + c8);
+							DOT8i_6(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_6(M2.s0, f02, V0, 12 + c8);
+							DOT8i_6(M2.s1, f02, V0, 14 + c8);
+							DOT8i_6(M3.s0, f02, V8, 0 + c8);
+							DOT8i_6(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_6(M4.s0, f02, V8, 4 + c8);
+							DOT8i_6(M4.s1, f02, V8, 6 + c8);
+							DOT8i_6(M5.s0, f02, V8, 8 + c8);
+							DOT8i_6(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_6(M6.s0, f02, V8, 12 + c8);
+							DOT8i_6(M6.s1, f02, V8, 14 + c8);
+
+
+							// f2[c8] x v[2 .. 16]
+							DOT8i_7(M0.s0, f02, V0, 4 + c8);
+							DOT8i_7(M0.s1, f02, V0, 6 + c8);
+							DOT8i_7(M1.s0, f02, V0, 8 + c8);
+							DOT8i_7(M1.s1, f02, V0, 10 + c8);
+
+							DOT8i_7(M2.s0, f02, V0, 12 + c8);
+							DOT8i_7(M2.s1, f02, V0, 14 + c8);
+							DOT8i_7(M3.s0, f02, V8, 0 + c8);
+							DOT8i_7(M3.s1, f02, V8, 2 + c8);
+
+							DOT8i_7(M4.s0, f02, V8, 4 + c8);
+							DOT8i_7(M4.s1, f02, V8, 6 + c8);
+							DOT8i_7(M5.s0, f02, V8, 8 + c8);
+							DOT8i_7(M5.s1, f02, V8, 10 + c8);
+
+							DOT8i_7(M6.s0, f02, V8, 12 + c8);
+							DOT8i_7(M6.s1, f02, V8, 14 + c8);
+
+						}
+				}
+				barrier(CLK_LOCAL_MEM_FENCE);
+		}
+
+	//barrier(CLK_LOCAL_MEM_FENCE);
+
+
+	// Store multiplies in SLM.
+		{
+			//barrier(CLK_LOCAL_MEM_FENCE);
+			__local UNIT_TYPE_2 *M_write = (__local UNIT_TYPE_2 *)&V[lz * 7 * 8];
+			M_write += lx;
+
+			M_write[0 * 16] = M0;
+			M_write[1 * 16] = M1;
+			M_write[2 * 16] = M2;
+			M_write[3 * 16] = M3;
+			M_write[4 * 16] = M4;
+			M_write[5 * 16] = M5;
+			M_write[6 * 16] = M6;
+
+			barrier(CLK_LOCAL_MEM_FENCE);
+		}
+
+		//if ((gz) % 2) return;
+
+		if (lz < 7)
+		{
+			// Load multiplies from SLM.
+			__local const UNIT_TYPE_2 *M_read = (__local UNIT_TYPE_2*)&V[lz * 8 ];
+			M_read += lx;
+
+			UNIT_TYPE_2 M0 = M_read[0 * 112];
+			UNIT_TYPE_2 M1 = M_read[1 * 112];
+			UNIT_TYPE_2 M2 = M_read[2 * 112];
+			UNIT_TYPE_2 M3 = M_read[3 * 112];
+			UNIT_TYPE_2 M4 = M_read[4 * 112];
+			UNIT_TYPE_2 M5 = M_read[5 * 112];
+			UNIT_TYPE_2 M6 = M_read[6 * 112];
+			UNIT_TYPE_2 M7 = M_read[7 * 112];
+
+			// Inverse Transform.
+			UNIT_TYPE_2 x0 = M1 + M2;
+			UNIT_TYPE_2 x1 = M1 - M2;
+
+			UNIT_TYPE_2 x2 = M3 + M4;
+			UNIT_TYPE_2 x3 = M3 - M4;
+
+			UNIT_TYPE_2 x4 = M5 + M6;
+			UNIT_TYPE_2 x5 = M5 - M6;
+
+			UNIT_TYPE_2 S0 = M0 + x0 + x2 + x4;
+			UNIT_TYPE_2 S1 = x1 + ((UNIT_TYPE)2.f)*x3 + ((UNIT_TYPE)0.5f)*x5;
+			UNIT_TYPE_2 S2 = x0 + ((UNIT_TYPE)4.f)*x2 + ((UNIT_TYPE)0.25f)*x4;
+			UNIT_TYPE_2 S3 = x1 + ((UNIT_TYPE)8.f)*x3 + ((UNIT_TYPE)0.125f)*x5;
+			UNIT_TYPE_2 S4 = x0 + ((UNIT_TYPE)16.f)*x2 + ((UNIT_TYPE)0.0625f)*x4;
+			UNIT_TYPE_2 S5 = x1 + ((UNIT_TYPE)32.f)*x3 + ((UNIT_TYPE)0.03125f)*x5 + M7;
+
+			// Store output to global memory.
+			uint p = gy * 6 + OUTPUT_PAD_BEFORE_SIZE_Y;
+			uint q = gx * 14 + lz * 2 + OUTPUT_PAD_BEFORE_SIZE_X;
+			uint k = gk * 16 + lx;
+
+			// bias and activation
+#if BIAS_TERM
+#if BIAS_PER_OUTPUT
+			const unsigned bias_index0 = k*OUTPUT_SIZE_X*OUTPUT_SIZE_Y + trow*OUTPUT_SIZE_X + q;
+			const unsigned bias_index1 = bias_index0 + 1;
+#else
+			const unsigned bias_index0 = k;
+			const unsigned bias_index1 = bias_index0 + 1;
+#endif
+#endif
+
+#if OUTPUT_LAYOUT_BYXF
+			uint outindex = gn*PQK + p*Q*FILTER_OFM_NUM + q*FILTER_OFM_NUM + k;
+			__global UNIT_TYPE *O_write = (__global UNIT_TYPE *)&O[outindex];
+#else
+			__global UNIT_TYPE *O_write_0 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p + 0)*Q + q]);
+			__global UNIT_TYPE *O_write_1 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p + 1)*Q + q]);
+			__global UNIT_TYPE *O_write_2 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p + 2)*Q + q]);
+			__global UNIT_TYPE *O_write_3 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p + 3)*Q + q]);
+			__global UNIT_TYPE *O_write_4 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p + 4)*Q + q]);
+			__global UNIT_TYPE *O_write_5 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p + 5)*Q + q]);
+#endif
+
+			// TODO: clip output by P, Q
+			bool q0_in = q < Q - OUTPUT_PAD_AFTER_SIZE_X;
+			bool q1_in = q + 1 < Q - OUTPUT_PAD_AFTER_SIZE_X;
+
+			const uint K = FILTER_OFM_NUM;
+
+			if (k < FILTER_OFM_NUM) {
+				if (p < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+					if (q0_in) {
+
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[0 * QK + 0 * K] = ACTIVATION(S0.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[0 * QK + 0 * K] = ACTIVATION(S0.s0 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_0[0] = ACTIVATION(S0.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_0[0] = ACTIVATION(S0.s0 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+					if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[0 * QK + 1 * K] = ACTIVATION(S0.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[0 * QK + 1 * K] = ACTIVATION(S0.s1 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_0[1] = ACTIVATION(S0.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+)__krnl"
+R"__krnl(						O_write_0[1] = ACTIVATION(S0.s1 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+				}
+
+				// row 1
+				if (p + 1 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+					if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[1 * QK + 0 * K] = ACTIVATION(S1.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[1 * QK + 0 * K] = ACTIVATION(S1.s0 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_1[0] = ACTIVATION(S1.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_1[0] = ACTIVATION(S1.s0 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+					if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[1 * QK + 1 * K] = ACTIVATION(S1.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[1 * QK + 1 * K] = ACTIVATION(S1.s1 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_1[1] = ACTIVATION(S1.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_1[1] = ACTIVATION(S1.s1 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+				}
+
+				// row 2
+				if (p + 2 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+					if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[2 * QK + 0 * K] = ACTIVATION(S2.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[2 * QK + 0 * K] = ACTIVATION(S2.s0 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_2[0] = ACTIVATION(S2.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_2[0] = ACTIVATION(S2.s0 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+					if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[2 * QK + 1 * K] = ACTIVATION(S2.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[2 * QK + 1 * K] = ACTIVATION(S2.s1 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_2[1] = ACTIVATION(S2.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_2[1] = ACTIVATION(S2.s1 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+				}
+
+				// row 3
+				if (p + 3 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+					if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[3 * QK + 0 * K] = ACTIVATION(S3.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[3 * QK + 0 * K] = ACTIVATION(S3.s0 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_3[0] = ACTIVATION(S3.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_3[0] = ACTIVATION(S3.s0 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+					if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+						O_write[3 * QK + 1 * K] = ACTIVATION(S3.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write[3 * QK + 1 * K] = ACTIVATION(S3.s1 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+						O_write_3[1] = ACTIVATION(S3.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+						O_write_3[1] = ACTIVATION(S3.s1 * scl, NL_M, NL_N);
+#endif
+#endif
+					}
+				}
+			}
+
+			// row 4
+			if (p + 4 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+				if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+					O_write[4 * QK + 0 * K] = ACTIVATION(S4.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write[4 * QK + 0 * K] = ACTIVATION(S4.s0 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+					O_write_4[0] = ACTIVATION(S4.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write_4[0] = ACTIVATION(S4.s0 * scl, NL_M, NL_N);
+#endif
+#endif
+				}
+				if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+					O_write[4 * QK + 1 * K] = ACTIVATION(S4.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write[4 * QK + 1 * K] = ACTIVATION(S4.s1 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+					O_write_4[1] = ACTIVATION(S4.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write_4[1] = ACTIVATION(S4.s1 * scl, NL_M, NL_N);
+#endif
+#endif
+				}
+			}
+
+			// row 5
+			if (p + 5 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+				if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+					O_write[5 * QK + 0 * K] = ACTIVATION(S5.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write[5 * QK + 0 * K] = ACTIVATION(S5.s0 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+					O_write_5[0] = ACTIVATION(S5.s0 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write_5[0] = ACTIVATION(S5.s0 * scl, NL_M, NL_N);
+#endif
+#endif
+				}
+				if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+					O_write[5 * QK + 1 * K] = ACTIVATION(S5.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write[5 * QK + 1 * K] = ACTIVATION(S5.s1 * scl, NL_M, NL_N);
+#endif
+#else
+#if BIAS_TERM
+					O_write_5[1] = ACTIVATION(S5.s1 * scl + bias[bias_index0], NL_M, NL_N);
+#else
+					O_write_5[1] = ACTIVATION(S5.s1 * scl, NL_M, NL_N);
+#endif
+#endif
+				}
+			}
+		}
+
+}
+#undef UNIT_TYPE_2
+#undef UNIT_TYPE_4
+#undef UNIT_TYPE_8
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_1x1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if FP16_UNIT_USED
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_half8(intel_sub_group_block_read_us8((const __global ushort*)(ptr) + (byte_offset)))
+
+    #define MULTIPLY_BLOCKS_16x8_8x16(_result, _blockA, _blockB) \
+    { \
+        const half16 acol0 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s0 ); \
+        const half16 acol1 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s1 ); \
+        const half16 acol2 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s2 ); \
+        const half16 acol3 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s3 ); \
+        const half16 acol4 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s4 ); \
+        const half16 acol5 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s5 ); \
+        const half16 acol6 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s6 ); \
+        const half16 acol7 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s7 ); \
+        _result = fma( _blockB.s0, acol0, _result ); \
+        _result = fma( _blockB.s1, acol1, _result ); \
+        _result = fma( _blockB.s2, acol2, _result ); \
+        _result = fma( _blockB.s3, acol3, _result ); \
+        _result = fma( _blockB.s4, acol4, _result ); \
+        _result = fma( _blockB.s5, acol5, _result ); \
+        _result = fma( _blockB.s6, acol6, _result ); \
+        _result = fma( _blockB.s7, acol7, _result ); \
+    }
+#else
+    // Block read - currently block is 4 bytes aligned.
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_float8(intel_sub_group_block_read8((const __global uint*)(ptr) + (byte_offset)))
+
+    #define MULTIPLY_BLOCKS_16x8_8x16(_result, _blockA, _blockB) \
+    { \
+        const float16 acol0 = TRANSPOSE_BLOCK_16( _blockA.s0 ); \
+        const float16 acol1 = TRANSPOSE_BLOCK_16( _blockA.s1 ); \
+        const float16 acol2 = TRANSPOSE_BLOCK_16( _blockA.s2 ); \
+        const float16 acol3 = TRANSPOSE_BLOCK_16( _blockA.s3 ); \
+        const float16 acol4 = TRANSPOSE_BLOCK_16( _blockA.s4 ); \
+        const float16 acol5 = TRANSPOSE_BLOCK_16( _blockA.s5 ); \
+        const float16 acol6 = TRANSPOSE_BLOCK_16( _blockA.s6 ); \
+        const float16 acol7 = TRANSPOSE_BLOCK_16( _blockA.s7 ); \
+        _result = fma( _blockB.s0, acol0, _result ); \
+        _result = fma( _blockB.s1, acol1, _result ); \
+        _result = fma( _blockB.s2, acol2, _result ); \
+        _result = fma( _blockB.s3, acol3, _result ); \
+        _result = fma( _blockB.s4, acol4, _result ); \
+        _result = fma( _blockB.s5, acol5, _result ); \
+)__krnl"
+R"__krnl(        _result = fma( _blockB.s6, acol6, _result ); \
+        _result = fma( _blockB.s7, acol7, _result ); \
+    }
+#endif
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_bfyx_1x1)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+    uint split_idx)
+{
+    const uint xy = get_group_id(0) * 16 + get_sub_group_local_id();
+    const uint x = xy % OUTPUT_SIZE_X;
+    const uint y = xy / OUTPUT_SIZE_X;
+    const uint f = get_group_id(1) * 16 + get_sub_group_local_id();//get_global_id(1);
+    const uint b = get_global_id(2);
+    const uint group_f = get_group_id(1) * 16;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 16) blockC00 = UNIT_VAL_ZERO;
+
+#if BIAS_TERM
+    #if   BIAS_PER_OUTPUT
+        const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+    #elif BIAS_PER_OFM
+        const uint bias_index = f;
+    #endif
+    for(uint i = 0; i < 16; i++)
+    {
+        blockC00[i] = intel_sub_group_shuffle(biases[bias_index], i);
+    }
+#endif
+
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (f / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint filter_offset = group_f * ((FILTER_OFM_PITCH + 8 - 1) / 8) * 8;//f*FILTER_OFM_PITCH;
+    const uint xy_block_num = (INPUT0_FEATURE_PITCH + 16 - 1) / 16;
+    const uint f_block_num = (INPUT0_FEATURE_NUM + 8 - 1) / 8;
+    const uint input_offset = in_split_offset + xy * 8 + b * xy_block_num * f_block_num * 128;//b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    for (uint k = 0; k < (FILTER_IFM_NUM + 8 - 1) / 8; ++k)
+    {
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA00;
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockB00;
+
+        uint input_idx = input_offset + k * 8 * xy_block_num * 16;
+        uint filter_idx = filter_offset + k * 8 * 16;
+
+        blockA00 = ALIGNED_BLOCK_READ8(input, input_idx);
+        blockB00 = ALIGNED_BLOCK_READ8(weights, filter_idx);
+
+        MULTIPLY_BLOCKS_16x8_8x16(blockC00, blockB00, blockA00);
+    }
+
+    if(xy >= INPUT0_SIZE_X * INPUT0_SIZE_Y)
+        return;
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+
+    for(uint i = 0; i < 16; i++)
+    {
+    #if OUTPUT_LAYOUT_BF8_XY16
+        const uint dst_index = GET_DATA_BF8_XY16_INDEX(OUTPUT, b, group_f+i, y, x) + out_split_offset;
+    #else
+        const uint dst_index = GET_DATA_INDEX(OUTPUT, b, group_f+i, y, x) + out_split_offset;
+    #endif
+    #if LEFTOVERS
+        if(group_f+i < OUTPUT_FEATURE_NUM)
+    #endif
+        output[dst_index] = ACTIVATION(blockC00[i], NL_M, NL_N);
+    }
+}
+
+#undef ALIGNED_BLOCK_READ8
+#undef MULTIPLY_BLOCKS_16x8_8x16
+#undef CONCAT_TOKEN
+#undef CONCAT_TOKEN_HANDLER1
+#undef MULTIPLY_BLOCKS_16x16
+#undef MAKE_VECTOR_TYPE
+
+)__krnl"},
+
+{"reorder_weights_winograd_6x3_s1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(reorder_weights_winograd_6x3_s1)(const __global INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+    const uint input_tile_width = 1;
+    const uint input_tile_height = 3;
+    const uint in_tile_x_idx = get_global_id(1);
+    const uint in_tile_y_idx = get_global_id(0);
+
+    const uint output_tile_width = 8;
+    const uint output_tile_height = 1;
+
+    const uint tile_x_idx = get_global_id(0);
+    const uint tile_y_idx = get_global_id(1);
+    const uint feature_idx = get_global_id(2) % INPUT0_IFM_NUM;
+    const uint batch_idx = get_global_id(2) / INPUT0_IFM_NUM;
+
+    uint in_idx = batch_idx * INPUT0_OFM_PITCH
+                 + feature_idx * INPUT0_IFM_PITCH
+                 + in_tile_y_idx * input_tile_height * INPUT0_Y_PITCH
+                 + in_tile_x_idx * input_tile_width * INPUT0_X_PITCH;
+
+    MAKE_VECTOR_TYPE(INPUT0_TYPE, 4) tile;
+    tile.x = input[in_idx]; in_idx += INPUT0_Y_PITCH;
+    tile.y = input[in_idx]; in_idx += INPUT0_Y_PITCH;
+    tile.z = input[in_idx];
+
+    const uint weightsOSplit = 16;
+    const uint oDivSplit = OUTPUT_OFM_NUM / 16;
+
+    uint out_idx = batch_idx % 16 +
+        tile_y_idx * output_tile_height * weightsOSplit +
+        batch_idx / 16 * weightsOSplit * OUTPUT_SIZE_Y +
+        feature_idx * weightsOSplit * OUTPUT_SIZE_Y * oDivSplit +
+        tile_x_idx * output_tile_width * weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+
+    output[out_idx] = TO_OUTPUT_TYPE(+90.0 / 90 * tile.x); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(-20.0 / 90 * tile.x - 20.0 / 90 * tile.y - 20.0 / 90 * tile.z); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(-20.0 / 90 * tile.x + 20.0 / 90 * tile.y - 20.0 / 90 * tile.z); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(+1.0 / 90 * tile.x + 2.0 / 90 * tile.y + 4.0 / 90 * tile.z); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(+1.0 / 90 * tile.x - 2.0 / 90 * tile.y + 4.0 / 90 * tile.z); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(+64.0 / 90 * tile.x + 32.0 / 90 * tile.y + 16.0 / 90 * tile.z); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(+64.0 / 90 * tile.x - 32.0 / 90 * tile.y + 16.0 / 90 * tile.z); out_idx += weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    output[out_idx] = TO_OUTPUT_TYPE(+90.0 / 90 * tile.z);
+)__krnl"
+R"__krnl(}
+
+)__krnl"},
+
+{"mvn_gpu_ref_within_channels",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+
+KERNEL (mvn_gpu_ref_within_channels)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    const uint b = get_global_id(0);
+    const uint f = get_global_id(1);
+    float mean = 0.f;
+
+    const uint input_first = INPUT0_OFFSET + b * INPUT0_BATCH_PITCH + f * INPUT0_FEATURE_PITCH;
+
+    // Compute mean
+    uint input_idx = input_first;
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            mean += (float)input[input_idx];
+            input_idx += INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+    }
+    mean /= INPUT0_SIZE_X * INPUT0_SIZE_Y;
+
+    uint output_idx = OUTPUT_OFFSET + b * OUTPUT_BATCH_PITCH + f * OUTPUT_FEATURE_PITCH;
+
+#if NORMALIZE_VARIANCE == 0
+    //subtract mean
+    input_idx = input_first;
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            output[output_idx] = ACTIVATION(input[input_idx] - UNIT_CVT_FUNC(mean), NL_M, NL_N);
+            input_idx += INPUT0_X_PITCH;
+            output_idx += OUTPUT_X_PITCH;
+        }
+        input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+        output_idx += OUTPUT_Y_PITCH - INPUT0_SIZE_X*OUTPUT_X_PITCH;
+    }
+#else //NORMALIZE_VARIANCE
+    float variance = 0.f;
+
+    //compute variance
+    input_idx = input_first;
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            float res = (float)input[input_idx] - mean;
+            variance = fma(res, res, variance);
+            input_idx += INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+    }
+
+    //normalize variance
+    variance /= INPUT0_SIZE_Y * INPUT0_SIZE_X;
+    variance = native_powr(variance + (float)EPSILON, -0.5f);
+
+    input_idx = input_first;
+    for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+    {
+        for (uint x = 0; x < INPUT0_SIZE_X; x++)
+        {
+            output[output_idx] = ACTIVATION((input[input_idx] - UNIT_CVT_FUNC(mean)) * UNIT_CVT_FUNC(variance), NL_M, NL_N);
+            input_idx += INPUT0_X_PITCH;
+            output_idx += OUTPUT_X_PITCH;
+        }
+        input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+        output_idx += OUTPUT_Y_PITCH - INPUT0_SIZE_X*OUTPUT_X_PITCH;
+    }
+#endif
+}
+
+
+#undef UNIT_CVT_FUNC
+
+)__krnl"},
+
+{"arg_max_min_axis",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+#define GLOBAL_SIZE 128
+#define LOCAL_SIZE GLOBAL_SIZE
+
+typedef struct /* Index and Value type that holds index and value used in this kernel */
+{
+    uint index;
+    UNIT_TYPE value;
+} iav_type;
+
+#ifdef BATCH_AXIS
+    #define GAP_SIZE (INPUT0_FEATURE_NUM * INPUT0_SIZE_X * INPUT0_SIZE_Y)
+    #define VALUES_NUM INPUT0_BATCH_NUM
+    #define FIRST_DIM_SIZE INPUT0_SIZE_X
+    #define SECOND_DIM_SIZE INPUT0_SIZE_Y
+    #define FIRST_DIM_MUL 1
+    #define SECOND_DIM_MUL INPUT0_SIZE_X
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y)
+#endif
+#ifdef FEATURE_AXIS
+    #define GAP_SIZE (INPUT0_SIZE_X * INPUT0_SIZE_Y)
+    #define VALUES_NUM INPUT0_FEATURE_NUM
+    #define FIRST_DIM_SIZE INPUT0_SIZE_X
+    #define SECOND_DIM_SIZE INPUT0_SIZE_Y
+    #define FIRST_DIM_MUL 1
+    #define SECOND_DIM_MUL INPUT0_SIZE_X
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM)
+#endif
+#ifdef Y_AXIS
+    #define GAP_SIZE INPUT0_SIZE_X
+    #define VALUES_NUM INPUT0_SIZE_Y
+    #define FIRST_DIM_SIZE INPUT0_SIZE_X
+    #define SECOND_DIM_SIZE INPUT0_FEATURE_NUM
+    #define FIRST_DIM_MUL 1
+    #define SECOND_DIM_MUL (INPUT0_SIZE_Y * INPUT0_SIZE_X)
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM)
+#endif
+#ifdef X_AXIS
+    #define GAP_SIZE 1
+    #define VALUES_NUM INPUT0_SIZE_X
+    #define FIRST_DIM_SIZE INPUT0_SIZE_Y
+    #define SECOND_DIM_SIZE INPUT0_FEATURE_NUM
+    #define FIRST_DIM_MUL INPUT0_SIZE_X
+    #define SECOND_DIM_MUL (INPUT0_SIZE_Y * INPUT0_SIZE_X)
+    #define THIRD_DIM_MUL (INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM)
+#endif
+
+#ifdef MAX_OUT
+    #define COMPARE_SIGN <
+    #define UNIT_FILL_VAL UNIT_VAL_MIN
+#else
+    #define COMPARE_SIGN >
+    #define UNIT_FILL_VAL UNIT_VAL_MAX
+#endif
+
+__attribute__((reqd_work_group_size(LOCAL_SIZE, 1, 1)))
+KERNEL(arg_max_gpu_axis)(const __global UNIT_TYPE* input, __global float* output)
+{
+    uint results[TOP_K];
+    __local iav_type scratch[LOCAL_SIZE];
+    const uint first_dim_id = (uint)get_global_id(1);
+    const uint second_third_dim_id = (uint)get_global_id(2);
+    const uint second_dim_id = second_third_dim_id % SECOND_DIM_SIZE;
+    const uint third_dim_id = second_third_dim_id / SECOND_DIM_SIZE;
+    const uint output_index = (first_dim_id + second_dim_id * FIRST_DIM_SIZE + third_dim_id * FIRST_DIM_SIZE * SECOND_DIM_SIZE) * TOP_K;
+    const uint offset = first_dim_id * FIRST_DIM_MUL + second_dim_id * SECOND_DIM_MUL + third_dim_id * THIRD_DIM_MUL;
+    uint local_index = get_local_id(0);
+    uint global_index = offset + local_index * GAP_SIZE;
+
+    iav_type accumulator;
+
+    uint temp_index = global_index;
+    uint start_index = (global_index - offset) / GAP_SIZE;
+    __attribute__((opencl_unroll_hint))
+    for (uint i = 0; i < TOP_K; i++)
+    {
+        accumulator.index = start_index;
+        accumulator.value = input[global_index];
+        for (int j = 0; j < i; j++)
+        {
+            if (accumulator.index == results[j])
+                accumulator.value = UNIT_FILL_VAL;
+        }
+        global_index += GLOBAL_SIZE * GAP_SIZE;
+        uint element_index = start_index + GLOBAL_SIZE;
+        while (global_index < offset + VALUES_NUM * GAP_SIZE)
+        {
+            iav_type element;
+            element.value = input[global_index];
+            element.index = element_index;
+            for (int j = 0; j < i; j++){
+                if (element.index == results[j])
+                    element.value = UNIT_FILL_VAL;
+            }
+            if(accumulator.value COMPARE_SIGN element.value)
+            {
+                accumulator.value = element.value;
+                accumulator.index = element.index;
+            }
+            element_index += GLOBAL_SIZE;
+            global_index += GLOBAL_SIZE * GAP_SIZE;
+        }
+        if (local_index < VALUES_NUM)
+            scratch[local_index] = accumulator;
+        else
+            scratch[local_index].value = UNIT_FILL_VAL;
+
+        barrier(CLK_LOCAL_MEM_FENCE);
+
+        __attribute__((opencl_unroll_hint))
+        for(uint scratch_offset = LOCAL_SIZE / 2; scratch_offset > 0; scratch_offset /= 2)
+        {
+            if (local_index < scratch_offset)
+            {
+                iav_type other = scratch[local_index + scratch_offset];
+                iav_type mine = scratch[local_index];
+
+                if(mine.value COMPARE_SIGN other.value)
+                {
+                    scratch[local_index] = other;
+                }
+            }
+            barrier(CLK_LOCAL_MEM_FENCE);
+        }
+
+        if (local_index == 0)
+        {
+)__krnl"
+R"__krnl(            output[output_index + i] = scratch[0].index;
+        }
+        global_index = temp_index;
+        results[i] = scratch[0].index;
+    }
+}
+
+#undef COMPARE_SIGN
+#undef UNIT_FILL_VAL
+#undef GAP_SIZE
+#undef VALUES_NUM
+#undef FIRST_DIM_SIZE
+#undef SECOND_DIM_SIZE
+#undef FIRST_DIM_MUL
+#undef SECOND_DIM_MUL
+#undef THIRD_DIM_MUL
+
+)__krnl"},
+
+{"reorg_yolo_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+#if OUTPUT_LAYOUT_BFYX
+    #define IW INPUT0_SIZES[0]
+    #define IH INPUT0_SIZES[1]
+    #define IC INPUT0_SIZES[2]
+    #define B  INPUT0_SIZES[3]
+
+#elif OUTPUT_LAYOUT_YXFB
+    #define IW INPUT0_SIZES[3]
+    #define IH INPUT0_SIZES[2]
+    #define IC INPUT0_SIZES[1]
+    #define B  INPUT0_SIZES[0]
+#endif
+
+#define ic_off (IC / (STRIDE * STRIDE))
+#define ih_off (IH * STRIDE)
+#define iw_off (IW * STRIDE)
+
+KERNEL (reorg_yolo_ref)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+#if OUTPUT_LAYOUT_BFYX
+    int ic = get_global_id(2);
+    int ih = get_global_id(1);
+    int iw = get_global_id(0);
+        for (int b = 0; b < B; b++) {
+        int dstIndex = b*IC*IH*IW + ic*IH*IW + ih*IW + iw;
+
+        int oc = ic % ic_off;
+        int offset = ic / ic_off;
+
+        int ow = iw * STRIDE + offset % STRIDE;
+        int oh = ih * STRIDE + offset / STRIDE;
+
+        int srcIndex = b*ic_off*ih_off*iw_off + oc*ih_off*iw_off + oh*iw_off + ow;
+
+        output[dstIndex] = input[srcIndex];
+    }
+#elif OUTPUT_LAYOUT_YXFB
+    int ic = get_global_id(0) / B;
+    int ib = get_global_id(0) % B;
+    int ih = get_global_id(2);
+    int iw = get_global_id(1);
+    for (int b = 0; b < B; b++) {
+        int dstIndex = ib + ic*B + ih*IC*B + iw*IH*IC*B;
+
+        int oc = ic % ic_off;
+        int offset = ic / ic_off;
+
+        int ow = iw * STRIDE + offset % STRIDE;
+        int oh = ih * STRIDE + offset / STRIDE;
+
+        int srcIndex = b*ic_off*ih_off*iw_off + oc*ih_off*iw_off + oh*iw_off + ow;
+
+        output[dstIndex] = input[srcIndex];
+    }
+#endif
+
+
+}
+
+)__krnl"},
+
+{"convolution_gpu_mmad",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+)__krnl"
+R"__krnl(        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+#define FILTER_IFM_MMAD_NUM ((FILTER_IFM_NUM + 31) / 32)
+#define FILTER_OFM_MMAD_NUM ((FILTER_OFM_NUM + 7) / 8)
+#define FILTER_IFM_ALIGNED (FILTER_IFM_MMAD_NUM * 32)
+#define FILTER_OFM_ALIGNED (FILTER_OFM_MMAD_NUM * 8)
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+KERNEL(convolution_MMAD)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+#if QUANTIZATION_TERM
+    const __global float* quantizations,
+#endif
+#if CALIBRATION_TERM
+    const __global float* calibrations,
+#endif
+    uint split_idx)
+{
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(2);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(2) % FILTER_OFM_ALIGNED;
+    const uint b = get_global_id(2) / FILTER_OFM_ALIGNED;
+#endif
+
+#if QUANTIZATION_TERM
+    int dotProd = 0;
+#else
+    UNIT_TYPE dotProd = UNIT_VAL_ZERO;
+#endif
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+
+    const uint filter_offset = (get_group_id(2) % FILTER_OFM_MMAD_NUM) * FILTER_OFM_BLOCK_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    for (uint k = 0; k < FILTER_IFM_MMAD_NUM; ++k)
+    {
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+            const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+            if(!zero_y)
+            {
+                for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+                {
+                    const int input_offset_x = input_x + i * DILATION_SIZE_X;
+                    const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                    if(!zero_x)
+                    {
+                        uint input_idx = input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH + k*32;
+                        uint filter_idx = filter_offset + k*FILTER_Y_PITCH * FILTER_SIZE_Y + j*FILTER_Y_PITCH + i*FILTER_X_PITCH;
+
+						int input_data = as_int(intel_sub_group_block_read((const __global uint*)(input + input_idx)));
+						int8 activations;  //activations of all lanes
+						activations.s0 = sub_group_broadcast(input_data, 0);
+                        activations.s1 = sub_group_broadcast(input_data, 1);
+                        activations.s2 = sub_group_broadcast(input_data, 2);
+                        activations.s3 = sub_group_broadcast(input_data, 3);
+                        activations.s4 = sub_group_broadcast(input_data, 4);
+                        activations.s5 = sub_group_broadcast(input_data, 5);
+                        activations.s6 = sub_group_broadcast(input_data, 6);
+                        activations.s7 = sub_group_broadcast(input_data, 7);
+
+						int8 weights_data = as_int8(intel_sub_group_block_read8((const __global uint*)(weights + filter_idx)));
+
+						dotProd = MMAD_8(activations, weights_data, dotProd);
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+#if QUANTIZATION_TERM
+#if CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+#else // QUANTIZATION_TERM
+    dotProd += (UNIT_TYPE)biases[bias_index];
+#endif // QUANTIZATION_TERM
+#endif // BIAS_TERM
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y, x) + out_split_offset;
+#if QUANTIZATION_TERM
+    output[dst_index] = ACTIVATION(convert_char(dotProd), NL_M, NL_N);
+#else
+    output[dst_index] = ACTIVATION(dotProd, NL_M, NL_N);
+#endif
+}
+
+#undef FILTER_IFM_MMAD_NUM
+#undef FILTER_OFM_MMAD_NUM
+#undef FILTER_IFM_ALIGNED
+#undef FILTER_OFM_ALIGNED
+
+)__krnl"},
+
+{"arg_max_min_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+#define GLOBAL_SIZE 128
+#define LOCAL_SIZE GLOBAL_SIZE
+
+typedef struct /* Index and Value type that holds index and value used in this kernel */
+{
+    uint index;
+    UNIT_TYPE value;
+} iav_type;
+
+#ifdef MAX_OUT
+    #define COMPARE_SIGN <
+    #define UNIT_FILL_VAL UNIT_VAL_MIN
+#else
+    #define COMPARE_SIGN >
+    #define UNIT_FILL_VAL UNIT_VAL_MAX
+#endif
+
+__attribute__((reqd_work_group_size(LOCAL_SIZE, 1, 1)))
+KERNEL(arg_max_gpu_top_k)(const __global UNIT_TYPE* input, __global float* output)
+{
+    uint results[TOP_K];
+    __local iav_type scratch[LOCAL_SIZE];
+
+    const uint current_batch = (uint)get_global_id(1);
+    uint local_index = get_local_id(0);
+#ifdef INPUT0_LAYOUT_BFYX
+    const uint size = INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM;
+    const uint batch_offset = current_batch * size;
+    uint global_index = batch_offset + local_index;
+#else
+    const uint size = INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM * INPUT0_BATCH_NUM;
+    const uint fyx_size = INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM;
+    uint global_index = current_batch + local_index*INPUT0_BATCH_NUM;
+#endif
+
+    iav_type accumulator;
+
+    uint temp_index = global_index;
+
+    __attribute__((opencl_unroll_hint))
+    for (uint i = 0; i < TOP_K; i++){
+        accumulator.index = global_index;
+        accumulator.value = input[global_index];
+        for (int j = 0; j < i; j++){
+            if (accumulator.index % size == results[j])
+                accumulator.value = UNIT_FILL_VAL;
+        }
+        global_index += GLOBAL_SIZE;
+#ifdef INPUT0_LAYOUT_BFYX
+            while (global_index < size + batch_offset)
+#else
+            while (global_index < size)
+#endif
+        {
+            iav_type element;
+            element.value = input[global_index];
+            element.index = global_index;
+            for (int j = 0; j < i; j++){
+                if (element.index % size == results[j])
+                    element.value = UNIT_FILL_VAL;
+            }
+            if(accumulator.value COMPARE_SIGN element.value)
+            {
+                accumulator.value = element.value;
+                accumulator.index = element.index;
+            }
+#ifdef INPUT0_LAYOUT_BFYX
+            global_index += GLOBAL_SIZE;
+#else
+            global_index += GLOBAL_SIZE * INPUT0_BATCH_NUM;
+#endif
+        }
+
+#ifdef INPUT0_LAYOUT_BFYX
+        if (local_index < size)
+            scratch[local_index] = accumulator;
+        else
+            scratch[local_index].value = UNIT_FILL_VAL;
+#else
+        if (local_index < fyx_size)
+            scratch[local_index] = accumulator;
+        else
+            scratch[local_index].value = UNIT_FILL_VAL;
+#endif
+
+
+        barrier(CLK_LOCAL_MEM_FENCE);
+
+        __attribute__((opencl_unroll_hint))
+        for(uint offset = LOCAL_SIZE / 2; offset > 0; offset /= 2)
+        {
+            if (local_index < offset)
+            {
+                iav_type other = scratch[local_index + offset];
+                iav_type mine = scratch[local_index];
+
+                if(mine.value COMPARE_SIGN other.value)
+                {
+                    scratch[local_index] = other;
+                }
+            }
+            barrier(CLK_LOCAL_MEM_FENCE);
+        }
+
+#ifdef INPUT0_LAYOUT_BFYX
+        if (local_index == 0)
+        {
+            output[current_batch * TOP_K + i] = scratch[0].index % size;
+        }
+        global_index = temp_index;
+        results[i] = scratch[0].index % size;
+#else
+        if (local_index == 0)
+        {
+            output[current_batch + i*INPUT0_BATCH_NUM] = scratch[0].index / INPUT0_BATCH_NUM;
+        }
+        global_index = temp_index;
+        results[i] = scratch[0].index;
+#endif
+    }
+}
+
+#undef COMPARE_SIGN
+#undef UNIT_FILL_VAL
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_depthwise_weights_lwg",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if FP16_UNIT_USED
+    #define ALIGNED_BLOCK_READ(ptr, byte_offset) as_half(intel_sub_group_block_read_us8((const __global ushort*)(ptr) + (byte_offset)))
+    #define ALIGNED_BLOCK_WRITE(ptr, byte_offset, val) intel_sub_group_block_write_us((__global ushort*)(ptr) + (byte_offset), as_ushort8(val))
+#else
+    #define ALIGNED_BLOCK_READ(ptr, byte_offset) as_float(intel_sub_group_block_read((const __global uint*)(ptr) + (byte_offset)))
+    #define ALIGNED_BLOCK_WRITE(ptr, byte_offset, val) intel_sub_group_block_write((__global uint*)(ptr) + (byte_offset), as_uint8(val))
+#endif
+
+__attribute__((intel_reqd_sub_group_size(16)))
+__attribute__((reqd_work_group_size(16, 1, 1)))
+KERNEL(convolution_depthwise_weights_lwg)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+    uint split_idx)
+{
+    const uint yx = get_global_id(0);
+    const uint x = yx % OUTPUT_SIZE_X;
+    const uint y = yx / OUTPUT_SIZE_X;
+    const uint f = get_global_id(1);
+    const uint b = get_global_id(2);
+
+    UNIT_TYPE dotProd = UNIT_VAL_ZERO;
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    const uint in_split_offset = (f / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+
+    const uint filter_offset = f*FILTER_OFM_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+#if FILTER_SIZE_Y * FILTER_SIZE_X % 16 == 0 && !FP16_UNIT_USED
+    UNIT_TYPE w = ALIGNED_BLOCK_READ(weights, filter_offset);
+#else
+    const uint lid = get_local_id(0);
+    UNIT_TYPE w = UNIT_VAL_ZERO;
+    if(lid < FILTER_SIZE_X * FILTER_SIZE_Y)
+        w = weights[filter_offset + lid];
+)__krnl"
+R"__krnl(#endif
+
+    __attribute__((opencl_unroll_hint(FILTER_SIZE_Y)))
+    for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+    {
+        const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+#if BOUNDARY_CHECK
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+#endif
+            __attribute__((opencl_unroll_hint(FILTER_SIZE_X)))
+            for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+            {
+                const int input_offset_x = input_x + i * DILATION_SIZE_X;
+#if BOUNDARY_CHECK
+                const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                if(!zero_x)
+                {
+#endif
+                    dotProd = mad(input[input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH],
+                                  intel_sub_group_shuffle( w, j*FILTER_Y_PITCH + i*FILTER_X_PITCH), dotProd);
+                }
+            }
+#if BOUNDARY_CHECK
+        }
+    }
+#endif
+
+    if(yx >= OUTPUT_SIZE_X * OUTPUT_SIZE_Y)
+        return;
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+    dotProd += (UNIT_TYPE)biases[bias_index];
+#endif
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y, x) + out_split_offset;
+    output[dst_index] = ACTIVATION(dotProd, NL_M, NL_N);
+
+}
+
+)__krnl"},
+
+{"region_yolo_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+#define IW INPUT0_SIZES[0]
+#define IH INPUT0_SIZES[1]
+#define IC INPUT0_SIZES[2]
+#define IB INPUT0_SIZES[3]
+
+inline UNIT_TYPE FUNC(logistic_activate)(UNIT_TYPE x) {
+    return 1. / (1. + exp(-x));
+}
+
+inline int FUNC(entry_index)(int width, int height, int coords, int classes,
+                       int outputs, int batch, int location,
+                       int entry) {
+    int n = location / (width * height);
+    int loc = location % (width * height);
+    return batch * outputs + n * width * height * (coords + classes + 1) +
+        entry * width * height + loc;
+}
+
+#if DO_SOFTMAX
+inline void FUNC(softmax_generic)(const __global UNIT_TYPE* src_data, __global UNIT_TYPE* dst_data,
+                            int B, int C, int W, int H, int i)
+{
+    for (int b = 0; b < B; b++) {
+        UNIT_TYPE max = src_data[b*C*H*W + i];
+        for (int c = 0; c < C; c++) {
+            UNIT_TYPE val = src_data[b*C*H*W + c*H*W + i];
+            if (val > max) max = val;
+        }
+
+        UNIT_TYPE expSum = 0;
+        for (int c = 0; c < C; c++) {
+            dst_data[b*C*H*W + c*H*W + i] = exp(src_data[b*C*H*W + c*H*W + i] - max);
+            expSum += dst_data[b*C*H*W + c*H*W + i];
+        }
+
+        for (int c = 0; c < C; c++) {
+            dst_data[b*C*H*W + c*H*W + i] = dst_data[b*C*H*W + c*H*W + i] / expSum;
+        }
+    }
+}
+#endif
+
+KERNEL (region_yolo_ref)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    int x = get_global_id(0);
+
+#if DO_SOFTMAX
+    #define ACTUAL_NUM (NUM)
+    #define CONF_CLASSES (1)
+#else
+    #define ACTUAL_NUM (MASK_SIZE)
+    #define CONF_CLASSES (CLASSES+1)
+#endif
+    #define INPUTS_COUNT (IH * IW * ACTUAL_NUM * (CLASSES + COORDS + 1))
+
+    for (int b = 0; b < IB; b++) {
+        for (int n = 0; n < ACTUAL_NUM; n++) {
+            // coords: x/y
+            int index = FUNC_CALL(entry_index)(IW, IH, COORDS, CLASSES, INPUTS_COUNT, b, n * IW * IH, 0);
+            int i = index + 2 * x;
+            output[i] = FUNC_CALL(logistic_activate)(input[i]);
+            output[i+1] = FUNC_CALL(logistic_activate)(input[i+1]);
+
+            // coords: w/h: directly copy?
+            index = FUNC_CALL(entry_index)(IW, IH, COORDS, CLASSES, INPUTS_COUNT, b, n * IW * IH, 2);
+            i = index + 2 * x;
+            output[i] = input[i];
+            output[i+1] = input[i+1];
+
+            // confidence
+            index = FUNC_CALL(entry_index)(IW, IH, COORDS, CLASSES, INPUTS_COUNT, b, n * IW * IH, COORDS);
+            for (int j = 0; j < CONF_CLASSES; j++)
+            {
+                i = index + x + j*IH*IW;
+                output[i] = FUNC_CALL(logistic_activate)(input[i]);
+            }
+        }
+    }
+
+#if DO_SOFTMAX
+    // the probability of classes
+    int index = FUNC_CALL(entry_index)(IW, IH, COORDS, CLASSES, INPUTS_COUNT, 0, 0, COORDS + 1);
+    int batch_offset = INPUTS_COUNT / NUM;
+    for (int b = 0; b < IB * NUM; b++)
+        FUNC_CALL(softmax_generic)(input + index + b * batch_offset, output + index + b * batch_offset,
+                                   1, CLASSES, IH, IW, x);
+#endif
+}
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_gemm_like_fp32",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define TILE_M          2
+#define TILE_K          FILTER_SIZE_X
+#define TILE_N          32
+
+__attribute__((intel_reqd_sub_group_size(8)))
+KERNEL(convolution_f32)(
+    const __global float *src0,
+    __global float *dst,
+    const __global float *src1,
+#if BIAS_TERM
+    const __global float *bias,
+#endif
+    uint split_idx)
+{
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+typedef struct half1  { half s0; }                                                               half1;
+typedef struct half5  { half s0; half s1; half s2; half s3; half s4; }                           half5;
+typedef struct half6  { half s0; half s1; half s2; half s3; half s4; half s5; }                  half6;
+typedef struct half7  { half s0; half s1; half s2; half s3; half s4; half s5; half s6; }         half7;
+typedef struct half9  { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; }                                                               half9;
+typedef struct half10 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; half s9; }                                                      half10;
+typedef struct half11 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; half s9; half sa; }                                             half11;
+)__krnl"
+R"__krnl(typedef struct half12 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb;}                                    half12;
+typedef struct half13 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb; half sc;}                           half13;
+typedef struct half14 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb; half sc; half se;}                  half14;
+typedef struct half15 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                       half s8;  half s9; half sa; half sb; half sc; half se; half sf;}          half15;
+typedef struct half0  { half s0; } half0; //never used but makes compiler happy.
+
+typedef struct float1 { float s0; } float1;
+typedef struct float5 { float s0; float s1; float s2; float s3; float s4; } float5;
+typedef struct float6 { float s0; float s1; float s2; float s3; float s4; float s5; } float6;
+typedef struct float7 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; } float7;
+typedef struct float9 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; float s7; float s8; } float9;
+typedef struct float10 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9;} float10;
+typedef struct float11 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa;} float11;
+typedef struct float12 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; } float12;
+typedef struct float13 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc;} float13;
+typedef struct float14 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; } float14;
+typedef struct float15 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; float se; } float15;
+typedef struct float0 { float s0; } float0; //never used but makes compiler happy.
+
+#if (KERNEL_WIDTH == 1)
+__constant half1 half_zeros= (half1){0};
+#elif (KERNEL_WIDTH == 2)
+    __constant half2 half_zeros = (half2)(0);
+#elif (KERNEL_WIDTH == 3)
+    __constant half3 half_zeros = (half3)(0);
+#elif (KERNEL_WIDTH == 4)
+    __constant half4 half_zeros = (half4)(0);
+#elif (KERNEL_WIDTH == 5)
+    __constant half5 half_zeros = (half5){0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 6)
+    __constant half6 half_zeros = (half6){0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 7)
+    __constant half7 half_zeros = (half7){0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 8)
+    __constant half8 half_zeros = (half8)(0);
+#elif (KERNEL_WIDTH == 9)
+    __constant half9 half_zeros = (half9){0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 10)
+    __constant half10 half_zeros = (half10){0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 11)
+    __constant half11 half_zeros = (half11){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 12)
+    __constant half12 half_zeros = (half12){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 13)
+    __constant half13 half_zeros = (half13){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 14)
+    __constant half14 half_zeros = (half14){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 15)
+    __constant half15 half_zeros = (half15){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 16)
+    __constant half16 half_zeros = (half16)(0);
+#endif
+
+
+    const unsigned group_x = get_group_id(0);
+    const unsigned group_y = get_group_id(1);
+    const unsigned global_x = get_global_id(0);
+    const unsigned global_y = get_global_id(1);
+    const unsigned global_z = get_global_id(2);
+
+    unsigned interleaved_y;
+    unsigned kernel_y;
+    unsigned kernel_idx;
+
+    // Result ctile (*dst) is M rows x N columns
+    // LWG size is 1x8.  Thus each thread calculates 8*M rows x N cols of ctile.
+    float8  blockC00 = 0.f;
+    float8  blockC10 = 0.f;
+    float8  blockC20 = 0.f;
+    float8  blockC30 = 0.f;
+    float8  blockC01 = 0.f;
+    float8  blockC11 = 0.f;
+    float8  blockC21 = 0.f;
+    float8  blockC31 = 0.f;
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * INPUT0_FEATURE_NUM;
+    // Src0 (patch input) is directly used as atile.
+    // Each work item points to the start of a different patch.
+    // atile is M rows x K columns.
+    const uint src0_read_offset0_const = INPUT0_OFFSET_WITH_PADDING + in_split_offset
+     + INPUT0_BATCH_PITCH * global_z                                                         // batch offset
+     + ( ( ( global_y * TILE_M + 0 ) / OUTPUT_SIZE_X ) * STRIDE_SIZE_Y * INPUT0_Y_PITCH )    // y offset
+     + ( ( ( global_y * TILE_M + 0 ) % OUTPUT_SIZE_X ) * STRIDE_SIZE_X );                    // x offset
+    const uint src0_read_offset1_const = INPUT0_OFFSET_WITH_PADDING + in_split_offset
+     + INPUT0_BATCH_PITCH * global_z                                                 // batch offset
+     + ( ( ( global_y * TILE_M + 1 ) / OUTPUT_SIZE_X ) * STRIDE_SIZE_Y * INPUT0_Y_PITCH )    // y offset
+     + ( ( ( global_y * TILE_M + 1 ) % OUTPUT_SIZE_X ) * STRIDE_SIZE_X );                    // x offset
+
+    // Src1 (filter) is directly used as btile.
+    // It starts at the top of src1 and walks down.
+    // btile is K rows x N columns.
+    uint src0_read_offset0 = src0_read_offset0_const;
+    uint src0_read_offset1 = src0_read_offset1_const;
+    uint src1_read_offset = ( global_x * TILE_N * 2);
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+    {   \
+        _result.s0 = mad( _rowA, sub_group_broadcast( colB,  0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, sub_group_broadcast( colB,  1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, sub_group_broadcast( colB,  2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, sub_group_broadcast( colB,  3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, sub_group_broadcast( colB,  4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, sub_group_broadcast( colB,  5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, sub_group_broadcast( colB,  6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, sub_group_broadcast( colB,  7 ), _result.s7 );  \
+    }
+
+    // Walk DOWN src0 (patch 0, 1, 2, ...) and DOWN src1.
+    // Inner loop loads and FMADs one row (FILTER_SIZE_X) of each input patch
+    // and FILTER_SIZE_X/2 rows of interleaved filter.
+    unsigned patch_depth = 0;
+    do
+    {
+        unsigned patch_row = 0;
+        do
+        {
+            // Load atile and btile.
+            // Kernel data is partially interleaved.  Every 2 rows are interleaved at float8 granularity.
+            // The exception is that if FILTER_SIZE_X is odd the last row is not interleaved.  The non
+            // interleaved row is padded with zero to ensure same size as interleaved rows. This
+            // interleaving is done to ensure 0% GDR bank conflicts.  For example, this is how the
+            // kernel data would be arranged before/after interleaving for FILTER_SIZE_X=3.
+            // (0, 0) (8, 0) (16, 0) (24, 0) ...       (0, 0) (0, 1) (8, 0) (0, 1) (16, 0) (0, 1) (24, 0) ..
+            // (0, 1) (8, 1) (16, 1) (24, 1) ... =>    (0, 2) (8, 2) (16, 2) (24, 2) ...
+            // (0, 2) (8, 2) (16, 2) (24, 2) ...       ...
+            // ...
+            const bool kernel_width_is_odd = FILTER_SIZE_X % 2 == 1;
+
+            float blockA00[FILTER_SIZE_X];
+            float blockA01[FILTER_SIZE_X];
+
+            // in case the data is not aligned to sizeof(T)*FILTER_SIZE_X we need to use vload or set the data in a loop
+            {
+                unsigned i = 0;
+                LOOP(FILTER_SIZE_X, i,
+                {
+#if LEFTOVERS == 1
+                    if(src0_read_offset0_const + (FILTER_SIZE_Y - 1) * INPUT0_Y_PITCH + (INPUT0_FEATURE_NUM - 1) * (INPUT0_FEATURE_PITCH - ( FILTER_SIZE_Y * INPUT0_Y_PITCH )) >= INPUT0_BATCH_NUM * INPUT0_BATCH_PITCH)
+                    {
+                        if(src0_read_offset0 + i < INPUT0_BATCH_NUM * INPUT0_BATCH_PITCH)
+                            blockA00[i] = src0[src0_read_offset0 + i];
+                    }
+                    else
+#endif
+                        blockA00[i] = src0[src0_read_offset0 + i];
+
+#if LEFTOVERS == 1
+                    if(src0_read_offset1_const + (FILTER_SIZE_Y - 1) * INPUT0_Y_PITCH + (INPUT0_FEATURE_NUM - 1) * (INPUT0_FEATURE_PITCH - ( FILTER_SIZE_Y * INPUT0_Y_PITCH )) >= INPUT0_BATCH_NUM * INPUT0_BATCH_PITCH)
+                    {
+                        if(src0_read_offset1 + i < INPUT0_BATCH_NUM * INPUT0_BATCH_PITCH)
+                            blockA01[i] = src0[src0_read_offset1 + i];
+                    }
+                    else
+#endif
+                        blockA01[i] = src0[src0_read_offset1 + i];
+                } )
+            }
+
+            float*  pblockA00 = (float*)(&blockA00);
+            float*  pblockA01 = (float*)(&blockA01);
+
+            src0_read_offset0 += INPUT0_Y_PITCH;
+            src0_read_offset1 += INPUT0_Y_PITCH;
+
+
+            float blockB00[FILTER_SIZE_X*4];
+            float8* p8BlockB00 = (float8*)blockB00;
+            float4* p4BlockB00 = (float4*)blockB00;
+            float*  pBlockB00 =  (float* )blockB00;
+
+            interleaved_y = 0;
+            LOOP(FILTER_SIZE_X_DIV2, interleaved_y,
+            {
+                p8BlockB00[interleaved_y] = as_float8( intel_sub_group_block_read8( (const __global uint*)src1 + src1_read_offset ) );
+                src1_read_offset += ALIGNED_OFM * 2;
+            } )
+            if ( kernel_width_is_odd )
+            {
+                p4BlockB00[FILTER_SIZE_X - 1] = as_float4( intel_sub_group_block_read4( (const __global uint*)src1 + src1_read_offset ) );
+                src1_read_offset += ALIGNED_OFM * 2;
+            }
+
+            // Perform MADs
+            kernel_idx = 0;
+            interleaved_y = 0;
+            LOOP(FILTER_SIZE_X_DIV2, interleaved_y,
+            {
+                kernel_y = interleaved_y * 2;
+                DOT_PRODUCT_8( blockC00, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC01, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;
+)__krnl"
+R"__krnl(                DOT_PRODUCT_8( blockC00, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC01, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC10, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC11, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC10, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC11, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC20, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC21, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC20, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC21, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC30, pblockA00[kernel_y    ], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC31, pblockA01[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC30, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC31, pblockA01[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;
+            } )
+            if ( kernel_width_is_odd )
+            {
+                kernel_y = interleaved_y * 2;
+                DOT_PRODUCT_8( blockC00, pblockA00[kernel_y], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC01, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC10, pblockA00[kernel_y], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC11, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC20, pblockA00[kernel_y], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC21, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_8( blockC30, pblockA00[kernel_y], pBlockB00[kernel_idx] );
+                DOT_PRODUCT_8( blockC31, pblockA01[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;
+            }
+        }
+
+        //while( ++patch_row < 1 ); //debug
+        while( ++patch_row < FILTER_SIZE_Y );
+
+        src0_read_offset0 += INPUT0_FEATURE_PITCH - ( FILTER_SIZE_Y * INPUT0_Y_PITCH ); // reset to start of next slice of patch
+        src0_read_offset1 += INPUT0_FEATURE_PITCH - ( FILTER_SIZE_Y * INPUT0_Y_PITCH ); // reset to start of next slice of patch
+    }
+    //while ( ++patch_depth < 1 );  //debug
+    while ( ++patch_depth < INPUT0_FEATURE_NUM );
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    // Dst resembles a cube of width x height x (output channel * batches).  Each tile writes:
+    // (SIMD * TILE_M) x 1 x TILE_N.  Partial writes most likely generated if padding used.
+    __global float *out0 = dst + OUTPUT_OFFSET + out_split_offset
+     + global_z * OUTPUT_BATCH_PITCH                                                   // batch offset
+     + ( group_x * TILE_N ) * OUTPUT_FEATURE_PITCH                                     // channel offset
+     + ( ( global_y * TILE_M ) / OUTPUT_SIZE_X ) * OUTPUT_Y_PITCH                      // y offset
+     + ( ( global_y * TILE_M ) % OUTPUT_SIZE_X );                                      // x offset
+    __global float *out1 = dst + OUTPUT_OFFSET + out_split_offset
+     + global_z * OUTPUT_BATCH_PITCH                                                   // batch offset
+     + ( group_x * TILE_N ) * OUTPUT_FEATURE_PITCH                                     // channel offset
+     + ( ( global_y * TILE_M + 1 ) / OUTPUT_SIZE_X ) * OUTPUT_Y_PITCH                  // y offset
+     + ( ( global_y * TILE_M + 1 ) % OUTPUT_SIZE_X );                                  // x offset
+
+    #if BIAS_TERM
+    __global float8* biasPtr = (__global float8*) (bias + group_x * TILE_N);
+    #endif
+
+    if( global_y * TILE_M < OUTPUT_SIZE_X * OUTPUT_SIZE_Y )
+    {
+        if ( ( OUTPUT_FEATURE_NUM % TILE_N ) == 0 )
+        {
+            #if BIAS_TERM
+            blockC00 += *biasPtr;
+            blockC10 += *(biasPtr + 1);
+            blockC20 += *(biasPtr + 2);
+            blockC30 += *(biasPtr + 3);
+            #endif
+
+            blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+            blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+            blockC20 = ACTIVATION(blockC20, NL_M, NL_N);
+            blockC30 = ACTIVATION(blockC30, NL_M, NL_N);
+
+            for( unsigned i = 0; i < 8; i++ )
+            {
+                out0[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                out0[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+                out0[(16+i) * OUTPUT_FEATURE_PITCH] = blockC20[i];
+                out0[(24+i) * OUTPUT_FEATURE_PITCH] = blockC30[i];
+            }
+        }
+        else
+        {
+            if ( ( global_x + 1 ) < get_global_size(0) )
+            {
+                #if BIAS_TERM
+                blockC00 += *biasPtr;
+                blockC10 += *(biasPtr + 1);
+                blockC20 += *(biasPtr + 2);
+                blockC30 += *(biasPtr + 3);
+                #endif
+
+                blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+                blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+                blockC20 = ACTIVATION(blockC20, NL_M, NL_N);
+                blockC30 = ACTIVATION(blockC30, NL_M, NL_N);
+
+                for ( unsigned i = 0; i < 8; i++ )
+                {
+                    out0[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                    out0[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+                    out0[(16+i) * OUTPUT_FEATURE_PITCH] = blockC20[i];
+                    out0[(24+i) * OUTPUT_FEATURE_PITCH] = blockC30[i];
+                }
+            }
+            else
+            {
+                if ( ( OUTPUT_FEATURE_NUM % TILE_N ) >= 24 )
+                {
+                    #if BIAS_TERM
+                    blockC00 += *biasPtr;
+                    blockC10 += *(biasPtr + 1);
+                    blockC20 += *(biasPtr + 2);
+                    if (( OUTPUT_FEATURE_NUM % TILE_N) > 24 ) blockC30 += *(biasPtr + 3);
+                    #endif
+
+                    blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+                    blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+                    blockC20 = ACTIVATION(blockC20, NL_M, NL_N);
+
+                    for (unsigned i = 0; i < 8; i++)
+                    {
+                        out0[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                        out0[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+                        out0[(16+i) * OUTPUT_FEATURE_PITCH] = blockC20[i];
+                    }
+
+                    // remaining output channels
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out0[(24+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC30[i], NL_M, NL_N);
+                    }
+                }
+                else if ( ( OUTPUT_FEATURE_NUM % TILE_N ) >= 16 )
+                {
+                    #if BIAS_TERM
+                    blockC00 += *biasPtr;
+                    blockC10 += *(biasPtr + 1);
+                    if (( OUTPUT_FEATURE_NUM % TILE_N) > 16 )
+                        blockC20 += *(biasPtr + 2);
+                    #endif
+
+                    blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+                    blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+
+                    for (unsigned i = 0; i < 8; i++)
+                    {
+                        out0[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                        out0[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+                    }
+
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out0[(16+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC20[i], NL_M, NL_N);
+
+                    }
+                }
+                else if ( ( OUTPUT_FEATURE_NUM % TILE_N ) >= 8 )
+                {
+                    #if BIAS_TERM
+                    blockC00 += *biasPtr;
+                    if (( OUTPUT_FEATURE_NUM % TILE_N) > 8 )
+                        blockC10 += *(biasPtr + 1);
+                    #endif
+
+                    blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+
+                    for (unsigned i = 0; i < 8; i++)
+                    {
+                        out0[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                    }
+
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out0[(8+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC10[i], NL_M, NL_N);
+                    }
+                }
+                else
+                {
+                    #if BIAS_TERM
+                    blockC00 += *biasPtr;
+                    #endif
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out0[( 0+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC00[i], NL_M, NL_N);
+                    }
+                }
+            }
+        }
+    }
+
+    if ((global_y * TILE_M + 1) < OUTPUT_SIZE_X * OUTPUT_SIZE_Y )
+    {
+        if ( ( OUTPUT_FEATURE_NUM % TILE_N ) == 0 )
+        {
+            #if BIAS_TERM
+            blockC01 += *biasPtr;
+            blockC11 += *(biasPtr + 1);
+            blockC21 += *(biasPtr + 2);
+            blockC31 += *(biasPtr + 3);
+            #endif
+)__krnl"
+R"__krnl(
+            blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+            blockC11 = ACTIVATION(blockC11, NL_M, NL_N);
+            blockC21 = ACTIVATION(blockC21, NL_M, NL_N);
+            blockC31 = ACTIVATION(blockC31, NL_M, NL_N);
+
+            for( unsigned i = 0; i < 8; i++ )
+            {
+                out1[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC01[i];
+                out1[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC11[i];
+                out1[(16+i) * OUTPUT_FEATURE_PITCH] = blockC21[i];
+                out1[(24+i) * OUTPUT_FEATURE_PITCH] = blockC31[i];
+            }
+        }
+        else
+        {
+            if ( ( global_x + 1 ) < get_global_size(0) )
+            {
+                #if BIAS_TERM
+                blockC01 += *biasPtr;
+                blockC11 += *(biasPtr + 1);
+                blockC21 += *(biasPtr + 2);
+                blockC31 += *(biasPtr + 3);
+                #endif
+
+                blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+                blockC11 = ACTIVATION(blockC11, NL_M, NL_N);
+                blockC21 = ACTIVATION(blockC21, NL_M, NL_N);
+                blockC31 = ACTIVATION(blockC31, NL_M, NL_N);
+
+                for ( unsigned i = 0; i < 8; i++ )
+                {
+                    out1[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC01[i];
+                    out1[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC11[i];
+                    out1[(16+i) * OUTPUT_FEATURE_PITCH] = blockC21[i];
+                    out1[(24+i) * OUTPUT_FEATURE_PITCH] = blockC31[i];
+                }
+            }
+            else
+            {
+                if ( ( OUTPUT_FEATURE_NUM % TILE_N ) >= 24 )
+                {
+                    #if BIAS_TERM
+                    blockC01 += *biasPtr;
+                    blockC11 += *(biasPtr + 1);
+                    blockC21 += *(biasPtr + 2);
+                    if ( ( OUTPUT_FEATURE_NUM % TILE_N ) > 24 ) blockC31 += *(biasPtr + 3);
+                    #endif
+
+                    blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+                    blockC11 = ACTIVATION(blockC11, NL_M, NL_N);
+                    blockC21 = ACTIVATION(blockC21, NL_M, NL_N);
+
+                    for (unsigned i = 0; i < 8; i++)
+                    {
+                        out1[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC01[i];
+                        out1[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC11[i];
+                        out1[(16+i) * OUTPUT_FEATURE_PITCH] = blockC21[i];
+                    }
+
+                    // Remaining channels
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out1[(24+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC31[i], NL_M, NL_N);
+                    }
+                }
+                else if ( ( OUTPUT_FEATURE_NUM % TILE_N ) >= 16 )
+                {
+                    #if BIAS_TERM
+                    blockC01 += *biasPtr;
+                    blockC11 += *(biasPtr + 1);
+                    if ( ( OUTPUT_FEATURE_NUM % TILE_N ) > 16 ) blockC21 += *(biasPtr + 2);
+                    #endif
+
+                    blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+                    blockC11 = ACTIVATION(blockC11, NL_M, NL_N);
+
+                    for (unsigned i = 0; i < 8; i++)
+                    {
+                        out1[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC01[i];
+                        out1[( 8+i) * OUTPUT_FEATURE_PITCH] = blockC11[i];
+                    }
+
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out1[(16+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC21[i], NL_M, NL_N);
+                    }
+                }
+                else if ( ( OUTPUT_FEATURE_NUM % TILE_N ) >= 8 )
+                {
+                    #if BIAS_TERM
+                    blockC01 += *biasPtr;
+                    if ( ( OUTPUT_FEATURE_NUM % TILE_N ) > 8 ) blockC11 += *(biasPtr + 1);
+                    #endif
+
+                    blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+
+                    for (unsigned i = 0; i < 8; i++)
+                    {
+                        out1[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC01[i];
+                    }
+
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out1[(8+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC11[i], NL_M, NL_N);
+                    }
+                }
+                else
+                {
+                    #if BIAS_TERM
+                    blockC01 += *biasPtr;
+                    #endif
+
+                    for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 8; i++)
+                    {
+                        out1[( 0+i) * OUTPUT_FEATURE_PITCH] = ACTIVATION(blockC01[i], NL_M, NL_N);
+                    }
+                }
+            }
+        }
+    }
+}
+
+)__krnl"},
+
+{"pooling_gpu_byxf_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define VECTOR_TYPE MAKE_VECTOR_TYPE(UNIT_TYPE,8)
+#define FEATURE_PER_ITEM 8
+#define FEATURE_BLOCK_NUM (OUTPUT_FEATURE_NUM / 8)
+
+#if   defined MAX_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_MIN
+#elif defined AVG_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_ZERO
+#else
+#error
+#endif
+
+inline VECTOR_TYPE FUNC(apply_pooling)(VECTOR_TYPE tmp, VECTOR_TYPE in)
+{
+#if defined MAX_POOLING
+    return max(tmp, in);
+#elif defined AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+KERNEL(pooling_gpu_byxf_opt)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    VECTOR_TYPE out;
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf / INPUT0_BATCH_NUM * FEATURE_PER_ITEM;
+    const uint b    = bf % INPUT0_BATCH_NUM;
+
+    VECTOR_TYPE feature_block;
+
+    if ((x >= OUTPUT_SIZE_X) || (y >= OUTPUT_SIZE_Y))
+        return;
+
+    const int offset_x = (int)x*STRIDE_SIZE_X;
+    const int offset_y = (int)y*STRIDE_SIZE_Y;
+
+    int input_idx = b*FEATURE_BLOCK_NUM*INPUT0_SIZE_X*INPUT0_SIZE_Y + FEATURE_BLOCK_NUM*INPUT0_SIZE_X*offset_y + FEATURE_BLOCK_NUM*offset_x + bf / INPUT0_BATCH_NUM;
+
+    out = UNIT_INIT_VAL;
+)__krnl"
+R"__krnl(
+    __attribute__((opencl_unroll_hint))
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        __attribute__((opencl_unroll_hint))
+        for(uint i = 0; i < POOL_SIZE_X; i++)
+        {
+            feature_block = vload8(input_idx+FEATURE_BLOCK_NUM*i, input);
+            out = FUNC_CALL(apply_pooling)(out, feature_block);
+        }
+        input_idx += FEATURE_BLOCK_NUM*INPUT0_SIZE_X;
+    }
+
+    uint output_pos = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+    __attribute__((opencl_unroll_hint))
+    for(uint i = 0; i < FEATURE_PER_ITEM; i++)
+    {
+        if(f+i < INPUT0_FEATURE_NUM){
+#if defined MAX_POOLING
+            output[output_pos+i] = ACTIVATION(out[i], NL_M ,NL_N);
+#elif defined AVG_POOLING
+            output[output_pos+i] = ACTIVATION(out[i]/(UNIT_TYPE)(POOL_SIZE_X*POOL_SIZE_Y), NL_M ,NL_N);
+#endif
+        }
+    }
+}
+
+#undef FEATURE_BLOCK_NUM
+#undef FEATURE_PER_ITEM
+#undef UNIT_INIT_VAL
+#undef VECTOR_TYPE
+
+)__krnl"},
+
+{"convolution_gpu_byxf_af32_depthwise",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+)__krnl"
+R"__krnl(        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+KERNEL(convolution_gpu_byxf_af32_depthwise)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+#if QUANTIZATION_TERM
+    __global float* quantizations,
+#endif
+#if CALIBRATION_TERM
+    __global float* calibrations,
+#endif
+    uint split_idx)
+{
+    const uint x = get_global_id(1);
+    const uint y = get_global_id(2);
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(0);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(0) % OUTPUT_FEATURE_NUM;
+    const uint b = get_global_id(0) / OUTPUT_FEATURE_NUM;
+#endif
+
+#if QUANTIZATION_TERM
+    int dotProd = 0;
+#else
+    UNIT_TYPE dotProd = UNIT_VAL_ZERO;
+#endif
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (f / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint filter_offset = f*FILTER_OFM_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    for (uint k = 0; k < FILTER_IFM_NUM; ++k)
+    {
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+            const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+            if(!zero_y)
+            {
+                for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+                {
+                    const int input_offset_x = input_x + i * DILATION_SIZE_X;
+                    const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                    if(!zero_x)
+                    {
+                        uint input_idx = input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH + k*INPUT0_FEATURE_PITCH;
+                        uint filter_idx = filter_offset + k*FILTER_IFM_PITCH + j*FILTER_Y_PITCH + i*FILTER_X_PITCH;
+#if QUANTIZATION_TERM
+                        dotProd += (int)input[input_idx] * (int)weights[filter_idx];
+#else
+                        dotProd += input[input_idx] * weights[filter_idx];
+#endif
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+#if QUANTIZATION_TERM
+#if CALIBRATION_TERM
+
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+#else  // QUANTIZATION_TERM
+    dotProd += (UNIT_TYPE)biases[bias_index];
+#endif // QUANTIZATION_TERM
+#endif
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y, x) + out_split_offset;
+
+#if QUANTIZATION_TERM
+    output[dst_index] = ACTIVATION(convert_char(dotProd), NL_M, NL_N);
+#else
+    output[dst_index] = ACTIVATION(dotProd, NL_M, NL_N);
+#endif
+
+}
+
+)__krnl"},
+
+{"convolution_gpu_yxfb_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(convolution_gpu_yxfb_ref)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    const __global UNIT_TYPE* bias,
+#endif
+    uint split_idx)
+{
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+
+    const uint batch_offset = (uint)get_global_id(0) % INPUT0_BATCH_NUM;
+    const uint ofm_offset   = (uint)get_global_id(0) / INPUT0_BATCH_NUM;
+    const uint out_x        = (uint)get_global_id(1);
+    const uint out_y        = (uint)get_global_id(2);
+
+    const int x = (int)out_x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int y = (int)out_y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (ofm_offset / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint input_offset = INPUT0_OFFSET + batch_offset*INPUT0_BATCH_PITCH + in_split_offset;
+
+    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i * DILATION_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j * DILATION_SIZE_X;
+                const bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                if(!zero)
+                {
+                    uint input_idx = input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH;
+                    uint filter_idx = ofm_offset*FILTER_OFM_PITCH + i*FILTER_Y_PITCH + j*FILTER_X_PITCH;
+)__krnl"
+R"__krnl(
+                    for (uint h = 0; h < FILTER_IFM_NUM; h++)
+                    {
+                        result = fma(input[input_idx], filter[filter_idx], result);
+                        filter_idx += FILTER_IFM_PITCH;
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+                }
+            }
+        }
+    }
+#if BIAS_TERM
+    result += bias[ofm_offset];
+#endif
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * FILTER_OFM_NUM;
+    const uint dst_index = batch_offset*OUTPUT_BATCH_PITCH + ofm_offset*OUTPUT_FEATURE_PITCH + out_y*OUTPUT_Y_PITCH + out_x*OUTPUT_X_PITCH + OUTPUT_OFFSET + out_split_offset;
+    output[dst_index] = ACTIVATION(result, NL_M, NL_N);
+}
+
+)__krnl"},
+
+{"select_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define GET_INDEX(prefix, num)                                                      \
+    CAT(CAT(prefix, num), _OFFSET) +                                                \
+    (d1 % CAT(CAT(prefix, num), _SIZES)[0])*CAT(CAT(prefix, num), _PITCHES)[0] +    \
+    (d2 % CAT(CAT(prefix, num), _SIZES)[1])*CAT(CAT(prefix, num), _PITCHES)[1] +    \
+    (d3 % CAT(CAT(prefix, num), _SIZES)[2])*CAT(CAT(prefix, num), _PITCHES)[2] +    \
+    (d4 % CAT(CAT(prefix, num), _SIZES)[3])*CAT(CAT(prefix, num), _PITCHES)[3]
+
+#define INPUT_0 input0[GET_INDEX(INPUT, 0)]
+#define INPUT_1 input1[GET_INDEX(INPUT, 1)]
+#define INPUT_2 input2[GET_INDEX(INPUT, 2)]
+
+KERNEL(select)(
+    INPUTS_DECLS
+    __global OUTPUT_TYPE* output)
+{
+
+const uint d1  = (uint) get_global_id(0);
+const uint d2  = (uint) get_global_id(1);
+const uint d34 = (uint) get_global_id(2);
+
+const uint d3  = d34 % OUTPUT_SIZES[2];
+const uint d4  = d34 / OUTPUT_SIZES[2];
+
+uint output_offset = OUTPUT_OFFSET +
+                     d1*OUTPUT_PITCHES[0] +
+                     d2*OUTPUT_PITCHES[1] +
+                     d3*OUTPUT_PITCHES[2] +
+                     d4*OUTPUT_PITCHES[3];
+
+const OUTPUT_TYPE res = select(INPUT_1, INPUT_0, MASK);
+
+output[output_offset] = res;
+}
+
+)__krnl"},
+
+{"lrn_within_channel_byxf_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+)__krnl"
+R"__krnl(                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+)__krnl"
+R"__krnl(        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define VECTOR_TYPE MAKE_VECTOR_TYPE(UNIT_TYPE,8)
+#define ACCUMULATOR_VECTOR_TYPE MAKE_VECTOR_TYPE(ACCUMULATOR_TYPE, 8)
+#define FEATURE_PER_ITEM 8
+#define FEATURE_BLOCK_NUM (OUTPUT_FEATURE_NUM / 8)
+
+KERNEL(lrn_within_channel_byxf_opt)(__global const INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+    const uint b = get_global_id(GWS_BATCH);
+    const uint f = get_global_id(GWS_FEATURE)*FEATURE_PER_ITEM;
+    const uint y = get_global_id(GWS_YX) / INPUT0_SIZE_X;
+    const uint x = get_global_id(GWS_YX) % INPUT0_SIZE_X;
+
+    const uint input_index = GET_DATA_INDEX(INPUT0, b, f, y, x);
+    const uint output_index = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+
+    ACCUMULATOR_VECTOR_TYPE sum = 0.0f;
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    uint num_elementes = 0;
+#endif
+
+    const int x_start = ((int)x - PADDING);
+    const int y_start = ((int)y - PADDING);
+    int input_offset = (GET_DATA_INDEX(INPUT0, b, f, y_start, x_start))/8;
+
+    VECTOR_TYPE feature_block;
+
+    for (int j = 0; j < LOCAL_SIZE; ++j)
+    {
+        for (int i = 0; i < LOCAL_SIZE; ++i)
+        {
+            int input_offset_x = x_start + i;
+            int input_offset_y = y_start + j;
+            bool zero = false;
+            zero = input_offset_x < 0 ? true : zero;
+            zero = input_offset_y < 0 ? true : zero;
+            zero = input_offset_x >= INPUT0_SIZE_X ? true : zero;
+            zero = input_offset_y >= INPUT0_SIZE_Y ? true : zero;
+
+)__krnl"
+R"__krnl(            VECTOR_TYPE val = zero ? UNIT_VAL_ZERO : vload8(input_offset+FEATURE_BLOCK_NUM*i, input);
+
+            sum = mad(val,val,sum);
+#ifdef DYNAMIC_KERNEL_DIVIDER
+            num_elementes += zero ? 0 : 1;
+#endif
+        }
+        input_offset += INPUT0_Y_PITCH/FEATURE_PER_ITEM;
+    }
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    const UNIT_TYPE num_elementes_div = UNIT_VAL_ONE / TO_UNIT_TYPE(num_elementes);
+#else
+    const UNIT_TYPE num_elementes_div = NUM_ELEMENTS_DIV;
+#endif
+
+    const VECTOR_TYPE base = mad((ACCUMULATOR_TYPE)ALPHA*num_elementes_div, sum, TO_UNIT_TYPE(K));
+    const VECTOR_TYPE normalization_factor = native_powr(base, TO_UNIT_TYPE(-BETA));
+    const VECTOR_TYPE val = vload8(input_index/FEATURE_PER_ITEM, input);
+    const VECTOR_TYPE normres = val*normalization_factor;
+
+    for(uint i = 0; i < FEATURE_PER_ITEM; i++)
+    {
+        output[output_index+i] = ACTIVATION(normres[i], NL_M ,NL_N);
+    }
+}
+
+#undef FEATURE_BLOCK_NUM
+#undef FEATURE_PER_ITEM
+#undef VECTOR_TYPE
+#undef ACCUMULATOR_VECTOR_TYPE
+
+)__krnl"},
+
+{"gemm_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+KERNEL(gemm_ref)
+	(const __global UNIT_TYPE* input0,
+	const __global UNIT_TYPE* input1,
+#if OUT_BIAS_TERM
+	const __global UNIT_TYPE* input2,
+#endif
+	__global UNIT_TYPE* output)
+{
+    const uint x = (uint)get_global_id(0);
+	const uint y = (uint)get_global_id(1);
+	const uint b = (uint)get_global_id(2);
+	uint in0_idx=0;
+	uint in1_idx=0;
+	float value = 0;
+
+#if TRANSPOSE_INPUT1
+for (uint i = 0; i < Y1; ++i)
+	{
+		in0_idx = i * X1 + x + b * X1 * Y1;
+#else
+	for (uint i = 0; i < X1; ++i)
+	{
+		in0_idx = x * X1 + i + b * X1 * Y1;
+#endif
+
+#if TRANSPOSE_INPUT2
+	in1_idx = y * X2 + i + b * X2 * Y2;
+#else
+	in1_idx = i * X2 + y + b * X2 * Y2;
+#endif
+
+		value = fma(input0[in0_idx], input1[in1_idx], value);
+	}
+#if TRANSPOSE_INPUT1 && TRANSPOSE_INPUT2
+	uint out_idx = y * X1 + x + b * X1 * Y2;
+#elif TRANSPOSE_INPUT1
+	uint out_idx = x * X2 + y + b * X1 * Y1;
+#elif TRANSPOSE_INPUT2
+	uint out_idx = x * Y2 + y + b * X2 * Y2;
+#else
+	uint out_idx = x * X2 + y + b * X2 * Y1;
+#endif
+
+	float beta_out = 0;
+#if OUT_BIAS_TERM
+	beta_out = BETA * input2[out_idx];
+#endif
+	output[out_idx] = fma(ALPHA, value, beta_out);
+}
+
+
+)__krnl"},
+
+{"fully_connected_gpu_bfyx_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(fc)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    const __global FILTER_TYPE* weights
+#if BIAS_TERM
+    , const __global BIAS_TYPE* biases
+#endif
+#if QUANTIZATION_TERM
+    ,const __global float* quantizations
+#endif
+#if CALIBRATION_TERM
+    ,const __global float* calibrations
+#endif
+    )
+{
+    const uint ofm = get_global_id(0);
+    const uint b = get_global_id(1);
+
+#if QUANTIZATION_TERM
+    int dotProd = 0;
+#else
+    ACCUMULATOR_TYPE dotProd = 0;
+#endif
+
+    for (uint ifm = 0; ifm < INPUT0_FEATURE_NUM; ++ifm)
+    {
+       for (uint y = 0; y < INPUT0_SIZE_Y; ++y)
+       {
+           for(uint x = 0; x < INPUT0_SIZE_X; ++x )
+           {
+               const uint input0_idx = GET_DATA_INDEX(INPUT0, b, ifm, y, x);
+               const uint filter_idx = GET_FILTER_INDEX(FILTER, ofm, ifm, y, x);
+#if QUANTIZATION_TERM
+               dotProd += (int)input[input0_idx] * (int)weights[filter_idx];
+#else
+               dotProd += (ACCUMULATOR_TYPE)(input[input0_idx] * weights[filter_idx]);
+#endif
+          }
+       }
+    }
+
+)__krnl"
+R"__krnl(    const uint output_idx = GET_DATA_INDEX(OUTPUT, b, ofm, 0, 0);
+
+#if BIAS_TERM
+    const uint bias_index = ofm;
+#endif
+
+#if BIAS_TERM
+#if QUANTIZATION_TERM
+#if CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[ofm] * I_QF + biases[bias_index]) * calibrations[ofm]);
+#else  // CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[ofm] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+#else  // QUANTIZATION_TERM
+    dotProd += (ACCUMULATOR_TYPE)biases[bias_index];
+#endif // QUANTIZATION_TERM
+#endif
+
+#if QUANTIZATION_TERM
+    output[output_idx] = ACTIVATION(convert_char(dotProd), NL_M, NL_N);
+#else
+    output[output_idx] = ACTIVATION((UNIT_TYPE)dotProd, NL_M, NL_N);
+#endif
+}
+
+)__krnl"},
+
+{"convolution_gpu_mmad_blocks",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+)__krnl"
+R"__krnl(        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+#define FILTER_IFM_MMAD_NUM ((FILTER_IFM_NUM + 31) / 32)
+#define FILTER_OFM_MMAD_NUM ((FILTER_OFM_NUM + 7) / 8)
+#define FILTER_IFM_ALIGNED (FILTER_IFM_MMAD_NUM * 32)
+#define FILTER_OFM_ALIGNED (FILTER_OFM_MMAD_NUM * 8)
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+KERNEL(convolution_MMAD_blocks)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+    __global float* quantizations,
+#if CALIBRATION_TERM
+    __global float* calibrations,
+#endif
+    uint split_idx)
+{
+    const uint x = get_global_id(0) * OUTPUT_BLOCK_WIDTH;
+    const uint y = get_global_id(1) * OUTPUT_BLOCK_HEIGHT;
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(2);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(2) % FILTER_OFM_ALIGNED;
+    const uint b = get_global_id(2) / FILTER_OFM_ALIGNED;
+#endif
+
+    // our output values
+    int out[OUTPUT_BLOCK_WIDTH * OUTPUT_BLOCK_HEIGHT] = { 0 };
+    int in[IN_BLOCK_ARRAY_SIZE];
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+
+    const uint filter_offset = (get_group_id(2) % FILTER_OFM_MMAD_NUM) * FILTER_OFM_BLOCK_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    uint in_addr = input_offset + input_x * INPUT0_X_PITCH + input_y * INPUT0_Y_PITCH;
+    uint filter_idx = filter_offset;
+
+   	__attribute__((opencl_unroll_hint(1)))
+    for (uint k = 0; k < FILTER_IFM_MMAD_NUM; ++k)
+    {
+        // preload input data
+        for(uint in_block_pos = 0; in_block_pos < IN_BLOCK_ARRAY_SIZE; in_block_pos++)
+        {
+            uint block_x = in_block_pos % IN_BLOCK_WIDTH;
+            uint block_y = in_block_pos / IN_BLOCK_WIDTH;
+            uint input_idx = in_addr + block_x * INPUT0_X_PITCH + block_y * INPUT0_Y_PITCH;
+            in[in_block_pos] = as_int(intel_sub_group_block_read((const __global uint*)(input + input_idx)));
+        }
+        // end of preloading input data
+
+        __attribute__((opencl_unroll_hint(FILTER_SIZE_Y)))
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+		    __attribute__((opencl_unroll_hint(FILTER_SIZE_X)))
+            for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+            {
+                int8 weights_data = as_int8(intel_sub_group_block_read8((const __global uint*)(weights + filter_idx)));
+
+			    __attribute__((opencl_unroll_hint(OUTPUT_BLOCK_HEIGHT)))
+                for(uint br = 0; br < OUTPUT_BLOCK_HEIGHT; br++)
+                {
+				    __attribute__((opencl_unroll_hint(OUTPUT_BLOCK_WIDTH)))
+                    for(uint bc = 0; bc < OUTPUT_BLOCK_WIDTH; bc++)
+                    {
+                        int input_data = in[(br * STRIDE_SIZE_Y + j) * IN_BLOCK_WIDTH + bc * STRIDE_SIZE_X + i];
+                        int8 activations;  //activations of all lanes
+                        activations.s0 = sub_group_broadcast(input_data, 0);
+                        activations.s1 = sub_group_broadcast(input_data, 1);
+                        activations.s2 = sub_group_broadcast(input_data, 2);
+                        activations.s3 = sub_group_broadcast(input_data, 3);
+                        activations.s4 = sub_group_broadcast(input_data, 4);
+                        activations.s5 = sub_group_broadcast(input_data, 5);
+                        activations.s6 = sub_group_broadcast(input_data, 6);
+                        activations.s7 = sub_group_broadcast(input_data, 7);
+
+                        out[br * OUTPUT_BLOCK_WIDTH + bc] = MMAD_8(activations, weights_data, out[br * OUTPUT_BLOCK_WIDTH + bc]);
+                    }
+                }
+                filter_idx += 32*8; // 32 features per channel * 8 output features per SIMD channel
+            }
+        }
+        in_addr += 32; // 4 features per channel * 8 SIMD channels
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+
+    for(uint br = 0; br < OUTPUT_BLOCK_HEIGHT; br++)
+    {
+        for(uint bc = 0; bc < OUTPUT_BLOCK_WIDTH; bc++)
+        {
+#if CALIBRATION_TERM
+            out[br * OUTPUT_BLOCK_WIDTH + bc] = (UNIT_TYPE)round(((float)out[br * OUTPUT_BLOCK_WIDTH + bc] * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+            out[br * OUTPUT_BLOCK_WIDTH + bc] = (UNIT_TYPE)round(((float)out[br * OUTPUT_BLOCK_WIDTH + bc] * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+        }
+    }
+#endif // BIAS_TERM
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    for(uint br = 0; br < OUTPUT_BLOCK_HEIGHT; br++)
+    {
+        if(y + br < OUTPUT_SIZE_Y)
+        {
+            for(uint bc = 0; bc < OUTPUT_BLOCK_WIDTH; bc++)
+            {
+                if(x + bc < OUTPUT_SIZE_X)
+                {
+                    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y+br, x+bc) + out_split_offset;
+                    output[dst_index] = ACTIVATION(convert_char(out[br * OUTPUT_BLOCK_WIDTH + bc]), NL_M, NL_N);
+                }
+            }
+        }
+    }
+}
+
+#undef FILTER_IFM_MMAD_NUM
+#undef FILTER_OFM_MMAD_NUM
+#undef FILTER_IFM_ALIGNED
+#undef FILTER_OFM_ALIGNED
+
+)__krnl"},
+
+{"convolution_gpu_mmad_batched",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline int FUNC(mmad_4)(char4 input, char4 weight, int acc)
+{
+	acc += (input[0] * weight[0]);
+	acc += (input[1] * weight[1]);
+	acc += (input[2] * weight[2]);
+	acc += (input[3] * weight[3]);
+	return acc;
+}
+
+inline int FUNC(mmad8)(int8 A_scalars, int8 B_vectors, int acc)
+{
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[0]), as_char4(B_vectors[0]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[1]), as_char4(B_vectors[1]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[2]), as_char4(B_vectors[2]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[3]), as_char4(B_vectors[3]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[4]), as_char4(B_vectors[4]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[5]), as_char4(B_vectors[5]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[6]), as_char4(B_vectors[6]), acc);
+	acc = FUNC_CALL(mmad_4)(as_char4(A_scalars[7]), as_char4(B_vectors[7]), acc);
+
+	return acc;
+}
+
+inline int4 FUNC(mmad4x8)(int4 A_vectors, int8 B_vectors, int4 acc)
+{
+    int4 ret;
+    for(uint i = 0; i < 4; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+)__krnl"
+R"__krnl(        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+inline int8 FUNC(mmad8x8)(int8 A_vectors, int8 B_vectors, int8 acc)
+{
+    int8 ret;
+    for(uint i = 0; i < 8; i++)
+    {
+        int8 A_scalars;
+        A_scalars.s0 = sub_group_broadcast(A_vectors[i], 0);
+        A_scalars.s1 = sub_group_broadcast(A_vectors[i], 1);
+        A_scalars.s2 = sub_group_broadcast(A_vectors[i], 2);
+        A_scalars.s3 = sub_group_broadcast(A_vectors[i], 3);
+        A_scalars.s4 = sub_group_broadcast(A_vectors[i], 4);
+        A_scalars.s5 = sub_group_broadcast(A_vectors[i], 5);
+        A_scalars.s6 = sub_group_broadcast(A_vectors[i], 6);
+        A_scalars.s7 = sub_group_broadcast(A_vectors[i], 7);
+        ret[i] = FUNC_CALL(mmad8)(A_scalars, B_vectors, acc[i]);
+    }
+    return ret;
+}
+
+
+#define MMAD_8(A, B, C) FUNC_CALL(mmad8)(A, B, C)
+#define MMAD_4x8(A, B, C) FUNC_CALL(mmad4x8)(A, B, C)
+#define MMAD_8x8(A, B, C) FUNC_CALL(mmad8x8)(A, B, C)
+
+
+#define FILTER_IFM_MMAD_NUM ((FILTER_IFM_NUM + 31) / 32)
+#define FILTER_OFM_MMAD_NUM ((FILTER_OFM_NUM + 7) / 8)
+#define FILTER_IFM_ALIGNED (FILTER_IFM_MMAD_NUM * 32)
+#define FILTER_OFM_ALIGNED (FILTER_OFM_MMAD_NUM * 8)
+// input data is in blocks 4batch x 32 features
+// each SIMD process 4 batches and 8 output features
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+KERNEL(convolution_mmad_batched)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+#if QUANTIZATION_TERM
+    const __global float* quantizations,
+#endif
+#if CALIBRATION_TERM
+    const __global float* calibrations,
+#endif
+    uint split_idx)
+{
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+
+    const uint f = get_global_id(2) % FILTER_OFM_ALIGNED;
+    const uint b_block = get_global_id(2) / FILTER_OFM_ALIGNED;
+    const uint f_block = f / 32;
+
+    int4 dotProd = 0;
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    const uint filter_offset = (get_group_id(2) % FILTER_OFM_MMAD_NUM) * FILTER_OFM_BLOCK_PITCH;
+    const uint input_offset = IN_OFFSET + IN_B_BLOCK_PITCH * b_block;
+
+    for (uint k = 0; k < FILTER_IFM_MMAD_NUM; ++k)
+    {
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+            const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+            if(!zero_y)
+            {
+                for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+                {
+                    const int input_offset_x = input_x + i * DILATION_SIZE_X;
+                    const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                    if(!zero_x)
+                    {
+                        uint input_idx = input_offset + input_offset_y * IN_Y_PITCH + input_offset_x * IN_X_PITCH + k * IN_F_BLOCK_PITCH;
+                        uint filter_idx = filter_offset + k*FILTER_Y_PITCH * FILTER_SIZE_Y + j*FILTER_Y_PITCH + i*FILTER_X_PITCH;
+
+						int4 input_data = as_int4(intel_sub_group_block_read4((const __global uint*)(input + input_idx)));
+                        int8 weights_data = as_int8(intel_sub_group_block_read8((const __global uint*)(weights + filter_idx)));
+
+                        dotProd = MMAD_4x8(input_data, weights_data, dotProd);
+                    }
+                }
+            }
+        }
+    }
+
+for(uint b = 0; b < 4; b++)
+{
+
+#if BIAS_TERM
+    const uint bias_index = f;
+#if QUANTIZATION_TERM
+#if CALIBRATION_TERM
+    dotProd[b] = (UNIT_TYPE)round(((float)dotProd[b] * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    dotProd[b] = (UNIT_TYPE)round(((float)dotProd[b] * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+#else // QUANTIZATION_TERM
+    dotProd[b] += (UNIT_TYPE)biases[bias_index];
+#endif // QUANTIZATION_TERM
+#endif // BIAS_TERM
+
+    const uint dst_index = GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(OUTPUT, b_block*4 + b, f, y, x);
+#if QUANTIZATION_TERM
+    output[dst_index] = ACTIVATION(convert_char(dotProd[b]), NL_M, NL_N);
+#else
+    output[dst_index] = ACTIVATION(dotProd[b], NL_M, NL_N);
+#endif
+}
+}
+
+#undef FILTER_IFM_MMAD_NUM
+#undef FILTER_OFM_MMAD_NUM
+#undef FILTER_IFM_ALIGNED
+#undef FILTER_OFM_ALIGNED
+
+)__krnl"},
+
+{"fully_connected_gpu_bs_f_bsv16_b1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Just-in-time macro definitions:
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Required JIT constants:
+//  - FP16_SUPPORTED        - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED        - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE             - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO         - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT0_BATCH_NUM      - [int] Batch size for input. Number of input sets of spatial and feature data that
+//                                  are grouped to be processed in single batch.
+//  - INPUT0_ELEMENTS_COUNT - [int] Cumulative number of elements in single data set from batch.
+//  - FILTER_OFM_NUM        - [int] Cumulative number of elements that are outputted for single input set from batch.
+//                           Number of layer responses per single input set from batch.
+//  - RELU                  - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE        - [float] Factor for negative output values (required when ReLU is specified).
+//
+//  - SUB_GROUP_SIZE        - [int] Size of used subgroup (SIMD).
+//  - UNIT_BYTE_SIZE        - [int] Size of unit of input/output/weight/bias in bytes.
+//  - CHUNK_TYPE            - Type of chunk of data read by work item using sub-group operation (OpenCL scalar type).
+//  - CHUNK_BYTE_SIZE       - [int] Size of chunk of data read by work item using sub-group operation in bytes.
+//  - UNITS_PER_CHUNK       - [int] Number of units stored in single chunk of read data.
+//                                  Must be equal CHUNK_BYTE_SIZE / UNIT_BYTE_SIZE (and this division must not have
+//                                  remainder). Added as helper for manual loop unrolling.
+//  - BYTES_PER_SG_READ     - [int] Number of bytes read by single sub-group read operation (read by entire sub-group).
+//                                  Must be equal (CHUNK_BYTE_SIZE * SUB_GROUP_SIZE). Added as helper for manual loop
+//                                  unrolling.
+//  - UNITS_PER_SG_READ     - [int] Number of units read by single sub-group read operation (read by entire sub-group).
+//                                  Must be equal (UNIT_BYTE_SIZE * SUB_GROUP_SIZE). Added as helper for manual loop
+//                                  unrolling.
+//
+//  - RESPONSES_PER_SG_EXEC      - [int] Number of neural responses processed/executed by single sub-group.
+//  - IN_CHUNK_PREFETCH_SIZE     - [int] Size of array of CHUNK_TYPE use to cache/prefetch input data.
+//  - FILTER_CHUNK_PREFETCH_SIZE - [int] Size of array of CHUNK_TYPE use to cache/prefetch filter/weights data.
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Helpers:
+// ---------------------------------------------------------------------------------------------------------------------
+
+#define CONCAT_TOKEN_HANDLER1(prefix, suffix) prefix##suffix
+
+)__krnl"
+R"__krnl(// Expands and concatenates two tokens into one.
+#define CONCAT_TOKEN(prefix, suffix) CONCAT_TOKEN_HANDLER1(prefix, suffix)
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Converts scalar expression to scalar of unit type.
+#if FP16_UNIT_USED
+    #define CVT_UNIT(expression) CONCAT_TOKEN(convert_, UNIT_TYPE)(expression)
+#else
+    #define CVT_UNIT(expression) (expression)
+#endif
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// - CHUNK_UNITS_TYPE - Type of scalar or vector of UNIT_TYPE that can be reinterpreted as CHUNK_TYPE.
+#if UNITS_PER_CHUNK == 1
+    #define CHUNK_UNITS_TYPE UNIT_TYPE
+#else
+    #define CHUNK_UNITS_TYPE MAKE_VECTOR_TYPE(UNIT_TYPE, UNITS_PER_CHUNK)
+#endif
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Reinterpretation between CHUNK_TYPE and CHUNK_UNITS_TYPE.
+#define AS_CHUNK(expression) CONCAT_TOKEN(as_, CHUNK_TYPE)(expression)
+#define AS_UNITS(expression) CONCAT_TOKEN(as_, CHUNK_UNITS_TYPE)(expression)
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Extracts one scalar element of UNIT_TYPE from work-item chunk;
+//     chunk - name of chunk variable, idx - 0-based index of element.
+#if UNITS_PER_CHUNK == 4
+    #define CHUNK_UNIT_SELECT(chunk, idx) ((idx) > 1 ? ((idx) > 2 ? AS_UNITS(chunk).s3 : AS_UNITS(chunk).s2) : ((idx) ? AS_UNITS(chunk).s1 : AS_UNITS(chunk).s0))
+#elif UNITS_PER_CHUNK == 2
+    #define CHUNK_UNIT_SELECT(chunk, idx) ((idx) ? AS_UNITS(chunk).s1 : AS_UNITS(chunk).s0)
+#elif UNITS_PER_CHUNK == 1
+    #define CHUNK_UNIT_SELECT(chunk, idx) AS_UNITS(chunk)
+#else
+    #error Unsupported number of units per chunk.
+#endif
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Sub-group operations:
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Extracts one scalar element of UNIT_TYPE from sub-group chunk;
+//     chunk - name of chunk variable, idx - 0-based index of element.
+#define SG_UNIT_SELECT(chunk, idx) CHUNK_UNIT_SELECT(intel_sub_group_shuffle(chunk, (idx) / UNITS_PER_CHUNK), (idx) % UNITS_PER_CHUNK)
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Reads / Writes:
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Type of chunk salar/vector returned by or passed to read/write macros.
+#define CHUNK_VEC1_TYPE CHUNK_TYPE
+#define CHUNK_VEC2_TYPE MAKE_VECTOR_TYPE(CHUNK_TYPE, 2)
+#define CHUNK_VEC4_TYPE MAKE_VECTOR_TYPE(CHUNK_TYPE, 4)
+#define CHUNK_VEC8_TYPE MAKE_VECTOR_TYPE(CHUNK_TYPE, 8)
+
+// Expands vector of chunks to array of chunks (using order of components);
+//     array - name of chunk array variable, idx - 0-based start index in array where vector should be expanded,
+//     chunk_vec - vector to expand.
+#define EXPAND_CHUNK_VEC1_TO_CHUNK_ARRAY(array, idx, chunk_vec) ((void)((array)[(idx)] = chunk_vec))
+#define EXPAND_CHUNK_VEC2_TO_CHUNK_ARRAY(array, idx, chunk_vec) ((void)((array)[(idx)] = chunk_vec.s0, (array)[(idx) + 1] = chunk_vec.s1))
+#define EXPAND_CHUNK_VEC4_TO_CHUNK_ARRAY(array, idx, chunk_vec) ((void)((array)[(idx)]     = chunk_vec.s0, (array)[(idx) + 1] = chunk_vec.s1, \
+                                                                        (array)[(idx) + 2] = chunk_vec.s2, (array)[(idx) + 3] = chunk_vec.s3))
+#define EXPAND_CHUNK_VEC8_TO_CHUNK_ARRAY(array, idx, chunk_vec) ((void)((array)[(idx)]     = chunk_vec.s0, (array)[(idx) + 1] = chunk_vec.s1, \
+                                                                        (array)[(idx) + 2] = chunk_vec.s2, (array)[(idx) + 3] = chunk_vec.s3, \
+                                                                        (array)[(idx) + 4] = chunk_vec.s4, (array)[(idx) + 5] = chunk_vec.s5, \
+                                                                        (array)[(idx) + 6] = chunk_vec.s6, (array)[(idx) + 7] = chunk_vec.s7))
+
+// Currently block read is 4 bytes aligned.
+#define ALIGNED_BLOCK_READ1(ptr, byte_offset) intel_sub_group_block_read((const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+#define ALIGNED_BLOCK_READ2(ptr, byte_offset) intel_sub_group_block_read2((const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+#define ALIGNED_BLOCK_READ4(ptr, byte_offset) intel_sub_group_block_read4((const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+#define ALIGNED_BLOCK_READ8(ptr, byte_offset) intel_sub_group_block_read8((const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+
+// Currently read is 4 bytes aligned.
+#define ALIGNED_READ1(ptr, byte_offset) (*(const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+#define ALIGNED_READ2(ptr, byte_offset) vload2(0, (const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+#define ALIGNED_READ4(ptr, byte_offset) vload4(0, (const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+#define ALIGNED_READ8(ptr, byte_offset) vload8(0, (const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+
+// Currently block write is 16 bytes aligned.
+#define ALIGNED_BLOCK_WRITE1(ptr, byte_offset, val) intel_sub_group_block_write((__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)), (val))
+#define ALIGNED_BLOCK_WRITE2(ptr, byte_offset, val) intel_sub_group_block_write2((__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)), (val))
+#define ALIGNED_BLOCK_WRITE4(ptr, byte_offset, val) intel_sub_group_block_write4((__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)), (val))
+#define ALIGNED_BLOCK_WRITE8(ptr, byte_offset, val) intel_sub_group_block_write8((__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)), (val))
+
+// Currently block write is 4 bytes aligned.
+#define ALIGNED_WRITE1(ptr, byte_offset, val) ((void)(*(__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)) = (val)))
+#define ALIGNED_WRITE2(ptr, byte_offset, val) vstore2((val), 0, (__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)))
+#define ALIGNED_WRITE4(ptr, byte_offset, val) vstore4((val), 0, (__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)))
+#define ALIGNED_WRITE8(ptr, byte_offset, val) vstore8((val), 0, (__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)))
+
+
+
+// Kernel-specific JIT requirements.
+#if INPUT0_BATCH_NUM != 1
+    #error Kernel does not support specified input batch size.
+#endif
+#if UNITS_PER_SG_READ <= 0 || RESPONSES_PER_SG_EXEC <= 0 || UNITS_PER_CHUNK <= 0 || UNITS_PER_SG_READ % RESPONSES_PER_SG_EXEC != 0 || RESPONSES_PER_SG_EXEC % UNITS_PER_CHUNK != 0
+    #error Kernel does not support specified number of responses processed by single sub-group.
+#endif
+#if IN_CHUNK_PREFETCH_SIZE <= 0 || FILTER_CHUNK_PREFETCH_SIZE <= 0 || (IN_CHUNK_PREFETCH_SIZE * RESPONSES_PER_SG_EXEC) % FILTER_CHUNK_PREFETCH_SIZE != 0
+    #error Kernel does not support specified prefetch sizes.
+#endif
+
+
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+KERNEL (fully_connected_gpu_bx_bs_x_bsv16_b1)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    // constexpr:
+    const uint input_byte_size  = INPUT0_ELEMENTS_COUNT * UNIT_BYTE_SIZE;
+
+    const uint output_size      = FILTER_OFM_NUM;
+
+    // Identifier of work item element in processing sub-group.
+    const uint sg_elem_id       = get_sub_group_local_id();
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+    // non-constexpr:
+    // Identifier of processing sub-group (each sub-group process RESPONSES_PER_SG_EXEC output responses).
+    const uint sg_id          = get_group_id(0);
+
+    // Input base offset in bytes (bfyx/bx format of input).
+    const uint input_base     = 0;
+
+    // Filter base offset in bytes (bs_x_bsv16 format of weights).
+    const uint filter_base    = sg_id * input_byte_size * RESPONSES_PER_SG_EXEC;
+
+    // [SCATTERED] Output base identifier/element offset to use (bx format of output).
+    const uint output_base_id = sg_id * RESPONSES_PER_SG_EXEC + sg_elem_id;
+#if BIAS_TERM
+    // [SCATTERED] Bias base identifier/element offset to use (x/f format of biases).
+    const uint bias_base_id = output_base_id;
+#endif //
+    // Filter/input byte offsets in sub-group used duering read/write operations.
+    const uint sg_elem_offset = sg_elem_id * CHUNK_BYTE_SIZE;
+
+
+    // Accumulator for fully connected. Contains one or more sum of multiples on yxf plane. If there is more than one it needs to be sum up to single one.
+    CHUNK_TYPE acc = 0;
+
+    uint input_offset = input_base;   // Non-scattered offset (all work items in sub-group must have the same value, so the loop will not diverge in sub-group).
+    uint filter_offset = filter_base; // Non-scattered (to support different sub-group scatter in remainder processing and avoid TPM issue during shuffling).
+    while (input_offset + IN_CHUNK_PREFETCH_SIZE * BYTES_PER_SG_READ <= input_byte_size)
+    {
+        // Contains chached IN_CHUNK_PREFETCH_SIZE * UNITS_PER_SG_READ input elements.
+        // Currently consecutive fyx data elements are stored in consecutive work-items in sub-group (elems in array seen from work-item are offseted by UNITS_PER_SG_READ).
+        CHUNK_TYPE input_val[IN_CHUNK_PREFETCH_SIZE];
+
+#if IN_CHUNK_PREFETCH_SIZE % 8 == 0
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_SIZE; input_val_idx += 8)
+        {
+            CHUNK_VEC8_TYPE input_vals = ALIGNED_BLOCK_READ8(input, input_offset + 8 * sg_elem_offset);
+            input_offset += 8 * BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC8_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+#elif IN_CHUNK_PREFETCH_SIZE % 4 == 0
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_SIZE; input_val_idx += 4)
+        {
+            CHUNK_VEC4_TYPE input_vals = ALIGNED_BLOCK_READ4(input, input_offset + 4 * sg_elem_offset);
+            input_offset += 4 * BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC4_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+#elif IN_CHUNK_PREFETCH_SIZE % 2 == 0
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_SIZE; input_val_idx += 2)
+        {
+            CHUNK_VEC2_TYPE input_vals = ALIGNED_BLOCK_READ2(input, input_offset + 2 * sg_elem_offset);
+            input_offset += 2 * BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC2_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+#else
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_SIZE; input_val_idx += 1)
+        {
+            CHUNK_VEC1_TYPE input_vals = ALIGNED_BLOCK_READ1(input, input_offset + sg_elem_offset);
+            input_offset += BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC1_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+#endif
+
+        __attribute__((opencl_unroll_hint))
+        for (uint elem_base_idx = 0; elem_base_idx < IN_CHUNK_PREFETCH_SIZE * UNITS_PER_SG_READ; elem_base_idx += FILTER_CHUNK_PREFETCH_SIZE * UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC)
+        {
+)__krnl"
+R"__krnl(            // Contains group of weights for RESPONSES_PER_SG_EXEC responses and for (FILTER_CHUNK_PREFETCH_SIZE * UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC) spatial points.
+            // Currently for floats:
+            // sub-group-id |        0 |        1 |        2 | ... |        15
+            // -------------+----------+----------+----------+-----+----------
+            // [0]          | (s0, r0) | (s0, r1) | (s0, r2) | ... | (s0, r15)
+            // [1]          | (s1, r0) | (s1, r1) | (s1, r2) | ... | (s1, r15)
+            // [2]          | (s2, r0) | (s2, r1) | (s2, r2) | ... | (s2, r15)
+            // ...          |   ...    |   ...    |   ...    | ... |   ...
+            // Currently for halfs:
+            // sub-group-id |          0 |          1 |          2 | ... |            7 |          8 |          9 |         10 | ... |           15
+            // -------------+------------+------------+------------+-----+--------------+------------+------------+------------+-----+-------------
+            // [0]          | (s0, r0-1) | (s0, r2-3) | (s0, r4-5) | ... | (s0, r14-15) | (s1, r0-1) | (s1, r2-3) | (s1, r4-5) | ... | (s1, r14-15)
+            // [1]          | (s2, r0-1) | (s2, r2-3) | (s2, r4-5) | ... | (s2, r14-15) | (s3, r0-1) | (s3, r2-3) | (s3, r4-5) | ... | (s3, r14-15)
+            // [2]          | (s4, r0-1) | (s4, r2-3) | (s4, r4-5) | ... | (s4, r14-15) | (s5, r0-1) | (s5, r2-3) | (s5, r4-5) | ... | (s5, r14-15)
+            // ...          |    ...     |    ...     |    ...     | ... |     ...      |    ...     |    ...     |    ...     | ... |     ...
+            CHUNK_TYPE filter_val[FILTER_CHUNK_PREFETCH_SIZE];
+
+#if FILTER_CHUNK_PREFETCH_SIZE % 8 == 0
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < FILTER_CHUNK_PREFETCH_SIZE; filter_val_idx += 8)
+            {
+                CHUNK_VEC8_TYPE filter_vals = ALIGNED_BLOCK_READ8(weight, filter_offset + 8 * sg_elem_offset);
+                filter_offset += 8 * BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC8_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#elif FILTER_CHUNK_PREFETCH_SIZE % 4 == 0
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < FILTER_CHUNK_PREFETCH_SIZE; filter_val_idx += 4)
+            {
+                CHUNK_VEC4_TYPE filter_vals = ALIGNED_BLOCK_READ4(weight, filter_offset + 4 * sg_elem_offset);
+                filter_offset += 4 * BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC4_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#elif FILTER_CHUNK_PREFETCH_SIZE % 2 == 0
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < FILTER_CHUNK_PREFETCH_SIZE; filter_val_idx += 2)
+            {
+                CHUNK_VEC2_TYPE filter_vals = ALIGNED_BLOCK_READ2(weight, filter_offset + 2 * sg_elem_offset);
+                filter_offset += 2 * BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC2_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#else
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < FILTER_CHUNK_PREFETCH_SIZE; filter_val_idx += 1)
+            {
+                CHUNK_VEC1_TYPE filter_vals = ALIGNED_BLOCK_READ1(weight, filter_offset + sg_elem_offset);
+                filter_offset += BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC1_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#endif
+
+            // Processing of cached filter chunks.
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < FILTER_CHUNK_PREFETCH_SIZE; ++filter_val_idx)
+            {
+                const uint input_base_elem_idx = elem_base_idx + filter_val_idx * UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC;
+
+                // Select different input for every SUB_GROUP_SIZE * RESPONSES_PER_SG_EXEC / UNITS_PER_SG_READ work-items in sub-group.
+                // This code is suboptimal because get_sub_group_local_id() is not treated as constexpr (compiler issue).
+#if UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC == 4
+                UNIT_TYPE rearranged_input = sg_elem_id < SUB_GROUP_SIZE / 2
+                    ? (sg_elem_id < SUB_GROUP_SIZE / 4
+                        ? (SG_UNIT_SELECT(input_val[input_base_elem_idx / UNITS_PER_SG_READ], input_base_elem_idx % UNITS_PER_SG_READ))
+                        : (SG_UNIT_SELECT(input_val[(input_base_elem_idx + 1) / UNITS_PER_SG_READ], (input_base_elem_idx + 1) % UNITS_PER_SG_READ)))
+                    : (sg_elem_id < 3 * SUB_GROUP_SIZE / 4
+                        ? (SG_UNIT_SELECT(input_val[(input_base_elem_idx + 2) / UNITS_PER_SG_READ], (input_base_elem_idx + 2) % UNITS_PER_SG_READ))
+                        : (SG_UNIT_SELECT(input_val[(input_base_elem_idx + 3) / UNITS_PER_SG_READ], (input_base_elem_idx + 3) % UNITS_PER_SG_READ)));
+#elif UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC == 2
+                UNIT_TYPE rearranged_input = sg_elem_id < SUB_GROUP_SIZE / 2
+                    ? (SG_UNIT_SELECT(input_val[input_base_elem_idx / UNITS_PER_SG_READ], input_base_elem_idx % UNITS_PER_SG_READ))
+                    : (SG_UNIT_SELECT(input_val[(input_base_elem_idx + 1) / UNITS_PER_SG_READ], (input_base_elem_idx + 1) % UNITS_PER_SG_READ));
+#elif UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC == 1
+                UNIT_TYPE rearranged_input = SG_UNIT_SELECT(input_val[input_base_elem_idx / UNITS_PER_SG_READ], input_base_elem_idx % UNITS_PER_SG_READ);
+#else
+    #error Selected RESPONSES_PER_SG_EXEC is not supported.
+#endif
+
+                acc = AS_CHUNK(fma(rearranged_input, AS_UNITS(filter_val[filter_val_idx]), AS_UNITS(acc)));
+            }
+        }
+    }
+
+
+// Processing input remainder (if needed).
+#define INPUT0_ELEMENTS_REMAINDER             (INPUT0_ELEMENTS_COUNT % (IN_CHUNK_PREFETCH_SIZE * UNITS_PER_SG_READ))
+#define IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE ((INPUT0_ELEMENTS_REMAINDER + UNITS_PER_SG_READ - 1) / UNITS_PER_SG_READ)
+#if INPUT0_ELEMENTS_REMAINDER != 0
+
+    {
+        CHUNK_TYPE input_val[IN_CHUNK_PREFETCH_SIZE];
+
+    #if IN_CHUNK_PREFETCH_SIZE % 8 == 0 && (IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE % 8 == 0 || IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE >= 16)
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE; input_val_idx += 8)
+        {
+            CHUNK_VEC8_TYPE input_vals = ALIGNED_BLOCK_READ8(input, input_offset + 8 * sg_elem_offset);
+            input_offset += 8 * BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC8_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+    #elif IN_CHUNK_PREFETCH_SIZE % 4 == 0 && (IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE % 4 == 0 || IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE >= 8)
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE; input_val_idx += 4)
+        {
+            CHUNK_VEC4_TYPE input_vals = ALIGNED_BLOCK_READ4(input, input_offset + 4 * sg_elem_offset);
+            input_offset += 4 * BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC4_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+    #elif IN_CHUNK_PREFETCH_SIZE % 2 == 0 && (IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE % 2 == 0 || IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE >= 4)
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE; input_val_idx += 2)
+        {
+            CHUNK_VEC2_TYPE input_vals = ALIGNED_BLOCK_READ2(input, input_offset + 2 * sg_elem_offset);
+            input_offset += 2 * BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC2_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+    #else
+        __attribute__((opencl_unroll_hint))
+        for (uint input_val_idx = 0; input_val_idx < IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE; input_val_idx += 1)
+        {
+            CHUNK_VEC1_TYPE input_vals = ALIGNED_BLOCK_READ1(input, input_offset + sg_elem_offset);
+            input_offset += BYTES_PER_SG_READ;
+            EXPAND_CHUNK_VEC1_TO_CHUNK_ARRAY(input_val, input_val_idx, input_vals);
+        }
+    #endif
+
+        __attribute__((opencl_unroll_hint))
+        for (uint elem_base_idx = 0; elem_base_idx < INPUT0_ELEMENTS_REMAINDER; elem_base_idx += FILTER_CHUNK_PREFETCH_SIZE * UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC)
+        {
+            // Size of array of CHUNK_TYPE needed to contain filter elements for input elements in range [elem_base_idx; INPUT0_ELEMENTS_REMAINDER).
+            const uint filter_chunk_remainder_size = ((INPUT0_ELEMENTS_REMAINDER - elem_base_idx) * RESPONSES_PER_SG_EXEC + UNITS_PER_SG_READ - 1) / UNITS_PER_SG_READ;
+            const uint filter_chunk_prefetch_req_size = filter_chunk_remainder_size < FILTER_CHUNK_PREFETCH_SIZE ? filter_chunk_remainder_size : FILTER_CHUNK_PREFETCH_SIZE;
+
+            CHUNK_TYPE filter_val[FILTER_CHUNK_PREFETCH_SIZE];
+
+#if FILTER_CHUNK_PREFETCH_SIZE % 8 == 0
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < filter_chunk_prefetch_req_size; filter_val_idx += 8)
+            {
+                CHUNK_VEC8_TYPE filter_vals = ALIGNED_BLOCK_READ8(weight, filter_offset + 8 * sg_elem_offset);
+                filter_offset += 8 * BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC8_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#elif FILTER_CHUNK_PREFETCH_SIZE % 4 == 0
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < filter_chunk_prefetch_req_size; filter_val_idx += 4)
+            {
+                CHUNK_VEC4_TYPE filter_vals = ALIGNED_BLOCK_READ4(weight, filter_offset + 4 * sg_elem_offset);
+                filter_offset += 4 * BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC4_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#elif FILTER_CHUNK_PREFETCH_SIZE % 2 == 0
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < filter_chunk_prefetch_req_size; filter_val_idx += 2)
+            {
+                CHUNK_VEC2_TYPE filter_vals = ALIGNED_BLOCK_READ2(weight, filter_offset + 2 * sg_elem_offset);
+                filter_offset += 2 * BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC2_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#else
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < filter_chunk_prefetch_req_size; filter_val_idx += 1)
+            {
+                CHUNK_VEC1_TYPE filter_vals = ALIGNED_BLOCK_READ1(weight, filter_offset + sg_elem_offset);
+                filter_offset += BYTES_PER_SG_READ;
+                EXPAND_CHUNK_VEC1_TO_CHUNK_ARRAY(filter_val, filter_val_idx, filter_vals);
+            }
+#endif
+
+            // Processing of cached filter chunks.
+            __attribute__((opencl_unroll_hint))
+            for (uint filter_val_idx = 0; filter_val_idx < filter_chunk_prefetch_req_size; ++filter_val_idx)
+            {
+                const uint input_base_elem_idx = elem_base_idx + filter_val_idx * UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC;
+
+                // Select different input for every SUB_GROUP_SIZE * RESPONSES_PER_SG_EXEC / UNITS_PER_SG_READ work-items in sub-group.
+                // This code is suboptimal because get_sub_group_local_id() is not treated as constexpr (compiler issue).
+#if UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC == 4
+                UNIT_TYPE rearranged_input = sg_elem_id < SUB_GROUP_SIZE / 2
+                    ? (sg_elem_id < SUB_GROUP_SIZE / 4
+                        ? (input_base_elem_idx     < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[input_base_elem_idx       / UNITS_PER_SG_READ], input_base_elem_idx % UNITS_PER_SG_READ)       : UNIT_VAL_ZERO)
+                        : (input_base_elem_idx + 1 < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[(input_base_elem_idx + 1) / UNITS_PER_SG_READ], (input_base_elem_idx + 1) % UNITS_PER_SG_READ) : UNIT_VAL_ZERO))
+                    : (sg_elem_id < 3 * SUB_GROUP_SIZE / 4
+                        ? (input_base_elem_idx + 2 < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[(input_base_elem_idx + 2) / UNITS_PER_SG_READ], (input_base_elem_idx + 2) % UNITS_PER_SG_READ) : UNIT_VAL_ZERO)
+                        : (input_base_elem_idx + 3 < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[(input_base_elem_idx + 3) / UNITS_PER_SG_READ], (input_base_elem_idx + 3) % UNITS_PER_SG_READ) : UNIT_VAL_ZERO));
+#elif UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC == 2
+                UNIT_TYPE rearranged_input = sg_elem_id < SUB_GROUP_SIZE / 2
+                    ? (input_base_elem_idx     < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[input_base_elem_idx       / UNITS_PER_SG_READ], input_base_elem_idx % UNITS_PER_SG_READ)       : UNIT_VAL_ZERO)
+                    : (input_base_elem_idx + 1 < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[(input_base_elem_idx + 1) / UNITS_PER_SG_READ], (input_base_elem_idx + 1) % UNITS_PER_SG_READ) : UNIT_VAL_ZERO);
+#elif UNITS_PER_SG_READ / RESPONSES_PER_SG_EXEC == 1
+                UNIT_TYPE rearranged_input = input_base_elem_idx < INPUT0_ELEMENTS_REMAINDER ? SG_UNIT_SELECT(input_val[input_base_elem_idx / UNITS_PER_SG_READ], input_base_elem_idx % UNITS_PER_SG_READ) : UNIT_VAL_ZERO;
+#else
+    #error Selected RESPONSES_PER_SG_EXEC is not supported.
+#endif
+
+                acc = AS_CHUNK(fma(rearranged_input, AS_UNITS(filter_val[filter_val_idx]), AS_UNITS(acc)));
+            }
+        }
+    }
+
+#endif
+)__krnl"
+R"__krnl(#undef INPUT0_ELEMENTS_REMAINDER
+#undef IN_CHUNK_PREFETCH_REMAINDER_REQ_SIZE
+
+
+    // Secondary accumulator that will contain final sum (special reducing over work-items in sub-group).
+    CHUNK_TYPE reduced_acc = acc;
+    CHUNK_TYPE zero = 0;
+
+    for (uint sg_reduce_offset = SUB_GROUP_SIZE * RESPONSES_PER_SG_EXEC / UNITS_PER_SG_READ;
+         sg_reduce_offset < SUB_GROUP_SIZE;
+         sg_reduce_offset += SUB_GROUP_SIZE * RESPONSES_PER_SG_EXEC / UNITS_PER_SG_READ)
+    {
+        reduced_acc = AS_CHUNK(AS_UNITS(reduced_acc) + AS_UNITS(intel_sub_group_shuffle_down(acc, zero, sg_reduce_offset)));
+    }
+
+
+    // Expand accumulator chunks to units.
+    const uint expanded_acc_size = (RESPONSES_PER_SG_EXEC + SUB_GROUP_SIZE - 1) / SUB_GROUP_SIZE;
+
+    __attribute__((opencl_unroll_hint))
+    for (uint expanded_acc_idx = 0; expanded_acc_idx < expanded_acc_size; ++expanded_acc_idx)
+    {
+        const uint output_id = output_base_id + expanded_acc_idx * SUB_GROUP_SIZE;
+#if BIAS_TERM
+        const uint bias_id = bias_base_id + expanded_acc_idx * SUB_GROUP_SIZE;
+#endif
+        UNIT_TYPE expanded_acc = SG_UNIT_SELECT(reduced_acc, expanded_acc_idx * SUB_GROUP_SIZE + sg_elem_id);
+
+        if (output_id < output_size)
+        {
+#if BIAS_TERM
+            expanded_acc += bias[bias_id];
+#endif
+            output[output_id] = ACTIVATION(expanded_acc, NL_M, NL_N);
+        }
+    }
+}
+
+#undef CONCAT_TOKEN_HANDLER1
+#undef CONCAT_TOKEN
+#undef MAKE_VECTOR_TYPE
+#undef CVT_UNIT
+#undef CHUNK_UNITS_TYPE
+#undef AS_CHUNK
+#undef AS_UNITS
+#undef CHUNK_UNIT_SELECT
+
+#undef SG_UNIT_SELECT
+#undef CHUNK_VEC1_TYPE
+#undef CHUNK_VEC2_TYPE
+#undef CHUNK_VEC4_TYPE
+#undef CHUNK_VEC8_TYPE
+#undef EXPAND_CHUNK_VEC1_TO_CHUNK_ARRAY
+#undef EXPAND_CHUNK_VEC2_TO_CHUNK_ARRAY
+#undef EXPAND_CHUNK_VEC4_TO_CHUNK_ARRAY
+#undef EXPAND_CHUNK_VEC8_TO_CHUNK_ARRAY
+#undef ALIGNED_BLOCK_READ1
+#undef ALIGNED_BLOCK_READ2
+#undef ALIGNED_BLOCK_READ4
+#undef ALIGNED_BLOCK_READ8
+#undef ALIGNED_READ1
+#undef ALIGNED_READ2
+#undef ALIGNED_READ4
+#undef ALIGNED_READ8
+#undef ALIGNED_BLOCK_WRITE1
+#undef ALIGNED_BLOCK_WRITE2
+#undef ALIGNED_BLOCK_WRITE4
+#undef ALIGNED_BLOCK_WRITE8
+#undef ALIGNED_WRITE1
+#undef ALIGNED_WRITE2
+#undef ALIGNED_WRITE4
+#undef ALIGNED_WRITE8
+
+
+)__krnl"},
+
+{"mvn_gpu_ref_accross_channels",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+
+KERNEL (mvn_gpu_ref_accross_channels)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    const uint b = get_global_id(0);
+    float mean = 0.f;
+
+    const uint input_first = INPUT0_OFFSET + b * INPUT0_BATCH_PITCH;
+
+    // Compute mean
+    uint input_idx = input_first;
+    for (uint f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+        for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+        {
+            for (uint x = 0; x < INPUT0_SIZE_X; x++)
+            {
+                mean += (float)input[input_idx];
+                input_idx += INPUT0_X_PITCH;
+            }
+            input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_FEATURE_PITCH - INPUT0_Y_PITCH*INPUT0_SIZE_Y;
+    }
+    uint output_idx = OUTPUT_OFFSET + b * OUTPUT_BATCH_PITCH;
+    mean /= INPUT0_FEATURE_NUM * INPUT0_SIZE_Y * INPUT0_SIZE_X;
+
+#if NORMALIZE_VARIANCE == 0
+    // Subtract mean / compute variance if needed
+    input_idx = input_first;
+    for (uint f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+        for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+        {
+            for (uint x = 0; x < INPUT0_SIZE_X; x++)
+            {
+                output[output_idx] = ACTIVATION(input[input_idx] - UNIT_CVT_FUNC(mean), NL_M, NL_N);
+                input_idx += INPUT0_X_PITCH;
+                output_idx += OUTPUT_X_PITCH;
+            }
+            input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+            output_idx += OUTPUT_Y_PITCH - INPUT0_SIZE_X*OUTPUT_X_PITCH;
+        }
+        input_idx += INPUT0_FEATURE_PITCH - INPUT0_Y_PITCH*INPUT0_SIZE_Y;
+        output_idx += OUTPUT_FEATURE_PITCH - INPUT0_SIZE_Y*OUTPUT_Y_PITCH;
+    }
+
+#else //NORMALIZE_VARIANCE
+    float variance = 0.f;
+
+    //compute variance
+    input_idx = input_first;
+    for (uint f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+        for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+        {
+            for (uint x = 0; x < INPUT0_SIZE_X; x++)
+            {
+                float res = (float)input[input_idx] - mean;
+                variance = fma(res, res, variance);
+                input_idx += INPUT0_X_PITCH;
+            }
+            input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_FEATURE_PITCH - INPUT0_Y_PITCH*INPUT0_SIZE_Y;
+    }
+
+    //normalize variance
+    variance /= INPUT0_FEATURE_NUM * INPUT0_SIZE_Y * INPUT0_SIZE_X;
+    variance = native_powr(variance + (float)EPSILON, -0.5f);
+
+    input_idx = input_first;
+    for (uint f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+        for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+        {
+            for (uint x = 0; x < INPUT0_SIZE_X; x++)
+            {
+                output[output_idx] = ACTIVATION((input[input_idx] - UNIT_CVT_FUNC(mean)) * UNIT_CVT_FUNC(variance), NL_M, NL_N);
+                input_idx += INPUT0_X_PITCH;
+                output_idx += OUTPUT_X_PITCH;
+            }
+            input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+            output_idx += OUTPUT_Y_PITCH - INPUT0_SIZE_X*OUTPUT_X_PITCH;
+        }
+        input_idx += INPUT0_FEATURE_PITCH - INPUT0_Y_PITCH*INPUT0_SIZE_Y;
+        output_idx += OUTPUT_FEATURE_PITCH - INPUT0_SIZE_Y*OUTPUT_Y_PITCH;
+    }
+#endif
+}
+
+
+#undef UNIT_CVT_FUNC
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_os_iyx_osv16",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Just-in-time macro definitions:
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Required JIT constants:
+//  - INPUT                - [tensor] Input dimensions (batch, spatial and feature).
+//  - OUTPUT               - [tensor] Output dimensions (batch, spatial and feature).
+//  - STRIDE               - [tensor] Stride (only spatial). Factors that describe step size in X or Y dimension of
+//                           input position of application of convolution filter when next ouput value
+//                           (step 1 in in X or Y dimension of output) is computed.
+//  - INPUT0_OFFSET        - [tensor] Offset for the first element
+//                           initial offset input position of application of convolution filter and output position.
+//  - FP16_SUPPORTED       - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED       - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE            - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO        - Literal of current UNIT_TYPE that represents 0.
+//  - RELU                 - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE       - [float] Factor for negative output values (required when ReLU is specified).
+//
+//  - SUB_GROUP_SIZE       - [int] Size of used subgroup (SIMD).
+//  - LEFTOVERS            - [int] Optional parameter, required only when number of ofm is not dividable by SUB_GROUP_SIZE
+//                           see comment for FEATURES_THREADS_PER_BATCH for more informations
+
+/*
+gpu::make_jit_constant("OUTPUT_LIMIT",              output_size),
+gpu::make_jit_constant("FILTER",                    filter_mem.argument().size),
+gpu::make_jit_constant("FILTER_ARRAY_NUM",          split),
+gpu::make_jit_constant("OUTPUT_BLOCK_WIDTH",        _kernel_data.block_width));
+gpu::make_jit_constant("OUTPUT_BLOCK_HEIGHT",       _kernel_data.block_height));
+gpu::make_jit_constant("IN_BLOCK_ARRAY_SIZE",       _kernel_data.input_block_array_size));
+gpu::make_jit_constant("IN_BLOCK_WIDTH",            _kernel_data.input_block_width));
+gpu::make_jit_constant("PREFETCH",                  _kernel_data.prefetch));
+if (_kernel_data.leftovers)
+    gpu::make_jit_constant("LEFTOVERS",             _kernel_data.leftovers));
+*/
+
+// FEATURES_THREADS_PER_BATCH defines how many threads in z-dimension are processing single batch.
+// ideally, z-dimension of value n should indicate processing of n-th output feature. however, since
+// threads are stack in groups of SUB_GROUP_SIZE, when number of ofm is not dividable by SUB_GROUP_SIZE
+// there are dummy threads added in z-dimension in count of LEFTOVERS. We need to take them into consideration
+// while calculating batch's id (see lines 86-87). Values calculated by dummy threads are discarded at line 210.
+#ifdef LEFTOVERS
+#define FEATURES_THREADS_PER_BATCH (FILTER_OFM_NUM + LEFTOVERS)
+#else
+#define FEATURES_THREADS_PER_BATCH (FILTER_OFM_NUM)
+#endif
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+__attribute__((reqd_work_group_size(1, 1, SUB_GROUP_SIZE)))
+KERNEL(convolution_gpu_bfyx_os_iyx_osv16)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weights,
+#if BIAS_TERM
+    const __global UNIT_TYPE* bias,
+#endif
+    uint split_idx) // TODO: removing this parameter cause a performance degradation... :)
+{
+    const uint oc  = (uint)get_global_id(0) * OUTPUT_BLOCK_WIDTH;  // oc = Output Column
+    const uint or  = (uint)get_global_id(1) * OUTPUT_BLOCK_HEIGHT; // or = Output Row
+    const uint fm  = get_global_id(2);                    // fm = Feature Map = od = Output Depth
+    const uint lid = get_sub_group_local_id();
+
+    uint batch_idx = fm / FEATURES_THREADS_PER_BATCH;
+    uint feature_idx = fm % FEATURES_THREADS_PER_BATCH;
+    uint fmg = feature_idx / SUB_GROUP_SIZE;
+
+    UNIT_TYPE in[IN_BLOCK_ARRAY_SIZE];
+    UNIT_TYPE out[OUTPUT_BLOCK_WIDTH * OUTPUT_BLOCK_HEIGHT];
+    UNIT_TYPE w[PREFETCH];
+    uint in_addr;
+    uint weight_addr = fmg * FILTER_IFM_NUM * FILTER_SIZE_X * FILTER_SIZE_Y * SUB_GROUP_SIZE + lid;
+
+    for(int i = 0; i < (OUTPUT_BLOCK_WIDTH * OUTPUT_BLOCK_HEIGHT); i++) {
+        out[i] = UNIT_VAL_ZERO;
+    }
+
+    uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+    in_addr = batch_idx * INPUT0_BATCH_PITCH;
+    in_addr += in_split_offset + INPUT0_OFFSET_WITH_PADDING + or * STRIDE_SIZE_Y * INPUT0_Y_PITCH + oc * STRIDE_SIZE_X + lid;
+
+    for(int kd = 0; kd < FILTER_IFM_NUM; kd++)  // _ID = 3, RGB
+    {
+        uint tmp_in_addr = in_addr;
+
+#if IN_BLOCK_WIDTH % SUB_GROUP_SIZE == 0
+        __attribute__((opencl_unroll_hint(IN_BLOCK_ARRAY_SIZE)))
+        for(uint in_block_pos = 0; in_block_pos < IN_BLOCK_ARRAY_SIZE * SUB_GROUP_SIZE; in_block_pos += SUB_GROUP_SIZE) {
+            // Horizontal position in input block after read.
+            const uint in_block_next_x_pos = in_block_pos % IN_BLOCK_WIDTH + SUB_GROUP_SIZE;
+
+            in[in_block_pos / SUB_GROUP_SIZE] = input[tmp_in_addr + in_block_pos % IN_BLOCK_WIDTH];
+
+            // If we have row break, move to the next row.
+            if (in_block_next_x_pos == IN_BLOCK_WIDTH)
+                tmp_in_addr += INPUT0_Y_PITCH;
+        }
+#elif (2 * IN_BLOCK_WIDTH) % SUB_GROUP_SIZE == 0
+        __attribute__((opencl_unroll_hint(IN_BLOCK_ARRAY_SIZE)))
+        for(uint in_block_pos = 0; in_block_pos < IN_BLOCK_ARRAY_SIZE * SUB_GROUP_SIZE; in_block_pos += SUB_GROUP_SIZE) {
+            // Horizontal position in input block after read.
+            const uint in_block_next_x_pos = in_block_pos % IN_BLOCK_WIDTH + SUB_GROUP_SIZE;
+
+            if (in_block_next_x_pos <= IN_BLOCK_WIDTH) { //
+                in[in_block_pos / SUB_GROUP_SIZE] = input[tmp_in_addr + in_block_pos % IN_BLOCK_WIDTH];
+
+                // If we have row break, move to the next row.
+                if (in_block_next_x_pos == IN_BLOCK_WIDTH)
+                    tmp_in_addr += INPUT0_Y_PITCH;
+            }
+            else {
+                // TODO: Generalize this step to relax IN_BLOCK_WIDTH restrictions.
+                // Position in sub-group on which new row need to be read.
+                const uint sg_br_pos = IN_BLOCK_WIDTH - in_block_pos % IN_BLOCK_WIDTH;
+
+                if (lid < sg_br_pos)
+                    in[in_block_pos / SUB_GROUP_SIZE] = input[tmp_in_addr + in_block_pos % IN_BLOCK_WIDTH];
+                // We have row break inside sub-group. Need to move to next line.
+                tmp_in_addr += INPUT0_Y_PITCH;
+                if (lid >= sg_br_pos)
+                    in[in_block_pos / SUB_GROUP_SIZE] = input[tmp_in_addr - sg_br_pos];
+
+                // If we have another row break, move to the next row.
+                if (in_block_next_x_pos == 2 * IN_BLOCK_WIDTH)
+                    tmp_in_addr += INPUT0_Y_PITCH;
+            }
+)__krnl"
+R"__krnl(        }
+#else
+    #error IN_BLOCK_WIDTH must be multiple of SUB_GROUP_SIZE or half of SUB_GROUP_SIZE. Other scenarios are not currently implemented.
+#endif
+
+        //move to next filter
+        in_addr += INPUT0_FEATURE_PITCH;
+
+        for(int pf=0; pf<PREFETCH; pf++) {
+            w[pf] = weights[weight_addr]; weight_addr += SUB_GROUP_SIZE;
+        }
+
+        uint wi = 0;
+        uint kr = 0; // kr = Kernel Row
+        LOOP(FILTER_SIZE_Y, kr,  // LOOP is a macro that unrolls the loop.
+        {
+            uint kc = 0; // kc = Kernel Column
+            LOOP(FILTER_SIZE_X, kc,
+            {
+                //w = weights[weight_addr];
+                for(uint br=0; br<OUTPUT_BLOCK_HEIGHT; br++) {
+                    for(uint bc=0; bc<OUTPUT_BLOCK_WIDTH; bc++) {
+
+#if IN_BLOCK_WIDTH != SUB_GROUP_SIZE
+                        //if we fix the programming model, then we could use a nice simple 2d array: val = in[br * STRIDE_SIZE_Y + kr][bc * STRIDE_SIZE_X + kc];
+                        UNIT_TYPE val = intel_sub_group_shuffle( in[(((br * STRIDE_SIZE_Y + kr * DILATION_SIZE_Y) * IN_BLOCK_WIDTH) + (bc * STRIDE_SIZE_X + kc * DILATION_SIZE_X)) / SUB_GROUP_SIZE],
+                                                                    (((br * STRIDE_SIZE_Y + kr * DILATION_SIZE_Y) * IN_BLOCK_WIDTH) + (bc * STRIDE_SIZE_X + kc * DILATION_SIZE_X)) % SUB_GROUP_SIZE);
+#else
+                        UNIT_TYPE val = intel_sub_group_shuffle( in[br * STRIDE_SIZE_Y + kr * DILATION_SIZE_Y], bc * STRIDE_SIZE_X + kc * DILATION_SIZE_X);
+#endif
+
+                        out[br * OUTPUT_BLOCK_WIDTH + bc] = mad(w[wi % PREFETCH], val, out[br * OUTPUT_BLOCK_WIDTH + bc]);
+                    }
+                }
+                w[wi % PREFETCH] = weights[weight_addr];
+                weight_addr += SUB_GROUP_SIZE; // weights must be stored in just the right SIMD swizzled format for this to work, see host code for details.
+                wi++;
+            });
+        });
+        // addr went beyond due to prefetch so move it back to correct location.
+        weight_addr -= PREFETCH * SUB_GROUP_SIZE;
+    }
+
+    uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * FILTER_OFM_NUM;
+    uint out_addr = OUTPUT_OFFSET;
+    out_addr += batch_idx * OUTPUT_BATCH_PITCH;
+    out_addr += out_split_offset + feature_idx * OUTPUT_FEATURE_PITCH; // out_addr indices into start of 16 feature maps.
+    out_addr += or * OUTPUT_Y_PITCH + oc;  // offset for the 4x3 block that this workitem is working on;
+
+#if BIAS_TERM
+    for(uint r = 0; r < OUTPUT_BLOCK_HEIGHT; r++) {
+        for(uint c = 0; c < OUTPUT_BLOCK_WIDTH; c++) {
+#if BIAS_PER_OUTPUT
+            const unsigned bias_index = feature_idx*OUTPUT_SIZE_X*OUTPUT_SIZE_Y + or*OUTPUT_SIZE_X + oc;
+#else
+            const unsigned bias_index = feature_idx;
+#endif
+            out[r * OUTPUT_BLOCK_WIDTH + c] += bias[bias_index];
+        }
+    }
+#endif
+
+
+    for(uint r = 0; r < OUTPUT_BLOCK_HEIGHT; r++) {
+        for(uint c = 0; c < OUTPUT_BLOCK_WIDTH; c++) {
+            out[r * OUTPUT_BLOCK_WIDTH + c] = ACTIVATION(out[r * OUTPUT_BLOCK_WIDTH + c], NL_M, NL_N);
+        }
+    }
+
+#ifdef LEFTOVERS
+    if (feature_idx < OUTPUT_FEATURE_NUM)
+#endif
+    for(uint r = 0; r < OUTPUT_BLOCK_HEIGHT; r++) {
+        if(!(or + r >= OUTPUT_SIZE_Y))
+        {
+            for(uint c = 0; c < OUTPUT_BLOCK_WIDTH; c++) {
+                // this does a scattered write to 16 different feature maps, so that data within one map is contiguous, thus ready for input to next layer.
+                if(!(oc + c >= OUTPUT_SIZE_X))
+                    output[out_addr + r * OUTPUT_Y_PITCH + c] = out[r * OUTPUT_BLOCK_WIDTH + c];
+            }
+        }
+    }
+}
+
+#undef FEATURES_THREADS_PER_BATCH
+
+)__krnl"},
+
+{"convolution_gpu_yxfb_yxio_b16_fp32",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(convolution_gpu_yxfb_yxio_b16)(
+    const __global float* input,
+    __global UNIT_TYPE* output,
+    const __global float* filter,
+#if BIAS_TERM
+    const __global float* bias,
+#endif
+    uint split_idx)
+{
+    // get_global_size(0) -> Number of work items needed to compute all features and all batches for single output spatial position
+    //                       (single (x, y) point in output).
+    // get_global_size(1) -> Output size in X-dimension.
+    // get_global_size(2) -> Output size in Y-dimension.
+    // get_global_id(0)   -> Id of work item computing single spatial point of output indicated by get_global_id(1), get_global_id(2).
+    // get_global_id(1)   -> Current x-position in output.
+    // get_global_id(2)   -> Current y-position in output.
+    //
+    // WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS -> Number of work items needed to compute entire one batch for at least one feature and one spatial point.
+    //                                           (this number in current implementation computes also OFM_PER_WORK_ITEM output features at the same time).
+    // FILTER_ARRAY_NUM                       -> Number of filters groups (split size).
+
+    const uint out_x = get_global_id(1);
+    const uint out_y = get_global_id(2);
+
+    const uint output_f_size = OUTPUT_PAD_BEFORE_FEATURE_NUM + OUTPUT_FEATURE_NUM + OUTPUT_PAD_AFTER_FEATURE_NUM;
+    const uint output_x_size = OUTPUT_PAD_BEFORE_SIZE_X + OUTPUT_SIZE_X + OUTPUT_PAD_AFTER_SIZE_X;
+    const uint linear_id_xy = OUTPUT_PAD_BEFORE_SIZE_X + out_x + output_x_size * (out_y + OUTPUT_PAD_BEFORE_SIZE_Y);
+    uint global_id = (((uint)get_global_id(0) / WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS) + (linear_id_xy * FILTER_ARRAY_NUM + split_idx) * (output_f_size / OFM_PER_WORK_ITEM)) * WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS;
+
+    const uint sub_group_id = get_local_id(0);
+
+#if defined(USE_BLOCK_READ_2) || defined(USE_BLOCK_READ_1)
+    const uint chunk_size = sizeof(uint)/sizeof(UNIT_TYPE);
+#else
+    const uint chunk_size = 1;
+#endif
+
+    const uint out_batch_id = chunk_size * sub_group_id + LOCAL_WORK_GROUP_SIZE * BATCHES_PER_WORK_ITEM * ((uint)get_group_id(0) % LOCAL_WORK_GROUPS_PER_SINGLE_BATCHES_ELEMENTS);
+
+    const uint out_id = (global_id / WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS) * OFM_PER_WORK_ITEM * OUTPUT_FEATURE_PITCH + OUTPUT_PAD_BEFORE_FEATURE_NUM * OUTPUT_FEATURE_PITCH + OUTPUT_PAD_BEFORE_BATCH_NUM + out_batch_id;
+
+)__krnl"
+R"__krnl(    const uint ofm_offset = ((global_id * OFM_PER_WORK_ITEM) / WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS) % output_f_size;
+
+    // Each component of vector element contains computation for separate output feature.
+    float8 _data[BATCHES_PER_WORK_ITEM];
+    for(uint i = 0; i < BATCHES_PER_WORK_ITEM; i++)
+    {
+        _data[i] = UNIT_VAL_ZERO;
+    }
+
+    const int x = (int)out_x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int y = (int)out_y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i * DILATION_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j * DILATION_SIZE_X;
+                const bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                if(!zero)
+                {
+                    uint input_idx = input_offset_x*INPUT0_X_PITCH + input_offset_y*INPUT0_Y_PITCH;
+                    input_idx += INPUT0_OFFSET + split_idx * FILTER_IFM_NUM * INPUT0_FEATURE_PITCH;
+                    input_idx += out_batch_id;
+
+                    //sub_group_id used as offset to make each workitem load different filter, and then shuffle it
+                    uint filter_idx = ofm_offset + sub_group_id + i*FILTER_Y_PITCH + j*FILTER_X_PITCH;
+
+                    for (uint h = 0; h < FILTER_IFM_NUM; h++)
+                    {
+#ifdef USE_BLOCK_READ_2
+                        float2 _input = as_float2(intel_sub_group_block_read2((const __global uint*)input + input_idx));
+                        float8 filter_transp = TRANSPOSE_BLOCK_8(filter[filter_idx]);
+                        _data[0] = fma(_input.s0, filter_transp, _data[0]);
+                        _data[1] = fma(_input.s1, filter_transp, _data[1]);
+                        input_idx += INPUT0_FEATURE_PITCH;
+#else
+                        float8 filter_transp = TRANSPOSE_BLOCK_8(filter[filter_idx]);
+                        for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+                        {
+                            _data[s] = fma(input[input_idx], filter_transp, _data[s]);
+                            input_idx += LOCAL_WORK_GROUP_SIZE;
+                        }
+                        input_idx += INPUT0_FEATURE_PITCH - BATCHES_PER_WORK_ITEM * LOCAL_WORK_GROUP_SIZE;
+#endif
+                        filter_idx += FILTER_IFM_PITCH;
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+    float bias_val = bias[ofm_offset + sub_group_id];
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+    {
+        ADD_BIAS_8(_data[s], bias_val);
+    }
+#endif
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+    {
+        _data[s] = ACTIVATION(_data[s], NL_M, NL_N);
+    }
+
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+    {
+        uint _out_id = OUTPUT_VIEW_OFFSET + out_id + s * LOCAL_WORK_GROUP_SIZE;
+        output[_out_id] = _data[s].s0; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s1; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s2; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s3; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s4; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s5; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s6; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s7; _out_id += OUTPUT_FEATURE_PITCH;
+    }
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_image_tutorial",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(fully_connected_gpu_image_tutorial)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    read_only image2d_t weights
+#if BIAS_TERM
+    , const __global BIAS_TYPE* biases
+#endif
+    )
+{
+    const uint ofm = get_global_id(0);
+    const uint b = get_global_id(1);
+    DECLARE_SAMPLER;
+
+    ACCUMULATOR_TYPE dotProd = 0;
+
+    for (uint iyx = 0; iyx < (INPUT0_FEATURE_NUM * INPUT0_SIZE_Y * INPUT0_SIZE_X + 3) / 4; ++iyx)
+    {
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 4) weights_val = IMAGE_READ(weights, (int2)(iyx, ofm));
+        const uint input0_idx = INPUT0_OFFSET + b * INPUT0_BATCH_PITCH + iyx * 4;
+
+        dotProd += (ACCUMULATOR_TYPE)(input[input0_idx] * weights_val.x);
+        if(iyx*4 + 1 >= INPUT0_BATCH_PITCH) break;
+        dotProd += (ACCUMULATOR_TYPE)(input[input0_idx + 1] * weights_val.y);
+        if(iyx*4 + 2 >= INPUT0_BATCH_PITCH) break;
+        dotProd += (ACCUMULATOR_TYPE)(input[input0_idx + 2] * weights_val.z);
+        if(iyx*4 + 3 >= INPUT0_BATCH_PITCH) break;
+        dotProd += (ACCUMULATOR_TYPE)(input[input0_idx + 3] * weights_val.w);
+    }
+
+    const uint output_idx = GET_DATA_INDEX(OUTPUT, b, ofm, 0, 0);
+
+#if BIAS_TERM
+    dotProd += (ACCUMULATOR_TYPE)biases[ofm];
+#endif
+
+    output[output_idx] = ACTIVATION((UNIT_TYPE)dotProd, NL_M, NL_N);
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 4) weights_val = IMAGE_READ(weights, (int2)(1, 0));
+}
+
+)__krnl"},
+
+{"lstm_elt_gpu_bfyx_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define ACTIVATION_LOGISTIC(input)                      (UNIT_VAL_ONE/(UNIT_VAL_ONE + exp(-input)))
+#define ACTIVATION_HYPERBOLIC_TAN(input)                (tanh(input))
+
+// tempGEMM = [ batch, direction, 1, 4 * hidden_size ]
+// cell     = [ batch, direction, 1,     hidden_size ] optional
+// output   = [ batch, direction, 2,     hidden_size ] output
+KERNEL(lstm_elt)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output
+#if CELL_TERM
+    ,const __global OUTPUT_TYPE* cell
+#endif
+    )
+{
+    const uint x = get_global_id(0);
+    const uint b = get_global_id(1);
+
+    ACCUMULATOR_TYPE it = input[GET_DATA_INDEX(INPUT0, b, 0, 0, x + GEMM_OFFSET_I)];
+    ACCUMULATOR_TYPE ot = input[GET_DATA_INDEX(INPUT0, b, 0, 0, x + GEMM_OFFSET_O)]; // pass constant offsets here
+    ACCUMULATOR_TYPE zt = input[GET_DATA_INDEX(INPUT0, b, 0, 0, x + GEMM_OFFSET_Z)];
+
+    ACCUMULATOR_TYPE val = ACTIVATION_LOGISTIC(CLIP(it)) * ACTIVATION_HYPERBOLIC_TAN(CLIP(zt));
+
+#if CELL_TERM || INPUT_FORGET
+    ACCUMULATOR_TYPE ft = input[GET_DATA_INDEX(INPUT0, b, 0, 0, x + GEMM_OFFSET_F)];
+#endif
+
+#if INPUT_FORGET
+    val *= ((ACCUMULATOR_TYPE)1 - ft);
+#endif
+
+#if CELL_TERM
+    val += cell[GET_DATA_INDEX(CELL, b, 0, 0, x)] * ACTIVATION_LOGISTIC(CLIP(ft));
+#endif
+
+    output[GET_DATA_INDEX(OUTPUT, b, 0, 0, x)] = ACTIVATION_HYPERBOLIC_TAN(val) * ACTIVATION_LOGISTIC(ot); // hidden
+    output[GET_DATA_INDEX(OUTPUT, b, 1, 0, x)] = (OUTPUT_TYPE)val; // cell
+}
+
+)__krnl"},
+
+{"convolution_gpu_yxfb_yxio_b8_fp32",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((reqd_work_group_size(LOCAL_WORK_GROUP_SIZE, 1, 1)))
+KERNEL(convolution_gpu_yxfb_yxio_b8)(
+    const __global float* input,
+    __global float* output,
+    const __global float* filter,
+#if BIAS_TERM
+    const __global float* bias,
+#endif
+    uint split_idx)
+{
+    const uint batch_num = INPUT0_BATCH_NUM;
+
+    const uint linear_id_xy = get_global_id(1) + get_global_size(1) * get_global_id(2);
+    // we're computing 8 OUTPUT_FEATURE_MAP so we must divide by 8, but we got 8 batches, so no division is needed.
+    uint global_id = ((uint)get_global_id(0) / batch_num) * batch_num + (linear_id_xy * FILTER_ARRAY_NUM + split_idx) * (FILTER_OFM_NUM / OFM_PER_WORK_ITEM) * batch_num;
+
+    const uint out_batch_id = get_local_id(0);
+    const uint out_x = get_global_id(1);
+    const uint out_y = get_global_id(2);
+
+    const uint out_id = (global_id / batch_num) * OFM_PER_WORK_ITEM * batch_num + out_batch_id;
+
+    const uint ofm_offset = (global_id * OFM_PER_WORK_ITEM) / batch_num % FILTER_OFM_NUM;
+
+    const uint sub_group_id = get_local_id(0);
+
+    float8 _data0 = 0.f;
+#if OFM_PER_WORK_ITEM == 16
+    float8 _data1 = 0.f;
+#endif
+
+    const int x = (int)out_x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int y = (int)out_y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i * DILATION_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+)__krnl"
+R"__krnl(            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j * DILATION_SIZE_X;
+                const bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                if(!zero)
+                {
+                    uint input_idx = input_offset_x*INPUT0_X_PITCH + input_offset_y*INPUT0_Y_PITCH;
+                    input_idx += INPUT0_OFFSET + split_idx * FILTER_IFM_NUM * INPUT0_FEATURE_PITCH;
+                    input_idx += out_batch_id;
+
+                    //sub_group_id used as offset to make each workitem load different filter, and then shuffle it
+                    uint filter_idx = ofm_offset + sub_group_id + i*FILTER_Y_PITCH + j*FILTER_X_PITCH;
+#if OFM_PER_WORK_ITEM == 16
+                    uint filter_idx2 = filter_idx + 8;
+#endif
+                    for (uint h = 0; h < FILTER_IFM_NUM / 8; h++)
+                    {
+                        float8 _input = as_float8(intel_sub_group_block_read8((const __global uint*)input + input_idx));
+
+                        DOT_PRODUCT_8(_data0, _input.s0, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s0, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s1, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s1, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s2, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s2, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s3, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s3, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s4, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s4, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s5, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s5, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s6, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s6, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        DOT_PRODUCT_8(_data0, _input.s7, filter[filter_idx]) filter_idx += FILTER_OFM_NUM;
+#if OFM_PER_WORK_ITEM == 16
+                        DOT_PRODUCT_8(_data1, _input.s7, filter[filter_idx2]) filter_idx2 += FILTER_OFM_NUM;
+#endif
+                        input_idx += 8 * INPUT0_FEATURE_PITCH;
+                    }
+                    for (uint h = FILTER_IFM_NUM - (FILTER_IFM_NUM % 8); h < FILTER_IFM_NUM; h++)
+                    {
+                        float8 _filter = TRANSPOSE_BLOCK_8(filter[filter_idx]); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(input[input_idx], _filter, _data0);
+#if OFM_PER_WORK_ITEM == 16
+                        float8 _filter2 = TRANSPOSE_BLOCK_8(filter[filter_idx2]); filter_idx2 += FILTER_OFM_NUM;
+                        _data1 = mad(input[input_idx], _filter2, _data1);
+#endif
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+    ADD_BIAS_8(_data0, bias[ofm_offset + sub_group_id]);
+#if OFM_PER_WORK_ITEM == 16
+    ADD_BIAS_8(_data1, bias[ofm_offset + sub_group_id + 8]);
+#endif
+#endif // #if BIAS_TERM
+    _data0 = ACTIVATION(_data0, NL_M, NL_N);
+#if OFM_PER_WORK_ITEM == 16
+    _data1 = ACTIVATION(_data1, NL_M, NL_N);
+#endif
+
+    const uint _out_id = OUTPUT_OFFSET + out_id;
+    intel_sub_group_block_write8((__global uint*)output + _out_id, as_uint8(_data0));
+#if OFM_PER_WORK_ITEM == 16
+    intel_sub_group_block_write8((__global uint*)output + _out_id + 8 * INPUT0_FEATURE_PITCH, as_uint8(_data1));
+#endif
+}
+
+)__krnl"},
+
+{"reorder_to_winograd_2x3_s1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// --------------------------------------------------------------------------------------------------------------------------------
+// Convert the signal using the forward F(2,3) Winograd transform.
+// --------------------------------------------------------------------------------------------------------------------------------
+KERNEL(reorder_to_winograd_2x3_s1)(global const UNIT_TYPE* input, global UNIT_TYPE* output_winograd)
+{
+    const uint input_tile_width = 4; //how much data is needed to produce one winograd tile (in x-dim)
+    const uint input_tile_height = 1; //how much data is needed to produce on winograd tile (in y-dim)
+    const uint input_tile_stride_x = 2; //how much do we need to proceed in input's x-dim to read data for new winograd tile
+    const uint input_tile_stride_y = 1; //how much do we need to proceed in input's y-dim to read data for new winograd tile
+    const uint winograd_tile_width = 4; //dimensions of resulting tile
+    const uint winograd_tile_height = 1;
+
+    const int batch_idx = get_global_id(0) / INPUT0_FEATURE_NUM;
+    const int feature_idx = get_global_id(0) % INPUT0_FEATURE_NUM; //which feature do we process
+    const int tile_idx_x = get_global_id(1); //which tile do we process (in x-dim)
+    const int tile_idx_y = get_global_id(2); //which tile do we process (in y-dim)
+
+    int in_idx = (INPUT0_PAD_BEFORE_BATCH_NUM + batch_idx) * INPUT0_BATCH_PITCH +
+                  (INPUT0_PAD_BEFORE_FEATURE_NUM + feature_idx) * INPUT0_FEATURE_PITCH +
+                  (INPUT0_PAD_BEFORE_SIZE_Y + (tile_idx_y * input_tile_stride_y) + INPUT0_OFFSET_SIZE_Y) * INPUT0_Y_PITCH +
+                  (INPUT0_PAD_BEFORE_SIZE_X + (tile_idx_x * input_tile_stride_x) + INPUT0_OFFSET_SIZE_X) * INPUT0_X_PITCH;
+
+    // storage for input tile
+    UNIT_TYPE input_tile[input_tile_width * input_tile_height];
+
+    // input tile is 4x1 so read 4 consecutive values in x-dim from input
+    input_tile[0] = input[in_idx]; in_idx += INPUT0_X_PITCH;
+    input_tile[1] = input[in_idx]; in_idx += INPUT0_X_PITCH;
+    input_tile[2] = input[in_idx]; in_idx += INPUT0_X_PITCH;
+    input_tile[3] = input[in_idx];
+
+    // output is in byxf -- no paddings allowed in winograd domain
+    int out_idx = batch_idx * OUTPUT_BATCH_PITCH +
+                   feature_idx * OUTPUT_FEATURE_PITCH +
+                   (tile_idx_y * winograd_tile_height) * OUTPUT_Y_PITCH +
+                   (tile_idx_x * winograd_tile_width) * OUTPUT_X_PITCH;
+
+    //produce single 4x1 winograd tile ==> write 4 consecutive values in x-dim to output
+    output_winograd[out_idx] = input_tile[0] - input_tile[2]; out_idx += OUTPUT_X_PITCH;
+    output_winograd[out_idx] = input_tile[1] + input_tile[2]; out_idx += OUTPUT_X_PITCH;
+    output_winograd[out_idx] = input_tile[2] - input_tile[1]; out_idx += OUTPUT_X_PITCH;
+    output_winograd[out_idx] = input_tile[1] - input_tile[3];
+};
+
+)__krnl"},
+
+{"concatenation_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL (concatenation_gpu_ref)(__global UNIT_TYPE* input, __global UNIT_TYPE* output, uint output_offset_in_concat_axis)
+{
+    const uint d1 = get_global_id(0);
+    const uint d2 = get_global_id(1);
+#ifdef CHECK_FEATURES
+    if(d2 >= INPUT0_FEATURE_NUM)
+        return;
+#endif
+    const uint d3 = get_global_id(2);
+
+    uint input_offset  = INPUT0_OFFSET + d1*INPUT0_PITCHES[INPUT_DIM_1] + d2*INPUT0_PITCHES[INPUT_DIM_2] + d3*INPUT0_PITCHES[INPUT_DIM_3];
+    uint output_offset = OUTPUT_OFFSET + d1*OUTPUT_PITCHES[1] + d2*OUTPUT_PITCHES[2] + d3*OUTPUT_PITCHES[3] + output_offset_in_concat_axis*OUTPUT_PITCHES[CONCAT_AXIS_INDEX];
+
+    for (size_t idx = 0; idx < INPUT0_SIZES[INPUT_DIM_0]; ++idx)
+    {
+        output[output_offset] = ACTIVATION(input[input_offset], NL_M, NL_N);
+        input_offset  += INPUT0_PITCHES[INPUT_DIM_0];
+        output_offset += OUTPUT_PITCHES[0];
+    }
+}
+
+)__krnl"},
+
+{"border_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+
+KERNEL(border_gpu_ref)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output)
+{
+    // [CONSTEXPR]
+    // Border sizes (left-top set and right-bottom set):
+    const uint blt_sx = LT_SIZES_SIZE_X;
+    const uint blt_sy = LT_SIZES_SIZE_Y;
+    const uint blt_sf = LT_SIZES_FEATURE_NUM;
+    const uint blt_sb = LT_SIZES_BATCH_NUM;
+
+    const uint brb_sx = RB_SIZES_SIZE_X;
+    const uint brb_sy = RB_SIZES_SIZE_Y;
+    const uint brb_sf = RB_SIZES_FEATURE_NUM;
+    const uint brb_sb = RB_SIZES_BATCH_NUM;
+
+    // Input sizes:
+    const uint in_sx = INPUT0_SIZE_X;
+    const uint in_sy = INPUT0_SIZE_Y;
+    const uint in_sf = INPUT0_FEATURE_NUM;
+    const uint in_sb = INPUT0_BATCH_NUM;
+
+    // Input limits (exclusive; tested on output position):
+    const uint in_lx = in_sx + blt_sx;
+    const uint in_ly = in_sy + blt_sy;
+    const uint in_lf = in_sf + blt_sf;
+    const uint in_lb = in_sb + blt_sb;
+
+
+    const uint out_x  = (uint) get_global_id(0);
+    const uint out_y  = (uint) get_global_id(1);
+    const uint out_fb = (uint) get_global_id(2);
+
+    const uint out_f  = out_fb % OUTPUT_FEATURE_NUM;
+    const uint out_b  = out_fb / OUTPUT_FEATURE_NUM;
+
+#ifdef BORDER_TYPE_CONSTANT
+    UNIT_TYPE in_val = TO_UNIT_TYPE(BORDER_VALUE);
+    if (out_x >= blt_sx & out_x < in_lx &
+        out_y >= blt_sy & out_y < in_ly &
+        out_f >= blt_sf & out_f < in_lf &
+)__krnl"
+R"__krnl(        out_b >= blt_sb & out_b < in_lb)
+    {
+        const uint in_x = out_x - blt_sx;
+        const uint in_y = out_y - blt_sy;
+        const uint in_f = out_f - blt_sf;
+        const uint in_b = out_b - blt_sb;
+
+        const uint in_pos = GET_DATA_INDEX(INPUT0, in_b, in_f, in_y, in_x);
+        in_val = input[in_pos];
+    }
+#elif defined BORDER_TYPE_EDGE
+    const uint in_x = (out_x >= blt_sx & out_x < in_lx) ? out_x - blt_sx : (out_x < blt_sx ? 0 : in_sx - 1);
+    const uint in_y = (out_y >= blt_sy & out_y < in_ly) ? out_y - blt_sy : (out_y < blt_sy ? 0 : in_sy - 1);
+    const uint in_f = (out_f >= blt_sf & out_f < in_lf) ? out_f - blt_sf : (out_f < blt_sf ? 0 : in_sf - 1);
+    const uint in_b = (out_b >= blt_sb & out_b < in_lb) ? out_b - blt_sb : (out_b < blt_sb ? 0 : in_sb - 1);
+
+    const uint in_pos = GET_DATA_INDEX(INPUT0, in_b, in_f, in_y, in_x);
+    UNIT_TYPE in_val = input[in_pos];
+#elif defined BORDER_TYPE_MIRROR
+    const uint in_x = (out_x >= blt_sx & out_x < in_lx) ? out_x - blt_sx : (out_x < blt_sx ? blt_sx - 1 - out_x : in_sx + in_lx - 1 - out_x);
+    const uint in_y = (out_y >= blt_sy & out_y < in_ly) ? out_y - blt_sy : (out_y < blt_sy ? blt_sy - 1 - out_y : in_sy + in_ly - 1 - out_y);
+    const uint in_f = (out_f >= blt_sf & out_f < in_lf) ? out_f - blt_sf : (out_f < blt_sf ? blt_sf - 1 - out_f : in_sf + in_lf - 1 - out_f);
+    const uint in_b = (out_b >= blt_sb & out_b < in_lb) ? out_b - blt_sb : (out_b < blt_sb ? blt_sb - 1 - out_b : in_sb + in_lb - 1 - out_b);
+
+    const uint in_pos = GET_DATA_INDEX(INPUT0, in_b, in_f, in_y, in_x);
+    UNIT_TYPE in_val = input[in_pos];
+#elif defined BORDER_TYPE_MIRROR_101
+    const uint in_x = (out_x >= blt_sx & out_x < in_lx) ? out_x - blt_sx : (out_x < blt_sx ? blt_sx - out_x : in_sx + in_lx - 2 - out_x);
+    const uint in_y = (out_y >= blt_sy & out_y < in_ly) ? out_y - blt_sy : (out_y < blt_sy ? blt_sy - out_y : in_sy + in_ly - 2 - out_y);
+    const uint in_f = (out_f >= blt_sf & out_f < in_lf) ? out_f - blt_sf : (out_f < blt_sf ? blt_sf - out_f : in_sf + in_lf - 2 - out_f);
+    const uint in_b = (out_b >= blt_sb & out_b < in_lb) ? out_b - blt_sb : (out_b < blt_sb ? blt_sb - out_b : in_sb + in_lb - 2 - out_b);
+
+    const uint in_pos = GET_DATA_INDEX(INPUT0, in_b, in_f, in_y, in_x);
+    UNIT_TYPE in_val = input[in_pos];
+#else
+    #error Unsupported border type.
+#endif
+
+    const uint out_pos = GET_DATA_INDEX(OUTPUT, out_b, out_f, out_y, out_x);
+    output[out_pos] = in_val;
+}
+
+)__krnl"},
+
+{"tile_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+#if DENSE
+__attribute__((intel_reqd_sub_group_size(16)))
+__attribute__((reqd_work_group_size(16, 1, 1)))
+#endif
+KERNEL (tile_ref)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+#if DENSE
+
+    const uint id = get_global_id(0);
+    const uint group_id = id / 16;
+    const uint lid = get_local_id(0);
+    const uint idx = min((uint)(id), (uint)(OUTER_SIZE - 1));
+    UNIT_TYPE val = input[idx];
+
+    for (int t = 0; t < TILES; t++)
+    {
+        UNIT_TYPE save_val = intel_sub_group_shuffle(val, (t*16 + lid)/TILES);
+        int offset = group_id*16*TILES + t*16 + lid;
+        if (offset < OUTPUT_ELEMENTS)
+            output[offset] = save_val;
+    }
+#else
+    const uint outer_idx = get_global_id(0);
+    const uint inner_idx = get_global_id(1);
+    if (inner_idx >= AXIS_PITCH) return;
+
+    for (int t = 0; t < TILES; t++)
+    {
+        output[outer_idx*TILES*AXIS_PITCH + t*AXIS_PITCH + inner_idx] = input[outer_idx*AXIS_PITCH + inner_idx];
+    }
+#endif
+}
+
+)__krnl"},
+
+{"softmax_gpu_items_class_optimized",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#define DATA_PER_WORKITEM ( (INPUT0_CLASS_NUM + (WORKITEMS_PER_CLASSES - 1) ) / WORKITEMS_PER_CLASSES)
+#define FULL_ITERATIONS_NUM (INPUT0_CLASS_NUM / WORKITEMS_PER_CLASSES)
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(softmax_items_class_optimized)(__global INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+    const uint other0 = get_group_id(0);
+    const uint other1 = get_group_id(1);
+    const uint batch  = get_group_id(2);
+    const uint simd_lane = get_sub_group_local_id();
+
+    const uint in_depth_offset  = batch*INPUT0_BATCH_PITCH + other1*INPUT0_OTHER1_PITCH + other0*INPUT0_OTHER0_PITCH + INPUT0_OFFSET;
+    const uint out_depth_offset = batch*OUTPUT_BATCH_PITCH + other1*OUTPUT_OTHER1_PITCH + other0*OUTPUT_OTHER0_PITCH + OUTPUT_OFFSET;
+
+    UNIT_TYPE max_value = UNIT_VAL_MIN;
+    UNIT_TYPE data[DATA_PER_WORKITEM];
+
+    // PART 1. Calculate MAX value
+    uint input_idx = in_depth_offset + simd_lane * INPUT0_CLASS_PITCH;
+    for (uint cls = 0; cls < FULL_ITERATIONS_NUM; cls++)
+    {
+        UNIT_TYPE in = input[input_idx];
+        max_value = max(max_value, in);
+        data[cls] = in;
+        input_idx += WORKITEMS_PER_CLASSES*INPUT0_CLASS_PITCH;
+    }
+    if(simd_lane < LEFTOVERS)
+    {
+        UNIT_TYPE in = input[input_idx];
+        max_value = max(max_value, in);
+        data[DATA_PER_WORKITEM-1] = in;
+    }
+    max_value = sub_group_reduce_max(max_value);
+
+    // PART 2. Calculate DENOMINATOR
+    // TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+    ACCUMULATOR_TYPE denominator = 0.0;
+    for (uint cls = 0; cls < FULL_ITERATIONS_NUM; cls++)
+    {
+        data[cls] = native_exp(data[cls] - max_value);
+        denominator += data[cls];
+    }
+    if(simd_lane < LEFTOVERS)
+    {
+        data[DATA_PER_WORKITEM-1] = native_exp(data[DATA_PER_WORKITEM-1] - max_value);
+        denominator += data[DATA_PER_WORKITEM-1];
+    }
+
+    denominator = sub_group_reduce_add(denominator);
+
+    // PART 3. Write out results
+    uint output_idx = out_depth_offset + simd_lane * OUTPUT_CLASS_PITCH;
+    for (uint cls = 0; cls < FULL_ITERATIONS_NUM; cls++)
+    {
+        const UNIT_TYPE res = data[cls] / (UNIT_TYPE)denominator;
+        output[output_idx] = ACTIVATION(res, NL_M, NL_N);
+        output_idx += WORKITEMS_PER_CLASSES * OUTPUT_CLASS_PITCH;
+    }
+    if(simd_lane < LEFTOVERS)
+    {
+        const UNIT_TYPE res = data[DATA_PER_WORKITEM-1] / (UNIT_TYPE)denominator;
+        output[output_idx] = ACTIVATION(res, NL_M, NL_N);
+    }
+}
+
+#undef FULL_ITERATIONS_NUM
+#undef DATA_PER_WORKITEM
+
+)__krnl"},
+
+{"fully_connected_gpu_fb_io_b8_f8",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((reqd_work_group_size(8, 1, 1)))
+KERNEL (fully_connected_gpu_xb_xb_b8_x8)(
+    const __global float* input,
+    __global float* output,
+    const __global float* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint global_id = get_global_id(0);
+    const int x = get_global_id(0);
+    const uint batch_id = x % INPUT0_BATCH_NUM;
+
+    uint neuronIdx = (x / INPUT0_BATCH_NUM) * NEURONS_PER_WORK_ITEM;
+
+    const uint sub_group_id = get_local_id(0);
+    const uint batch_num = INPUT0_BATCH_NUM;
+
+    const int out_id = (global_id / batch_num) * NEURONS_PER_WORK_ITEM * batch_num + batch_id;
+
+    const int ofm_offset = (global_id * NEURONS_PER_WORK_ITEM) / batch_num;
+
+    float8 _data0 = 0.f;
+#if NEURONS_PER_WORK_ITEM > 8
+    float8 _data1 = 0.f;
+#endif
+
+    uint weight_offset = sub_group_id + neuronIdx;
+
+    for(uint h = 0; h < INPUT0_ELEMENTS_COUNT; h++)
+    {
+        DOT_PRODUCT_8(_data0, input[h * batch_num + batch_id], weight[weight_offset])
+#if NEURONS_PER_WORK_ITEM > 8
+        DOT_PRODUCT_8(_data1, input[h * batch_num + batch_id], weight[weight_offset + 8])
+#endif
+        weight_offset+= FILTER_OFM_NUM;
+    }
+
+#if BIAS_TERM
+)__krnl"
+R"__krnl(    ADD_BIAS_8(_data0, bias[neuronIdx + sub_group_id]);
+#if NEURONS_PER_WORK_ITEM > 8
+    ADD_BIAS_8(_data1, bias[neuronIdx + sub_group_id + 8]);
+#endif
+#endif
+    _data0 = ACTIVATION(_data0, NL_M, NL_N);
+#if NEURONS_PER_WORK_ITEM > 8
+    _data1 = ACTIVATION(_data1, NL_M, NL_N);
+#endif
+
+    intel_sub_group_block_write8((__global uint*)output + out_id, as_uint8(_data0));
+#if NEURONS_PER_WORK_ITEM > 8
+    intel_sub_group_block_write8((__global uint*)output + out_id + 8 * batch_num, as_uint8(_data1));
+#endif
+}
+
+)__krnl"},
+
+{"max_unpooling_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(pooling_gpu)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output, const __global float* arg_max)
+{
+#if OUTPUT_LAYOUT_BFYX  || OUTPUT_LAYOUT_BYXF
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf % INPUT0_FEATURE_NUM;
+    const uint b    = bf / INPUT0_FEATURE_NUM;
+
+    if (x >= INPUT0_SIZE_X)
+    {
+        return;
+    }
+#elif OUTPUT_LAYOUT_YXFB
+    const uint x    = (uint)get_global_id(1);
+    const uint y    = (uint)get_global_id(2);
+    const uint bf   = (uint)get_global_id(0);
+    const uint f    = bf / INPUT0_BATCH_NUM;
+    const uint b    = bf % INPUT0_BATCH_NUM;
+#endif
+
+    const uint input_id = GET_DATA_INDEX(INPUT0, b, f, y, x);
+    const uint arg_max_id = GET_DATA_INDEX(INPUT1, b, f, y, x);
+    const uint pool_idx = convert_uint(arg_max[arg_max_id]);
+
+#if OUTPUT_PADDED
+    const uint x_output = pool_idx % OUTPUT_SIZE_X;
+    const uint y_output = (pool_idx / OUTPUT_SIZE_X) % OUTPUT_SIZE_Y;
+    const uint f_output = (pool_idx / OUTPUT_SIZE_X / OUTPUT_SIZE_Y) % OUTPUT_FEATURE_NUM;
+    const uint b_output = pool_idx / OUTPUT_SIZE_X / OUTPUT_SIZE_Y / OUTPUT_FEATURE_NUM;
+
+    const uint output_pos = GET_DATA_INDEX(OUTPUT, b_output, f_output, y_output, x_output);
+    output[output_pos] += ACTIVATION(input[input_id], NL_M ,NL_N);
+#else
+    output[pool_idx] += ACTIVATION(input[input_id], NL_M ,NL_N);
+#endif
+}
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_1x1_hgemm_buf_16x1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define GET_COL_8( _block, _col )                               \
+        (float8)( intel_sub_group_shuffle( _block, _col ));
+
+#define GET_COL_2( _block, _col )                               \
+        (float2)( intel_sub_group_shuffle( _block, _col ));
+
+#define GET_COL_4( _block, _col )                               \
+        (float4)( intel_sub_group_shuffle( _block, _col ));
+
+#define DOT4i( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s0, sub_group_broadcast( _B.s0, i), _result);	\
+	_result = mad(_A.s1, sub_group_broadcast( _B.s1, i), _result);	\
+	_result = mad(_A.s2, sub_group_broadcast( _B.s2, i), _result);	\
+	_result = mad(_A.s3, sub_group_broadcast( _B.s3, i), _result);	\
+    }
+
+#define DOT4i_LO( _result, _A, _B, i)					\
+    {									\
+	_result = mad(_A.s0, sub_group_broadcast( _B.s0, i), _result);	\
+	_result = mad(_A.s1, sub_group_broadcast( _B.s1, i), _result);	\
+	_result = mad(_A.s2, sub_group_broadcast( _B.s2, i), _result);	\
+	_result = mad(_A.s3, sub_group_broadcast( _B.s3, i), _result);	\
+    }
+
+#define DOT4i_HI( _result, _A, _B, i)					\
+    {									\
+)__krnl"
+R"__krnl(	_result = mad(_A.s4, sub_group_broadcast( _B.s0, i), _result);	\
+	_result = mad(_A.s5, sub_group_broadcast( _B.s1, i), _result);	\
+	_result = mad(_A.s6, sub_group_broadcast( _B.s2, i), _result);	\
+	_result = mad(_A.s7, sub_group_broadcast( _B.s3, i), _result);	\
+    }
+
+#define DOT8i( _result, _A, _B, i)					\
+    {									\
+	_result = fma(_A.s0, sub_group_broadcast( _B.s0, i), _result);	\
+	_result = fma(_A.s1, sub_group_broadcast( _B.s1, i), _result);	\
+	_result = fma(_A.s2, sub_group_broadcast( _B.s2, i), _result);	\
+	_result = fma(_A.s3, sub_group_broadcast( _B.s3, i), _result);	\
+	_result = fma(_A.s4, sub_group_broadcast( _B.s4, i), _result);	\
+	_result = fma(_A.s5, sub_group_broadcast( _B.s5, i), _result);	\
+	_result = fma(_A.s6, sub_group_broadcast( _B.s6, i), _result);	\
+	_result = fma(_A.s7, sub_group_broadcast( _B.s7, i), _result);	\
+    }
+
+#define MULTIPLY_BLOCKS_8x8( _result, _blockA, _blockB )                \
+        {                                                               \
+            const float8    acol0 = GET_COL_8( _blockA, 0 );            \
+            const float8    acol1 = GET_COL_8( _blockA, 1 );            \
+            const float8    acol2 = GET_COL_8( _blockA, 2 );            \
+            const float8    acol3 = GET_COL_8( _blockA, 3 );            \
+            const float8    acol4 = GET_COL_8( _blockA, 4 );            \
+            const float8    acol5 = GET_COL_8( _blockA, 5 );            \
+            const float8    acol6 = GET_COL_8( _blockA, 6 );            \
+            const float8    acol7 = GET_COL_8( _blockA, 7 );            \
+            _result = mad( (float8)(_blockB.s0), acol0, _result );      \
+            _result = mad( (float8)(_blockB.s1), acol1, _result );      \
+            _result = mad( (float8)(_blockB.s2), acol2, _result );      \
+            _result = mad( (float8)(_blockB.s3), acol3, _result );      \
+            _result = mad( (float8)(_blockB.s4), acol4, _result );      \
+            _result = mad( (float8)(_blockB.s5), acol5, _result );      \
+            _result = mad( (float8)(_blockB.s6), acol6, _result );      \
+            _result = mad( (float8)(_blockB.s7), acol7, _result );      \
+        }
+
+#define MULTIPLY_BLOCKS_4x8( _result, _blockA, _blockB )                \
+        {                                                               \
+            const float4    acol0 = GET_COL_4( _blockA, 0 );            \
+            const float4    acol1 = GET_COL_4( _blockA, 1 );            \
+            const float4    acol2 = GET_COL_4( _blockA, 2 );            \
+            const float4    acol3 = GET_COL_4( _blockA, 3 );            \
+            const float4    acol4 = GET_COL_4( _blockA, 4 );            \
+            const float4    acol5 = GET_COL_4( _blockA, 5 );            \
+            const float4    acol6 = GET_COL_4( _blockA, 6 );            \
+            const float4    acol7 = GET_COL_4( _blockA, 7 );            \
+            _result = mad( (float4)(_blockB.s0), acol0, _result );      \
+            _result = mad( (float4)(_blockB.s1), acol1, _result );      \
+            _result = mad( (float4)(_blockB.s2), acol2, _result );      \
+            _result = mad( (float4)(_blockB.s3), acol3, _result );      \
+            _result = mad( (float4)(_blockB.s4), acol4, _result );      \
+            _result = mad( (float4)(_blockB.s5), acol5, _result );      \
+            _result = mad( (float4)(_blockB.s6), acol6, _result );      \
+            _result = mad( (float4)(_blockB.s7), acol7, _result );      \
+        }
+
+#define MULTIPLY_BLOCKS_2x8( _result, _blockA, _blockB )                \
+        {                                                               \
+            const float2    acol0 = GET_COL_2( _blockA, 0 );            \
+            const float2    acol1 = GET_COL_2( _blockA, 1 );            \
+            const float2    acol2 = GET_COL_2( _blockA, 2 );            \
+            const float2    acol3 = GET_COL_2( _blockA, 3 );            \
+            const float2    acol4 = GET_COL_2( _blockA, 4 );            \
+            const float2    acol5 = GET_COL_2( _blockA, 5 );            \
+            const float2    acol6 = GET_COL_2( _blockA, 6 );            \
+            const float2    acol7 = GET_COL_2( _blockA, 7 );            \
+            _result = mad( (float2)(_blockB.s0), acol0, _result );      \
+            _result = mad( (float2)(_blockB.s1), acol1, _result );      \
+            _result = mad( (float2)(_blockB.s2), acol2, _result );      \
+            _result = mad( (float2)(_blockB.s3), acol3, _result );      \
+            _result = mad( (float2)(_blockB.s4), acol4, _result );      \
+            _result = mad( (float2)(_blockB.s5), acol5, _result );      \
+            _result = mad( (float2)(_blockB.s6), acol6, _result );      \
+            _result = mad( (float2)(_blockB.s7), acol7, _result );      \
+        }
+
+#define MULTIPLY_BLOCKS_8x8_NO_ACCUMULATE( _result, _blockA, _blockB )  \
+        {                                                               \
+            const float8    acol0 = GET_COL_8( _blockA, 0 );            \
+            const float8    acol1 = GET_COL_8( _blockA, 1 );            \
+            const float8    acol2 = GET_COL_8( _blockA, 2 );            \
+            const float8    acol3 = GET_COL_8( _blockA, 3 );            \
+            const float8    acol4 = GET_COL_8( _blockA, 4 );            \
+            const float8    acol5 = GET_COL_8( _blockA, 5 );            \
+            const float8    acol6 = GET_COL_8( _blockA, 6 );            \
+            const float8    acol7 = GET_COL_8( _blockA, 7 );            \
+            _result = (float8)(_blockB.s0) * acol0;                     \
+            _result = mad( (float8)(_blockB.s1), acol1, _result );      \
+            _result = mad( (float8)(_blockB.s2), acol2, _result );      \
+            _result = mad( (float8)(_blockB.s3), acol3, _result );      \
+            _result = mad( (float8)(_blockB.s4), acol4, _result );      \
+            _result = mad( (float8)(_blockB.s5), acol5, _result );      \
+            _result = mad( (float8)(_blockB.s6), acol6, _result );      \
+            _result = mad( (float8)(_blockB.s7), acol7, _result );      \
+        }
+
+#define MULTIPLY_BLOCKS_16x8_LO( _result, _blockA, _blockB )	\
+    {								\
+	const float8    acol0 = GET_COL_8( _blockA, 0 );	\
+	const float8    acol1 = GET_COL_8( _blockA, 1 );	\
+	const float8    acol2 = GET_COL_8( _blockA, 2 );	\
+	const float8    acol3 = GET_COL_8( _blockA, 3 );	\
+	const float8    acol4 = GET_COL_8( _blockA, 4 );	\
+	const float8    acol5 = GET_COL_8( _blockA, 5 );	\
+	const float8    acol6 = GET_COL_8( _blockA, 6 );	\
+	const float8    acol7 = GET_COL_8( _blockA, 7 );	\
+	_result = mad( (float8)(_blockB.s0), acol0, _result );	\
+	_result = mad( (float8)(_blockB.s1), acol1, _result );	\
+	_result = mad( (float8)(_blockB.s2), acol2, _result );	\
+	_result = mad( (float8)(_blockB.s3), acol3, _result );	\
+	_result = mad( (float8)(_blockB.s4), acol4, _result );	\
+	_result = mad( (float8)(_blockB.s5), acol5, _result );	\
+	_result = mad( (float8)(_blockB.s6), acol6, _result );	\
+	_result = mad( (float8)(_blockB.s7), acol7, _result );	\
+    }
+
+#define MULTIPLY_BLOCKS_16x8_HI( _result, _blockA, _blockB )	\
+    {								\
+	const float8    acol0 = GET_COL_8( _blockA, 8 );	\
+	const float8    acol1 = GET_COL_8( _blockA, 9 );	\
+	const float8    acol2 = GET_COL_8( _blockA, 10 );	\
+	const float8    acol3 = GET_COL_8( _blockA, 11 );	\
+	const float8    acol4 = GET_COL_8( _blockA, 12 );	\
+	const float8    acol5 = GET_COL_8( _blockA, 13 );	\
+	const float8    acol6 = GET_COL_8( _blockA, 14 );	\
+	const float8    acol7 = GET_COL_8( _blockA, 15 );	\
+	_result = mad( (float8)(_blockB.s0), acol0, _result );	\
+	_result = mad( (float8)(_blockB.s1), acol1, _result );	\
+	_result = mad( (float8)(_blockB.s2), acol2, _result );	\
+	_result = mad( (float8)(_blockB.s3), acol3, _result );	\
+	_result = mad( (float8)(_blockB.s4), acol4, _result );	\
+	_result = mad( (float8)(_blockB.s5), acol5, _result );	\
+	_result = mad( (float8)(_blockB.s6), acol6, _result );	\
+	_result = mad( (float8)(_blockB.s7), acol7, _result );	\
+    }
+
+
+#define WRITE_BLOCK_2(ptr_,     block0_, block1_, row_)                 \
+    if (row_ < max_row) {                                               \
+        const float2 vec = (float2)(block0_.s ## row_,                  \
+                                    block1_.s ## row_);                 \
+        intel_sub_group_block_write2((__global uint*)&ptr_[N*row_], as_uint2(vec)); \
+    }
+
+#define WRITE_BLOCK_4(ptr_, block0_, block1_, block2_, block3_, row_)   \
+    if (row_ < max_row) {                                               \
+        const float4 vec = (float4)(block0_.s ## row_,                  \
+                                    block1_.s ## row_,                  \
+                                    block2_.s ## row_,                  \
+                                    block3_.s ## row_);                 \
+        intel_sub_group_block_write4((__global uint*)&ptr_[N*row_], as_uint4(vec)); \
+    }
+
+
+#define MULT(C_, A_, i_)                   \
+    DOT8i(C_,  B0, A_, i_ + 0);            \
+    DOT8i(C_,  B8, A_, i_ + 1);            \
+    DOT8i(C_, B16, A_, i_ + 2);            \
+    DOT8i(C_, B24, A_, i_ + 3);
+
+__attribute__((reqd_work_group_size(16, TY, 1)))
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_gpu_bfyx_1x1_hgemm_buf_16x1)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __read_only image2d_t weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+    uint split_idx)
+{
+
+    const uint local_x = get_local_id(0);
+    const uint local_y = get_local_id(1);
+    const uint group_x = get_group_id(0);
+    const uint group_y = get_group_id(1);
+    const uint batch = get_global_id(2);
+
+#if BIAS_TERM
+    const uint bias_index = group_x * TILE_N + local_x;
+    half8 C0 = biases[bias_index];
+    half8 C8 = biases[bias_index];
+#else
+    //      i [16.1]
+    half8 C0 = 0.0h;
+    half8 C8 = 0.0h;
+#endif
+
+    uint lxd4 = local_x >> 2;
+    uint lxm4 = local_x % 4;
+
+    uint i = TILE_M * group_y + local_y * 16 + lxd4;
+
+    __global const half8 *A_load = (__global const half8*)&input[batch * INPUT0_BATCH_PITCH + i*K + (lxm4<<3)];
+
+    uint j = group_x << 4;
+
+    // YX->KN
+)__krnl"
+R"__krnl(    int2 coordB = (int2)(j * sizeof(short), 0);
+
+    for (uint k8 = 0; k8 < K8; k8 += 4) {
+
+        // 512 MADs
+
+        half8 B0 = as_half8(intel_sub_group_block_read_us8(weights, coordB));
+        coordB.y += 8;
+        half8 B8 = as_half8(intel_sub_group_block_read_us8(weights, coordB));
+        coordB.y += 8;
+
+        half8 B16 = as_half8(intel_sub_group_block_read_us8(weights, coordB));
+        coordB.y += 8;
+        half8 B24 = as_half8(intel_sub_group_block_read_us8(weights, coordB));
+        coordB.y += 8;
+
+        half8 A0 = A_load[K8*0 + k8];
+        half8 A4 = A_load[K8*4 + k8];
+
+        MULT(C0.s0, A0, 0);
+        MULT(C0.s1, A0, 4);
+        MULT(C0.s2, A0, 8);
+        MULT(C0.s3, A0, 12);
+        MULT(C0.s4, A4, 0);
+        MULT(C0.s5, A4, 4);
+        MULT(C0.s6, A4, 8);
+        MULT(C0.s7, A4, 12);
+
+        A0 = A_load[K8* 8 + k8];
+        A4 = A_load[K8*12 + k8];
+
+        MULT(C8.s0, A0, 0);
+        MULT(C8.s1, A0, 4);
+        MULT(C8.s2, A0, 8);
+        MULT(C8.s3, A0, 12);
+        MULT(C8.s4, A4, 0);
+        MULT(C8.s5, A4, 4);
+        MULT(C8.s6, A4, 8);
+        MULT(C8.s7, A4, 12);
+    }
+
+    uint y0 = group_y * TILE_M + (local_y << 4);
+    __global half *C_write = &output[batch * OUTPUT_BATCH_PITCH + group_x * TILE_N + y0 * N + local_x];
+
+    if (group_y < NUM_WHOLE_GROUPS_Y || local_y < NUM_WHOLE_SUBGROUPS_Y) {
+        C_write[0*N] = ACTIVATION(C0.s0, NL_M, NL_N);
+        C_write[1*N] = ACTIVATION(C0.s1, NL_M, NL_N);
+        C_write[2*N] = ACTIVATION(C0.s2, NL_M, NL_N);
+        C_write[3*N] = ACTIVATION(C0.s3, NL_M, NL_N);
+        C_write[4*N] = ACTIVATION(C0.s4, NL_M, NL_N);
+        C_write[5*N] = ACTIVATION(C0.s5, NL_M, NL_N);
+        C_write[6*N] = ACTIVATION(C0.s6, NL_M, NL_N);
+        C_write[7*N] = ACTIVATION(C0.s7, NL_M, NL_N);
+        C_write[8*N] = ACTIVATION(C8.s0, NL_M, NL_N);
+        C_write[9*N] = ACTIVATION(C8.s1, NL_M, NL_N);
+        C_write[10*N] = ACTIVATION(C8.s2, NL_M, NL_N);
+        C_write[11*N] = ACTIVATION(C8.s3, NL_M, NL_N);
+        C_write[12*N] = ACTIVATION(C8.s4, NL_M, NL_N);
+        C_write[13*N] = ACTIVATION(C8.s5, NL_M, NL_N);
+        C_write[14*N] = ACTIVATION(C8.s6, NL_M, NL_N);
+        C_write[15*N] = ACTIVATION(C8.s7, NL_M, NL_N);
+    } else {
+#if 0 < LAST_LOCAL_Y
+        C_write[0*N] = ACTIVATION(C0.s0, NL_M, NL_N);
+#endif
+#if 1 < LAST_LOCAL_Y
+        C_write[1*N] = ACTIVATION(C0.s1, NL_M, NL_N);
+#endif
+#if 2 < LAST_LOCAL_Y
+        C_write[2*N] = ACTIVATION(C0.s2, NL_M, NL_N);
+#endif
+#if 3 < LAST_LOCAL_Y
+        C_write[3*N] = ACTIVATION(C0.s3, NL_M, NL_N);
+#endif
+#if 4 < LAST_LOCAL_Y
+        C_write[4*N] = ACTIVATION(C0.s4, NL_M, NL_N);
+#endif
+#if 5 < LAST_LOCAL_Y
+        C_write[5*N] = ACTIVATION(C0.s5, NL_M, NL_N);
+#endif
+#if 6 < LAST_LOCAL_Y
+        C_write[6*N] = ACTIVATION(C0.s6, NL_M, NL_N);
+#endif
+#if 7 < LAST_LOCAL_Y
+        C_write[7*N] = ACTIVATION(C0.s7, NL_M, NL_N);
+#endif
+#if 8 < LAST_LOCAL_Y
+        C_write[8*N] = ACTIVATION(C8.s0, NL_M, NL_N);
+#endif
+#if 9 < LAST_LOCAL_Y
+        C_write[9*N] = ACTIVATION(C8.s1, NL_M, NL_N);
+#endif
+#if 10 < LAST_LOCAL_Y
+        C_write[10*N] = ACTIVATION(C8.s2, NL_M, NL_N);
+#endif
+#if 11 < LAST_LOCAL_Y
+        C_write[11*N] = ACTIVATION(C8.s3, NL_M, NL_N);
+#endif
+#if 12 < LAST_LOCAL_Y
+        C_write[12*N] = ACTIVATION(C8.s4, NL_M, NL_N);
+#endif
+#if 13 < LAST_LOCAL_Y
+        C_write[13*N] = ACTIVATION(C8.s5, NL_M, NL_N);
+#endif
+#if 14 < LAST_LOCAL_Y
+        C_write[14*N] = ACTIVATION(C8.s6, NL_M, NL_N);
+#endif
+#if 15 < LAST_LOCAL_Y
+        C_write[15*N] = ACTIVATION(C8.s7, NL_M, NL_N);
+#endif
+    }
+}
+
+)__krnl"},
+
+{"pooling_gpu_byxf_padding_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define VECTOR_TYPE MAKE_VECTOR_TYPE(UNIT_TYPE,8)
+#define FEATURE_PER_ITEM 8
+#define FEATURE_BLOCK_NUM (OUTPUT_FEATURE_NUM / 8)
+
+#if   defined MAX_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_MIN
+#elif defined AVG_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_ZERO
+#else
+#error
+#endif
+
+inline VECTOR_TYPE FUNC(apply_pooling)(VECTOR_TYPE tmp, VECTOR_TYPE in)
+{
+#if   defined MAX_POOLING
+    return max(tmp, in);
+#elif defined AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+KERNEL(pooling_gpu_byxf_opt)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    VECTOR_TYPE out;
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf / INPUT0_BATCH_NUM * FEATURE_PER_ITEM;
+    const uint b    = bf % INPUT0_BATCH_NUM;
+
+    VECTOR_TYPE feature_block;
+
+    if ((x >= OUTPUT_SIZE_X) || (y >= OUTPUT_SIZE_Y))
+        return;
+
+    const int offset_x = (int)x*STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int offset_y = (int)y*STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+#ifdef CHECK_BOUNDRY
+    if (offset_x + POOL_SIZE_X < 0 || offset_x >= INPUT0_SIZE_X ||
+        offset_y + POOL_SIZE_Y < 0 || offset_y >= INPUT0_SIZE_Y)
+)__krnl"
+R"__krnl(    {
+        return;
+    }
+#endif
+    int input_idx = b*FEATURE_BLOCK_NUM*INPUT0_SIZE_X*INPUT0_SIZE_Y + FEATURE_BLOCK_NUM*INPUT0_SIZE_X*offset_y + FEATURE_BLOCK_NUM*offset_x + bf / INPUT0_BATCH_NUM;
+
+    out = UNIT_INIT_VAL;
+
+    __attribute__((opencl_unroll_hint))
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        int input_offset_y = offset_y + j;
+        bool zero = input_offset_y < 0 || input_offset_y >= INPUT0_SIZE_Y;
+        if(!zero)
+        {
+            __attribute__((opencl_unroll_hint))
+            for(uint i = 0; i < POOL_SIZE_X; i++)
+            {
+                int input_offset_x = offset_x + i;
+                zero = input_offset_x < 0 || input_offset_x >= INPUT0_SIZE_X;
+                if (!zero)
+                {
+                    feature_block = vload8(input_idx+FEATURE_BLOCK_NUM*i, input);
+                    out = FUNC_CALL(apply_pooling)(out, feature_block);
+                }
+            }
+        }
+        input_idx += FEATURE_BLOCK_NUM*INPUT0_SIZE_X;
+    }
+
+    uint output_pos = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+    __attribute__((opencl_unroll_hint))
+    for(uint i = 0; i < FEATURE_PER_ITEM; i++)
+    {
+        if(f+i < INPUT0_FEATURE_NUM)
+        {
+#if defined MAX_POOLING
+            output[output_pos+i] = ACTIVATION(out[i], NL_M ,NL_N);
+#elif defined AVG_POOLING
+            output[output_pos+i] = ACTIVATION(out[i]/(UNIT_TYPE)(POOL_SIZE_X*POOL_SIZE_Y), NL_M ,NL_N);
+#endif
+        }
+    }
+}
+
+#undef FEATURE_BLOCK_NUM
+#undef FEATURE_PER_ITEM
+#undef UNIT_INIT_VAL
+#undef VECTOR_TYPE
+
+)__krnl"},
+
+{"convolution_tutorial",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifdef ADVANCED_TUTORIAL
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// change this function with your own idea. please note that it's a naive implementation.
+KERNEL(convolution_tutorial)(
+    __global INPUT0_TYPE* input,        // input buffer
+    __global OUTPUT_TYPE* output,       // output buffer
+    __global FILTER_TYPE* weights,      // weights buffer (training output)
+#if BIAS_TERM                           // in case we have bias in convolution params
+    __global BIAS_TYPE* biases,         // bias buffer (training output)
+#endif
+    uint split_idx)                     // which split index to process
+{
+#if defined OUTPUT_LAYOUT_YXFB                  // in Case of YXFB we need a different processing order than BFYX (from performance aspect)
+    const uint x = get_global_id(1);
+    const uint y = get_global_id(2);
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(0);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(0) % OUTPUT_FEATURE_NUM;
+    const uint b = get_global_id(0) / OUTPUT_FEATURE_NUM;
+#endif
+#else
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(2);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(2) % OUTPUT_FEATURE_NUM;
+    const uint b = get_global_id(2) / OUTPUT_FEATURE_NUM;
+#endif
+#endif
+
+    UNIT_TYPE dotProd = UNIT_VAL_ZERO;                                          // UNIT_TYPE - half/float/etc
+
+#if BIAS_TERM
+    #if   BIAS_PER_OUTPUT
+        const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);               // helper macro to cacluate indices
+    #elif BIAS_PER_OFM
+        const uint bias_index = f;
+    #endif
+)__krnl"
+R"__krnl(    dotProd = biases[bias_index];
+#endif
+
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    // in case of depth separable optimization we have to dynamically calculate the split index
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (f / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint filter_offset = f*FILTER_OFM_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    for (uint k = 0; k < FILTER_IFM_NUM; ++k)
+    {
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+            const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+            if(!zero_y)
+            {
+                for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+                {
+                    const int input_offset_x = input_x + i * DILATION_SIZE_X;
+                    const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                    if(!zero_x)
+                    {
+                        uint input_idx = input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH + k*INPUT0_FEATURE_PITCH;
+                        uint filter_idx = filter_offset + k*FILTER_IFM_PITCH + j*FILTER_Y_PITCH + i*FILTER_X_PITCH;
+                        dotProd += input[input_idx]*weights[filter_idx];    // finally the convolution calcualtion.
+                    }
+                }
+            }
+        }
+    }
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;    // calculating output split offset
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y, x) + out_split_offset;           // helper macro to calculate output index
+    output[dst_index] = ACTIVATION(dotProd, NL_M, NL_N);                                    // run activation functions (RelU in most cases) and set output
+}
+
+#else
+
+//#include "put here your include files"
+
+__kernel void convolution_tutorial(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* filter,
+    const __global UNIT_TYPE* bias)
+{
+    // fill here your kernel
+}
+
+#endif
+
+)__krnl"},
+
+{"pooling_gpu_fs_bs_yx_bsv4_fsv32",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if MAX_POOLING
+    #define INIT_VAL CHAR_MIN
+#elif AVG_POOLING
+    #define INIT_VAL 0
+#else
+#error
+#endif
+
+
+inline int FUNC(apply_pooling)(int tmp, int in)
+{
+#if MAX_POOLING
+    return max(tmp, in);
+#elif AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+__attribute__((intel_reqd_sub_group_size(8)))
+KERNEL(pooling_gpu_fs_bs_yx_bsv4_fsv32)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output)
+{
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+	// we process 4 features per workitem that's why we need to divide it
+    const uint aligned32_features = ((INPUT0_FEATURE_NUM + 31) / 32) * 32;
+    const uint f    = 4 * (bf % (aligned32_features / 4));
+    const uint b_block = bf / (aligned32_features / 4);
+
+    if (x >= OUTPUT_SIZE_X)
+    {
+        return;
+    }
+
+    const int offset_x = (int)x*STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int offset_y = (int)y*STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    int4 result[4];
+    for(uint b = 0; b < 4; b++)
+)__krnl"
+R"__krnl(    {
+        result[b] = INIT_VAL;
+    }
+
+#ifdef CHECK_BOUNDRY
+    if (offset_x + POOL_SIZE_X < 0 || offset_x >= INPUT0_SIZE_X ||
+        offset_y + POOL_SIZE_Y < 0 || offset_y >= INPUT0_SIZE_Y)
+    {
+        return;
+    }
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    uint num_elementes = 0;
+#endif
+
+    const uint batch_and_feature_offset = GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(INPUT0, b_block * 4, f, 0, 0);
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        int input_offset_y = offset_y + j;
+        bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+        if(!zero_y)
+        {
+            for(uint i = 0; i < POOL_SIZE_X; i++)
+            {
+                int input_offset_x = offset_x + i;
+                bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+                if(!zero)
+                {
+                    const uint input_idx = batch_and_feature_offset + input_offset_y*IN_Y_PITCH + input_offset_x*IN_X_PITCH;
+
+                    int4 int_data = as_int4(intel_sub_group_block_read4((const __global uint*)(input + input_idx)));
+                    for(uint b = 0; b < 4; b++)
+                    {
+                        char4 input_data = as_char4(int_data[b]);
+                        result[b][0] = FUNC_CALL(apply_pooling)(result[b][0], (int)input_data[0]);
+                        result[b][1] = FUNC_CALL(apply_pooling)(result[b][1], (int)input_data[1]);
+                        result[b][2] = FUNC_CALL(apply_pooling)(result[b][2], (int)input_data[2]);
+                        result[b][3] = FUNC_CALL(apply_pooling)(result[b][3], (int)input_data[3]);
+
+                    }
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+                    num_elementes++;
+#endif
+                }
+            }
+        }
+    }
+#ifdef DYNAMIC_WITH_PADDING_KERNEL_DIVIDER
+    const int hend = min(offset_y + POOL_SIZE_Y, INPUT0_SIZE_Y + PADDING_SIZE_Y);
+    const int wend = min(offset_x + POOL_SIZE_X, INPUT0_SIZE_X + PADDING_SIZE_X);
+    const uint num_elementes = (hend - offset_y) * (wend - offset_x);
+#endif
+#else
+    uint input_idx = GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(INPUT0, b_block * 4, f, offset_y, offset_x);
+
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        for(uint i = 0; i < POOL_SIZE_X; i++)
+        {
+            int4 int_data = as_int4(intel_sub_group_block_read4((const __global uint*)(input + input_idx)));
+            for(uint b = 0; b < 4; b++)
+            {
+                char4 input_data = as_char4(int_data[b]);
+                result[b][0] = FUNC_CALL(apply_pooling)(result[b][0], (int)input_data[0]);
+                result[b][1] = FUNC_CALL(apply_pooling)(result[b][1], (int)input_data[1]);
+                result[b][2] = FUNC_CALL(apply_pooling)(result[b][2], (int)input_data[2]);
+                result[b][3] = FUNC_CALL(apply_pooling)(result[b][3], (int)input_data[3]);
+            }
+
+            input_idx += IN_X_PITCH;
+        }
+        input_idx += (IN_Y_PITCH - POOL_SIZE_X*IN_X_PITCH);
+    }
+
+#if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+    const uint num_elementes = POOL_SIZE_X*POOL_SIZE_Y;
+#endif
+#endif
+
+#if defined AVG_POOLING
+    #if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+        for(uint b = 0; b < 4; b++)
+        {
+            for(uint i = 0; i < 4; i++)
+            {
+                result[b][i] = convert_int(round(((float)result[b][i] / max(num_elementes, (uint)1)));
+            }
+        }
+    #else
+        for(uint b = 0; b < 4; b++)
+        {
+            for(uint i = 0; i < 4; i++)
+            {
+                result[b][i] = convert_int(round((float)result[b][i] / (int)(POOL_SIZE_Y * POOL_SIZE_X)));
+            }
+        }
+    #endif
+#endif
+
+for(uint b = 0; b < 4; b++)
+{
+    for(uint op = 0; op < 4; op++)
+    {
+        const uint output_pos = GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(OUTPUT, b_block*4 + b, f+op, y, x);
+        output[output_pos] = ACTIVATION(convert_char(result[b][op]), NL_M ,NL_N);
+    }
+}
+}
+
+#undef INIT_VAL
+
+)__krnl"},
+
+{"permute_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+KERNEL (permute_ref)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    uint4 input_indices, output_indices;
+
+    input_indices[0] = get_global_id(0);
+    input_indices[1] = get_global_id(1);
+    input_indices[2] = get_global_id(2) % INPUT0_SIZES[2];
+    input_indices[3] = get_global_id(2) / INPUT0_SIZES[2];
+
+    output_indices[0] = input_indices[PERMUTE_ORDER[0]];
+    output_indices[1] = input_indices[PERMUTE_ORDER[1]];
+    output_indices[2] = input_indices[PERMUTE_ORDER[2]];
+    output_indices[3] = input_indices[PERMUTE_ORDER[3]];
+
+    uint input_offset =  INPUT0_OFFSET +
+                         input_indices[0]*INPUT0_PITCHES[0] +
+                         input_indices[1]*INPUT0_PITCHES[1] +
+                         input_indices[2]*INPUT0_PITCHES[2] +
+                         input_indices[3]*INPUT0_PITCHES[3];
+    uint output_offset = OUTPUT_OFFSET +
+                         output_indices[0]*OUTPUT_PITCHES[0] +
+                         output_indices[1]*OUTPUT_PITCHES[1] +
+                         output_indices[2]*OUTPUT_PITCHES[2] +
+                         output_indices[3]*OUTPUT_PITCHES[3];
+
+    output[output_offset] = ACTIVATION(input[input_offset], NL_M, NL_N);
+}
+
+)__krnl"},
+
+{"softmax_gpu_fb",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+UNIT_TYPE FUNC(find_max_value)(__local UNIT_TYPE* partial_max, const int global_id, const int idx, const int batch_offset, const int data_sets_count, const __global UNIT_TYPE* input)
+{
+    UNIT_TYPE value = -UNIT_VAL_MAX;
+    for(int i = 0; i < ITEMS_NUM; i++)
+    {
+        value = max(value, input[LWS * i + global_id]);
+    }
+    value = max(value, global_id < LEFTOVERS? input[LWS * ITEMS_NUM + global_id] : -UNIT_VAL_MAX);
+    partial_max[global_id] = value;
+
+    barrier(CLK_LOCAL_MEM_FENCE);
+    if(global_id < data_sets_count)
+    {
+        for(int i = 1; i < LWS / data_sets_count; i++)
+        {
+            partial_max[batch_offset] = max(partial_max[batch_offset], partial_max[i*data_sets_count + batch_offset]);
+        };
+    }
+    barrier(CLK_LOCAL_MEM_FENCE);
+    return partial_max[batch_offset];
+}
+
+KERNEL (softmax_gpu_continoues_yxfb)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+    const uint data_sets_count = DATA_SETS_COUNT;   //how many data sets are in the processing payload
+
+    const int global_id = get_global_id(0);
+    const int idx = global_id / data_sets_count;
+
+    const int batch_offset = global_id % data_sets_count;
+
+    __local UNIT_TYPE partial_max[LWS];
+    const UNIT_TYPE max_value = FUNC_CALL(find_max_value)(partial_max, global_id, idx, batch_offset, data_sets_count, input);
+
+    UNIT_TYPE tmp_vals[ITEMS_NUM + 1];
+    for(int i = 0; i < ITEMS_NUM; i++)
+    {
+        tmp_vals[i] = native_exp(input[LWS * i + global_id] - max_value);
+    }
+    tmp_vals[ITEMS_NUM] = global_id < LEFTOVERS ? native_exp(input[LWS * ITEMS_NUM + global_id] - max_value) : UNIT_VAL_ZERO;
+
+    // accumulate all values;
+    __local UNIT_TYPE partial_acc[LWS]; // all values accumulated;
+    partial_acc[global_id] = UNIT_VAL_ZERO;
+    for(int i = 0; i < ITEMS_NUM + 1; i++)
+    {
+        partial_acc[global_id] += tmp_vals[i];
+    }
+
+    barrier(CLK_LOCAL_MEM_FENCE); // we must be sure that all threads calculated max of elements(we can remove it if simd32 and GWS <= 32
+    if(global_id < data_sets_count)
+    {
+        for(int i = 1; i < LWS/data_sets_count; i++)
+        {
+            partial_acc[batch_offset] += partial_acc[i*data_sets_count + batch_offset];
+        }
+    }
+    barrier(CLK_LOCAL_MEM_FENCE);
+    for(int i = 0; i < ITEMS_NUM; i++)
+    {
+        output[LWS * i + global_id] = ACTIVATION(tmp_vals[i] / partial_acc[batch_offset], NL_M ,NL_N);
+    }
+    if(global_id < LEFTOVERS)
+        output[LWS * ITEMS_NUM + global_id] = ACTIVATION(tmp_vals[ITEMS_NUM] / partial_acc[batch_offset], NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"pooling_gpu_byxf_af32",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if MAX_POOLING
+    #define INIT_VAL CHAR_MIN
+#elif AVG_POOLING
+    #define INIT_VAL 0
+#else
+#error
+#endif
+
+
+inline int FUNC(apply_pooling)(int tmp, int in)
+{
+#if MAX_POOLING
+    return max(tmp, in);
+#elif AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+KERNEL(pooling_gpu_byxf_af32)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output)
+{
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+	// we process 4 features per workitem that's why we need to divide it
+    const uint aligned32_features = ((INPUT0_FEATURE_NUM + 31) / 32) * 32;
+    const uint f    = 4 * (bf % (aligned32_features / 4));
+    const uint b    = bf / (aligned32_features / 4);
+
+    if (x >= OUTPUT_SIZE_X)
+    {
+        return;
+    }
+
+    const int offset_x = (int)x*STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int offset_y = (int)y*STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    int4 result = INIT_VAL;
+
+#ifdef CHECK_BOUNDRY
+)__krnl"
+R"__krnl(    if (offset_x + POOL_SIZE_X < 0 || offset_x >= INPUT0_SIZE_X ||
+        offset_y + POOL_SIZE_Y < 0 || offset_y >= INPUT0_SIZE_Y)
+    {
+        return;
+    }
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    uint num_elementes = 0;
+#endif
+
+    const uint batch_and_feature_offset = GET_DATA_INDEX(INPUT0, b, f, 0, 0);
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        int input_offset_y = offset_y + j;
+        bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+        if(!zero_y)
+        {
+            for(uint i = 0; i < POOL_SIZE_X; i++)
+            {
+                int input_offset_x = offset_x + i;
+                bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+                if(!zero)
+                {
+                    const uint input_idx = batch_and_feature_offset + input_offset_y*INPUT0_Y_PITCH + input_offset_x*INPUT0_X_PITCH;
+
+                    char4 input_data = as_char4(intel_sub_group_block_read((const __global uint*)(input + input_idx)));
+                    result[0] = FUNC_CALL(apply_pooling)(result[0], (int)input_data[0]);
+                    result[1] = FUNC_CALL(apply_pooling)(result[1], (int)input_data[1]);
+                    result[2] = FUNC_CALL(apply_pooling)(result[2], (int)input_data[2]);
+                    result[3] = FUNC_CALL(apply_pooling)(result[3], (int)input_data[3]);
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+                    num_elementes++;
+#endif
+                }
+            }
+        }
+    }
+#ifdef DYNAMIC_WITH_PADDING_KERNEL_DIVIDER
+    const int hend = min(offset_y + POOL_SIZE_Y, INPUT0_SIZE_Y + PADDING_SIZE_Y);
+    const int wend = min(offset_x + POOL_SIZE_X, INPUT0_SIZE_X + PADDING_SIZE_X);
+    const uint num_elementes = (hend - offset_y) * (wend - offset_x);
+#endif
+#else
+    uint input_idx = GET_DATA_INDEX(INPUT0, b, f, offset_y, offset_x);
+
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        for(uint i = 0; i < POOL_SIZE_X; i++)
+        {
+            char4 input_data = as_char4(intel_sub_group_block_read((const __global uint*)(input + input_idx)));
+            result[0] = FUNC_CALL(apply_pooling)(result[0], (int)input_data[0]);
+            result[1] = FUNC_CALL(apply_pooling)(result[1], (int)input_data[1]);
+            result[2] = FUNC_CALL(apply_pooling)(result[2], (int)input_data[2]);
+            result[3] = FUNC_CALL(apply_pooling)(result[3], (int)input_data[3]);
+
+            input_idx += INPUT0_X_PITCH;
+        }
+        input_idx += (INPUT0_Y_PITCH - POOL_SIZE_X*INPUT0_X_PITCH);
+    }
+
+#if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+    const uint num_elementes = POOL_SIZE_X*POOL_SIZE_Y;
+#endif
+#endif
+
+#if defined AVG_POOLING
+    #if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+        for(uint i = 0; i < 4; i++)
+        {
+            result[i] = convert_int(round(((float)result[i] / max(num_elementes, (uint)1)));
+        }
+    #else
+        for(uint i = 0; i < 4; i++)
+        {
+            result[i] = convert_int(round((float)result[i] / (int)(POOL_SIZE_Y * POOL_SIZE_X)));
+        }
+    #endif
+#endif
+
+for(uint op = 0; op < 4; op++)
+{
+    const uint output_pos = GET_DATA_INDEX(OUTPUT, b, f+op, y, x);
+    output[output_pos] = ACTIVATION(convert_char(result[op]), NL_M ,NL_N);
+}
+
+}
+
+#undef INIT_VAL
+
+)__krnl"},
+
+{"eltwise_simple_vload8",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(eltwise_gpu_vload8)(
+    INPUTS_DECLS
+    __global UNIT_TYPE* output)
+{
+    const uint global_id = get_global_id(0);
+
+    VLOAD_DECLS
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) res;
+
+    DO_ELTWISE
+
+    res = ACTIVATION(res, NL_M, NL_N);
+
+    vstore8(res, global_id, output);
+
+}
+
+)__krnl"},
+
+{"lrn_gpu_across_channel_multiple_features",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#ifdef FORCE_SIMD_16
+__attribute__((intel_reqd_sub_group_size(16)))
+#endif
+KERNEL (lrn_gpu_across_channel_multiple_features)(const __global INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+#if   defined OUTPUT_LAYOUT_BFYX
+// PERF NOTE: SIMD IS OVER global_id(0) so in SIMD global_id(1) and global_id(2) does not change, so we can use group_id to have SIMD1 instructions
+    const uint x            = get_global_id(0);
+    const uint y            = get_group_id(1);
+    const uint b_f          = get_group_id(2);
+    const uint batch_id     = (b_f * OFM_PER_SIMD) / INPUT0_FEATURE_NUM;
+    const uint feature_id   = (b_f % (INPUT0_FEATURE_NUM / OFM_PER_SIMD)) * OFM_PER_SIMD;
+
+    if (x >= INPUT0_SIZE_X)
+        return;
+#elif defined OUTPUT_LAYOUT_YXFB
+    const uint b_f          = get_global_id(0);
+    const uint x            = get_group_id(1);
+    const uint y            = get_group_id(2);
+    const uint feature_id   = (b_f / INPUT0_BATCH_NUM) * OFM_PER_SIMD;
+    const uint batch_id     = b_f % INPUT0_BATCH_NUM;
+#endif
+
+    uint input_id = INPUT0_OFFSET + batch_id*INPUT0_BATCH_PITCH + feature_id*INPUT0_FEATURE_PITCH + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH;
+
+    int input_offset_f = feature_id - PADDING;
+    uint input_idx = input_id - PADDING*INPUT0_FEATURE_PITCH;
+
+    input_idx =  MULTIPLY_OFFSET(UNIT_TYPE, input_idx);
+
+    UNIT_TYPE vals[OFM_PER_SIMD];
+    UNIT_TYPE results[OFM_PER_SIMD] = { UNIT_VAL_ZERO };
+
+    // prefetch
+    for(uint i = 0; i < OFM_PER_SIMD; i++)
+    {
+        bool zero = input_offset_f < 0 || input_offset_f >= INPUT0_FEATURE_NUM;
+        vals[i] = zero ? UNIT_VAL_ZERO : TO_UNIT_TYPE(ALPHA_VAL_FACTOR_DIV_BY_SIZE) * (*OFFSET_GLOBAL_PTR(UNIT_TYPE, input, input_idx));
+        input_offset_f++;
+        input_idx += MULTIPLY_OFFSET(UNIT_TYPE, INPUT0_FEATURE_PITCH);
+    }
+
+    for (uint i = 0; i < LOCAL_SIZE-1; i++)
+    {
+        for(uint j = 0; j < OFM_PER_SIMD; j++)
+        {
+            results[j] = mad(vals[j], vals[j], results[j]);
+        }
+        for(uint j = 0; j < OFM_PER_SIMD-1; j++)
+        {
+            vals[j] = vals[j+1];
+        }
+
+        bool zero = input_offset_f < 0 || input_offset_f >= INPUT0_FEATURE_NUM;
+        vals[OFM_PER_SIMD-1] = zero ? UNIT_VAL_ZERO : TO_UNIT_TYPE(ALPHA_VAL_FACTOR_DIV_BY_SIZE) * (*OFFSET_GLOBAL_PTR(UNIT_TYPE, input, input_idx));
+        input_offset_f++;
+        input_idx += MULTIPLY_OFFSET(UNIT_TYPE, INPUT0_FEATURE_PITCH);
+    }
+
+    for(uint j = 0; j < OFM_PER_SIMD; j++)
+    {
+        results[j] = mad(vals[j], vals[j], results[j]);
+    }
+
+    for(uint j = 0; j < OFM_PER_SIMD; j++)
+    {
+        results[j] = mad(results[j], TO_UNIT_TYPE(ALPHA_DIV_BY_SIZE), TO_UNIT_TYPE(K));
+        results[j] = native_powr(results[j], -TO_UNIT_TYPE(BETA));
+    }
+
+    uint output_idx = OUTPUT_OFFSET + batch_id*OUTPUT_BATCH_PITCH + feature_id*OUTPUT_FEATURE_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH;
+    for(uint j = 0; j < OFM_PER_SIMD; j++)
+    {
+        output[output_idx] = ACTIVATION(results[j] * input[input_id], NL_M ,NL_N);
+        output_idx += OUTPUT_FEATURE_PITCH;
+        input_id += INPUT0_FEATURE_PITCH;
+    }
+}
+
+)__krnl"},
+
+{"normalize_gpu_across_spatial_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+
+KERNEL (normalize_gpu_across_spatial_bfyx)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output, const __global UNIT_TYPE* scale_input)
+{
+    const uint b = get_global_id(0);
+
+    float norm = EPSILON;
+
+    const uint input_first = INPUT0_OFFSET + b * INPUT0_BATCH_PITCH;
+
+    // Compute norm
+    uint input_idx = input_first;
+    for (uint f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+        for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+        {
+            for (uint x = 0; x < INPUT0_SIZE_X; x++)
+            {
+                float value = (float)input[input_idx];
+                norm = mad(value, value, norm);
+                input_idx += INPUT0_X_PITCH;
+            }
+            input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+        }
+        input_idx += INPUT0_FEATURE_PITCH - INPUT0_Y_PITCH*INPUT0_SIZE_Y;
+    }
+    if(norm <= THRESHOLD)
+    {
+        norm = 0;
+    }
+    else
+    {
+        norm = native_powr(norm, -0.5f);
+    }
+
+    uint output_idx = OUTPUT_OFFSET + b * OUTPUT_BATCH_PITCH;
+
+    // Scale the input
+    input_idx = input_first;
+    for (uint f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+#if SCALE_TABLE_FEATURE_NUM == 1
+        const uint scale_index = 0;
+#elif INPUT0_FEATURE_NUM <= SCALE_TABLE_FEATURE_NUM
+        const uint scale_index = f;
+#else
+        const uint scale_index = f % SCALE_TABLE_FEATURE_NUM;
+#endif
+
+        for (uint y = 0; y < INPUT0_SIZE_Y; y++)
+        {
+            for (uint x = 0; x < INPUT0_SIZE_X; x++)
+            {
+                output[output_idx] = ACTIVATION(UNIT_CVT_FUNC(norm) * input[input_idx] * scale_input[scale_index], NL_M, NL_N);
+                input_idx += INPUT0_X_PITCH;
+                output_idx += OUTPUT_X_PITCH;
+            }
+            input_idx += INPUT0_Y_PITCH - INPUT0_SIZE_X*INPUT0_X_PITCH;
+            output_idx += OUTPUT_Y_PITCH - INPUT0_SIZE_X*OUTPUT_X_PITCH;
+        }
+        input_idx += INPUT0_FEATURE_PITCH - INPUT0_Y_PITCH*INPUT0_SIZE_Y;
+        output_idx += OUTPUT_FEATURE_PITCH - INPUT0_SIZE_Y*OUTPUT_Y_PITCH;
+    }
+}
+
+
+#undef UNIT_CVT_FUNC
+
+)__krnl"},
+
+{"fully_connected_gpu_bs_f_bsv16_af8_vload",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if FP16_UNIT_USED
+    // Block read - currently block is 4 bytes aligned.
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_half8(intel_sub_group_block_read_us8((const __global ushort*)(ptr) + (byte_offset)))
+
+    #define MULTIPLY_BLOCKS_16x8(_result, _blockA, _blockB)  \
+    {   \
+        const half16 acol0 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s0 ); \
+        const half16 acol1 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s1 ); \
+        const half16 acol2 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s2 ); \
+        const half16 acol3 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s3 ); \
+        const half16 acol4 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s4 ); \
+        const half16 acol5 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s5 ); \
+        const half16 acol6 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s6 ); \
+        const half16 acol7 = TRANSPOSE_BLOCK_16_FP16_HALF_TYPE( _blockA.s7 ); \
+        _result = fma( _blockB.s0, acol0, _result ); \
+        _result = fma( _blockB.s1, acol1, _result ); \
+        _result = fma( _blockB.s2, acol2, _result ); \
+        _result = fma( _blockB.s3, acol3, _result ); \
+        _result = fma( _blockB.s4, acol4, _result ); \
+        _result = fma( _blockB.s5, acol5, _result ); \
+        _result = fma( _blockB.s6, acol6, _result ); \
+        _result = fma( _blockB.s7, acol7, _result ); \
+    }
+#else
+    // Block read - currently block is 4 bytes aligned.
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_float8(intel_sub_group_block_read8((const __global uint*)(ptr) + (byte_offset)))
+
+    #define MULTIPLY_BLOCKS_8x8(_result, _blockA, _blockB)  \
+    {   \
+        const float8 acol0 = TRANSPOSE_BLOCK_8( _blockA.s0 ); \
+        const float8 acol1 = TRANSPOSE_BLOCK_8( _blockA.s1 ); \
+        const float8 acol2 = TRANSPOSE_BLOCK_8( _blockA.s2 ); \
+        const float8 acol3 = TRANSPOSE_BLOCK_8( _blockA.s3 ); \
+        const float8 acol4 = TRANSPOSE_BLOCK_8( _blockA.s4 ); \
+        const float8 acol5 = TRANSPOSE_BLOCK_8( _blockA.s5 ); \
+        const float8 acol6 = TRANSPOSE_BLOCK_8( _blockA.s6 ); \
+        const float8 acol7 = TRANSPOSE_BLOCK_8( _blockA.s7 ); \
+        _result = mad( _blockB.s0, acol0, _result ); \
+        _result = mad( _blockB.s1, acol1, _result ); \
+        _result = mad( _blockB.s2, acol2, _result ); \
+        _result = mad( _blockB.s3, acol3, _result ); \
+)__krnl"
+R"__krnl(        _result = mad( _blockB.s4, acol4, _result ); \
+        _result = mad( _blockB.s5, acol5, _result ); \
+        _result = mad( _blockB.s6, acol6, _result ); \
+        _result = mad( _blockB.s7, acol7, _result ); \
+    }
+#endif
+
+#define SUB_GROUP_SIZE 16
+
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+KERNEL (fully_connected_gpu_xb_bs_xs_xsv8_bsv16_vload)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint global_id = get_global_id(0);
+    const uint group_id = get_group_id(0);
+    const uint batch_group_id = get_global_id(1); // which part of batches we are computing, for example for batch 64 we compute batches 0..31 for batch_group_id == 0 and batches 32..65 for batch_group_id == 1
+    const uint id_in_sub_group = get_sub_group_local_id();
+
+    const uint out_id = (id_in_sub_group * BATCHES_PER_WORK_ITEM * (uint)get_global_size(1)) / SUB_GROUP_SIZE + group_id * BATCHES_PER_WORK_ITEM * (uint)get_global_size(1) + (BATCHES_PER_WORK_ITEM * batch_group_id) / SUB_GROUP_SIZE;
+
+    uint neuronIdx = id_in_sub_group + group_id * SUB_GROUP_SIZE;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 16) blockC00 = UNIT_VAL_ZERO;
+
+    uint weight_offset = id_in_sub_group + SUB_GROUP_SIZE * group_id * INPUT0_ELEMENTS_COUNT;
+
+    uint input_idx = id_in_sub_group + batch_group_id * BATCHES_PER_WORK_ITEM * INPUT0_ELEMENTS_COUNT;
+    for(uint h = 0; h < INPUT0_ELEMENTS_COUNT / 8; h++)
+    {
+        // read input data in blocks ( 16 batch * 8 x )
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA00 = ALIGNED_BLOCK_READ8(input, input_idx);
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockB00 = ALIGNED_BLOCK_READ8(weight, weight_offset);
+
+        MULTIPLY_BLOCKS_16x8(blockC00, blockA00, blockB00)
+
+        weight_offset += 128;
+        input_idx     += 128; // 128 = 16x8 - because of input format which have blocks of 128 elements
+    }
+
+#if BIAS_TERM
+    blockC00 += bias[neuronIdx];
+#endif // #if BIAS_TERM
+
+    blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+
+    vstore16(blockC00, out_id, output);
+
+}
+
+#undef SUB_GROUP_SIZE
+#undef ALIGNED_BLOCK_READ8
+#undef MAKE_VECTOR_TYPE
+#undef CONCAT_TOKEN
+#undef CONCAT_TOKEN_HANDLER1
+#undef MULTIPLY_BLOCKS_16x16
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_direct_10_12_16",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+//////////////////////////////////////////////////////////////////////////////
+// Direct Convolution
+#if defined(cl_intel_subgroups_short)
+
+#define TILE_M          DY      // Height of tile in input patches (src0)
+#define TILE_K          DX      // Width of tile in input patches (src0)
+#define TILE_N          16      // Num filter channels per tile (src1)
+
+#define TILE_X          12      // Width of tile loaded in input (src0)
+#define TILE_Y          10      // Height of tile loaded in input (src0)
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_f16_10x12x16)(
+    const __global half *src0,
+    __global half *dst,
+    const __global half *src1,
+#if BIAS_TERM
+    const __global half *biases,
+#endif
+    uint split_idx)
+{
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+typedef struct half1  { half s0; }                                                               half1;
+typedef struct half5  { half s0; half s1; half s2; half s3; half s4; }                           half5;
+typedef struct half6  { half s0; half s1; half s2; half s3; half s4; half s5; }                  half6;
+)__krnl"
+R"__krnl(typedef struct half7  { half s0; half s1; half s2; half s3; half s4; half s5; half s6; }         half7;
+typedef struct half9  { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; }                                                               half9;
+typedef struct half10 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; half s9; }                                                      half10;
+typedef struct half11 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; half s9; half sa; }                                             half11;
+typedef struct half12 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb;}                                    half12;
+typedef struct half13 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb; half sc;}                           half13;
+typedef struct half14 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb; half sc; half se;}                  half14;
+typedef struct half15 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                       half s8;  half s9; half sa; half sb; half sc; half se; half sf;}          half15;
+typedef struct half0  { half s0; } half0; //never used but makes compiler happy.
+
+typedef struct float1 { float s0; } float1;
+typedef struct float5 { float s0; float s1; float s2; float s3; float s4; } float5;
+typedef struct float6 { float s0; float s1; float s2; float s3; float s4; float s5; } float6;
+typedef struct float7 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; } float7;
+typedef struct float9 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; float s7; float s8; } float9;
+typedef struct float10 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9;} float10;
+typedef struct float11 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa;} float11;
+typedef struct float12 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; } float12;
+typedef struct float13 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc;} float13;
+typedef struct float14 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; } float14;
+typedef struct float15 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; float se; } float15;
+typedef struct float0 { float s0; } float0; //never used but makes compiler happy.
+
+#if (KERNEL_WIDTH == 1)
+__constant half1 half_zeros= (half1){0};
+#elif (KERNEL_WIDTH == 2)
+    __constant half2 half_zeros = (half2)(0);
+#elif (KERNEL_WIDTH == 3)
+    __constant half3 half_zeros = (half3)(0);
+#elif (KERNEL_WIDTH == 4)
+    __constant half4 half_zeros = (half4)(0);
+#elif (KERNEL_WIDTH == 5)
+    __constant half5 half_zeros = (half5){0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 6)
+    __constant half6 half_zeros = (half6){0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 7)
+    __constant half7 half_zeros = (half7){0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 8)
+    __constant half8 half_zeros = (half8)(0);
+#elif (KERNEL_WIDTH == 9)
+    __constant half9 half_zeros = (half9){0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 10)
+    __constant half10 half_zeros = (half10){0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 11)
+    __constant half11 half_zeros = (half11){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 12)
+    __constant half12 half_zeros = (half12){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 13)
+    __constant half13 half_zeros = (half13){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 14)
+    __constant half14 half_zeros = (half14){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 15)
+    __constant half15 half_zeros = (half15){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 16)
+    __constant half16 half_zeros = (half16)(0);
+#endif
+
+
+    const unsigned global_x = get_global_id(0);
+    const unsigned global_y = get_global_id(1);
+    const unsigned global_z = get_global_id(2);
+    const unsigned out_fm   = global_z % ALIGNED_OFM;
+    const unsigned batch_id = global_z / ALIGNED_OFM;
+    const unsigned group_x = get_group_id(0);
+    const unsigned group_z = get_group_id(2);
+    const unsigned max_group_x = get_num_groups(0);
+    const unsigned local_z = get_local_id(2);
+
+    half blockC[TILE_M * TILE_K] = { 0 };
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * INPUT0_FEATURE_NUM;
+    uint src0_offset_tile = INPUT0_OFFSET_WITH_PADDING      // data offset
+     + in_split_offset
+     + batch_id * INPUT0_BATCH_PITCH                        // batch offset
+     + ( global_y * TILE_M * STRIDE_SIZE_Y ) * INPUT0_Y_PITCH    // y offset
+     + ( global_x * TILE_K * STRIDE_SIZE_X );                    // x offset
+    uint src0_offset = src0_offset_tile
+     + ( local_z / ( TILE_X / 4 ) ) * INPUT0_Y_PITCH        // y tile offset
+     + ( local_z % ( TILE_X / 4 ) ) * 4;                    // x tile offset
+
+    const __global half *src1_read = src1 + ( group_z * TILE_N % ALIGNED_OFM ) * 2;
+
+    unsigned patch_depth = 0;
+    __attribute__((opencl_unroll_hint(3)))
+    do
+    {
+        // Load atile (input) and btile (filters).
+        // Kernel data is partially interleaved.  Every 2 rows are interleaved at float16 granularity.
+        // The exception is that if FILTER_SIZE_X is odd the last row is not interleaved.  The non
+        // interleaved row is padded with zero to ensure same size as interleaved rows. This
+        // interleaving is done to increase consecutive data to fetch which reduces loads required.
+        // For example, this is how the kernel data would be arranged before/after interleaving for FILTER_SIZE_X=3.
+        // (0, 0) (8, 0) (16, 0) (24, 0) ...       (0, 0) (0, 1) (8, 0) (0, 1) (16, 0) (0, 1) (24, 0) ..
+        // (0, 1) (8, 1) (16, 1) (24, 1) ... =>    (0, 2) (8, 2) (16, 2) (24, 2) ...
+        // (0, 2) (8, 2) (16, 2) (24, 2) ...       ...
+        // ...
+
+        #if ((INPUT0_Y_PITCH) % 4) == 0
+        // aligned - can ignore vload
+        half4 blockA0 = *(const __global half4 *)( src0 + src0_offset );
+        half4 blockA1 = *(const __global half4 *)( src0 + src0_offset + INPUT0_Y_PITCH * 5 );
+        #elif ((INPUT0_Y_PITCH) % 2) == 0
+        // in case the data is not aligned to sizeof(T)*4 we need to use vload or set the data in a loop
+        // first one is aligned
+        half4 blockA0 = *(const __global half4 *)( src0 + src0_offset );
+        half4 blockA1 = vload4(0, src0 + src0_offset + INPUT0_Y_PITCH * 5 );
+        #else
+        half4 blockA0 = vload4(0, src0 + src0_offset );
+        half4 blockA1 = vload4(0, src0 + src0_offset + INPUT0_Y_PITCH * 5 );
+        #endif
+        src0_offset += INPUT0_FEATURE_PITCH;
+
+        half blockB[FILTER_SIZE_X * FILTER_SIZE_Y];
+        ushort2* p2BlockB = (ushort2*)blockB;
+        ushort*  pBlockB =  (ushort* )blockB;
+
+        const bool kernel_slice_is_odd = ( FILTER_SIZE_X * FILTER_SIZE_Y ) % 2 == 1;
+        unsigned interleaved_y = 0;
+        LOOP(KERNEL_SLICE_DIV2, interleaved_y,
+        {
+            p2BlockB[interleaved_y] = intel_sub_group_block_read_us2( (const __global ushort*)src1_read );
+            src1_read += ALIGNED_OFM * 2;
+        } )
+        if ( kernel_slice_is_odd )
+        {
+            pBlockB[FILTER_SIZE_X * FILTER_SIZE_Y - 1] = intel_sub_group_block_read_us( (const __global ushort*)src1_read );
+            src1_read += ALIGNED_OFM * 2;
+        }
+
+#define BLOCK_A(n) ( (n < 60) \
+    ? sub_group_broadcast( blockA0[(n)%4], (n)/4 ) \
+    : sub_group_broadcast( blockA1[(n-60)%4], (n-60)/4 ) )
+
+        // Perform MADs
+        // Loop through all patches in tile (patch_x/y)
+        // For each patch, sum values (x/y)
+        unsigned patch_y=0;
+        LOOP(TILE_M, patch_y,
+        {
+            unsigned patch_x=0;
+            LOOP(TILE_K, patch_x,
+            {
+                unsigned tile_idx = patch_y * TILE_X * STRIDE_SIZE_Y + patch_x * STRIDE_SIZE_X;
+                unsigned out_idx  = patch_y * TILE_K + patch_x;
+
+                unsigned y=0;
+                LOOP(FILTER_SIZE_Y, y,
+                {
+                    unsigned x=0;
+                    LOOP(FILTER_SIZE_X, x,
+                    {
+                        unsigned offset_idx = y * TILE_X + x;
+                        unsigned out_chan_idx = y * FILTER_SIZE_X + x;
+
+                        blockC[out_idx] = mad( BLOCK_A( tile_idx + offset_idx ), blockB[out_chan_idx], blockC[out_idx] );
+                    } )
+                } )
+            } )
+        } )
+    }
+    while ( ++patch_depth < INPUT0_FEATURE_NUM );
+
+    // Dst resembles a cube of width x height x (output channel * batches).  Each tile writes:
+    // TILE_K x TILE_M x SIMD.  Partial writes most likely generated if output padding used.
+    // Group stores into vectors to expedite writeback.  One large write is faster than many
+    // small saves. Right-most column may be smaller if output width not divisible by tile width.
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    __global half *out = dst + OUTPUT_OFFSET + out_split_offset +
+     + batch_id * OUTPUT_BATCH_PITCH            // batch offset
+     + out_fm * OUTPUT_FEATURE_PITCH              // channel offset
+     + ( global_y * TILE_M ) * OUTPUT_Y_PITCH // y offset
+     + ( global_x * TILE_K );                // x offset
+
+    if ( batch_id < OUTPUT_BATCH_NUM && out_fm < OUTPUT_FEATURE_NUM )
+    {
+#if BIAS_TERM == 0
+        const half bias = 0.h;
+#elif BIAS_PER_OFM
+        const half bias = biases[out_fm];
+#endif
+
+        if ( OUTPUT_SIZE_X % TILE_K == 0 ||
+             group_x < max_group_x - 1 )
+        {
+            typedef CAT( half, TILE_K ) half_t;
+            for( unsigned y = 0; y < TILE_M; y++ )
+            {
+)__krnl"
+R"__krnl(                if ( global_y * TILE_M + y < OUTPUT_SIZE_Y )
+                {
+                    half_t vBlockC;
+                    half *pvBlockC = (half*)&vBlockC;
+                    for (unsigned i = 0; i < TILE_K; i++)
+                    {
+                    #if BIAS_TERM && BIAS_PER_OUTPUT
+                        const unsigned bias_index = out_fm*OUTPUT_SIZE_X*OUTPUT_SIZE_Y + ( global_y * TILE_M + y )*OUTPUT_SIZE_X + ( global_x * TILE_K + i);
+                        const half bias = biases[bias_index];
+                    #endif
+                        pvBlockC[i] = ACTIVATION(blockC[y * TILE_K + i] + bias, NL_M, NL_N);
+                        ((__global half*)(out + y * OUTPUT_Y_PITCH))[i] = pvBlockC[i];
+                    }
+                    //*(__global half_t*)(out + y * OUTPUT_Y_PITCH) = vBlockC;
+                }
+            }
+        }
+        else
+        {
+            typedef CAT( half, RIGHT_PARTIAL_TILE_K ) half_t;
+            for( unsigned y = 0; y < TILE_M; y++ )
+            {
+                if ( global_y * TILE_M + y < OUTPUT_SIZE_Y )
+                {
+                    half_t vBlockC;
+                    half *pvBlockC = (half*)&vBlockC;
+                    for (unsigned i = 0; i < RIGHT_PARTIAL_TILE_K; i++)
+                    {
+                    #if BIAS_TERM && BIAS_PER_OUTPUT
+                        const unsigned bias_index = out_fm*OUTPUT_SIZE_X*OUTPUT_SIZE_Y + ( global_y * TILE_M + y )*OUTPUT_SIZE_X + ( global_x * TILE_K + i);
+                        const half bias = biases[bias_index];
+                    #endif
+                        pvBlockC[i] = ACTIVATION(blockC[y * TILE_K + i] + bias, NL_M, NL_N);
+                        ((__global half*)(out + y * OUTPUT_Y_PITCH))[i] = pvBlockC[i];
+                    }
+                    //*(__global half_t*)(out + y * OUTPUT_Y_PITCH) = vBlockC;
+                }
+            }
+        }
+    }
+}
+#endif // cl_intel_subgroups_short
+
+)__krnl"},
+
+{"pooling_gpu_bfyx_block_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if MAX_POOLING || MAX_WITH_ARGMAX_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_MIN
+#elif AVG_POOLING
+    #define UNIT_INIT_VAL UNIT_VAL_ZERO
+#else
+#error
+#endif
+
+
+inline UNIT_TYPE FUNC(apply_pooling)(UNIT_TYPE tmp, UNIT_TYPE in)
+{
+#if MAX_POOLING || MAX_WITH_ARGMAX_POOLING
+    return max(tmp, in);
+#elif AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+KERNEL(pooling_gpu)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output
+#if MAX_WITH_ARGMAX_POOLING
+, __global float* arg_max
+#endif
+)
+{
+
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1) * POOL_SIZE_Y;
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf % INPUT0_FEATURE_NUM;
+    const uint b    = bf / INPUT0_FEATURE_NUM;
+
+    if ((x >= OUTPUT_SIZE_X) || (y >= OUTPUT_SIZE_Y))
+        return;
+
+    const int offset_x = (int)x*STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int offset_y = (int)y*STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    UNIT_TYPE result = UNIT_INIT_VAL;
+
+    uint input_idx = GET_DATA_INDEX(INPUT0, b, f, offset_y, offset_x);
+    UNIT_TYPE max_x[BLOCK_SIZE_Y];
+)__krnl"
+R"__krnl(    UNIT_TYPE out[POOL_SIZE_Y];
+
+#if MAX_WITH_ARGMAX_POOLING
+    uint arg_max_x[BLOCK_SIZE_Y] = { 0 };
+    uint arg_max_out[POOL_SIZE_Y] = { 0 };
+    uint input_idx_bfyx_no_padding = offset_x + INPUT0_SIZE_X * (offset_y + INPUT0_SIZE_Y * (f + INPUT0_FEATURE_NUM * b));
+#endif
+
+    for(uint i = 0; i < BLOCK_SIZE_Y; i++)
+    {
+        max_x[i] = UNIT_INIT_VAL;
+    }
+
+    // we do max in "x" dimension
+    for(uint j = 0; j < BLOCK_SIZE_Y; j++)
+    {
+        for(uint i = 0; i < POOL_SIZE_X; i++)
+        {
+
+#if MAX_WITH_ARGMAX_POOLING
+            if(input[input_idx] > max_x[j])
+                arg_max_x[j] = input_idx_bfyx_no_padding;
+#endif
+            max_x[j] = FUNC_CALL(apply_pooling)(max_x[j], input[input_idx]);
+            input_idx += INPUT0_X_PITCH;
+
+#if MAX_WITH_ARGMAX_POOLING
+            input_idx_bfyx_no_padding++;
+#endif
+
+        }
+        input_idx += (INPUT0_Y_PITCH - POOL_SIZE_X*INPUT0_X_PITCH);
+
+#if MAX_WITH_ARGMAX_POOLING
+        input_idx_bfyx_no_padding += (INPUT0_SIZE_X - POOL_SIZE_X);
+#endif
+    }
+
+    for(uint i = 0; i < POOL_SIZE_Y; i++)
+    {
+        out[i] = max_x[i * STRIDE_SIZE_Y];
+
+#if MAX_WITH_ARGMAX_POOLING
+        arg_max_out[i] = arg_max_x[i * STRIDE_SIZE_Y];
+#endif
+    }
+
+    // now we do max in "y" dimension
+    for(uint i = 0; i < POOL_SIZE_Y; i++)
+    {
+        for(uint j = 1; j < POOL_SIZE_Y; j++)
+        {
+
+#if MAX_WITH_ARGMAX_POOLING
+            if(max_x[j + i * STRIDE_SIZE_Y] > out[i])
+                arg_max_out[i] = arg_max_x[j + i * STRIDE_SIZE_Y];
+#endif
+
+            out[i] = FUNC_CALL(apply_pooling)(out[i], max_x[j + i * STRIDE_SIZE_Y]);
+        }
+    }
+
+    uint output_pos = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+
+#if MAX_WITH_ARGMAX_POOLING
+    uint arg_max_pos = GET_DATA_INDEX(INPUT1, b, f, y, x);
+#endif
+
+    for(uint i = 0; i < POOL_SIZE_Y; i++)
+    {
+        if((y + i) < OUTPUT_SIZE_Y)
+        {
+#if defined AVG_POOLING
+            out[i] /= (UNIT_TYPE)(POOL_SIZE_Y * POOL_SIZE_X);
+#endif
+            output[output_pos] = ACTIVATION(out[i], NL_M ,NL_N);
+            output_pos += OUTPUT_Y_PITCH;
+
+#if MAX_WITH_ARGMAX_POOLING
+            arg_max[arg_max_pos] = arg_max_out[i];
+            arg_max_pos += INPUT1_Y_PITCH;
+#endif
+        }
+    }
+}
+
+#undef UNIT_INIT_VAL
+
+)__krnl"},
+
+{"fully_connected_grad_input_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(fully_connected_grad_input_gpu_ref)(
+    const __global INPUT0_TYPE* input_grad,
+    __global OUTPUT_TYPE* output,
+    const __global FILTER_TYPE* weights,
+    const __global INPUT1_TYPE* input
+    )
+{
+    const uint x            = get_global_id(1);
+    const uint y            = get_global_id(2);
+    const uint b_f          = get_global_id(0);
+    const uint batch_id     = b_f % INPUT0_BATCH_NUM;
+    const uint feature_id   = b_f / INPUT0_BATCH_NUM;
+
+    if(b_f >= INPUT1_FEATURE_NUM * INPUT0_BATCH_NUM)
+        return;
+
+    ACCUMULATOR_TYPE result = 0;
+
+    for (uint ofm = 0; ofm < FILTER_OFM_NUM; ++ofm)
+    {
+        const uint input_grad_idx = GET_DATA_INDEX(INPUT0, batch_id, 0, 0, ofm);
+        const uint filter_idx = GET_FILTER_INDEX(FILTER, ofm, feature_id, y, x);
+
+        result += (ACCUMULATOR_TYPE)(input_grad[input_grad_idx] * weights[filter_idx]);
+    }
+
+    const uint output_idx = GET_DATA_INDEX(OUTPUT, batch_id, feature_id, y, x);
+    output[output_idx] = result;
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_bf_io_input_spatial",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// Required JIT constants:
+//  - FP16_SUPPORTED        - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED        - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE             - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO         - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT0_BATCH_NUM      - [int] Number of elements from single spatial and single feature that are grouped in single batch in input.
+//  - INPUT0_ELEMENTS_COUNT - [int] Cumulative number of elements from input that are processed in single batch.
+//  - FILTER_OFM_NUM        - [int] Cumulative number of elements that are outputted in single batch.
+//  - RELU                  - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE        - [float] Factor for negative output values (required when ReLU is specified).
+
+#define ACC_TYPE float
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL (fully_connected_gpu_bf_io_input_spatial)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint x = get_global_id(0);
+    const uint batch_id = get_global_id(1);
+
+    const uint outXIdx = batch_id * FILTER_OFM_NUM + x;
+    ACC_TYPE result = UNIT_VAL_ZERO;
+
+    uint input_idx = batch_id * INPUT0_ELEMENTS_COUNT + get_sub_group_local_id();
+    input_idx = MULTIPLY_OFFSET(UNIT_TYPE, input_idx);
+    uint weight_idx = MULTIPLY_OFFSET(UNIT_TYPE, outXIdx);
+    const uint weight_idx_base = weight_idx;
+    uint s_w_idx = MULTIPLY_OFFSET(UNIT_TYPE, get_group_id(0) * 16 + get_sub_group_local_id() * FILTER_OFM_NUM);
+    const uint input_slices = INPUT0_ELEMENTS_COUNT / 16;
+    for (uint i = 0; i < input_slices; i++)
+    {
+        UNIT_TYPE _inG = *OFFSET_GLOBAL_PTR(UNIT_TYPE, input, input_idx);
+        uint it_w_addr = _inG == UNIT_VAL_ZERO ? weight_idx_base : s_w_idx;
+        for(uint j = 0; j < 16; j++)
+)__krnl"
+R"__krnl(        {
+            UNIT_TYPE _in = intel_sub_group_shuffle(_inG, j);
+            uint wi_w_addr = intel_sub_group_shuffle(it_w_addr, j);
+            wi_w_addr += MULTIPLY_OFFSET(UNIT_TYPE, get_sub_group_local_id());
+            UNIT_TYPE _w = *OFFSET_GLOBAL_PTR(UNIT_TYPE, weight, wi_w_addr);
+            result += _in * _w;
+        }
+        input_idx  += MULTIPLY_OFFSET(UNIT_TYPE, 16);
+        s_w_idx += MULTIPLY_OFFSET(UNIT_TYPE, FILTER_OFM_NUM * 16);
+    }
+    input_idx -=  MULTIPLY_OFFSET(UNIT_TYPE, get_sub_group_local_id());
+    weight_idx += MULTIPLY_OFFSET(UNIT_TYPE, input_slices * FILTER_OFM_NUM);
+    for (uint i = 0; i < INPUT0_ELEMENTS_COUNT % 16; i++)
+    {
+        UNIT_TYPE _in = *OFFSET_GLOBAL_PTR(UNIT_TYPE, input, input_idx);
+        UNIT_TYPE _w = *OFFSET_GLOBAL_PTR(UNIT_TYPE, weight, weight_idx);
+        result += _in * _w;
+        input_idx  += MULTIPLY_OFFSET(UNIT_TYPE, 1);
+        weight_idx += MULTIPLY_OFFSET(UNIT_TYPE, FILTER_OFM_NUM);
+    }
+
+#if BIAS_TERM
+    result += bias[outXIdx];
+#endif
+    if(x < FILTER_OFM_NUM)
+    {
+        output[x] = ACTIVATION((UNIT_TYPE)(result), NL_M, NL_N);
+    }
+}
+
+
+)__krnl"},
+
+{"lrn_gpu_across_channel_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+
+KERNEL (lrn_gpu_across_channel_ref)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+#if   defined OUTPUT_LAYOUT_BFYX
+    const uint x            = get_global_id(0);
+    const uint y            = get_global_id(1);
+    const uint b_f          = get_global_id(2);
+    const uint batch_id     = b_f / INPUT0_FEATURE_NUM;
+    const uint feature_id   = b_f % INPUT0_FEATURE_NUM;
+
+    if (x >= INPUT0_SIZE_X)
+        return;
+#else
+    const uint b_f          = get_global_id(0);
+    const uint x            = (uint)get_global_id(1);
+    const uint y            = (uint)get_global_id(2);
+    const uint feature_id   = b_f / INPUT0_BATCH_NUM;
+    const uint batch_id     = b_f % INPUT0_BATCH_NUM;
+#endif
+
+    const uint input_id = INPUT0_OFFSET + batch_id*INPUT0_BATCH_PITCH + feature_id*INPUT0_FEATURE_PITCH + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH;
+
+    UNIT_TYPE acc = UNIT_VAL_ZERO;
+
+    int input_offset_f = feature_id - PADDING;
+    int input_idx = (int)input_id - PADDING*INPUT0_FEATURE_PITCH;
+
+    for (int i = 0; i < LOCAL_SIZE; i++)
+    {
+        bool zero = input_offset_f < 0 || input_offset_f >= INPUT0_FEATURE_NUM;
+
+        UNIT_TYPE value = zero ? UNIT_VAL_ZERO : UNIT_CVT_FUNC(ALPHA_VAL_FACTOR_DIV_BY_SIZE) * input[input_idx];
+        acc = mad(value, value, acc);
+
+        input_offset_f++;
+        input_idx += INPUT0_FEATURE_PITCH;
+    }
+    acc = mad(acc, UNIT_CVT_FUNC(ALPHA_DIV_BY_SIZE), UNIT_CVT_FUNC(K));
+    acc = native_powr(acc, -UNIT_CVT_FUNC(BETA));
+
+    const uint output_idx = OUTPUT_OFFSET + batch_id*OUTPUT_BATCH_PITCH + feature_id*OUTPUT_FEATURE_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH;
+    output[output_idx] = ACTIVATION(acc * input[input_id], NL_M ,NL_N);
+}
+
+#undef UNIT_CVT_FUNC
+
+)__krnl"},
+
+{"fully_connected_gpu_fb_oi_b8_fp32_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+
+__attribute__((reqd_work_group_size(8, 1, 1)))
+KERNEL (fully_connected_gpu_xb_bx_b8)(
+    const __global float* input,
+    __global float* output,
+    const __global float* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint batch_id = get_global_id(0);
+
+    uint outXIdx = get_global_id(1);
+    uint weight_offset = outXIdx * INPUT0_ELEMENTS_COUNT + batch_id;
+#if BIAS_TERM
+    float result = bias[outXIdx];
+#else
+    float result = 0.0f;
+#endif
+
+    float8 _data = 0.f;
+    const uint sub_group_id = get_local_id(0);
+
+    for(uint _i = 0; _i < INPUT0_ELEMENTS_COUNT/8; _i++)
+    {
+        uint i = _i * 8;
+        const float weight_val = weight[weight_offset];
+        const float8 _input = as_float8(intel_sub_group_block_read8((const __global uint*)input + i * INPUT0_BATCH_NUM + batch_id));
+        _data.s0 = fma(_input.s0, intel_sub_group_shuffle(weight_val, 0), _data.s0);
+        _data.s1 = fma(_input.s1, intel_sub_group_shuffle(weight_val, 1), _data.s1);
+        _data.s2 = fma(_input.s2, intel_sub_group_shuffle(weight_val, 2), _data.s2);
+        _data.s3 = fma(_input.s3, intel_sub_group_shuffle(weight_val, 3), _data.s3);
+        _data.s4 = fma(_input.s4, intel_sub_group_shuffle(weight_val, 4), _data.s4);
+        _data.s5 = fma(_input.s5, intel_sub_group_shuffle(weight_val, 5), _data.s5);
+        _data.s6 = fma(_input.s6, intel_sub_group_shuffle(weight_val, 6), _data.s6);
+        _data.s7 = fma(_input.s7, intel_sub_group_shuffle(weight_val, 7), _data.s7);
+        weight_offset += 8;
+    }
+    for(uint i = INPUT0_ELEMENTS_COUNT - (INPUT0_ELEMENTS_COUNT % 8); i < INPUT0_ELEMENTS_COUNT; i++)
+)__krnl"
+R"__krnl(    {
+        result += input[i * INPUT0_BATCH_NUM + batch_id] * weight[weight_offset++];
+    }
+    result += _data.s0 + _data.s1 + _data.s2 + _data.s3 +
+              _data.s4 + _data.s5 + _data.s6 + _data.s7;
+
+    output[outXIdx * INPUT0_BATCH_NUM + batch_id] = ACTIVATION(result, NL_M, NL_N);
+}
+
+)__krnl"},
+
+{"reorder_weights_image_winograd_6x3_s1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(reorder_weights_image_winograd_6x3_s1)(const __global INPUT0_TYPE* input, write_only image2d_t output)
+{
+    const uint input_tile_width = 1;
+    const uint input_tile_height = 3;
+    const uint in_tile_x_idx = get_global_id(1);
+    const uint in_tile_y_idx = get_global_id(0);
+
+    const uint output_tile_width = 8;
+    const uint output_tile_height = 1;
+
+    const uint tile_x_idx = get_global_id(0);
+    const uint tile_y_idx = get_global_id(1);
+    const uint feature_idx = get_global_id(2) % INPUT0_IFM_NUM;
+    const uint batch_idx = get_global_id(2) / INPUT0_IFM_NUM;
+
+    uint in_idx = batch_idx * INPUT0_OFM_PITCH
+        + feature_idx * INPUT0_IFM_PITCH
+        + in_tile_y_idx * input_tile_height * INPUT0_Y_PITCH
+        + in_tile_x_idx * input_tile_width * INPUT0_X_PITCH;
+
+    MAKE_VECTOR_TYPE(INPUT0_TYPE, 4) tile;
+    tile.x = input[in_idx]; in_idx += INPUT0_Y_PITCH;
+    tile.y = input[in_idx]; in_idx += INPUT0_Y_PITCH;
+    tile.z = input[in_idx];
+
+    const uint weightsOSplit = 16;
+    const uint oDivSplit = OUTPUT_OFM_NUM / 16;
+
+#if OUTPUT_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_FBXYB
+    const uint ySize = OUTPUT_OFM_NUM * OUTPUT_SIZE_X * OUTPUT_SIZE_Y;
+    uint idx = batch_idx % 16 +
+        tile_y_idx * output_tile_height * weightsOSplit +
+        tile_x_idx * output_tile_width * weightsOSplit * OUTPUT_SIZE_Y +
+        batch_idx / 16 * weightsOSplit * OUTPUT_SIZE_X * OUTPUT_SIZE_Y +
+        feature_idx * ySize;
+    uint idx_x = idx%ySize;
+    uint idx_y = idx/ySize;
+    const uint Stride = weightsOSplit * OUTPUT_SIZE_Y;
+
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+90.0 / 90 * tile.x)); idx_x += Stride; //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(-20.0 / 90 * tile.x - 20.0 / 90 * tile.y - 20.0 / 90 * tile.z)); idx_x += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(-20.0 / 90 * tile.x + 20.0 / 90 * tile.y - 20.0 / 90 * tile.z)); idx_x += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+)__krnl"
+R"__krnl(    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+1.0 / 90 * tile.x + 2.0 / 90 * tile.y + 4.0 / 90 * tile.z)); idx_x += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+1.0 / 90 * tile.x - 2.0 / 90 * tile.y + 4.0 / 90 * tile.z)); idx_x += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+64.0 / 90 * tile.x + 32.0 / 90 * tile.y + 16.0 / 90 * tile.z)); idx_x += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+64.0 / 90 * tile.x - 32.0 / 90 * tile.y + 16.0 / 90 * tile.z)); idx_x += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+90.0 / 90 * tile.z));
+
+
+#else // OUTPUT_LAYOUT_IMAGE_2D_WEIGHTS_WINOGRAD_6x3_S1_XFBYB
+    const uint ySize = OUTPUT_OFM_NUM * OUTPUT_SIZE_Y;
+    uint idx = batch_idx % 16 +
+        tile_y_idx * output_tile_height * weightsOSplit +
+        batch_idx / 16 * weightsOSplit * OUTPUT_SIZE_Y +
+        feature_idx * weightsOSplit * OUTPUT_SIZE_Y * oDivSplit +
+        tile_x_idx * output_tile_width * weightsOSplit * OUTPUT_SIZE_Y * oDivSplit * INPUT0_IFM_NUM;
+    uint idx_x = idx%ySize;
+    uint idx_y = idx/ySize;
+    const uint Stride = INPUT0_IFM_NUM;
+
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+90.0 / 90 * tile.x)); idx_y += Stride; //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(-20.0 / 90 * tile.x - 20.0 / 90 * tile.y - 20.0 / 90 * tile.z)); idx_y += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(-20.0 / 90 * tile.x + 20.0 / 90 * tile.y - 20.0 / 90 * tile.z)); idx_y += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+1.0 / 90 * tile.x + 2.0 / 90 * tile.y + 4.0 / 90 * tile.z)); idx_y += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+1.0 / 90 * tile.x - 2.0 / 90 * tile.y + 4.0 / 90 * tile.z)); idx_y += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+64.0 / 90 * tile.x + 32.0 / 90 * tile.y + 16.0 / 90 * tile.z)); idx_y += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+64.0 / 90 * tile.x - 32.0 / 90 * tile.y + 16.0 / 90 * tile.z)); idx_y += Stride;  //if (idx_x >= ySize) { idx_x = idx_x % ySize; idx_y++; }
+    write_imagef(output, (int2)(idx_x, idx_y), TO_OUTPUT_TYPE(+90.0 / 90 * tile.z));
+
+#endif
+
+
+}
+
+)__krnl"},
+
+{"deconvolution_gpu_bfyx_opt",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define WORK_GROUP_GROUP_SIZE 16
+
+__attribute__((reqd_work_group_size(WORK_GROUP_GROUP_SIZE, 1, 1)))
+KERNEL(deconvolution_gpu_bfyx_opt)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    const __global UNIT_TYPE* bias,
+#endif
+    uint split_idx
+#if FUSED_ELTWISE
+	, const __global UNIT_TYPE* fuse_input
+#endif
+	)
+{
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+
+    const uint b_f          = get_global_id(2);
+    const uint batch_offset = b_f / OUTPUT_FEATURE_NUM;
+    const uint ofm_offset   = b_f % OUTPUT_FEATURE_NUM;
+
+    const uint global_x_group    = get_group_id(0);
+    const uint global_y_group    = get_group_id(1);
+
+    const uint local_x        = get_local_id(0);
+    const uint local_y        = get_local_id(1);
+
+    const uint stride_x_id = global_x_group % STRIDE_SIZE_X;
+    const uint stride_y_id = global_y_group % STRIDE_SIZE_Y;
+
+    const uint id_x = (global_x_group / STRIDE_SIZE_X) * STRIDE_SIZE_X * WORK_GROUP_GROUP_SIZE + local_x * STRIDE_SIZE_X + stride_x_id;
+
+    if (id_x >= OUTPUT_SIZE_X)
+        return;
+
+    const uint id_y = (global_y_group / STRIDE_SIZE_Y) * STRIDE_SIZE_Y + local_y * STRIDE_SIZE_Y + stride_y_id;
+    const int in_x = (int)id_x + PADDING_SIZE_X - (FILTER_SIZE_X - 1);
+    const int in_y = (int)id_y + PADDING_SIZE_Y - (FILTER_SIZE_Y - 1);
+
+    const uint start_x = (STRIDE_SIZE_X - (in_x % STRIDE_SIZE_X)) % STRIDE_SIZE_X;
+    const uint start_y = (STRIDE_SIZE_Y - (in_y % STRIDE_SIZE_Y)) % STRIDE_SIZE_Y;
+)__krnl"
+R"__krnl(
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (ofm_offset / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint input_offset = INPUT0_OFFSET + batch_offset*INPUT0_BATCH_PITCH + in_split_offset;
+
+    for (uint i = start_y; i < FILTER_SIZE_Y; i+=STRIDE_SIZE_Y)
+    {
+        const int input_offset_y = in_y + i;
+        const bool zero_y = (input_offset_y >= INPUT0_SIZE_Y * STRIDE_SIZE_Y) || (input_offset_y < 0);
+
+        if(!zero_y)
+        {
+            for (uint j = start_x; j < FILTER_SIZE_X; j+=STRIDE_SIZE_X)
+            {
+                const int input_offset_x = in_x + j;
+                const bool zero_x = (input_offset_x >= INPUT0_SIZE_X * STRIDE_SIZE_X) || (input_offset_x < 0);
+
+                if(!zero_x)
+                {
+                    uint fixed_input_offset_x = (uint)input_offset_x / STRIDE_SIZE_X;
+                    uint fixed_input_offset_y = (uint)input_offset_y / STRIDE_SIZE_Y;
+                    uint input_idx = input_offset + (uint)fixed_input_offset_x*INPUT0_X_PITCH + (uint)fixed_input_offset_y*INPUT0_Y_PITCH;
+
+#if GRADIENT
+                    uint filter_idx = ofm_offset*FILTER_IFM_PITCH + (FILTER_SIZE_Y - i - 1)*FILTER_Y_PITCH + (FILTER_SIZE_X - j - 1)*FILTER_X_PITCH;
+                    for (uint h = 0; h < FILTER_OFM_NUM; h++)
+                    {
+                        result = fma(input[input_idx], filter[filter_idx], result);
+                        filter_idx += FILTER_OFM_PITCH;
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+#else
+                    uint filter_idx = ofm_offset*FILTER_OFM_PITCH + (FILTER_SIZE_Y - i - 1)*FILTER_Y_PITCH + (FILTER_SIZE_X - j - 1)*FILTER_X_PITCH;
+                    for (uint h = 0; h < FILTER_IFM_NUM; h++)
+                    {
+                        result = fma(input[input_idx], filter[filter_idx], result);
+                        filter_idx += FILTER_IFM_PITCH;
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+#endif
+                }
+            }
+        }
+    }
+#if BIAS_TERM
+    result += bias[ofm_offset];
+#endif
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * FILTER_OFM_NUM;
+    const uint dst_index = OUTPUT_OFFSET + out_split_offset + batch_offset*OUTPUT_BATCH_PITCH + ofm_offset*OUTPUT_FEATURE_PITCH + id_y*OUTPUT_Y_PITCH + id_x*OUTPUT_X_PITCH;
+#if FUSED_ELTWISE
+    const uint fused_index = INPUT1_OFFSET + split_idx * INPUT1_FEATURE_PITCH * FILTER_OFM_NUM + batch_offset*INPUT1_BATCH_PITCH + ofm_offset*INPUT1_FEATURE_PITCH + id_y*INPUT1_Y_PITCH + id_x*INPUT1_X_PITCH;
+#if !GRADIENT
+	output[dst_index] = ACTIVATION(result + fuse_input[fused_index], NL_M, NL_N);
+#else
+	output[dst_index] = result + fuse_input[fused_index];
+#endif
+#else
+    output[dst_index] = ACTIVATION(result, NL_M, NL_N);
+#endif
+}
+
+#undef ACTIVATION
+#undef WORK_GROUP_GROUP_SIZE
+
+)__krnl"},
+
+{"convolution_gpu_yxfb_yxio_b1_block_fp32",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((reqd_work_group_size(LOCAL_WORK_GROUP_SIZE, 1, 1)))
+KERNEL(convolution_gpu_yxfb_yxio_b1_block)(
+    const __global float* input,
+    __global float* output,
+    const __global float* filter,
+#if BIAS_TERM
+    const __global float* bias,
+#endif
+    uint split_idx)
+{
+#ifdef USE_VECTOR_8
+    #define VECTOR_FLOAT float8
+    #define BLOCK_READ(IN) as_float8(intel_sub_group_block_read8((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write8((__global uint*)OUT, as_uint8(DATA));
+#endif
+#ifdef USE_VECTOR_4
+    #define VECTOR_FLOAT float4
+    #define BLOCK_READ(IN) as_float4(intel_sub_group_block_read4((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write4((__global uint*)OUT, as_uint4(DATA));
+#endif
+#ifdef USE_VECTOR_2
+    #define VECTOR_FLOAT float2
+    #define BLOCK_READ(IN) as_float2(intel_sub_group_block_read2((const __global uint*)IN))
+    #define BLOCK_WRITE(OUT, DATA) intel_sub_group_block_write2((__global uint*)OUT, as_uint2(DATA));
+#endif
+
+    const uint batch_num = INPUT0_BATCH_NUM;
+    const uint linear_id_xy = get_group_id(1) + get_global_size(1) * get_group_id(2);
+    uint global_id = (((uint)get_group_id(0) * LOCAL_WORK_GROUP_SIZE) / batch_num) * batch_num + (linear_id_xy * FILTER_ARRAY_NUM + split_idx) * (FILTER_OFM_NUM / OFM_PER_WORK_ITEM) * batch_num;
+
+    const uint out_batch_id = (uint)get_local_id(0) % INPUT0_BATCH_NUM;
+    const uint out_x = get_group_id(1);
+    const uint out_y = get_group_id(2);
+
+    const uint out_id = (global_id / batch_num) * OFM_PER_WORK_ITEM * batch_num + out_batch_id;
+
+    const uint ofm_offset = (global_id * (OFM_PER_WORK_ITEM / batch_num)) % FILTER_OFM_NUM;
+
+    const uint sub_group_id = (uint)get_local_id(0) % INPUT0_BATCH_NUM;
+
+    VECTOR_FLOAT _data0 = 0.f;
+
+)__krnl"
+R"__krnl(    const int x = (int)out_x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int y = (int)out_y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i * DILATION_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j * DILATION_SIZE_X;
+                const bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                if(!zero)
+                {
+                    uint input_idx = input_offset_x*INPUT0_X_PITCH + input_offset_y*INPUT0_Y_PITCH;
+                    input_idx += INPUT0_OFFSET + split_idx * FILTER_IFM_NUM * INPUT0_FEATURE_PITCH;
+                    input_idx += out_batch_id;
+
+                    uint filter_idx = ofm_offset + sub_group_id + i*FILTER_Y_PITCH + j*FILTER_X_PITCH;
+
+#if INPUT0_BATCH_NUM == 1
+                    for(uint h = 0; h < FILTER_IFM_NUM / 8; h++)
+                    {
+                        float _in = as_float(intel_sub_group_block_read((const __global uint*)input + input_idx));
+                        float8 _input = TRANSPOSE_BLOCK_8(_in);
+
+                        VECTOR_FLOAT _filter;
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s0, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s1, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s2, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s3, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s4, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s5, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s6, _filter, _data0);
+
+                        _filter = BLOCK_READ(filter + filter_idx); filter_idx += FILTER_OFM_NUM;
+                        _data0 = mad(_input.s7, _filter, _data0);
+
+                        input_idx += 8 * INPUT0_FEATURE_PITCH;
+                    }
+                    for (uint h = FILTER_IFM_NUM - (FILTER_IFM_NUM % 8); h < FILTER_IFM_NUM; h++)
+#else
+                    for (uint h = 0; h < FILTER_IFM_NUM; h++)
+#endif
+                    {
+                        VECTOR_FLOAT _filter = BLOCK_READ(filter + filter_idx);
+                        _data0 = mad(input[input_idx], _filter, _data0);
+                        filter_idx += FILTER_IFM_PITCH;
+                        input_idx += INPUT0_FEATURE_PITCH;
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+    _data0 += BLOCK_READ(bias + ofm_offset);
+#endif
+    _data0 = ACTIVATION(_data0, NL_M, NL_N);
+
+    uint _out_id = OUTPUT_OFFSET + out_id;
+    BLOCK_WRITE(output + _out_id, _data0);
+#if defined(USE_VECTOR_8) || defined(USE_VECTOR_4) || defined(USE_VECTOR_2)
+    #undef VECTOR_FLOAT
+    #undef BLOCK_READ
+    #undef BLOCK_WRITE
+#endif
+}
+
+)__krnl"},
+
+{"embed_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+KERNEL(embed_ref)(const __global UNIT_TYPE* input0, __global UNIT_TYPE* output, __global UNIT_TYPE* weights, __global UNIT_TYPE* biases)
+{
+    const uint x = (uint)get_global_id(0);
+	const uint y = (uint)get_global_id(1);
+	const uint b = (uint)get_global_id(2);
+	uint output_idx = (b*INPUT0_ELEMENTS_COUNT*NUM_OUTPUT_SIZE)+(uint)(x*NUM_OUTPUT_SIZE+y);
+    output[output_idx] = weights[(uint)(input0[(b*INPUT0_ELEMENTS_COUNT)+x]*NUM_OUTPUT_SIZE+y)] + biases[y];
+}
+
+
+)__krnl"},
+
+{"reorder_weights",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+
+///////////////////////// Input Index /////////////////////////
+inline uint FUNC(get_input_index)(uint o, uint i, uint y, uint x)
+{
+#if   INPUT0_SIMPLE
+    return GET_FILTER_INDEX(INPUT0, o, i, y, x);
+#elif defined INPUT0_LAYOUT_OS_IYX_OSV16    || \
+      defined INPUT0_LAYOUT_OS_I_OSV16      || \
+      defined INPUT0_LAYOUT_OS_I_OSV8__AI8  || \
+      defined INPUT0_LAYOUT_OS_I_OSV16__AI8
+    return GET_FILTER_OS_IYX_OSV8_INDEX(INPUT0, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_OS_IYX_OSV16_ROTATE_180
+    return GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(INPUT0, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_I_YXS_OS_YXSV2_OSV16
+    return GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(INPUT0, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_IY_XS_OS_XSV2_OSV16__AO32 || defined OUTPUT_LAYOUT_IY_XS_OS_XSV2_OSV8__AO32
+    return GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(INPUT0, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_IMAGE_2D_WEIGHTS_C1_B_FYX
+    #error - not supported yet
+#elif defined INPUT0_LAYOUT_OS_IS_YX_ISA8_OSV8_ISV4
+	return GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(INPUT0, o, i, y, x);
+#elif defined INPUT0_LAYOUT_IS_O_YX_ISV32
+    return GET_FILTER_IS_O_YX_ISV32(INPUT0, o, i, y, x);
+#else
+#error reorder_weights.cl: input format - not supported
+#endif
+}
+
+///////////////////////// Output Index /////////////////////////
+
+inline uint FUNC(get_output_index)(uint o, uint i, uint y, uint x)
+{
+#if   OUTPUT_SIMPLE
+    return GET_FILTER_INDEX(OUTPUT, o, i, y, x);
+#elif defined OUTPUT_LAYOUT_OS_IYX_OSV16    || \
+      defined OUTPUT_LAYOUT_OS_I_OSV16      || \
+      defined OUTPUT_LAYOUT_OS_I_OSV8__AI8  || \
+      defined OUTPUT_LAYOUT_OS_I_OSV16__AI8
+    return GET_FILTER_OS_IYX_OSV8_INDEX(OUTPUT, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined OUTPUT_LAYOUT_OS_IYX_OSV16_ROTATE_180
+    return GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(OUTPUT, o, i, y, x, SUB_GROUP_SIZE);
+)__krnl"
+R"__krnl(#elif defined OUTPUT_LAYOUT_I_YXS_OS_YXSV2_OSV16
+    return GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(OUTPUT, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined OUTPUT_LAYOUT_IY_XS_OS_XSV2_OSV16__AO32 || defined OUTPUT_LAYOUT_IY_XS_OS_XSV2_OSV8__AO32
+    return GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(OUTPUT, o, i, y, x, SUB_GROUP_SIZE);
+#elif defined OUTPUT_LAYOUT_IMAGE_2D_WEIGHTS_C1_B_FYX
+    return 0; //will not be used for images
+#elif defined OUTPUT_LAYOUT_OS_IS_YX_ISA8_OSV8_ISV4
+	return GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(OUTPUT, o, i, y, x);
+#elif defined OUTPUT_LAYOUT_IS_O_YX_ISV32
+    return GET_FILTER_IS_O_YX_ISV32(OUTPUT, o, i, y, x);
+#else
+#error reorder_weights.cl: output format - not supported
+#endif
+}
+
+#if OUTPUT_LAYOUT_IMAGE_2D_WEIGHTS_C1_B_FYX
+KERNEL (reorder_weights)(const __global INPUT0_TYPE* input, write_only image2d_t output)
+{
+    const unsigned o = get_global_id(0);
+    const unsigned iyx = get_global_id(1);
+    const unsigned x = iyx % INPUT0_SIZE_X;
+    const unsigned y = (iyx / INPUT0_SIZE_X) % INPUT0_SIZE_Y;
+    const unsigned i = (iyx / INPUT0_SIZE_X) / INPUT0_SIZE_Y;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 4) input_val = (MAKE_VECTOR_TYPE(UNIT_TYPE, 4))(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    const int2 coord = (int2)(o, iyx);
+    uint4 ir = FUNC_CALL(reshape_dims)(o,i,y,x, OUTPUT_SIZE_Y, OUTPUT_SIZE_X, INPUT0_SIZE_Y, INPUT0_SIZE_X, OUTPUT_DIMS, INPUT0_DIMS);
+    input_val.s0 = TO_OUTPUT_TYPE(input[FUNC_CALL(get_input_index)(ir[0],ir[1],ir[2],ir[3])]);
+    IMAGE_WRITE(output, coord, input_val);
+}
+#else
+KERNEL (reorder_weights)(const __global INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+    const unsigned o = get_global_id(0);
+    const unsigned i = get_global_id(1);
+#if   OUTPUT_DIMS == 2
+    const unsigned y = 0;
+    const unsigned x = 0;
+#elif OUTPUT_DIMS == 4
+    const unsigned y = get_global_id(2) / INPUT0_SIZE_X;
+    const unsigned x = get_global_id(2) % INPUT0_SIZE_X;
+#endif
+    uint4 ir = FUNC_CALL(reshape_dims)(o,i,y,x, OUTPUT_SIZE_Y, OUTPUT_SIZE_X, INPUT0_SIZE_Y, INPUT0_SIZE_X, OUTPUT_DIMS, INPUT0_DIMS);
+    output[FUNC_CALL(get_output_index)(o, i, y, x)] = TO_OUTPUT_TYPE(input[FUNC_CALL(get_input_index)(ir[0],ir[1],ir[2],ir[3])]);
+}
+#endif
+
+)__krnl"},
+
+{"average_unpooling_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(average_unpooling_gpu)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output)
+{
+#if OUTPUT_LAYOUT_BFYX  || OUTPUT_LAYOUT_BYXF
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf % INPUT0_FEATURE_NUM;
+    const uint b    = bf / INPUT0_FEATURE_NUM;
+#elif OUTPUT_LAYOUT_YXFB
+    const uint x    = (uint)get_global_id(1);
+    const uint y    = (uint)get_global_id(2);
+    const uint bf   = (uint)get_global_id(0);
+    const uint f    = bf / INPUT0_BATCH_NUM;
+    const uint b    = bf % INPUT0_BATCH_NUM;
+#endif
+
+    if (x >= INPUT0_SIZE_X)
+    {
+        return;
+    }
+
+    const uint x_begin = x * STRIDE_SIZE_X;
+    const uint y_begin = y * STRIDE_SIZE_Y;
+    const uint x_end = min((uint)(x_begin + UNPOOL_SIZE_X), (uint)(OUTPUT_SIZE_X));
+    const uint y_end = min((uint)(y_begin + UNPOOL_SIZE_Y), (uint)(OUTPUT_SIZE_Y));
+
+    const uint window_x = x_end - x_begin;
+    const uint window_y = y_end - y_begin;
+
+    const uint input_offset = GET_DATA_INDEX(INPUT0, b, f, y, x);
+    uint out_index = GET_DATA_INDEX(OUTPUT, b, f, y_begin, x_begin);
+    UNIT_TYPE out_val = input[input_offset] / (window_x * window_y);
+
+    for(uint j = 0; j < window_y; j++)
+    {
+        for(uint i = 0; i < window_x; i++)
+        {
+            output[out_index] += ACTIVATION(out_val, NL_M ,NL_N);
+            out_index += OUTPUT_X_PITCH;
+        }
+        out_index += OUTPUT_Y_PITCH - window_x * OUTPUT_X_PITCH;
+)__krnl"
+R"__krnl(    }
+}
+
+)__krnl"},
+
+{"reorder_data",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+)__krnl"
+R"__krnl(        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+///////////////////////// Input Index /////////////////////////
+inline uint FUNC(get_input_index)(uint b, uint f, uint y, uint x)
+{
+)__krnl"
+R"__krnl(#if   INPUT0_SIMPLE
+    return GET_DATA_INDEX(INPUT0, b, f, y, x);
+#elif defined INPUT0_LAYOUT_BS_F_BSV8__AF8  || \
+      defined INPUT0_LAYOUT_BS_F_BSV16__AF8
+    return GET_DATA_BS_FYX_BSV8_INDEX(INPUT0, b, f, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_BF8_XY16
+    return GET_DATA_BF8_XY16_INDEX(INPUT0, b, f, y, x);
+#elif defined INPUT0_LAYOUT_BYXF_AF32
+	return GET_DATA_BYXF_AF32_INDEX(INPUT0, b, f, y, x);
+#elif defined INPUT0_LAYOUT_FS_BS_YX_BSV4_FSV32
+    return GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(INPUT0, b, f, y, x);
+#else
+#error reorder_data.cl: input format - not supported
+#endif
+}
+
+///////////////////////// Output Index /////////////////////////
+
+inline uint FUNC(get_output_index)(uint b, uint f, uint y, uint x)
+{
+#if   OUTPUT_SIMPLE
+    return GET_DATA_INDEX(OUTPUT, b, f, y, x);
+#elif defined OUTPUT_LAYOUT_BS_F_BSV8__AF8  || \
+      defined OUTPUT_LAYOUT_BS_F_BSV16__AF8
+    return GET_DATA_BS_FYX_BSV8_INDEX(OUTPUT, b, f, y, x, SUB_GROUP_SIZE);
+#elif defined OUTPUT_LAYOUT_BF8_XY16
+    return GET_DATA_BF8_XY16_INDEX(OUTPUT, b, f, y, x);
+#elif defined OUTPUT_LAYOUT_BYXF_AF32
+	return GET_DATA_BYXF_AF32_INDEX(OUTPUT, b, f, y, x);
+#elif defined OUTPUT_LAYOUT_FS_BS_YX_BSV4_FSV32
+    return GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(OUTPUT, b, f, y, x);
+#else
+#error reorder_data.cl: output format - not supported
+#endif
+}
+
+KERNEL (reorder_data)(
+    const __global INPUT_REORDER_TYPE* input,
+    __global OUTPUT_REORDER_TYPE* output
+#ifdef MEAN_SUBTRACT_IN_BUFFER
+    , __global MEAN_SUBTRACT_TYPE* mean_subtract
+#endif
+    )
+{
+    const uint b = get_global_id(GWS_BATCH);
+    const uint f = get_global_id(GWS_FEATURE);
+#if   INPUT0_DIMS == 2
+    const uint y = 0;
+    const uint x = 0;
+#elif INPUT0_DIMS == 4
+    const uint y = ((uint)(get_global_id(GWS_YX))) / INPUT0_SIZE_X;
+    const uint x = ((uint)(get_global_id(GWS_YX))) % INPUT0_SIZE_X;
+#endif
+
+    uint4 ov = FUNC_CALL(reshape_dims)(b,f,y,x, INPUT0_SIZE_Y, INPUT0_SIZE_X, OUTPUT_SIZE_Y, OUTPUT_SIZE_X, INPUT0_DIMS, OUTPUT_DIMS);
+    const uint input_idx  = FUNC_CALL(get_input_index)(b, f, y, x);
+    const uint output_idx = FUNC_CALL(get_output_index)(ov[0],ov[1],ov[2],ov[3]);
+
+#if defined MEAN_SUBTRACT_INSIDE_PARAMS
+    float res = TO_MEAN_TYPE(input[input_idx]);
+    res = MEAN_OP(res, VALUE_TO_SUBTRACT[f % VALUE_TO_SUBTRACT_SIZE]);
+#elif defined MEAN_SUBTRACT_IN_BUFFER
+#if defined MEAN_PER_FEATURE
+    MEAN_SUBTRACT_TYPE res = TO_MEAN_TYPE(input[input_idx]);
+    res = MEAN_OP(res, mean_subtract[f]);
+#else
+    MEAN_SUBTRACT_TYPE res = TO_MEAN_TYPE(input[input_idx]);
+    uint4 msv = FUNC_CALL(reshape_dims)(b,f,y,x, INPUT0_SIZE_Y, INPUT0_SIZE_X, MEAN_SUBTRACT_SIZE_Y, MEAN_SUBTRACT_SIZE_X, INPUT0_DIMS, MEAN_SUBTRACT_DIMS);
+    res = MEAN_OP(res, mean_subtract[GET_DATA_INDEX_SAFE(MEAN_SUBTRACT, msv[0], msv[1], msv[2], msv[3])]);
+#endif
+#else
+    CALC_TYPE res = TO_CALC_TYPE(input[input_idx]);
+#endif
+
+    output[output_idx] = ACTIVATION(TO_OUTPUT_REORDER_TYPE(res), NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"convolution_gpu_yxfb_yxio_b16_fp16",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+__attribute__((intel_reqd_sub_group_size(16)))
+__attribute__((reqd_work_group_size(16, 1, 1)))
+KERNEL(convolution_gpu_yxfb_yxio_b16)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* filter,
+#if BIAS_TERM
+    const __global UNIT_TYPE* bias,
+#endif
+    uint split_idx)
+{
+    // get_global_size(0) -> Number of work items needed to compute all features and all batches for single output spatial position
+    //                       (single (x, y) point in output).
+    // get_global_size(1) -> Output size in X-dimension.
+    // get_global_size(2) -> Output size in Y-dimension.
+    // get_global_id(0)   -> Id of work item computing single spatial point of output indicated by get_global_id(1), get_global_id(2).
+    // get_global_id(1)   -> Current x-position in output.
+    // get_global_id(2)   -> Current y-position in output.
+    //
+    // WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS -> Number of work items needed to compute entire one batch for at least one feature and one spatial point.
+    //                                           (this number in current implementation computes also OFM_PER_WORK_ITEM output features at the same time).
+    // FILTER_ARRAY_NUM                       -> Number of filters groups (split size).
+
+    const uint out_x = get_global_id(1);
+    const uint out_y = get_global_id(2);
+
+    const uint output_f_size = OUTPUT_PAD_BEFORE_FEATURE_NUM + OUTPUT_FEATURE_NUM + OUTPUT_PAD_AFTER_FEATURE_NUM;
+    const uint output_x_size = OUTPUT_PAD_BEFORE_SIZE_X + OUTPUT_SIZE_X + OUTPUT_PAD_AFTER_SIZE_X;
+    const uint linear_id_xy = OUTPUT_PAD_BEFORE_SIZE_X + out_x + output_x_size * (out_y + OUTPUT_PAD_BEFORE_SIZE_Y);
+    uint global_id = (((uint)get_global_id(0) / WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS) + (linear_id_xy * FILTER_ARRAY_NUM + split_idx) * (output_f_size / OFM_PER_WORK_ITEM)) * WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS;
+
+    const uint sub_group_id = get_local_id(0);
+
+#if defined(USE_BLOCK_READ_2) || defined(USE_BLOCK_READ_1)
+    const uint chunk_size = sizeof(uint)/sizeof(UNIT_TYPE);
+#else
+    const uint chunk_size = 1;
+#endif
+
+    const uint out_batch_id = chunk_size * sub_group_id + LOCAL_WORK_GROUP_SIZE * BATCHES_PER_WORK_ITEM * ((uint)get_group_id(0) % LOCAL_WORK_GROUPS_PER_SINGLE_BATCHES_ELEMENTS);
+
+)__krnl"
+R"__krnl(    const uint out_id = (global_id / WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS) * OFM_PER_WORK_ITEM * OUTPUT_FEATURE_PITCH + OUTPUT_PAD_BEFORE_FEATURE_NUM * OUTPUT_FEATURE_PITCH + OUTPUT_PAD_BEFORE_BATCH_NUM + out_batch_id;
+
+    const uint ofm_offset = ((global_id * OFM_PER_WORK_ITEM) / WORK_ITEMS_PER_SINGLE_BATCHES_ELEMENTS) % output_f_size;
+
+    // Each component of vector element contains computation for separate output feature.
+    half16 _data[BATCHES_PER_WORK_ITEM];
+    for(uint i = 0; i < BATCHES_PER_WORK_ITEM; i++)
+    {
+        _data[i] = UNIT_VAL_ZERO;
+    }
+
+    const int x = (int)out_x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int y = (int)out_y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    for (uint i = 0; i < FILTER_SIZE_Y; i++)
+    {
+        const int input_offset_y = y + i * DILATION_SIZE_Y;
+        const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+        if(!zero_y)
+        {
+            for (uint j = 0; j < FILTER_SIZE_X; j++)
+            {
+                const int input_offset_x = x + j * DILATION_SIZE_X;
+                const bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                if(!zero)
+                {
+                    uint input_idx = input_offset_x*INPUT0_X_PITCH + input_offset_y*INPUT0_Y_PITCH;
+                    input_idx += INPUT0_OFFSET + split_idx * FILTER_IFM_NUM * INPUT0_FEATURE_PITCH;
+                    input_idx += out_batch_id;
+
+                    //sub_group_id used as offset to make each workitem load different filter, and then shuffle it
+                    // 2 * sub_group_id is used because we group 2 halfs as one uint element.
+                    uint filter_idx = ofm_offset + 2*sub_group_id + i*FILTER_Y_PITCH + j*FILTER_X_PITCH;
+
+                    for (uint h = 0; h < FILTER_IFM_NUM; h++)
+                    {
+#if defined(USE_BLOCK_READ_2)
+                        half4 _input = as_half4(intel_sub_group_block_read2((const __global uint*)(input + input_idx)));
+                        uint filter_val_pair = *(const __global uint*)(filter + filter_idx);
+                        half16 filter_transp = TRANSPOSE_BLOCK_16_FP16(filter_val_pair);
+                        _data[0] = fma(_input.s0, filter_transp, _data[0]);
+                        _data[1] = fma(_input.s1, filter_transp, _data[1]);
+                        _data[2] = fma(_input.s2, filter_transp, _data[2]);
+                        _data[3] = fma(_input.s3, filter_transp, _data[3]);
+                        input_idx += INPUT0_FEATURE_PITCH;
+#elif defined(USE_BLOCK_READ_1)
+                        half2 _input = as_half2(intel_sub_group_block_read((const __global uint*)(input + input_idx)));
+                        uint filter_val_pair = *(const __global uint*)(filter + filter_idx);
+                        half16 filter_transp = TRANSPOSE_BLOCK_16_FP16(filter_val_pair);
+                        _data[0] = fma(_input.s0, filter_transp, _data[0]);
+                        _data[1] = fma(_input.s1, filter_transp, _data[1]);
+                        input_idx += INPUT0_FEATURE_PITCH;
+#else
+                        uint filter_val_pair = *(const __global uint*)(filter + filter_idx);
+                        half16 filter_transp = TRANSPOSE_BLOCK_16_FP16(filter_val_pair);
+                        for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+                        {
+                            _data[s] = fma(input[input_idx], filter_transp, _data[s]);
+                            input_idx += LOCAL_WORK_GROUP_SIZE;
+                        }
+                        input_idx += INPUT0_FEATURE_PITCH - BATCHES_PER_WORK_ITEM * LOCAL_WORK_GROUP_SIZE;
+#endif
+                        filter_idx += FILTER_IFM_PITCH;
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+    uint bias_val_pair = *(const __global uint*)(bias + (ofm_offset + 2 * sub_group_id));
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+    {
+        ADD_BIAS_16_FP16(_data[s], bias_val_pair);
+    }
+#endif
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+    {
+        _data[s] = ACTIVATION(_data[s], NL_M, NL_N);
+    }
+
+#if defined(USE_BLOCK_READ_2) || defined(USE_BLOCK_READ_1)
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM / 2; s++)
+    {
+        uint _out_id = OUTPUT_VIEW_OFFSET + out_id + chunk_size * s * LOCAL_WORK_GROUP_SIZE;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s0, _data[chunk_size * s + 1].s0)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s1, _data[chunk_size * s + 1].s1)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s2, _data[chunk_size * s + 1].s2)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s3, _data[chunk_size * s + 1].s3)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s4, _data[chunk_size * s + 1].s4)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s5, _data[chunk_size * s + 1].s5)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s6, _data[chunk_size * s + 1].s6)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s7, _data[chunk_size * s + 1].s7)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s8, _data[chunk_size * s + 1].s8)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].s9, _data[chunk_size * s + 1].s9)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].sa, _data[chunk_size * s + 1].sa)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].sb, _data[chunk_size * s + 1].sb)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].sc, _data[chunk_size * s + 1].sc)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].sd, _data[chunk_size * s + 1].sd)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].se, _data[chunk_size * s + 1].se)); _out_id += OUTPUT_FEATURE_PITCH;
+        *(__global uint*)(output + _out_id) = as_uint((half2)(_data[chunk_size * s].sf, _data[chunk_size * s + 1].sf)); _out_id += OUTPUT_FEATURE_PITCH;
+    }
+#else
+    for(uint s = 0; s < BATCHES_PER_WORK_ITEM; s++)
+    {
+        uint _out_id = OUTPUT_VIEW_OFFSET + out_id + s * LOCAL_WORK_GROUP_SIZE;
+        output[_out_id] = _data[s].s0; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s1; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s2; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s3; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s4; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s5; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s6; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s7; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s8; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].s9; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].sa; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].sb; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].sc; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].sd; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].se; _out_id += OUTPUT_FEATURE_PITCH;
+        output[_out_id] = _data[s].sf; _out_id += OUTPUT_FEATURE_PITCH;
+    }
+#endif
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_yxfb_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// Required JIT constants:
+//  - FP16_SUPPORTED       - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED       - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE            - Type of unit of input/output/weights/biases.
+//  - UNIT_VAL_ZERO        - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT_BATCH_NUM      - [int] Number of elements from single spatial and single feature that are grouped in single batch in input.
+//  - INPUT_ELEMENTS_COUNT - [int] Cumulative number of elements from input that are processed in single batch.
+//  - FILTER_OFM_NUM       - [int] Cumulative number of elements that are outputted in single batch.
+//  - RELU                 - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE       - [float] Factor for negative output values (required when ReLU is specified).
+
+KERNEL (fully_connected_gpu_yxfn)(
+    const __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    const __global FILTER_TYPE* weights
+#if BIAS_TERM
+    , const __global BIAS_TYPE* biases
+#endif
+    )
+{
+    const uint x = get_global_id(0);
+    const uint batch_id = x % INPUT0_BATCH_NUM;
+    const uint neuronIdx = x / INPUT0_BATCH_NUM;
+
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+
+    uint weight_offset = neuronIdx * FILTER_OFM_PITCH;
+    for (uint k = 0; k < INPUT0_FEATURE_NUM; k++)
+    {
+        for (uint j = 0; j < INPUT0_SIZE_Y; j++)
+        {
+            for(uint i = 0; i < INPUT0_SIZE_X; i++)
+            {
+                uint4 widx = FUNC(reshape_dims)(batch_id, k,j,i, INPUT0_SIZE_Y, INPUT0_SIZE_X, FILTER_SIZE_Y, FILTER_SIZE_X, INPUT0_DIMS, FILTER_DIMS);
+                uint weight_idx = weight_offset + widx[1]*FILTER_IFM_PITCH + widx[2]*FILTER_Y_PITCH + widx[3]*FILTER_X_PITCH;
+                uint input_idx = INPUT0_OFFSET + k*INPUT0_FEATURE_PITCH + j*INPUT0_Y_PITCH + i*INPUT0_X_PITCH + batch_id*INPUT0_BATCH_PITCH;
+                result += input[input_idx] * weights[weight_idx];
+            }
+        }
+    }
+    const uint output_idx = OUTPUT_OFFSET + batch_id*OUTPUT_BATCH_PITCH + neuronIdx*OUTPUT_FEATURE_PITCH;
+)__krnl"
+R"__krnl(
+#if BIAS_TERM
+    result += biases[neuronIdx];
+#endif
+    output[output_idx] = ACTIVATION(result, NL_M, NL_N);
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_fb_oi_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// Required JIT constants:
+//  - FP16_SUPPORTED       - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED       - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE            - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO        - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT_BATCH_NUM      - [int] Number of elements from single spatial and single feature that are grouped in single batch in input.
+//  - INPUT_ELEMENTS_COUNT - [int] Cumulative number of elements from input that are processed in single batch.
+//  - FILTER_OFM_NUM       - [int] Cumulative number of elements that are outputted in single batch.
+//  - RELU                 - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE       - [float] Factor for negative output values (required when ReLU is specified).
+
+
+KERNEL (fully_connected_gpu_xb_bx)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint x = get_global_id(0);
+    const uint batch_id = x % INPUT0_BATCH_NUM;
+    const uint outXIdx = x / INPUT0_BATCH_NUM;
+    UNIT_TYPE result = UNIT_VAL_ZERO;
+    uint weight_offset = outXIdx * FILTER_OFM_PITCH;
+    uint input_offset = INPUT0_OFFSET + batch_id*INPUT0_BATCH_PITCH;
+
+    for (uint i = 0; i < INPUT0_ELEMENTS_COUNT; i++)
+    {
+        result += input[input_offset] * weight[weight_offset];
+        input_offset += INPUT0_BATCH_NUM;
+        weight_offset++;
+    }
+#if BIAS_TERM
+    result += bias[outXIdx];
+#endif
+    output[x] = ACTIVATION(result, NL_M, NL_N);
+}
+
+)__krnl"},
+
+{"convolution_gpu_winograd_2x3_s1_fused",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// --------------------------------------------------------------------------------------------------------------------------------
+// L3_SIMD_4x8
+// Input matrices dimensions: M x K x N
+// Output matrix dimensions: M x N
+// --------------------------------------------------------------------------------------------------------------------------------
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#define DOT4i0( _result, _A, _B, i)					\
+    {	\
+	_result = mad(_A.s0, intel_sub_group_shuffle( _B.s0, (i)), _result);	\
+    }
+
+#define DOT4i1( _result, _A, _B, i)					\
+    {	\
+	_result = mad(_A.s1, intel_sub_group_shuffle( _B.s1, (i)), _result);	\
+    }
+
+#define DOT4i2( _result, _A, _B, i)					\
+    {	\
+	_result = mad(_A.s2, intel_sub_group_shuffle( _B.s2, (i)), _result);	\
+    }
+
+#define DOT4i3( _result, _A, _B, i)					\
+    {	\
+	_result = mad(_A.s3, intel_sub_group_shuffle( _B.s3, (i)), _result);	\
+    }
+
+#define UNIT_TYPE_2 CAT(UNIT_TYPE, 2)
+#define UNIT_TYPE_4 CAT(UNIT_TYPE, 4)
+#define UNIT_TYPE_8 CAT(UNIT_TYPE, 8)
+
+__attribute__((reqd_work_group_size(8, 2, 8)))
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_gpu_winograd_2x3_s1_fused)
+(
+    __global INPUT0_TYPE* I,
+    __global OUTPUT_TYPE* O,
+    __global FILTER_TYPE* U,
+#if BIAS_TERM
+    const __global UNIT_TYPE * bias,
+#endif
+    uint split_idx)
+{
+    //               (DxC2)x(UxWx8c)
+	const uint slmSize = (4*2)*(2*16*2);
+    __local UNIT_TYPE_4 V[slmSize*2]; // 8 KB
+
+    /* These constants are defined as precompiler macros during compilation. */
+     const uint WC = W*INPUT0_FEATURE_NUM;
+	 const uint HW = H*W;
+     const uint HWC = H*WC;
+     const uint WC4 = WC >> 2;
+     const uint K16 = FILTER_OFM_NUM >> 4;
+     const uint C4 = INPUT0_FEATURE_NUM >> 2;
+     const uint K2 = FILTER_OFM_NUM >> 1;
+     const uint QK2 = Q*K2;
+     const uint QK = Q*FILTER_OFM_NUM;
+     const uint PQK = P*QK;
+
+	const uint upperHalf = get_local_id(1);
+    uint gx = get_group_id(0);
+    uint gy = get_group_id(1)*2+(get_group_id(2)%2);
+    uint gz = (get_group_id(2)/2)*2+ upperHalf;
+    uint gk = gz % K16;
+    uint gn = gz / K16;
+
+	#define lx get_local_id(0)
+	#define lz get_local_id(2)
+
+    uint lxd4 = lx >> 2;
+    uint lxm4 = lx % 4;
+
+    uint lzd4 = lz >> 2;
+    uint lzm4 = lz % 4;
+
+    // Load 16x6 input tile, with 2 pixel overlap in X and y.
+    // Compute 14x4 output tile.
+    // Load 8 filters / thread.
+    // 8 threads total: 2 filters x 4 winograd components. 16 filters total.
+    int x = gx*14 + lz*2 + lxd4 - px;
+    int y = gy*4 - py;
+    uint k = gk*16 + lzd4*8;
+
+    // #                                  x->
+    // #     M0    M1    M2    M3    M4    M5    M6
+    // #   +------------------------------------------
+    // # u | 0 1 | 0 1 | 0 1 | 0 1 | 0 1 | 0 1 | 0 1 |
+    // # | | 2 3 | 2 3 | 2 3 | 2 3 | 2 3 | 2 3 | 2 3 |
+    // # v
+    // #
+
+    UNIT_TYPE_4 M0 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    UNIT_TYPE_4 M1 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    UNIT_TYPE_4 M2 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    UNIT_TYPE_4 M3 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    UNIT_TYPE_4 M4 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    UNIT_TYPE_4 M5 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+    UNIT_TYPE_4 M6 = (UNIT_TYPE_4)(UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO, UNIT_VAL_ZERO);
+
+#if INPUT0_LAYOUT_BYXF
+    uint adr = gn*HWC + ((uint)y)*WC + ((uint)x)*INPUT0_FEATURE_NUM;
+    const __global UNIT_TYPE_4 *I_load = ((const __global UNIT_TYPE_4*)&(I[adr]));
+#else
+	const __global UNIT_TYPE *I_load = (const __global UNIT_TYPE*)&I[gn*HWC + ((uint)y)*W + ((uint)x)];
+#endif
+
+
+    uint lxm2 = lx % 2;
+    uint lxb1 = (lx & 2)/2;
+
+    uint2 coordU0;
+    coordU0.x = (lzm4*24 + k*12);
+    coordU0.y = 0;
+
+	uint slmPipeStage = 0;
+
+    __attribute__((opencl_unroll_hint(1)))
+    for (uint c = lxm4; c < C4_up16; c += 4) {
+
+		__local UNIT_TYPE_4 *V_write = &V[lxb1 * 256 + lz * 4 + lxd4 * 2 + lxm2 + slmSize*slmPipeStage];
+		__local const UNIT_TYPE_4 *V_read = &V[lzm4 * 64 + lx + slmSize*slmPipeStage];
+		slmPipeStage = (slmPipeStage+1)%2;
+
+        // 2*14 * 3 * 16 = 1344 MADs
+
+        // Transform HxW x C        -> DxUxW x C
+        //           6x16x16 inputs -> 4x2x16x16 winograd components.
+		if (!upperHalf)
+        {
+			bool x_in =  0 <= x && x < W;
+			bool y0_in = 0 <= (y + 0) && (y + 0) < H && x_in;
+			bool y1_in = 0 <= (y + 1) && (y + 1) < H && x_in;
+			bool y2_in = 0 <= (y + 2) && (y + 2) < H && x_in;
+			bool y3_in = 0 <= (y + 3) && (y + 3) < H && x_in;
+			bool y4_in = 0 <= (y + 4) && (y + 4) < H && x_in;
+			bool y5_in = 0 <= (y + 5) && (y + 5) < H && x_in;
+
+			#if INPUT0_LAYOUT_BYXF
+
+/*				const  UNIT_TYPE_4 I_load_0 = y0_in ? I_load[0*WC4+c] : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_1 = y1_in ? I_load[1*WC4+c] : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_2 = y2_in ? I_load[2*WC4+c] : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_3 = y3_in ? I_load[3*WC4+c] : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_4 = y4_in ? I_load[4*WC4+c] : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_5 = y5_in ? I_load[5*WC4+c] : (UNIT_TYPE_4)(UNIT_VAL_ZERO);*/
+
+				const  UNIT_TYPE_4 I_load_0 = y0_in ? *((const __global UNIT_TYPE_4*)(I+adr+(0*WC4+c)*4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_1 = y1_in ? *((const __global UNIT_TYPE_4*)(I+adr+(1*WC4+c)*4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_2 = y2_in ? *((const __global UNIT_TYPE_4*)(I+adr+(2*WC4+c)*4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_3 = y3_in ? *((const __global UNIT_TYPE_4*)(I+adr+(3*WC4+c)*4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_4 = y4_in ? *((const __global UNIT_TYPE_4*)(I+adr+(4*WC4+c)*4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+				const  UNIT_TYPE_4 I_load_5 = y5_in ? *((const __global UNIT_TYPE_4*)(I+adr+(5*WC4+c)*4)) : (UNIT_TYPE_4)(UNIT_VAL_ZERO);
+
+
+			#else
+)__krnl"
+R"__krnl(				const __global UNIT_TYPE *I_load_0 = &I_load[0*W]; //y0_in ? &I_load[0*W] : zeros4;
+				const __global UNIT_TYPE *I_load_1 = &I_load[1*W]; //y1_in ? &I_load[1*W] : zeros4;
+				const __global UNIT_TYPE *I_load_2 = &I_load[2*W]; //y2_in ? &I_load[2*W] : zeros4;
+				const __global UNIT_TYPE *I_load_3 = &I_load[3*W]; //y3_in ? &I_load[3*W] : zeros4;
+				const __global UNIT_TYPE *I_load_4 = &I_load[4*W]; //y4_in ? &I_load[4*W] : zeros4;
+				const __global UNIT_TYPE *I_load_5 = &I_load[5*W]; //y5_in ? &I_load[5*W] : zeros4;
+			#endif
+
+#if INPUT0_LAYOUT_BYXF
+
+            UNIT_TYPE_4 I0 =  I_load_0;
+            UNIT_TYPE_4 I1 =  I_load_1;
+            UNIT_TYPE_4 I2 =  I_load_2;
+            UNIT_TYPE_4 I3 =  I_load_3;
+            UNIT_TYPE_4 I4 =  I_load_4;
+            UNIT_TYPE_4 I5 =  I_load_5;
+#else
+            UNIT_TYPE_4 I0 = y0_in ? (UNIT_TYPE_4)(I_load_0[c*HW*4], I_load_0[c*HW*4+HW], I_load_0[c*HW*4+HW*2], I_load_0[c*HW*4+HW*3]):(UNIT_TYPE_4)(UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO);
+            UNIT_TYPE_4 I1 = y1_in ? (UNIT_TYPE_4)(I_load_1[c*HW*4], I_load_1[c*HW*4+HW], I_load_1[c*HW*4+HW*2], I_load_1[c*HW*4+HW*3]):(UNIT_TYPE_4)(UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO);
+            UNIT_TYPE_4 I2 = y2_in ? (UNIT_TYPE_4)(I_load_2[c*HW*4], I_load_2[c*HW*4+HW], I_load_2[c*HW*4+HW*2], I_load_2[c*HW*4+HW*3]):(UNIT_TYPE_4)(UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO);
+            UNIT_TYPE_4 I3 = y3_in ? (UNIT_TYPE_4)(I_load_3[c*HW*4], I_load_3[c*HW*4+HW], I_load_3[c*HW*4+HW*2], I_load_3[c*HW*4+HW*3]):(UNIT_TYPE_4)(UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO);
+            UNIT_TYPE_4 I4 = y4_in ? (UNIT_TYPE_4)(I_load_4[c*HW*4], I_load_4[c*HW*4+HW], I_load_4[c*HW*4+HW*2], I_load_4[c*HW*4+HW*3]):(UNIT_TYPE_4)(UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO);
+            UNIT_TYPE_4 I5 = y5_in ? (UNIT_TYPE_4)(I_load_5[c*HW*4], I_load_5[c*HW*4+HW], I_load_5[c*HW*4+HW*2], I_load_5[c*HW*4+HW*3]):(UNIT_TYPE_4)(UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO,UNIT_VAL_ZERO);
+#endif
+
+            // Compute Winograd f2x3 data transform and store components in SLM.
+            V_write[0*64] = I0 - I2;
+            V_write[1*64] = I1 + I2;
+            V_write[2*64] = -I1 + I2;
+            V_write[3*64] = I1 - I3;
+
+            V_write[0*64 + 32] = I2 - I4;
+            V_write[1*64 + 32] = I3 + I4;
+            V_write[2*64 + 32] = -I3 + I4;
+            V_write[3*64 + 32] = I3 - I5;
+        }
+
+        barrier(CLK_LOCAL_MEM_FENCE);
+
+        __local const UNIT_TYPE_4 *V_read_c8 = V_read;
+
+        __attribute__((opencl_unroll_hint(1)))
+        for (uint c8 = 0; c8 < 2; ++c8) {
+
+            // 2*14 * 3 * 8 = 672 MADs
+
+			// Fetch 8 channels of Winograd input components, spread across subgroup.
+			// row 0
+			const UNIT_TYPE_4 V00 = V_read_c8[0 * 8 + 32 * 0 + c8 * 256];
+			const UNIT_TYPE_4 V01 = V_read_c8[1 * 8 + 32 * 0 + c8 * 256];
+			const UNIT_TYPE_4 V02 = V_read_c8[2 * 8 + 32 * 0 + c8 * 256];
+			const UNIT_TYPE_4 V03 = V_read_c8[3 * 8 + 32 * 0 + c8 * 256];
+
+			// row 1
+			const UNIT_TYPE_4 V10 = V_read_c8[0 * 8 + 32 * 1 + c8 * 256];
+			const UNIT_TYPE_4 V11 = V_read_c8[1 * 8 + 32 * 1 + c8 * 256];
+			const UNIT_TYPE_4 V12 = V_read_c8[2 * 8 + 32 * 1 + c8 * 256];
+			const UNIT_TYPE_4 V13 = V_read_c8[3 * 8 + 32 * 1 + c8 * 256];
+
+            __attribute__((opencl_unroll_hint(2)))
+            for (uint c4 = 0; c4 < 2; ++c4) {
+                // 2*14 * 3 * 4 = 336 MADs
+
+                // Fetch 4 channels of Winograd filter components.
+                //uint2 coordU = coordU0;
+				//uint coordU_x = coordU0.x + get_sub_group_local_id()%8;
+				const uint flatA = coordU0.y*FILTER_OFM_NUM*KCOLSW*KROWSW + coordU0.x + get_sub_group_local_id()%8;
+                const UNIT_TYPE_4 f0 = (UNIT_TYPE_4)(
+				*(__global UNIT_TYPE *)(&U[flatA+0*FILTER_OFM_NUM*KCOLSW*KROWSW]), // as_UNIT_TYPE_4(intel_sub_group_block_read4(U, coordU));
+				*(__global UNIT_TYPE *)(&U[flatA+1*FILTER_OFM_NUM*KCOLSW*KROWSW]), // as_UNIT_TYPE_4(intel_sub_group_block_read4(U, coordU));
+				*(__global UNIT_TYPE *)(&U[flatA+2*FILTER_OFM_NUM*KCOLSW*KROWSW]), // as_UNIT_TYPE_4(intel_sub_group_block_read4(U, coordU));
+				*(__global UNIT_TYPE *)(&U[flatA+3*FILTER_OFM_NUM*KCOLSW*KROWSW])); // as_UNIT_TYPE_4(intel_sub_group_block_read4(U, coordU));
+
+				// row 0
+
+				// f0 x v[0 .. 14]
+				DOT4i0(M0.s0, f0, V00, 0 + c4);
+				DOT4i0(M0.s1, f0, V00, 2 + c4);
+				DOT4i0(M1.s0, f0, V00, 4 + c4);
+				DOT4i0(M1.s1, f0, V00, 6 + c4);
+
+				DOT4i0(M2.s0, f0, V01, 0 + c4);
+				DOT4i0(M2.s1, f0, V01, 2 + c4);
+				DOT4i0(M3.s0, f0, V01, 4 + c4);
+				DOT4i0(M3.s1, f0, V01, 6 + c4);
+
+				DOT4i0(M4.s0, f0, V02, 0 + c4);
+				DOT4i0(M4.s1, f0, V02, 2 + c4);
+				DOT4i0(M5.s0, f0, V02, 4 + c4);
+				DOT4i0(M5.s1, f0, V02, 6 + c4);
+
+				DOT4i0(M6.s0, f0, V03, 0 + c4);
+				DOT4i0(M6.s1, f0, V03, 2 + c4);
+
+				// f0 x v[0 .. 14]
+				DOT4i0(M0.s2, f0, V10, 0 + c4);
+				DOT4i0(M0.s3, f0, V10, 2 + c4);
+				DOT4i0(M1.s2, f0, V10, 4 + c4);
+				DOT4i0(M1.s3, f0, V10, 6 + c4);
+
+				DOT4i0(M2.s2, f0, V11, 0 + c4);
+				DOT4i0(M2.s3, f0, V11, 2 + c4);
+				DOT4i0(M3.s2, f0, V11, 4 + c4);
+				DOT4i0(M3.s3, f0, V11, 6 + c4);
+
+				DOT4i0(M4.s2, f0, V12, 0 + c4);
+				DOT4i0(M4.s3, f0, V12, 2 + c4);
+				DOT4i0(M5.s2, f0, V12, 4 + c4);
+				DOT4i0(M5.s3, f0, V12, 6 + c4);
+
+				DOT4i0(M6.s2, f0, V13, 0 + c4);
+				DOT4i0(M6.s3, f0, V13, 2 + c4);
+
+
+				// row 1
+				DOT4i1(M0.s0, f0, V00, 0 + c4);
+				DOT4i1(M0.s1, f0, V00, 2 + c4);
+				DOT4i1(M1.s0, f0, V00, 4 + c4);
+				DOT4i1(M1.s1, f0, V00, 6 + c4);
+
+				DOT4i1(M2.s0, f0, V01, 0 + c4);
+				DOT4i1(M2.s1, f0, V01, 2 + c4);
+				DOT4i1(M3.s0, f0, V01, 4 + c4);
+				DOT4i1(M3.s1, f0, V01, 6 + c4);
+
+				DOT4i1(M4.s0, f0, V02, 0 + c4);
+				DOT4i1(M4.s1, f0, V02, 2 + c4);
+				DOT4i1(M5.s0, f0, V02, 4 + c4);
+				DOT4i1(M5.s1, f0, V02, 6 + c4);
+
+				DOT4i1(M6.s0, f0, V03, 0 + c4);
+				DOT4i1(M6.s1, f0, V03, 2 + c4);
+
+				// f0 x v[0 .. 14]
+				DOT4i1(M0.s2, f0, V10, 0 + c4);
+				DOT4i1(M0.s3, f0, V10, 2 + c4);
+				DOT4i1(M1.s2, f0, V10, 4 + c4);
+				DOT4i1(M1.s3, f0, V10, 6 + c4);
+
+				DOT4i1(M2.s2, f0, V11, 0 + c4);
+				DOT4i1(M2.s3, f0, V11, 2 + c4);
+				DOT4i1(M3.s2, f0, V11, 4 + c4);
+				DOT4i1(M3.s3, f0, V11, 6 + c4);
+
+				DOT4i1(M4.s2, f0, V12, 0 + c4);
+				DOT4i1(M4.s3, f0, V12, 2 + c4);
+				DOT4i1(M5.s2, f0, V12, 4 + c4);
+				DOT4i1(M5.s3, f0, V12, 6 + c4);
+
+				DOT4i1(M6.s2, f0, V13, 0 + c4);
+				DOT4i1(M6.s3, f0, V13, 2 + c4);
+
+				// f0 x v[0 .. 14]
+				DOT4i2(M0.s0, f0, V00, 0 + c4);
+				DOT4i2(M0.s1, f0, V00, 2 + c4);
+				DOT4i2(M1.s0, f0, V00, 4 + c4);
+				DOT4i2(M1.s1, f0, V00, 6 + c4);
+
+				DOT4i2(M2.s0, f0, V01, 0 + c4);
+				DOT4i2(M2.s1, f0, V01, 2 + c4);
+				DOT4i2(M3.s0, f0, V01, 4 + c4);
+				DOT4i2(M3.s1, f0, V01, 6 + c4);
+
+				DOT4i2(M4.s0, f0, V02, 0 + c4);
+				DOT4i2(M4.s1, f0, V02, 2 + c4);
+				DOT4i2(M5.s0, f0, V02, 4 + c4);
+				DOT4i2(M5.s1, f0, V02, 6 + c4);
+
+				DOT4i2(M6.s0, f0, V03, 0 + c4);
+				DOT4i2(M6.s1, f0, V03, 2 + c4);
+
+				// f0 x v[0 .. 14]
+				DOT4i2(M0.s2, f0, V10, 0 + c4);
+				DOT4i2(M0.s3, f0, V10, 2 + c4);
+				DOT4i2(M1.s2, f0, V10, 4 + c4);
+				DOT4i2(M1.s3, f0, V10, 6 + c4);
+
+				DOT4i2(M2.s2, f0, V11, 0 + c4);
+				DOT4i2(M2.s3, f0, V11, 2 + c4);
+				DOT4i2(M3.s2, f0, V11, 4 + c4);
+				DOT4i2(M3.s3, f0, V11, 6 + c4);
+
+				DOT4i2(M4.s2, f0, V12, 0 + c4);
+				DOT4i2(M4.s3, f0, V12, 2 + c4);
+				DOT4i2(M5.s2, f0, V12, 4 + c4);
+				DOT4i2(M5.s3, f0, V12, 6 + c4);
+
+				DOT4i2(M6.s2, f0, V13, 0 + c4);
+				DOT4i2(M6.s3, f0, V13, 2 + c4);
+
+
+				// f0 x v[0 .. 14]
+				DOT4i3(M0.s0, f0, V00, 0 + c4);
+				DOT4i3(M0.s1, f0, V00, 2 + c4);
+				DOT4i3(M1.s0, f0, V00, 4 + c4);
+				DOT4i3(M1.s1, f0, V00, 6 + c4);
+
+				DOT4i3(M2.s0, f0, V01, 0 + c4);
+				DOT4i3(M2.s1, f0, V01, 2 + c4);
+				DOT4i3(M3.s0, f0, V01, 4 + c4);
+)__krnl"
+R"__krnl(				DOT4i3(M3.s1, f0, V01, 6 + c4);
+
+				DOT4i3(M4.s0, f0, V02, 0 + c4);
+				DOT4i3(M4.s1, f0, V02, 2 + c4);
+				DOT4i3(M5.s0, f0, V02, 4 + c4);
+				DOT4i3(M5.s1, f0, V02, 6 + c4);
+
+				DOT4i3(M6.s0, f0, V03, 0 + c4);
+				DOT4i3(M6.s1, f0, V03, 2 + c4);
+
+				// f0 x v[0 .. 14]
+				DOT4i3(M0.s2, f0, V10, 0 + c4);
+				DOT4i3(M0.s3, f0, V10, 2 + c4);
+				DOT4i3(M1.s2, f0, V10, 4 + c4);
+				DOT4i3(M1.s3, f0, V10, 6 + c4);
+
+				DOT4i3(M2.s2, f0, V11, 0 + c4);
+				DOT4i3(M2.s3, f0, V11, 2 + c4);
+				DOT4i3(M3.s2, f0, V11, 4 + c4);
+				DOT4i3(M3.s3, f0, V11, 6 + c4);
+
+				DOT4i3(M4.s2, f0, V12, 0 + c4);
+				DOT4i3(M4.s3, f0, V12, 2 + c4);
+				DOT4i3(M5.s2, f0, V12, 4 + c4);
+				DOT4i3(M5.s3, f0, V12, 6 + c4);
+
+				DOT4i3(M6.s2, f0, V13, 0 + c4);
+				DOT4i3(M6.s3, f0, V13, 2 + c4);
+
+
+
+				//flatA += 8;
+				const UNIT_TYPE_4 f1 = (UNIT_TYPE_4)(
+					*(__global UNIT_TYPE *)(&U[flatA + 8 + 0 * FILTER_OFM_NUM*KCOLSW*KROWSW]),
+					*(__global UNIT_TYPE *)(&U[flatA + 8 + 1 * FILTER_OFM_NUM*KCOLSW*KROWSW]),
+					*(__global UNIT_TYPE *)(&U[flatA + 8 + 2 * FILTER_OFM_NUM*KCOLSW*KROWSW]),
+					*(__global UNIT_TYPE *)(&U[flatA + 8 + 3 * FILTER_OFM_NUM*KCOLSW*KROWSW]));
+
+
+				// f1[c4] x v[1 .. 15]
+				DOT4i0(M0.s0, f1, V00, 2 + c4);
+				DOT4i0(M0.s1, f1, V00, 4 + c4);
+				DOT4i0(M1.s0, f1, V00, 6 + c4);
+				DOT4i0(M1.s1, f1, V01, 0 + c4);
+
+				DOT4i0(M2.s0, f1, V01, 2 + c4);
+				DOT4i0(M2.s1, f1, V01, 4 + c4);
+				DOT4i0(M3.s0, f1, V01, 6 + c4);
+				DOT4i0(M3.s1, f1, V02, 0 + c4);
+
+				DOT4i0(M4.s0, f1, V02, 2 + c4);
+				DOT4i0(M4.s1, f1, V02, 4 + c4);
+				DOT4i0(M5.s0, f1, V02, 6 + c4);
+				DOT4i0(M5.s1, f1, V03, 0 + c4);
+
+				DOT4i0(M6.s0, f1, V03, 2 + c4);
+				DOT4i0(M6.s1, f1, V03, 4 + c4);
+
+
+				// f1 x v[1 .. 15]
+				DOT4i0(M0.s2, f1, V10, 2 + c4);
+				DOT4i0(M0.s3, f1, V10, 4 + c4);
+				DOT4i0(M1.s2, f1, V10, 6 + c4);
+				DOT4i0(M1.s3, f1, V11, 0 + c4);
+
+				DOT4i0(M2.s2, f1, V11, 2 + c4);
+				DOT4i0(M2.s3, f1, V11, 4 + c4);
+				DOT4i0(M3.s2, f1, V11, 6 + c4);
+				DOT4i0(M3.s3, f1, V12, 0 + c4);
+
+				DOT4i0(M4.s2, f1, V12, 2 + c4);
+				DOT4i0(M4.s3, f1, V12, 4 + c4);
+				DOT4i0(M5.s2, f1, V12, 6 + c4);
+				DOT4i0(M5.s3, f1, V13, 0 + c4);
+
+				DOT4i0(M6.s2, f1, V13, 2 + c4);
+				DOT4i0(M6.s3, f1, V13, 4 + c4);
+
+
+				// f1[c4] x v[1 .. 15]
+				DOT4i1(M0.s0, f1, V00, 2 + c4);
+				DOT4i1(M0.s1, f1, V00, 4 + c4);
+				DOT4i1(M1.s0, f1, V00, 6 + c4);
+				DOT4i1(M1.s1, f1, V01, 0 + c4);
+
+				DOT4i1(M2.s0, f1, V01, 2 + c4);
+				DOT4i1(M2.s1, f1, V01, 4 + c4);
+				DOT4i1(M3.s0, f1, V01, 6 + c4);
+				DOT4i1(M3.s1, f1, V02, 0 + c4);
+
+				DOT4i1(M4.s0, f1, V02, 2 + c4);
+				DOT4i1(M4.s1, f1, V02, 4 + c4);
+				DOT4i1(M5.s0, f1, V02, 6 + c4);
+				DOT4i1(M5.s1, f1, V03, 0 + c4);
+
+				DOT4i1(M6.s0, f1, V03, 2 + c4);
+				DOT4i1(M6.s1, f1, V03, 4 + c4);
+
+
+				// f1 x v[1 .. 15]
+				DOT4i1(M0.s2, f1, V10, 2 + c4);
+				DOT4i1(M0.s3, f1, V10, 4 + c4);
+				DOT4i1(M1.s2, f1, V10, 6 + c4);
+				DOT4i1(M1.s3, f1, V11, 0 + c4);
+
+				DOT4i1(M2.s2, f1, V11, 2 + c4);
+				DOT4i1(M2.s3, f1, V11, 4 + c4);
+				DOT4i1(M3.s2, f1, V11, 6 + c4);
+				DOT4i1(M3.s3, f1, V12, 0 + c4);
+
+				DOT4i1(M4.s2, f1, V12, 2 + c4);
+				DOT4i1(M4.s3, f1, V12, 4 + c4);
+				DOT4i1(M5.s2, f1, V12, 6 + c4);
+				DOT4i1(M5.s3, f1, V13, 0 + c4);
+
+				DOT4i1(M6.s2, f1, V13, 2 + c4);
+				DOT4i1(M6.s3, f1, V13, 4 + c4);
+
+				// f1[c4] x v[1 .. 15]
+				DOT4i2(M0.s0, f1, V00, 2 + c4);
+				DOT4i2(M0.s1, f1, V00, 4 + c4);
+				DOT4i2(M1.s0, f1, V00, 6 + c4);
+				DOT4i2(M1.s1, f1, V01, 0 + c4);
+
+				DOT4i2(M2.s0, f1, V01, 2 + c4);
+				DOT4i2(M2.s1, f1, V01, 4 + c4);
+				DOT4i2(M3.s0, f1, V01, 6 + c4);
+				DOT4i2(M3.s1, f1, V02, 0 + c4);
+
+				DOT4i2(M4.s0, f1, V02, 2 + c4);
+				DOT4i2(M4.s1, f1, V02, 4 + c4);
+				DOT4i2(M5.s0, f1, V02, 6 + c4);
+				DOT4i2(M5.s1, f1, V03, 0 + c4);
+
+				DOT4i2(M6.s0, f1, V03, 2 + c4);
+				DOT4i2(M6.s1, f1, V03, 4 + c4);
+
+				// f1 x v[1 .. 15]
+				DOT4i2(M0.s2, f1, V10, 2 + c4);
+				DOT4i2(M0.s3, f1, V10, 4 + c4);
+				DOT4i2(M1.s2, f1, V10, 6 + c4);
+				DOT4i2(M1.s3, f1, V11, 0 + c4);
+
+				DOT4i2(M2.s2, f1, V11, 2 + c4);
+				DOT4i2(M2.s3, f1, V11, 4 + c4);
+				DOT4i2(M3.s2, f1, V11, 6 + c4);
+				DOT4i2(M3.s3, f1, V12, 0 + c4);
+
+				DOT4i2(M4.s2, f1, V12, 2 + c4);
+				DOT4i2(M4.s3, f1, V12, 4 + c4);
+				DOT4i2(M5.s2, f1, V12, 6 + c4);
+				DOT4i2(M5.s3, f1, V13, 0 + c4);
+
+				DOT4i2(M6.s2, f1, V13, 2 + c4);
+				DOT4i2(M6.s3, f1, V13, 4 + c4);
+
+				// f1[c4] x v[1 .. 15]
+				DOT4i3(M0.s0, f1, V00, 2 + c4);
+				DOT4i3(M0.s1, f1, V00, 4 + c4);
+				DOT4i3(M1.s0, f1, V00, 6 + c4);
+				DOT4i3(M1.s1, f1, V01, 0 + c4);
+
+				DOT4i3(M2.s0, f1, V01, 2 + c4);
+				DOT4i3(M2.s1, f1, V01, 4 + c4);
+				DOT4i3(M3.s0, f1, V01, 6 + c4);
+				DOT4i3(M3.s1, f1, V02, 0 + c4);
+
+				DOT4i3(M4.s0, f1, V02, 2 + c4);
+				DOT4i3(M4.s1, f1, V02, 4 + c4);
+				DOT4i3(M5.s0, f1, V02, 6 + c4);
+				DOT4i3(M5.s1, f1, V03, 0 + c4);
+
+				DOT4i3(M6.s0, f1, V03, 2 + c4);
+				DOT4i3(M6.s1, f1, V03, 4 + c4);
+
+
+				// f1 x v[1 .. 15]
+				DOT4i3(M0.s2, f1, V10, 2 + c4);
+				DOT4i3(M0.s3, f1, V10, 4 + c4);
+				DOT4i3(M1.s2, f1, V10, 6 + c4);
+				DOT4i3(M1.s3, f1, V11, 0 + c4);
+
+				DOT4i3(M2.s2, f1, V11, 2 + c4);
+				DOT4i3(M2.s3, f1, V11, 4 + c4);
+				DOT4i3(M3.s2, f1, V11, 6 + c4);
+				DOT4i3(M3.s3, f1, V12, 0 + c4);
+
+				DOT4i3(M4.s2, f1, V12, 2 + c4);
+				DOT4i3(M4.s3, f1, V12, 4 + c4);
+				DOT4i3(M5.s2, f1, V12, 6 + c4);
+				DOT4i3(M5.s3, f1, V13, 0 + c4);
+
+				DOT4i3(M6.s2, f1, V13, 2 + c4);
+				DOT4i3(M6.s3, f1, V13, 4 + c4);
+
+
+				//flatA += 8;
+				const UNIT_TYPE_4 f2 = (UNIT_TYPE_4)(
+					*(__global UNIT_TYPE *)(&U[flatA + 16 + 0 * FILTER_OFM_NUM*KCOLSW*KROWSW]),
+					*(__global UNIT_TYPE *)(&U[flatA + 16 + 1 * FILTER_OFM_NUM*KCOLSW*KROWSW]),
+)__krnl"
+R"__krnl(					*(__global UNIT_TYPE *)(&U[flatA + 16 + 2 * FILTER_OFM_NUM*KCOLSW*KROWSW]),
+					*(__global UNIT_TYPE *)(&U[flatA + 16 + 3 * FILTER_OFM_NUM*KCOLSW*KROWSW]));
+				coordU0.y += 4;
+
+
+				// f2[c4] x v[2 .. 16]
+				DOT4i0(M0.s0, f2, V00, 4 + c4);
+				DOT4i0(M0.s1, f2, V00, 6 + c4);
+				DOT4i0(M1.s0, f2, V01, 0 + c4);
+				DOT4i0(M1.s1, f2, V01, 2 + c4);
+
+				DOT4i0(M2.s0, f2, V01, 4 + c4);
+				DOT4i0(M2.s1, f2, V01, 6 + c4);
+				DOT4i0(M3.s0, f2, V02, 0 + c4);
+				DOT4i0(M3.s1, f2, V02, 2 + c4);
+
+				DOT4i0(M4.s0, f2, V02, 4 + c4);
+				DOT4i0(M4.s1, f2, V02, 6 + c4);
+				DOT4i0(M5.s0, f2, V03, 0 + c4);
+				DOT4i0(M5.s1, f2, V03, 2 + c4);
+
+				DOT4i0(M6.s0, f2, V03, 4 + c4);
+				DOT4i0(M6.s1, f2, V03, 6 + c4);
+
+				// row 1
+
+				// f2 x v[2 .. 16]
+				DOT4i0(M0.s2, f2, V10, 4 + c4);
+				DOT4i0(M0.s3, f2, V10, 6 + c4);
+				DOT4i0(M1.s2, f2, V11, 0 + c4);
+				DOT4i0(M1.s3, f2, V11, 2 + c4);
+
+				DOT4i0(M2.s2, f2, V11, 4 + c4);
+				DOT4i0(M2.s3, f2, V11, 6 + c4);
+				DOT4i0(M3.s2, f2, V12, 0 + c4);
+				DOT4i0(M3.s3, f2, V12, 2 + c4);
+
+				DOT4i0(M4.s2, f2, V12, 4 + c4);
+				DOT4i0(M4.s3, f2, V12, 6 + c4);
+				DOT4i0(M5.s2, f2, V13, 0 + c4);
+				DOT4i0(M5.s3, f2, V13, 2 + c4);
+
+				DOT4i0(M6.s2, f2, V13, 4 + c4);
+				DOT4i0(M6.s3, f2, V13, 6 + c4);
+
+				///
+
+
+				// f2[c4] x v[2 .. 16]
+				DOT4i1(M0.s0, f2, V00, 4 + c4);
+				DOT4i1(M0.s1, f2, V00, 6 + c4);
+				DOT4i1(M1.s0, f2, V01, 0 + c4);
+				DOT4i1(M1.s1, f2, V01, 2 + c4);
+
+				DOT4i1(M2.s0, f2, V01, 4 + c4);
+				DOT4i1(M2.s1, f2, V01, 6 + c4);
+				DOT4i1(M3.s0, f2, V02, 0 + c4);
+				DOT4i1(M3.s1, f2, V02, 2 + c4);
+
+				DOT4i1(M4.s0, f2, V02, 4 + c4);
+				DOT4i1(M4.s1, f2, V02, 6 + c4);
+				DOT4i1(M5.s0, f2, V03, 0 + c4);
+				DOT4i1(M5.s1, f2, V03, 2 + c4);
+
+				DOT4i1(M6.s0, f2, V03, 4 + c4);
+				DOT4i1(M6.s1, f2, V03, 6 + c4);
+
+				// row 1
+
+
+				// f2 x v[2 .. 16]
+				DOT4i1(M0.s2, f2, V10, 4 + c4);
+				DOT4i1(M0.s3, f2, V10, 6 + c4);
+				DOT4i1(M1.s2, f2, V11, 0 + c4);
+				DOT4i1(M1.s3, f2, V11, 2 + c4);
+
+				DOT4i1(M2.s2, f2, V11, 4 + c4);
+				DOT4i1(M2.s3, f2, V11, 6 + c4);
+				DOT4i1(M3.s2, f2, V12, 0 + c4);
+				DOT4i1(M3.s3, f2, V12, 2 + c4);
+
+				DOT4i1(M4.s2, f2, V12, 4 + c4);
+				DOT4i1(M4.s3, f2, V12, 6 + c4);
+				DOT4i1(M5.s2, f2, V13, 0 + c4);
+				DOT4i1(M5.s3, f2, V13, 2 + c4);
+
+				DOT4i1(M6.s2, f2, V13, 4 + c4);
+				DOT4i1(M6.s3, f2, V13, 6 + c4);
+
+
+
+				// f2[c4] x v[2 .. 16]
+				DOT4i2(M0.s0, f2, V00, 4 + c4);
+				DOT4i2(M0.s1, f2, V00, 6 + c4);
+				DOT4i2(M1.s0, f2, V01, 0 + c4);
+				DOT4i2(M1.s1, f2, V01, 2 + c4);
+
+				DOT4i2(M2.s0, f2, V01, 4 + c4);
+				DOT4i2(M2.s1, f2, V01, 6 + c4);
+				DOT4i2(M3.s0, f2, V02, 0 + c4);
+				DOT4i2(M3.s1, f2, V02, 2 + c4);
+
+				DOT4i2(M4.s0, f2, V02, 4 + c4);
+				DOT4i2(M4.s1, f2, V02, 6 + c4);
+				DOT4i2(M5.s0, f2, V03, 0 + c4);
+				DOT4i2(M5.s1, f2, V03, 2 + c4);
+
+				DOT4i2(M6.s0, f2, V03, 4 + c4);
+				DOT4i2(M6.s1, f2, V03, 6 + c4);
+
+				//
+
+				// f2 x v[2 .. 16]
+				DOT4i2(M0.s2, f2, V10, 4 + c4);
+				DOT4i2(M0.s3, f2, V10, 6 + c4);
+				DOT4i2(M1.s2, f2, V11, 0 + c4);
+				DOT4i2(M1.s3, f2, V11, 2 + c4);
+
+				DOT4i2(M2.s2, f2, V11, 4 + c4);
+				DOT4i2(M2.s3, f2, V11, 6 + c4);
+				DOT4i2(M3.s2, f2, V12, 0 + c4);
+				DOT4i2(M3.s3, f2, V12, 2 + c4);
+
+				DOT4i2(M4.s2, f2, V12, 4 + c4);
+				DOT4i2(M4.s3, f2, V12, 6 + c4);
+				DOT4i2(M5.s2, f2, V13, 0 + c4);
+				DOT4i2(M5.s3, f2, V13, 2 + c4);
+
+				DOT4i2(M6.s2, f2, V13, 4 + c4);
+				DOT4i2(M6.s3, f2, V13, 6 + c4);
+
+
+				//
+				// row 1
+				// f0 x v[0 .. 14]
+
+				// f2[c4] x v[2 .. 16]
+				DOT4i3(M0.s0, f2, V00, 4 + c4);
+				DOT4i3(M0.s1, f2, V00, 6 + c4);
+				DOT4i3(M1.s0, f2, V01, 0 + c4);
+				DOT4i3(M1.s1, f2, V01, 2 + c4);
+
+				DOT4i3(M2.s0, f2, V01, 4 + c4);
+				DOT4i3(M2.s1, f2, V01, 6 + c4);
+				DOT4i3(M3.s0, f2, V02, 0 + c4);
+				DOT4i3(M3.s1, f2, V02, 2 + c4);
+
+				DOT4i3(M4.s0, f2, V02, 4 + c4);
+				DOT4i3(M4.s1, f2, V02, 6 + c4);
+				DOT4i3(M5.s0, f2, V03, 0 + c4);
+				DOT4i3(M5.s1, f2, V03, 2 + c4);
+
+				DOT4i3(M6.s0, f2, V03, 4 + c4);
+				DOT4i3(M6.s1, f2, V03, 6 + c4);
+
+
+				// f2 x v[2 .. 16]
+				DOT4i3(M0.s2, f2, V10, 4 + c4);
+				DOT4i3(M0.s3, f2, V10, 6 + c4);
+				DOT4i3(M1.s2, f2, V11, 0 + c4);
+				DOT4i3(M1.s3, f2, V11, 2 + c4);
+
+				DOT4i3(M2.s2, f2, V11, 4 + c4);
+				DOT4i3(M2.s3, f2, V11, 6 + c4);
+				DOT4i3(M3.s2, f2, V12, 0 + c4);
+				DOT4i3(M3.s3, f2, V12, 2 + c4);
+
+				DOT4i3(M4.s2, f2, V12, 4 + c4);
+				DOT4i3(M4.s3, f2, V12, 6 + c4);
+				DOT4i3(M5.s2, f2, V13, 0 + c4);
+				DOT4i3(M5.s3, f2, V13, 2 + c4);
+
+				DOT4i3(M6.s2, f2, V13, 4 + c4);
+				DOT4i3(M6.s3, f2, V13, 6 + c4);
+
+
+			}
+			//V_read_c8 += 256;
+
+        }
+
+        //barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+    // Store multiplies in SLM.
+    {
+		barrier(CLK_LOCAL_MEM_FENCE);
+        __local UNIT_TYPE_4 *M_write = &V[lz*7*8 + lx + slmSize*upperHalf];
+
+        M_write[0*8] = M0;
+        M_write[1*8] = M1;
+        M_write[2*8] = M2;
+        M_write[3*8] = M3;
+        M_write[4*8] = M4;
+        M_write[5*8] = M5;
+        M_write[6*8] = M6;
+
+        barrier(CLK_LOCAL_MEM_FENCE);
+    }
+
+)__krnl"
+R"__krnl(    if (lz < 7)
+	{
+        // Load multiplies from SLM.
+        __local const UNIT_TYPE_8 *M_read = (__local UNIT_TYPE_8*)&V[lz*8 + lxd4*224 + lxm4*2 + slmSize*upperHalf];
+
+        UNIT_TYPE_8 M0 = M_read[0*28];
+        UNIT_TYPE_8 M1 = M_read[1*28];
+        UNIT_TYPE_8 M2 = M_read[2*28];
+        UNIT_TYPE_8 M3 = M_read[3*28];
+
+        // Inverse Transform.
+        UNIT_TYPE_8 S0 = M0 + M1 + M2;
+        UNIT_TYPE_8 S1 = M1 - M2 - M3;
+
+        // Store output to global memory.
+        uint p = gy*4 + OUTPUT_PAD_BEFORE_SIZE_Y;
+        uint q = gx*14 + lz*2 + OUTPUT_PAD_BEFORE_SIZE_X;
+        uint k = gk*16 + lx*2;
+
+		// bias and activation
+		#if BIAS_TERM
+		#if BIAS_PER_OUTPUT
+            const unsigned bias_index0 = k*OUTPUT_SIZE_X*OUTPUT_SIZE_Y + trow*OUTPUT_SIZE_X + q;
+			const unsigned bias_index1 = bias_index0 + 1;
+		#else
+            const unsigned bias_index0 = k;
+			const unsigned bias_index1 = bias_index0 + 1;
+		#endif
+		#endif
+
+#if OUTPUT_LAYOUT_BYXF
+		uint outindex = gn*PQK + p*Q*FILTER_OFM_NUM + q*FILTER_OFM_NUM + k;
+        __global UNIT_TYPE_2 *O_write = (__global UNIT_TYPE_2 *)&O[outindex];
+#else
+        __global UNIT_TYPE *O_write_0 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p+0)*Q + q]);
+        __global UNIT_TYPE *O_write_1 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p+1)*Q + q]);
+        __global UNIT_TYPE *O_write_2 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p+2)*Q + q]);
+        __global UNIT_TYPE *O_write_3 = (__global UNIT_TYPE *)(&O[gn*PQK + k*Q*P + (p+3)*Q + q]);
+#endif
+
+        // TODO: clip output by P, Q
+        bool q0_in = q < Q - OUTPUT_PAD_AFTER_SIZE_X;
+        bool q1_in = q + 1 < Q - OUTPUT_PAD_AFTER_SIZE_X;
+
+		if (k < FILTER_OFM_NUM) {
+            if (p < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+                if (q0_in) {
+
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[0*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s0 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S0.s4 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[0*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s0, NL_M, NL_N), ACTIVATION(S0.s4, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_0[0] = ACTIVATION(S0.s0 + bias[bias_index0], NL_M, NL_N);
+                    O_write_0[0+Q*P] = ACTIVATION(S0.s4 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_0[0] = ACTIVATION(S0.s0, NL_M, NL_N);
+                    O_write_0[0+Q*P] = ACTIVATION(S0.s4, NL_M, NL_N);
+#endif
+#endif
+                }
+                if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[0*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s1 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S0.s5 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[0*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s1, NL_M, NL_N), ACTIVATION(S0.s5, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_0[1] = ACTIVATION(S0.s1 + bias[bias_index0], NL_M, NL_N);
+                    O_write_0[1+Q*P] = ACTIVATION(S0.s5 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_0[1] = ACTIVATION(S0.s1, NL_M, NL_N);
+                    O_write_0[1+Q*P] = ACTIVATION(S0.s5, NL_M, NL_N);
+#endif
+#endif
+                }
+            }
+
+            // row 1
+            if (p + 1 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+                if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[1*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s0 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S1.s4 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[1*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s0, NL_M, NL_N), ACTIVATION(S1.s4, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_1[0] = ACTIVATION(S1.s0 + bias[bias_index0], NL_M, NL_N);
+                    O_write_1[0+Q*P] = ACTIVATION(S1.s4 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_1[0] = ACTIVATION(S1.s0, NL_M, NL_N);
+                    O_write_1[0+Q*P] = ACTIVATION(S1.s4, NL_M, NL_N);
+#endif
+#endif
+                }
+                if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[1*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s1 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S1.s5 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[1*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s1, NL_M, NL_N), ACTIVATION(S1.s5, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_1[1] = ACTIVATION(S1.s1 + bias[bias_index0], NL_M, NL_N);
+                    O_write_1[1+Q*P] = ACTIVATION(S1.s5 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_1[1] = ACTIVATION(S1.s1, NL_M, NL_N);
+                    O_write_1[1+Q*P] = ACTIVATION(S1.s5, NL_M, NL_N);
+#endif
+#endif
+                }
+            }
+
+            // row 2
+            if (p + 2 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+                if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[2*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s2 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S0.s6 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[2*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s2, NL_M, NL_N), ACTIVATION(S0.s6, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_2[0] = ACTIVATION(S0.s2 + bias[bias_index0], NL_M, NL_N);
+                    O_write_2[0+Q*P] = ACTIVATION(S0.s6 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_2[0] = ACTIVATION(S0.s2, NL_M, NL_N);
+                    O_write_2[0+Q*P] = ACTIVATION(S0.s6, NL_M, NL_N);
+#endif
+#endif
+                }
+                if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[2*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s3 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S0.s7 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[2*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S0.s3, NL_M, NL_N), ACTIVATION(S0.s7, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_2[1] = ACTIVATION(S0.s3 + bias[bias_index0], NL_M, NL_N);
+                    O_write_2[1+Q*P] = ACTIVATION(S0.s7 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_2[1] = ACTIVATION(S0.s3, NL_M, NL_N);
+                    O_write_2[1+Q*P] = ACTIVATION(S0.s7, NL_M, NL_N);
+#endif
+#endif
+                }
+            }
+
+            // row 3
+            if (p + 3 < P - OUTPUT_PAD_AFTER_SIZE_Y) {
+                if (q0_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[3*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s2 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S1.s6 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[3*QK2 + 0*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s2, NL_M, NL_N), ACTIVATION(S1.s6, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_3[0] = ACTIVATION(S1.s2 + bias[bias_index0], NL_M, NL_N);
+                    O_write_3[0+Q*P] = ACTIVATION(S1.s6 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_3[0] = ACTIVATION(S1.s2, NL_M, NL_N);
+                    O_write_3[0+Q*P] = ACTIVATION(S1.s6, NL_M, NL_N);
+#endif
+#endif
+                }
+                if (q1_in) {
+#if OUTPUT_LAYOUT_BYXF
+#if BIAS_TERM
+                    O_write[3*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s3 + bias[bias_index0], NL_M, NL_N), ACTIVATION(S1.s7 + bias[bias_index1], NL_M, NL_N));
+#else
+                    O_write[3*QK2 + 1*K2] = (UNIT_TYPE_2)(ACTIVATION(S1.s3, NL_M, NL_N), ACTIVATION(S1.s7, NL_M, NL_N));
+#endif
+#else
+#if BIAS_TERM
+                    O_write_3[1] = ACTIVATION(S1.s3 + bias[bias_index0], NL_M, NL_N);
+                    O_write_3[1+Q*P] = ACTIVATION(S1.s7 + bias[bias_index1], NL_M, NL_N);
+#else
+                    O_write_3[1] = ACTIVATION(S1.s3, NL_M, NL_N);
+                    O_write_3[1+Q*P] = ACTIVATION(S1.s7, NL_M, NL_N);
+#endif
+#endif
+                }
+            }
+        }
+
+
+    }
+)__krnl"
+R"__krnl(}
+#undef UNIT_TYPE_2
+#undef UNIT_TYPE_4
+#undef UNIT_TYPE_8
+
+)__krnl"},
+
+{"broadcast_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+
+KERNEL(broadcast_gpu_ref)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output)
+{
+    // [CONSTEXPR]
+    // Input sizes:
+    const uint in_sx = INPUT0_SIZE_X;
+    const uint in_sy = INPUT0_SIZE_Y;
+    const uint in_sf = INPUT0_FEATURE_NUM;
+    const uint in_sb = INPUT0_BATCH_NUM;
+
+
+    const uint out_x  = (uint) get_global_id(0);
+    const uint out_y  = (uint) get_global_id(1);
+    const uint out_fb = (uint) get_global_id(2);
+
+    const uint out_f  = out_fb % OUTPUT_FEATURE_NUM;
+    const uint out_b  = out_fb / OUTPUT_FEATURE_NUM;
+
+
+    const uint in_x = out_x % in_sx;
+    const uint in_y = out_y % in_sy;
+    const uint in_f = out_f % in_sf;
+    const uint in_b = out_b % in_sb;
+
+    const uint in_pos  = GET_DATA_INDEX(INPUT0, in_b,  in_f,  in_y,  in_x);
+    const uint out_pos = GET_DATA_INDEX(OUTPUT, out_b, out_f, out_y, out_x);
+
+
+    output[out_pos] = input[in_pos];
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_fb_io_block_fp16",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Just-in-time macro definitions:
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Required JIT constants:
+//  - FP16_SUPPORTED       - [0/1] Value indicating whether device supports FP16 OpenCL extension (cl_khr_fp16).
+//  - FP16_UNIT_USED       - [0/1] Value indicating that current kernel should use FP16.
+//  - UNIT_TYPE            - Type of unit of input/output/weight/bias.
+//  - UNIT_VAL_ZERO        - Literal of current UNIT_TYPE that represents 0.
+//  - INPUT_BATCH_NUM      - [int] Batch size for input. Number of input sets of spatial and feature data that
+//                           are grouped to be processed in single batch.
+//  - INPUT_ELEMENTS_COUNT - [int] Cumulative number of elements in single data set from batch.
+//  - FILTER_OFM_NUM       - [int] Cumulative number of elements that are outputted for single input set from batch.
+//                           Number of layer responses per single input set from batch.
+//  - RELU                 - [0/1] Indicates that ReLU activation function should be used on output.
+//  - NEGATIVE_SLOPE       - [float] Factor for negative output values (required when ReLU is specified).
+//
+//  - SUB_GROUP_SIZE       - [int] Size of used subgroup (SIMD).
+//  - WORK_ITEMS_PER_BATCH - [int] Number of work items needed to process at least one element in all data sets
+//                           from batch.
+//  - UNIT_BYTE_SIZE       - [int] Size of unit of input/output/weight/bias in bytes.
+//  - CHUNK_TYPE           - Type of chunk of data read by work item using sub-group operation.
+//  - CHUNK_BYTE_SIZE      - [int] Size of chunk of data read by work item using sub-group operation in bytes.
+//  - UNITS_PER_CHUNK      - [int] Number of units stored in single chunk of read data.
+//  - BYTES_PER_SG_READ    - [int] Number of bytes read by single sub-group read operation (read by entire sub-group).
+//  - UNITS_PER_SG_READ    - [int] Number of units read by single sub-group read operation (read by entire sub-group).
+//  - RG_COUNT             - [int] Number of response groups. Each group (except last) writes units_per_sg_read
+//                           responses for at least one input data set from batch.
+//  - LAST_RG_SIZE         - [int] Number of responses in last group of written responses.
+//                           Responses are grouped in UNITS_PER_SG_READ-sized groups. The parameter describes how
+//                           many responses are in last group or 0, if group is full.
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Helpers:
+// ---------------------------------------------------------------------------------------------------------------------
+
+#define CONCAT_TOKEN_HANDLER1(prefix, suffix) prefix##suffix
+
+// Expands and concatenates two tokens into one.
+#define CONCAT_TOKEN(prefix, suffix) CONCAT_TOKEN_HANDLER1(prefix, suffix)
+
+)__krnl"
+R"__krnl(// ---------------------------------------------------------------------------------------------------------------------
+
+// Converts scalar expression to scalar of unit type.
+#if FP16_UNIT_USED
+    #define CVT_UNIT(expression) CONCAT_TOKEN(convert_, UNIT_TYPE)(expression)
+#else
+    #define CVT_UNIT(expression) (expression)
+#endif
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// - CHUNK_UNITS_TYPE - Type of scalar or vector of UNIT_TYPE that can be reinterpreted as CHUNK_TYPE.
+#if UNITS_PER_CHUNK == 1
+    #define CHUNK_UNITS_TYPE UNIT_TYPE
+#else
+    #define CHUNK_UNITS_TYPE MAKE_VECTOR_TYPE(UNIT_TYPE, UNITS_PER_CHUNK)
+#endif
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Reinterpretation between CHUNK_TYPE and CHUNK_UNITS_TYPE.
+#define AS_CHUNK(expression) CONCAT_TOKEN(as_, CHUNK_TYPE)(expression)
+#define AS_UNITS(expression) CONCAT_TOKEN(as_, CHUNK_UNITS_TYPE)(expression)
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Extracts one scalar element of UNIT_TYPE from work-item chunk;
+//     chunk - name of chunk variable, idx - 0-based index of element.
+#if UNITS_PER_CHUNK == 2
+    #define CHUNK_UNIT_SELECT(chunk, idx) ((idx) ? AS_UNITS(chunk).s1 : AS_UNITS(chunk).s0)
+#elif UNITS_PER_CHUNK == 1
+    #define CHUNK_UNIT_SELECT(chunk, idx) AS_UNITS(chunk)
+#else
+    #error Unsupported number of units per chunk.
+#endif
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Sub-group operations:
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Extracts one scalar element of UNIT_TYPE from sub-group chunk;
+//     chunk - name of chunk variable, idx - 0-based index of element.
+#define SG_UNIT_SELECT(chunk, idx) CHUNK_UNIT_SELECT(intel_sub_group_shuffle(chunk, (idx) / UNITS_PER_CHUNK), (idx) % UNITS_PER_CHUNK)
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+// Currently block read is 4 bytes aligned.
+#define ALIGNED_BLOCK_READ(ptr, byte_offset) intel_sub_group_block_read((const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+
+// Currently read is 4 bytes aligned.
+#define ALIGNED_READ(ptr, byte_offset) (*(const __global CHUNK_TYPE*)((const __global char*)(ptr) + (byte_offset)))
+
+// Currently block write is 16 bytes aligned.
+#define ALIGNED_BLOCK_WRITE(ptr, byte_offset, val) intel_sub_group_block_write((__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)), (val))
+
+// Currently block write is 4 bytes aligned.
+#define ALIGNED_WRITE(ptr, byte_offset, val) ((void)(*(__global CHUNK_TYPE*)((__global char*)(ptr) + (byte_offset)) = (val)))
+
+// Depends on batch size (aligned to greatest power of 2 which divides INPUT0_BATCH_NUM).
+#define INPUT0_READ(ptr, byte_offset) ALIGNED_READ(ptr, byte_offset)
+// Depends on number of responses (aligned to greatest power of 2 which divides FILTER_OFM_NUM).
+#define FILTER_READ(ptr, byte_offset) ALIGNED_READ(ptr, byte_offset)
+// Aligned to BYTES_PER_SG_READ.
+#define BIAS_READ(ptr, byte_offset) ALIGNED_READ(ptr, byte_offset)
+// Depends on batch size (aligned to greatest power of 2 which divides INPUT0_BATCH_NUM).
+#define OUTPUT_WRITE(ptr, byte_offset, val) ALIGNED_WRITE(ptr, byte_offset, val)
+
+
+/*
+#if FILTER_OFM_NUM % (2 * SUB_GROUP_SIZE) == 0 || (!FP16_UNIT_USED && FILTER_OFM_NUM % SUB_GROUP_SIZE == 0)
+    #define FILTER_READ(ptr, byte_offset) ALIGNED_BLOCK_READ(ptr, byte_offset)
+#elifs
+    #define FILTER_READ(ptr, byte_offset) ALIGNED_BLOCK_READ(ptr, byte_offset)
+#elif FILTER_OFM_NUM % 8 == 0
+#else
+#endif
+
+
+
+
+#if FP16_UNIT_USED
+    #define ALIGNED_FILTER_BLOCK_READ(ptr, byte_offset) as_half2(intel_sub_group_block_read((const __global uint*)((const __global char*)(ptr) + (byte_offset))))
+    #define FILTER_TYPE half2
+#else
+    #define ALIGNED_FILTER_BLOCK_READ(ptr, byte_offset) as_float(intel_sub_group_block_read((const __global uint*)((const __global char*)(ptr) + (byte_offset))))
+    #define FILTER_TYPE float
+#endif
+*/
+
+
+#if INPUT0_BATCH_NUM > 0 && INPUT0_BATCH_NUM % (SUB_GROUP_SIZE * CHUNK_BYTE_SIZE / UNIT_BYTE_SIZE) == 0
+#else
+    #error Kernel does not support specified input batch size.
+#endif
+
+
+
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+KERNEL (fully_connected_gpu_xb_xb_block_fp16)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    // constexpr:
+    const uint input_batch_byte_size       = INPUT0_BATCH_NUM * UNIT_BYTE_SIZE;
+    const uint input_byte_size             = INPUT0_ELEMENTS_COUNT * input_batch_byte_size;
+    const uint input_yxf_elems_per_sg_read = INPUT0_BATCH_NUM < UNITS_PER_SG_READ
+                                               ? UNITS_PER_SG_READ / INPUT0_BATCH_NUM
+                                               : 1;
+    const uint input_sg_reads_distance     = WORK_ITEMS_PER_BATCH * BYTES_PER_SG_READ;
+
+    // Size in bytes of all responses for single spatial/feature data point (the same as filter_yxf_elems_distance).
+    // Distance between two nearest xyf elements with the same response id.
+    const uint filter_response_byte_size = FILTER_OFM_NUM * UNIT_BYTE_SIZE;
+    // Cumulative size in bytes of all weights/filters.
+    const uint filters_byte_size         = INPUT0_ELEMENTS_COUNT * filter_response_byte_size;
+
+    const uint output_batch_byte_size = input_batch_byte_size;
+    const uint output_byte_size = FILTER_OFM_NUM * output_batch_byte_size;
+
+// ---------------------------------------------------------------------------------------------------------------------
+
+    // non-constexpr:
+    // Identifier of processing sub-group (each sub-group process UNITS_PER_SG_READ output responses for at least
+    // one data set in batch).
+    const uint sg_id          = get_group_id(0);
+    // Identifier of batch group (each batch group process up to UNITS_PER_SG_READ data sets from batch).
+    const uint batch_group_id = get_global_id(1);
+    // Identifier of work item element in processing sub-group.
+    const uint sg_elem_id     = get_sub_group_local_id();
+
+    // Input base offset in bytes (yxfb/xb format of input).
+    const uint input_base     = batch_group_id * BYTES_PER_SG_READ;
+
+    // Filter base offset in bytes (yxfb/xb format of weights).
+    const uint filter_base    = sg_id * BYTES_PER_SG_READ;
+
+    // Filter base offset in bytes (x/f format of biases).
+#if BIAS_TERM
+    const uint bias_base = filter_base;
+#endif
+    // Output base offset in bytes (xb format of output). INPUT0_BATCH_NUM is the same as OUTPUT_BATCH_NUM.
+    const uint output_base    = (sg_id * INPUT0_BATCH_NUM + batch_group_id) * BYTES_PER_SG_READ;
+
+    // Filter/input byte offsets in sub-group used duering read/write operations.
+    const uint sg_elem_offset = sg_elem_id * CHUNK_BYTE_SIZE;
+
+
+    // Accumulator over batch and response elements.
+    CHUNK_TYPE acc[UNITS_PER_SG_READ] = {};
+
+    // Iterate over yxf linear plane (both filters/weights and input).
+    for (uint input_offset = input_base, filter_offset = filter_base; input_offset < input_byte_size; input_offset += input_sg_reads_distance)
+    {
+        CHUNK_TYPE input_val = INPUT0_READ(input, input_offset + sg_elem_offset);
+
+        // Iterate over filters needed to process input read by sub-group.
+        for(uint elem_idx = 0; elem_idx < input_yxf_elems_per_sg_read; ++elem_idx)
+        {
+            CHUNK_TYPE filter_val = FILTER_READ(weight, filter_offset + sg_elem_offset);
+            filter_offset += filter_response_byte_size;
+
+            // MULTIPLY
+            // BATCH = 32x? (HF) / 16x? (F)
+            // Iterate over output features (indexed by acc_pos). acc[i] accumulates entire batch group for output feature i.
+            __attribute__((opencl_unroll_hint(UNITS_PER_SG_READ)))
+            for (uint acc_pos = 0; acc_pos < UNITS_PER_SG_READ; ++acc_pos)
+            {
+                acc[acc_pos] = AS_CHUNK(fma(AS_UNITS(input_val), SG_UNIT_SELECT(filter_val, acc_pos), AS_UNITS(acc[acc_pos])));
+            }
+        }
+    }
+
+    // WRITE OUTPUT
+    // BATCH = 32x? (HF) / 16x? (F)
+#if LAST_RG_SIZE > 0
+    if (sg_id < RG_COUNT - 1)
+#endif
+    {
+#if BIAS_TERM
+        CHUNK_TYPE bias_val = BIAS_READ(bias, bias_base + sg_elem_offset);
+#endif
+        uint output_offset = output_base;
+        __attribute__((opencl_unroll_hint(UNITS_PER_SG_READ)))
+        for (uint acc_pos = 0; acc_pos < UNITS_PER_SG_READ; ++acc_pos)
+        {
+#if BIAS_TERM
+            CHUNK_UNITS_TYPE output_val = AS_UNITS(acc[acc_pos]) + SG_UNIT_SELECT(bias_val, acc_pos);
+#else
+            CHUNK_UNITS_TYPE output_val = AS_UNITS(acc[acc_pos]);
+#endif
+            output_val = ACTIVATION(output_val, NL_M, NL_N);
+            OUTPUT_WRITE(output, output_offset + sg_elem_offset, AS_CHUNK(output_val));
+            output_offset += output_batch_byte_size;
+)__krnl"
+R"__krnl(        }
+    }
+#if LAST_RG_SIZE > 0
+    else
+    {
+        CHUNK_TYPE bias_val = BIAS_READ(bias, bias_base + sg_elem_offset);
+
+        uint output_offset = output_base;
+        __attribute__((opencl_unroll_hint(LAST_RG_SIZE)))
+        for (uint acc_pos = 0; acc_pos < LAST_RG_SIZE; ++acc_pos)
+        {
+#if BIAS_TERM
+            CHUNK_UNITS_TYPE output_val = AS_UNITS(acc[acc_pos]) + SG_UNIT_SELECT(bias_val, acc_pos);
+#else
+            CHUNK_UNITS_TYPE output_val = AS_UNITS(acc[acc_pos]);
+#endif
+            output_val = ACTIVATION(output_val, NL_M, NL_N);
+            OUTPUT_WRITE(output, output_offset + sg_elem_offset, AS_CHUNK(output_val));
+            output_offset += output_batch_byte_size;
+        }
+    }
+#endif
+}
+
+#undef CONCAT_TOKEN_HANDLER1
+#undef CONCAT_TOKEN
+#undef MAKE_VECTOR_TYPE
+#undef CVT_UNIT
+#undef CHUNK_UNITS_TYPE
+#undef AS_CHUNK
+#undef AS_UNITS
+#undef CHUNK_UNIT_SELECT
+
+#undef SG_UNIT_SELECT
+#undef ALIGNED_BLOCK_READ
+#undef ALIGNED_BLOCK_WRITE
+#undef INPUT0_READ
+#undef FILTER_READ
+#undef BIAS_READ
+#undef OUTPUT_WRITE
+
+)__krnl"},
+
+{"lookup_table_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+KERNEL(lookup_table)(const __global UNIT_TYPE* input0, const __global float* indices, __global UNIT_TYPE* output)
+{
+    const uint x    = (uint)get_global_id(0);
+    const uint b    = (uint)get_global_id(1);
+	const uint size = INPUT0_SIZE_X * INPUT0_SIZE_Y * INPUT0_FEATURE_NUM;
+    #ifdef INPUT0_LAYOUT_BFYX
+    const uint global_index = b * VAL_NUM + x;
+    output[global_index] = input0[(int)indices[global_index] + b*size];
+    #elif defined INPUT0_LAYOUT_YXFB
+    const uint global_index = b + x * INPUT0_BATCH_NUM;
+    output[global_index] = input0[(int)indices[global_index]*INPUT0_BATCH_NUM + b];
+    #endif
+}
+
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_direct_8_8_16",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+//#include "include/cnn_common.cl"
+
+//////////////////////////////////////////////////////////////////////////////
+// Direct Convolution
+#if defined(cl_intel_subgroups_short)
+
+#define TILE_M          DY      // Height of tile in input patches (src0)
+#define TILE_K          DX      // Width of tile in input patches (src0)
+#define TILE_N          16      // Num filter channels per tile (src1)
+
+#define TILE_X          8       // Width of tile loaded in input (src0)
+#define TILE_Y          8       // Height of tile loaded in input (src0)
+
+__attribute__((intel_reqd_sub_group_size(16)))
+__kernel void convolution_f16_8x8x16(
+    const __global half *src0,
+    __global half *dst,
+    const __global half *src1,
+    const __global half *biases)
+{
+    const unsigned global_x = get_global_id(0);
+    const unsigned global_y = get_global_id(1);
+    const unsigned global_z = get_global_id(2);
+    const unsigned out_fm   = global_z % ALIGNED_OFM;
+    const unsigned batch_id = global_z / ALIGNED_OFM;
+    const unsigned group_x = get_group_id(0);
+    const unsigned group_z = get_group_id(2);
+    const unsigned max_group_x = get_num_groups(0);
+    const unsigned local_z = get_local_id(2);
+
+    half blockC[TILE_M * TILE_K] = { 0 };
+
+    uint src0_offset_tile =
+       batch_id * INPUT_BATCH_PITCH                         // batch offset
+     + ( global_y * TILE_M * STRIDE_Y ) * INPUT_Y_PITCH   // y offset
+     + ( global_x * TILE_K * STRIDE_X );                    // x offset
+    uint src0_offset = src0_offset_tile
+     + ( local_z / ( TILE_X / 4 ) ) * INPUT_Y_PITCH       // y tile offset
+     + ( local_z % ( TILE_X / 4 ) ) * 4;                    // x tile offset
+
+    const __global half *src1_read = src1 + ( group_z * TILE_N % ALIGNED_OFM ) * 2;
+
+    unsigned patch_depth = 0;
+    __attribute__((opencl_unroll_hint(3)))
+    do
+    {
+        // Load atile (input) and btile (filters).
+        // Kernel data is partially interleaved.  Every 2 rows are interleaved at float16 granularity.
+        // The exception is that if KERNEL_WIDTH is odd the last row is not interleaved.  The non
+        // interleaved row is padded with zero to ensure same size as interleaved rows. This
+        // interleaving is done to increase consecutive data to fetch which reduces loads required.
+        // For example, this is how the kernel data would be arranged before/after interleaving for KERNEL_WIDTH=3.
+        // (0, 0) (8, 0) (16, 0) (24, 0) ...       (0, 0) (0, 1) (8, 0) (0, 1) (16, 0) (0, 1) (24, 0) ..
+        // (0, 1) (8, 1) (16, 1) (24, 1) ... =>    (0, 2) (8, 2) (16, 2) (24, 2) ...
+        // (0, 2) (8, 2) (16, 2) (24, 2) ...       ...
+        // ...
+
+        // in case the data is not aligned to sizeof(T)*KERNEL_WIDTH we need to use vload or set the data in a loop
+        half4 blockA = vload4(0, src0 + src0_offset );
+        src0_offset += INPUT_FEATURE_PITCH;
+
+        half blockB[KERNEL_WIDTH * KERNEL_HEIGHT];
+        ushort2* p2BlockB = (ushort2*)blockB;
+        ushort*  pBlockB =  (ushort* )blockB;
+
+        const bool kernel_slice_is_odd = ( KERNEL_WIDTH * KERNEL_HEIGHT ) % 2 == 1;
+        unsigned interleaved_y = 0;
+        LOOP(KERNEL_SLICE_DIV2, interleaved_y,
+        {
+            p2BlockB[interleaved_y] = intel_sub_group_block_read_us2( (const __global ushort*)src1_read );
+            src1_read += ALIGNED_OFM * 2;
+        } )
+        if ( kernel_slice_is_odd )
+        {
+            pBlockB[KERNEL_WIDTH * KERNEL_HEIGHT - 1] = intel_sub_group_block_read_us( (const __global ushort*)src1_read );
+            src1_read += ALIGNED_OFM * 2;
+        }
+
+#define BLOCK_A(n) sub_group_broadcast( blockA[(n)%4], (n)/4 )
+
+        // Perform MADs
+        // Loop through all patches in tile (patch_x/y)
+        // For each patch, sum values (x/y)
+        unsigned patch_y=0;
+        LOOP(TILE_M, patch_y,
+        {
+            unsigned patch_x=0;
+            LOOP(TILE_K, patch_x,
+            {
+                unsigned tile_idx = patch_y * TILE_X * STRIDE_Y + patch_x * STRIDE_X;
+                unsigned out_idx  = patch_y * TILE_K + patch_x;
+
+                unsigned y=0;
+                LOOP(KERNEL_HEIGHT, y,
+                {
+                    unsigned x=0;
+                    LOOP(KERNEL_WIDTH, x,
+                    {
+                        unsigned offset_idx = y * TILE_X + x;
+                        unsigned out_chan_idx = y * KERNEL_WIDTH + x;
+
+                        blockC[out_idx] = mad( BLOCK_A( tile_idx + offset_idx ), blockB[out_chan_idx], blockC[out_idx] );
+                    } )
+                } )
+            } )
+        } )
+    }
+    while ( ++patch_depth < INPUT_FEATURE_NUM );
+
+    // Dst resembles a cube of width x height x (output channel * batches).  Each tile writes:
+    // TILE_K x TILE_M x SIMD.  Partial writes most likely generated if output padding used.
+    // Group stores into vectors to expedite writeback.  One large write is faster than many
+    // small saves. Right-most column may be smaller if output width not divisible by tile width.
+    __global half *out = dst
+     + batch_id * OUTPUT_BATCH_PITCH            // batch offset
+     + out_fm * OUTPUT_FEATURE_PITCH              // channel offset
+     + ( global_y * TILE_M ) * OUTPUT_Y_PITCH // y offset
+     + ( global_x * TILE_K );                // x offset
+
+    if ( batch_id < OUTPUT_BATCH_NUM && out_fm < OUTPUT_FEATURE_NUM )
+    {
+        half bias = biases[out_fm];
+        if ( OUTPUT_SIZE_X % TILE_K == 0 ||
+             group_x < max_group_x - 1 )
+        {
+            typedef CAT( half, TILE_K ) half_t;
+            half bias = biases[out_fm];
+            for( unsigned y = 0; y < TILE_M; y++ )
+            {
+                if ( global_y * TILE_M + y < OUTPUT_SIZE_Y )
+                {
+                    half_t vBlockC;
+                    half *pvBlockC = (half*)&vBlockC;
+                    for (unsigned i = 0; i < TILE_K; i++) pvBlockC[i] = activation_function(blockC[y * TILE_K + i] + bias, NL_M, NL_N);
+                    *(__global half_t*)(out + y * OUTPUT_Y_PITCH) = vBlockC;
+                }
+            }
+        }
+        else
+        {
+            typedef CAT( half, RIGHT_PARTIAL_TILE_K ) half_t;
+            for( unsigned y = 0; y < TILE_M; y++ )
+            {
+                if ( global_y * TILE_M + y < OUTPUT_SIZE_Y )
+                {
+                    half_t vBlockC;
+                    half *pvBlockC = (half*)&vBlockC;
+                    for (unsigned i = 0; i < RIGHT_PARTIAL_TILE_K; i++) pvBlockC[i] = activation_function(blockC[y * TILE_K + i] + bias, NL_M, NL_N);
+                    *(__global half_t*)(out + y * OUTPUT_Y_PITCH) = vBlockC;
+                }
+            }
+        }
+    }
+}
+#endif // cl_intel_subgroups_short
+
+)__krnl"},
+
+{"reorder_weights_winograd_2x3_s1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(reorder_weights_winograd_2x3_s1)(const __global INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+#if OUTPUT_LAYOUT_WINOGRAD_2x3_S1_WEIGHTS
+    const uint input_tile_width = 3;
+    const uint input_tile_height = 1;
+    const uint in_tile_x_idx = get_global_id(0);
+    const uint in_tile_y_idx = get_global_id(1);
+#else //OUTPUT_LAYOUT_WINOGRAD_2x3_S1_FUSED_WEIGHTS
+    const uint input_tile_width = 1;
+    const uint input_tile_height = 3;
+    const uint in_tile_x_idx = get_global_id(1);
+    const uint in_tile_y_idx = get_global_id(0);
+#endif
+
+    const uint output_tile_width = 4;
+    const uint output_tile_height = 1;
+
+    const uint tile_x_idx = get_global_id(0);
+    const uint tile_y_idx = get_global_id(1);
+    const uint feature_idx = get_global_id(2) % INPUT0_IFM_NUM;
+    const uint batch_idx = get_global_id(2) / INPUT0_IFM_NUM;
+
+    uint in_idx = batch_idx * INPUT0_OFM_PITCH
+                 + feature_idx * INPUT0_IFM_PITCH
+                 + in_tile_y_idx * input_tile_height * INPUT0_Y_PITCH
+                 + in_tile_x_idx * input_tile_width * INPUT0_X_PITCH;
+
+#if OUTPUT_LAYOUT_WINOGRAD_2x3_S1_WEIGHTS
+    MAKE_VECTOR_TYPE(INPUT0_TYPE, 4) tile;
+    tile.x = input[in_idx]; in_idx += INPUT0_X_PITCH;
+    tile.y = input[in_idx]; in_idx += INPUT0_X_PITCH;
+    tile.z = input[in_idx];
+
+    uint out_idx = batch_idx * OUTPUT_OFM_PITCH
+                  + feature_idx * OUTPUT_IFM_PITCH
+                  + tile_y_idx * output_tile_height * OUTPUT_Y_PITCH
+                  + tile_x_idx * output_tile_width * OUTPUT_X_PITCH;
+
+    output[out_idx] = TO_OUTPUT_TYPE(tile.x); out_idx += OUTPUT_X_PITCH;
+    output[out_idx] = TO_OUTPUT_TYPE((tile.x + tile.y + tile.z) / 2.0f); out_idx += OUTPUT_X_PITCH;
+    output[out_idx] = TO_OUTPUT_TYPE((tile.x - tile.y + tile.z) / 2.0f); out_idx += OUTPUT_X_PITCH;
+    output[out_idx] = TO_OUTPUT_TYPE(tile.z);
+)__krnl"
+R"__krnl(#else //OUTPUT_LAYOUT_WINOGRAD_2x3_S1_FUSED_WEIGHTS
+    MAKE_VECTOR_TYPE(INPUT0_TYPE, 4) tile;
+    tile.x = input[in_idx]; in_idx += INPUT0_Y_PITCH;
+    tile.y = input[in_idx]; in_idx += INPUT0_Y_PITCH;
+    tile.z = input[in_idx];
+
+    const uint weightsOSplit = 8;
+    const uint oDivSplit = OUTPUT_OFM_NUM / 8;
+    uint out_idx = batch_idx % 8 + tile_y_idx * output_tile_height * weightsOSplit +
+        tile_x_idx * output_tile_width * weightsOSplit * OUTPUT_SIZE_Y +
+        batch_idx / 8 * weightsOSplit * OUTPUT_SIZE_X * OUTPUT_SIZE_Y +
+        feature_idx * weightsOSplit * OUTPUT_SIZE_X * OUTPUT_SIZE_Y * oDivSplit;
+
+    output[out_idx] = TO_OUTPUT_TYPE(tile.x); out_idx += weightsOSplit * OUTPUT_SIZE_Y;
+    output[out_idx] = TO_OUTPUT_TYPE((tile.x + tile.y + tile.z) / 2.0f); out_idx += weightsOSplit * OUTPUT_SIZE_Y;
+    output[out_idx] = TO_OUTPUT_TYPE((tile.x - tile.y + tile.z) / 2.0f); out_idx += weightsOSplit * OUTPUT_SIZE_Y;
+    output[out_idx] = TO_OUTPUT_TYPE(tile.z);
+#endif
+}
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+)__krnl"
+R"__krnl(        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+)__krnl"
+R"__krnl(    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(convolution)(
+    __global INPUT0_TYPE* input,
+    __global OUTPUT_TYPE* output,
+    __global FILTER_TYPE* weights,
+#if BIAS_TERM
+    __global BIAS_TYPE* biases,
+#endif
+#if QUANTIZATION_TERM
+    __global float* quantizations,
+#endif
+#if CALIBRATION_TERM
+    __global float* calibrations,
+#endif
+    uint split_idx)
+{
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+#if OUTPUT_BATCH_NUM == 1
+    const uint f = get_global_id(2);
+    const uint b = 0;
+#else
+    const uint f = get_global_id(2) % OUTPUT_FEATURE_NUM;
+    const uint b = get_global_id(2) / OUTPUT_FEATURE_NUM;
+#endif
+
+#if QUANTIZATION_TERM
+    int dotProd = 0;
+#else
+    UNIT_TYPE dotProd = UNIT_VAL_ZERO;
+#endif
+    const int input_x = x * STRIDE_SIZE_X - PADDING_SIZE_X;
+    const int input_y = y * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+#if DEPTHWISE_SEPARABLE_OPT
+    const uint in_split_offset = (f / FILTER_OFM_NUM) * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#else
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * FILTER_IFM_NUM;
+#endif
+    const uint filter_offset = f*FILTER_OFM_PITCH;
+    const uint input_offset = b*INPUT0_BATCH_PITCH + INPUT0_OFFSET + in_split_offset;
+
+    for (uint k = 0; k < FILTER_IFM_NUM; ++k)
+)__krnl"
+R"__krnl(    {
+        for (uint j = 0; j < FILTER_SIZE_Y ; ++j)
+        {
+            const int input_offset_y = input_y + j * DILATION_SIZE_Y;
+            const bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+
+            if(!zero_y)
+            {
+                for (uint i = 0; i < FILTER_SIZE_X ; ++i)
+                {
+                    const int input_offset_x = input_x + i * DILATION_SIZE_X;
+                    const bool zero_x = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+
+                    if(!zero_x)
+                    {
+                        uint input_idx = input_offset + (uint)input_offset_x*INPUT0_X_PITCH + (uint)input_offset_y*INPUT0_Y_PITCH + k*INPUT0_FEATURE_PITCH;
+                        uint filter_idx = filter_offset + k*FILTER_IFM_PITCH + j*FILTER_Y_PITCH + i*FILTER_X_PITCH;
+#if QUANTIZATION_TERM
+                        dotProd += (int)input[input_idx] * (int)weights[filter_idx];
+#else
+                        dotProd += input[input_idx] * weights[filter_idx];
+#endif
+                    }
+                }
+            }
+        }
+    }
+
+#if BIAS_TERM
+#if   BIAS_PER_OUTPUT
+    const uint bias_index = GET_DATA_INDEX(BIAS, b, f, y, x);
+#elif BIAS_PER_OFM
+    const uint bias_index = f;
+#endif
+#if QUANTIZATION_TERM
+#if CALIBRATION_TERM
+
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * calibrations[f]);
+#else  // CALIBRATION_TERM
+    dotProd = (UNIT_TYPE)round(((float)dotProd * quantizations[f] * I_QF + biases[bias_index]) * O_QF);
+#endif // CALIBRATION_TERM
+#else  // QUANTIZATION_TERM
+    dotProd += (UNIT_TYPE)biases[bias_index];
+#endif // QUANTIZATION_TERM
+#endif
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    const uint dst_index = GET_DATA_INDEX(OUTPUT, b, f, y, x) + out_split_offset;
+#if QUANTIZATION_TERM
+    output[dst_index] = ACTIVATION(convert_char(dotProd), NL_M, NL_N);
+#else
+    output[dst_index] = ACTIVATION(dotProd, NL_M, NL_N);
+#endif
+
+}
+
+)__krnl"},
+
+{"reorder_data_fast_b1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+)__krnl"
+R"__krnl(    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+
+
+///////////////////////// Input Index /////////////////////////
+inline uint FUNC(get_input_index)(uint b, uint f, uint y, uint x)
+{
+)__krnl"
+R"__krnl(#if   INPUT0_SIMPLE
+    return GET_DATA_INDEX(INPUT0, b, f, y, x);
+#elif defined INPUT0_LAYOUT_BS_F_BSV8__AF8  || \
+      defined INPUT0_LAYOUT_BS_F_BSV16__AF8
+    return GET_DATA_BS_FYX_BSV8_INDEX(INPUT0, b, f, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_BF8_XY16
+    return GET_DATA_BF8_XY16_INDEX(INPUT0, b, f, y, x);
+#else
+#error reorder_data_fast_b1.cl: input format - not supported
+#endif
+}
+
+///////////////////////// Output Index /////////////////////////
+
+inline uint FUNC(get_output_index)(uint b, uint f, uint y, uint x)
+{
+#if   OUTPUT_SIMPLE
+    return GET_DATA_INDEX(OUTPUT, b, f, y, x);
+#elif defined OUTPUT_LAYOUT_BS_F_BSV8__AF8  || \
+      defined OUTPUT_LAYOUT_BS_F_BSV16__AF8
+    return GET_DATA_BS_FYX_BSV8_INDEX(OUTPUT, b, f, y, x, SUB_GROUP_SIZE);
+#elif defined OUTPUT_LAYOUT_BF8_XY16
+    return GET_DATA_BF8_XY16_INDEX(OUTPUT, b, f, y, x);
+#else
+#error reorder_data_fast_b1.cl: output format - not supported
+#endif
+}
+
+KERNEL (reorder_data_fast_b1)(
+    const __global INPUT_REORDER_TYPE* input,
+    __global OUTPUT_REORDER_TYPE* output
+#ifdef MEAN_SUBTRACT_IN_BUFFER
+    , __global MEAN_SUBTRACT_TYPE* mean_subtract
+#endif
+    )
+{
+    uint data_idx = get_global_id(0);
+    if(data_idx >= ELEMENTS_COUNT)
+        return;
+
+#if !CHANGE_DATA_TYPE_ONLY
+ // We're checking output layout instead of input layout intentionally for performance reason
+#if defined OUTPUT_LAYOUT_BFYX
+    uint tmp_data_idx = data_idx / INPUT0_BATCH_NUM;
+    const uint b = data_idx - tmp_data_idx * INPUT0_BATCH_NUM;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_FEATURE_NUM;
+    const uint f = data_idx - tmp_data_idx * INPUT0_FEATURE_NUM;
+    data_idx = tmp_data_idx;
+
+    // We're first iterating over Y then over X for performance reason
+    // Otherwise we could compute X and Y in reverse order
+    tmp_data_idx = data_idx / INPUT0_SIZE_X;
+    const uint x = data_idx - tmp_data_idx * INPUT0_SIZE_X;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx  = data_idx / INPUT0_SIZE_Y;
+    const uint y = data_idx - tmp_data_idx * INPUT0_SIZE_Y;
+#elif defined OUTPUT_LAYOUT_YXFB
+    // We're first iterating over Y then over X for performance reason
+    // Otherwise we could compute X and Y in reverse order
+    uint tmp_data_idx = data_idx / INPUT0_SIZE_X;
+    const uint x = data_idx - tmp_data_idx * INPUT0_SIZE_X;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_SIZE_Y;
+    const uint y = data_idx - tmp_data_idx * INPUT0_SIZE_Y;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_FEATURE_NUM;
+    const uint f = data_idx - tmp_data_idx * INPUT0_FEATURE_NUM;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx  = data_idx / INPUT0_BATCH_NUM;
+    const uint b = data_idx - tmp_data_idx * INPUT0_BATCH_NUM;
+#else // BYXF?
+    uint tmp_data_idx = data_idx / INPUT0_BATCH_NUM;
+    const uint b = data_idx - tmp_data_idx * INPUT0_BATCH_NUM;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_SIZE_Y;
+    const uint y = data_idx - tmp_data_idx * INPUT0_SIZE_Y;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_SIZE_X;
+    const uint x = data_idx - tmp_data_idx * INPUT0_SIZE_X;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx  = data_idx / INPUT0_FEATURE_NUM;
+    const uint f = data_idx - tmp_data_idx * INPUT0_FEATURE_NUM;
+#endif
+#endif
+
+#if CHANGE_DATA_TYPE_ONLY
+    const uint input_idx  = data_idx;
+    const uint output_idx = data_idx;
+#else
+    uint4 ov = FUNC_CALL(reshape_dims)(b,f,y,x, INPUT0_SIZE_Y, INPUT0_SIZE_X, OUTPUT_SIZE_Y, OUTPUT_SIZE_X, INPUT0_DIMS, OUTPUT_DIMS);
+    const uint input_idx  = FUNC_CALL(get_input_index)(b, f, y, x);
+    const uint output_idx = FUNC_CALL(get_output_index)(ov[0],ov[1],ov[2],ov[3]);
+#endif
+
+#if   defined MEAN_SUBTRACT_INSIDE_PARAMS
+    float res = TO_MEAN_TYPE(input[input_idx]);
+    res -= VALUE_TO_SUBTRACT[f % VALUE_TO_SUBTRACT_SIZE];
+#elif defined MEAN_SUBTRACT_IN_BUFFER
+    MEAN_SUBTRACT_TYPE res = TO_MEAN_TYPE(input[input_idx]);
+    uint4 msv = FUNC_CALL(reshape_dims)(b,f,y,x, INPUT0_SIZE_Y, INPUT0_SIZE_X, MEAN_SUBTRACT_SIZE_Y, MEAN_SUBTRACT_SIZE_X, INPUT0_DIMS, MEAN_SUBTRACT_DIMS);
+    res -= mean_subtract[GET_DATA_INDEX_SAFE(MEAN_SUBTRACT, msv[0], msv[1], msv[2], msv[3])];
+#else
+    CALC_TYPE res = TO_CALC_TYPE(input[input_idx]);
+#endif
+
+    output[output_idx] = ACTIVATION(TO_OUTPUT_REORDER_TYPE(res), NL_M ,NL_N);
+}
+
+)__krnl"},
+
+{"normalize_gpu_within_spatial_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+#if FP16_UNIT_USED
+    #define UNIT_CVT_FUNC(val) convert_half(val)
+#else
+    #define UNIT_CVT_FUNC(val) (val)
+#endif
+
+
+KERNEL (normalize_gpu_within_spatial_bfyx)(const __global UNIT_TYPE* input, __global UNIT_TYPE* output, const __global UNIT_TYPE* scale_input)
+{
+    const uint x = get_global_id(0);
+    const uint y = get_global_id(1);
+    const uint b = get_global_id(2);
+
+    const uint input_first = INPUT0_OFFSET + b*INPUT0_BATCH_PITCH + y*INPUT0_Y_PITCH + x*INPUT0_X_PITCH;
+
+    // Compute norm
+    uint input_idx = input_first;
+    float norm = EPSILON;
+    for (int i = 0; i < INPUT0_FEATURE_NUM; i++)
+    {
+        float value = (float)input[input_idx];
+        norm = mad(value, value, norm);
+        input_idx += INPUT0_FEATURE_PITCH;
+    }
+
+    uint output_idx = OUTPUT_OFFSET + b*OUTPUT_BATCH_PITCH + y*OUTPUT_Y_PITCH + x*OUTPUT_X_PITCH;
+
+    if(norm <= THRESHOLD)
+    {
+        norm = 0;
+    }
+    else
+    {
+        norm = native_powr(norm, -0.5f);
+    }
+
+    // Scale the input
+    input_idx = input_first;
+    for (int f = 0; f < INPUT0_FEATURE_NUM; f++)
+    {
+#if SCALE_TABLE_FEATURE_NUM == 1
+        const uint scale_index = 0;
+#elif INPUT0_FEATURE_NUM <= SCALE_TABLE_FEATURE_NUM
+        const uint scale_index = f;
+#else
+        const uint scale_index = f % SCALE_TABLE_FEATURE_NUM;
+#endif
+
+        output[output_idx] = ACTIVATION(UNIT_CVT_FUNC(norm) * input[input_idx] * scale_input[scale_index], NL_M, NL_N);
+        output_idx += OUTPUT_FEATURE_PITCH;
+        input_idx += INPUT0_FEATURE_PITCH;
+    }
+}
+
+
+#undef UNIT_CVT_FUNC
+
+)__krnl"},
+
+{"eltwise_fs_bs_yx_bsv4_fsv32",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#define GET_INDEX(src) \
+    GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(src, d4, d3, d2, d1)
+
+int16 FUNC(get_int16)(const __global UNIT_TYPE* src, uint idx)
+{
+    int4 int_data = as_int4(intel_sub_group_block_read4((const __global uint*)(src + idx)));
+    int16 to_return;
+    for(uint b = 0; b < 4; b++)
+    {
+        for(uint f = 0; f < 4; f++)
+        {
+            to_return[b * 4 + f] = as_char4(int_data[b])[f];
+        }
+    }
+    return to_return;
+}
+#define GET_INPUT(A, B) FUNC_CALL(get_int16)(A, GET_INDEX(B))
+
+__attribute__((intel_reqd_sub_group_size(8)))
+KERNEL(eltwise_fs_bs_yx_bsv4_fsv32)(
+    INPUTS_DECLS
+    __global UNIT_TYPE* output
+#if CALIBRATION_TERM
+    , const __global float* calibrations
+#endif
+    )
+{
+    const uint of_32_aligned = ((OUTPUT_FEATURE_NUM + 31) / 32) * 32;
+    const uint d1 = get_global_id(0);   // X
+    const uint d2 = get_global_id(1);   // Y
+    const uint d3 = (get_global_id(2) * 4) % of_32_aligned; // Feature
+    const uint d4 = 4 * ((get_global_id(2) * 4) / of_32_aligned); // Batch
+
+    int16 res;
+
+    DO_ELTWISE;
+
+    int4 char_result;
+    for(uint b = 0; b < 4; b++)
+    {
+)__krnl"
+R"__krnl(        char4 char_res;
+        for(uint f = 0; f < 4; f++)
+        {
+            int res_tmp = res[b * 4 + f];
+        #if CALIBRATION_TERM
+            res_tmp = (int)round(((float)res_tmp) * calibrations[d3+f]);
+        #else  // CALIBRATION_TERM
+            res_tmp = (int)round(((float)res_tmp) * O_QF);
+        #endif // CALIBRATION_TERM
+            char_res[f] = ACTIVATION(convert_char(res_tmp), NL_M, NL_N);
+        }
+        // pack 4 chars into int
+        char_result[b] = as_int(char_res);
+    }
+
+    uint output_offset = GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(OUTPUT, d4, d3, d2, d1);
+    intel_sub_group_block_write4((__global uint*)(output + output_offset), as_uint4(char_result));
+}
+
+#undef GET_INDEX
+
+)__krnl"},
+
+{"concatenation_gpu_depth_bfyx_no_pitch",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+//
+// In this kernel we are processing "fyx" as flatten 1D "elements".
+// As long as we can we use block read/write.
+// For last SIMD in which we have to write only partial data we use normal read/write to buffer.
+//
+
+// must be 8 as long as we use block_read8/write8
+#define ELEMENTS_PER_WORK_ITEM 8
+#define WORK_GROUP_SIZE 16
+#define INPUT0_ELEMENTS_COUNT (INPUT0_LENGTH/INPUT0_BATCH_NUM)
+
+#if FP16_UNIT_USED
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_half8(intel_sub_group_block_read_us8((const __global ushort*)(ptr) + (byte_offset)))
+    #define ALIGNED_BLOCK_WRITE8(ptr, byte_offset, val) intel_sub_group_block_write_us8((__global ushort*)(ptr) + (byte_offset), as_ushort8(val))
+#else
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_float8(intel_sub_group_block_read8((const __global uint*)(ptr) + (byte_offset)))
+    #define ALIGNED_BLOCK_WRITE8(ptr, byte_offset, val) intel_sub_group_block_write8((__global uint*)(ptr) + (byte_offset), as_uint8(val))
+#endif
+
+__attribute__((reqd_work_group_size(1, WORK_GROUP_SIZE, 1)))
+__attribute__((intel_reqd_sub_group_size(WORK_GROUP_SIZE)))
+KERNEL (concatenation_gpu_depth_bfyx_no_padding)(__global UNIT_TYPE* input, __global UNIT_TYPE* output, uint output_offset_in_concat_axis)
+{
+    const uint batch_id = get_group_id(0);
+
+    // Which pack of 16*8 elements we are processing.
+    uint element_group_id = get_group_id(1);
+    uint element_offset = (uint)get_global_id(1) * ELEMENTS_PER_WORK_ITEM;
+
+    const uint element_group_offset = element_group_id * WORK_GROUP_SIZE * ELEMENTS_PER_WORK_ITEM;
+
+    const uint input_offset = INPUT0_OFFSET + element_group_offset + batch_id * INPUT0_BATCH_PITCH;
+    const uint output_batch_offset = batch_id * OUTPUT_BATCH_PITCH;
+    const uint output_offset = OUTPUT_OFFSET + element_group_offset + output_batch_offset + output_offset_in_concat_axis*OUTPUT_PITCHES[CONCAT_AXIS_INDEX];
+
+    //Check if current group in batch starts from 16-byte aligned pos. If not then move block read to 16-byte aligned position.
+    //Requirement for intel_sub_group_block_write8.
+    uint align_offset = 0;
+    const uint group_start_pos = output_offset;
+    if(group_start_pos % WORK_GROUP_SIZE != 0)
+    {
+)__krnl"
+R"__krnl(        uint next_aligned_pos = group_start_pos / WORK_GROUP_SIZE * WORK_GROUP_SIZE + WORK_GROUP_SIZE;
+        align_offset = next_aligned_pos - group_start_pos;
+    }
+
+    if(element_group_offset + align_offset + WORK_GROUP_SIZE * ELEMENTS_PER_WORK_ITEM < INPUT0_ELEMENTS_COUNT)
+    {
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) in = ALIGNED_BLOCK_READ8(input, input_offset + align_offset);
+        ALIGNED_BLOCK_WRITE8(output, output_offset + align_offset, ACTIVATION(in, NL_M, NL_N));
+
+        //Fill the values that were missed upon adding align_offset
+        if((align_offset != 0) && (element_offset + output_batch_offset < group_start_pos + align_offset))
+        {
+            for(uint i = 0; i < align_offset; i++)
+                output[output_offset + i] = ACTIVATION(input[input_offset + i], NL_M, NL_N);
+        }
+    }
+    else
+    {
+        // This is the last SIMD that needs to write only partial data.
+        uint element_offset_in_workitem = element_offset - element_group_offset;
+        for(uint i = 0; i < ELEMENTS_PER_WORK_ITEM; i++)
+        {
+            if(element_offset + i >= INPUT0_ELEMENTS_COUNT)
+                return;
+
+            output[output_offset + element_offset_in_workitem] = ACTIVATION(input[input_offset + element_offset_in_workitem], NL_M, NL_N);
+            element_offset_in_workitem++;
+        }
+    }
+}
+
+#undef INPUT0_ELEMENTS_COUNT
+#undef WORK_GROUP_SIZE
+#undef ELEMENTS_PER_WORK_ITEM
+#undef ALIGNED_BLOCK_READ8
+#undef ALIGNED_BLOCK_WRITE8
+
+)__krnl"},
+
+{"softmax_loss_grad_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(softmax_loss_grad_gpu_ref)(
+    const __global INPUT0_TYPE* input_pred,
+    __global OUTPUT_TYPE* output,
+    const __global INPUT1_TYPE* labels
+    )
+{
+    const uint b_x          = get_global_id(0);
+    const uint batch_id     = b_x / OUTPUT_SIZE_X;
+    const uint x            = b_x % OUTPUT_SIZE_X;
+
+    const uint input_pred_idx = GET_DATA_INDEX(INPUT0, batch_id, 0, 0, x);
+    const uint labels_idx = GET_DATA_INDEX(INPUT1, batch_id, 0, 0, 0);
+
+    UNIT_TYPE label = labels[labels_idx];
+    const uint output_idx = GET_DATA_INDEX(OUTPUT, batch_id, 0, 0, x);
+
+    if(label == x)
+        output[output_idx] = input_pred[input_pred_idx] - 1;
+    else
+        output[output_idx] = input_pred[input_pred_idx];
+}
+
+)__krnl"},
+
+{"fully_connected_gpu_bs_f_bsv8_af8_vload",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if FP16_UNIT_USED
+    // Block read - currently block is 4 bytes aligned.
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_half8(intel_sub_group_block_read_us8((const __global ushort*)(ptr) + (byte_offset)))
+
+    #define MULTIPLY_BLOCKS_8x8(_result, _blockA, _blockB)  \
+    {   \
+        const half8 acol0 = TRANSPOSE_BLOCK_8_FP16( _blockA.s0 ); \
+        const half8 acol1 = TRANSPOSE_BLOCK_8_FP16( _blockA.s1 ); \
+        const half8 acol2 = TRANSPOSE_BLOCK_8_FP16( _blockA.s2 ); \
+        const half8 acol3 = TRANSPOSE_BLOCK_8_FP16( _blockA.s3 ); \
+        const half8 acol4 = TRANSPOSE_BLOCK_8_FP16( _blockA.s4 ); \
+        const half8 acol5 = TRANSPOSE_BLOCK_8_FP16( _blockA.s5 ); \
+        const half8 acol6 = TRANSPOSE_BLOCK_8_FP16( _blockA.s6 ); \
+        const half8 acol7 = TRANSPOSE_BLOCK_8_FP16( _blockA.s7 ); \
+        _result = fma( _blockB.s0, acol0, _result ); \
+        _result = fma( _blockB.s1, acol1, _result ); \
+        _result = fma( _blockB.s2, acol2, _result ); \
+        _result = fma( _blockB.s3, acol3, _result ); \
+        _result = fma( _blockB.s4, acol4, _result ); \
+        _result = fma( _blockB.s5, acol5, _result ); \
+        _result = fma( _blockB.s6, acol6, _result ); \
+        _result = fma( _blockB.s7, acol7, _result ); \
+    }
+#else
+    // Block read - currently block is 4 bytes aligned.
+    #define ALIGNED_BLOCK_READ8(ptr, byte_offset) as_float8(intel_sub_group_block_read8((const __global uint*)(ptr) + (byte_offset)))
+
+    #define MULTIPLY_BLOCKS_8x8(_result, _blockA, _blockB)  \
+    {   \
+        const float8 acol0 = TRANSPOSE_BLOCK_8( _blockA.s0 ); \
+        const float8 acol1 = TRANSPOSE_BLOCK_8( _blockA.s1 ); \
+        const float8 acol2 = TRANSPOSE_BLOCK_8( _blockA.s2 ); \
+        const float8 acol3 = TRANSPOSE_BLOCK_8( _blockA.s3 ); \
+        const float8 acol4 = TRANSPOSE_BLOCK_8( _blockA.s4 ); \
+        const float8 acol5 = TRANSPOSE_BLOCK_8( _blockA.s5 ); \
+        const float8 acol6 = TRANSPOSE_BLOCK_8( _blockA.s6 ); \
+        const float8 acol7 = TRANSPOSE_BLOCK_8( _blockA.s7 ); \
+        _result = mad( _blockB.s0, acol0, _result ); \
+        _result = mad( _blockB.s1, acol1, _result ); \
+        _result = mad( _blockB.s2, acol2, _result ); \
+        _result = mad( _blockB.s3, acol3, _result ); \
+)__krnl"
+R"__krnl(        _result = mad( _blockB.s4, acol4, _result ); \
+        _result = mad( _blockB.s5, acol5, _result ); \
+        _result = mad( _blockB.s6, acol6, _result ); \
+        _result = mad( _blockB.s7, acol7, _result ); \
+    }
+#endif
+
+#define SUB_GROUP_SIZE 8
+
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+__attribute__((intel_reqd_sub_group_size(SUB_GROUP_SIZE)))
+KERNEL (fully_connected_gpu_xb_bs_xs_xsv8_bsv8_vload)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint global_id = get_global_id(0);
+    const uint group_id = get_group_id(0);
+    const uint batch_group_id = get_global_id(1); // which part of batches we are computing, for example for batch 64 we compute batches 0..31 for batch_group_id == 0 and batches 32..65 for batch_group_id == 1
+    const uint id_in_sub_group = get_sub_group_local_id();
+
+    const uint out_id = (id_in_sub_group * BATCHES_PER_WORK_ITEM * (uint)get_global_size(1)) / SUB_GROUP_SIZE + group_id * BATCHES_PER_WORK_ITEM * NEURONS_PER_WORK_ITEM * (uint)get_global_size(1) + (BATCHES_PER_WORK_ITEM * batch_group_id) / SUB_GROUP_SIZE;
+
+    uint neuronIdx = id_in_sub_group + group_id * SUB_GROUP_SIZE * NEURONS_PER_WORK_ITEM;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC00 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC10 = UNIT_VAL_ZERO;
+
+#if BATCHES_PER_WORK_ITEM >= 16
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC01 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC11 = UNIT_VAL_ZERO;
+#if BATCHES_PER_WORK_ITEM >= 32
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC02 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC12 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC03 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC13 = UNIT_VAL_ZERO;
+#endif
+#endif
+
+    uint weight_offset = id_in_sub_group + SUB_GROUP_SIZE * group_id * NEURONS_PER_WORK_ITEM * INPUT0_ELEMENTS_COUNT;
+#if NEURONS_PER_WORK_ITEM > 1
+
+    uint weight_offset2 = weight_offset + SUB_GROUP_SIZE * INPUT0_ELEMENTS_COUNT;
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+
+    uint input_idx = id_in_sub_group + batch_group_id * BATCHES_PER_WORK_ITEM * INPUT0_ELEMENTS_COUNT;
+    for(uint h = 0; h < INPUT0_ELEMENTS_COUNT / 8; h++)
+    {
+        // read input data in blocks ( 8 batch * 8 x )
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA00 = ALIGNED_BLOCK_READ8(input, input_idx);
+#if BATCHES_PER_WORK_ITEM >= 16
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA01 = ALIGNED_BLOCK_READ8(input, input_idx + (INPUT0_ELEMENTS_COUNT*8));
+#if BATCHES_PER_WORK_ITEM >= 32
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA02 = ALIGNED_BLOCK_READ8(input, input_idx + (INPUT0_ELEMENTS_COUNT*16));
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA03 = ALIGNED_BLOCK_READ8(input, input_idx + (INPUT0_ELEMENTS_COUNT*24));
+#endif
+#endif
+
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockB00 = ALIGNED_BLOCK_READ8(weight, weight_offset); weight_offset += 64;
+
+        MULTIPLY_BLOCKS_8x8(blockC00, blockA00, blockB00)
+#if BATCHES_PER_WORK_ITEM >= 16
+        MULTIPLY_BLOCKS_8x8(blockC01, blockA01, blockB00)
+#if BATCHES_PER_WORK_ITEM >= 32
+        MULTIPLY_BLOCKS_8x8(blockC02, blockA02, blockB00)
+        MULTIPLY_BLOCKS_8x8(blockC03, blockA03, blockB00)
+#endif
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockB10 = ALIGNED_BLOCK_READ8(weight, weight_offset2); weight_offset2 += 64;
+
+        MULTIPLY_BLOCKS_8x8(blockC10, blockA00, blockB10)
+#if BATCHES_PER_WORK_ITEM >= 16
+        MULTIPLY_BLOCKS_8x8(blockC11, blockA01, blockB10)
+#if BATCHES_PER_WORK_ITEM >= 32
+        MULTIPLY_BLOCKS_8x8(blockC12, blockA02, blockB10)
+        MULTIPLY_BLOCKS_8x8(blockC13, blockA03, blockB10)
+#endif
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+        input_idx += 64; // 64 because of input format which have blocks of 64 elements
+    }
+
+#if BIAS_TERM
+    blockC00 += bias[neuronIdx];
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC01 += bias[neuronIdx];
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC02 += bias[neuronIdx];
+    blockC03 += bias[neuronIdx];
+#endif
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+    blockC10 += bias[neuronIdx+8];
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC11 += bias[neuronIdx+8];
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC12 += bias[neuronIdx+8];
+    blockC13 += bias[neuronIdx+8];
+#endif
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+#endif // #if BIAS_TERM
+    blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC02 = ACTIVATION(blockC02, NL_M, NL_N);
+    blockC03 = ACTIVATION(blockC03, NL_M, NL_N);
+#endif
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+    blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC11 = ACTIVATION(blockC11, NL_M, NL_N);
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC12 = ACTIVATION(blockC12, NL_M, NL_N);
+    blockC13 = ACTIVATION(blockC13, NL_M, NL_N);
+#endif
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+
+    if(neuronIdx >= OUTPUT_ELEMENTS_COUNT)
+        return;
+
+    vstore8(blockC00, out_id, output);
+#if BATCHES_PER_WORK_ITEM >= 16
+    vstore8(blockC01, out_id + 1, output);
+#if BATCHES_PER_WORK_ITEM >= 32
+    vstore8(blockC02, out_id + 2, output);
+    vstore8(blockC03, out_id + 3, output);
+#endif
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+    if(neuronIdx + 8 >= OUTPUT_ELEMENTS_COUNT)
+        return;
+
+    vstore8(blockC10, out_id+INPUT0_BATCH_NUM, output);
+#if BATCHES_PER_WORK_ITEM >= 16
+    vstore8(blockC11, out_id+INPUT0_BATCH_NUM+1, output);
+#if BATCHES_PER_WORK_ITEM >= 32
+    vstore8(blockC12, out_id+INPUT0_BATCH_NUM+2, output);
+    vstore8(blockC13, out_id+INPUT0_BATCH_NUM+3, output);
+#endif
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+}
+
+#undef SUB_GROUP_SIZE
+#undef ALIGNED_BLOCK_READ8
+#undef MAKE_VECTOR_TYPE
+#undef CONCAT_TOKEN
+#undef CONCAT_TOKEN_HANDLER1
+#undef MULTIPLY_BLOCKS_8x8
+
+)__krnl"},
+
+{"fully_connected_gpu_fb_io_b8_f8_vload",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if FP16_UNIT_USED
+    #define MULTIPLY_BLOCKS_8x8(_result, _blockA, _blockB)  \
+    {   \
+        const half8 acol0 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 0 ); \
+        const half8 acol1 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 1 ); \
+        const half8 acol2 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 2 ); \
+        const half8 acol3 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 3 ); \
+        const half8 acol4 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 4 ); \
+        const half8 acol5 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 5 ); \
+        const half8 acol6 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 6 ); \
+        const half8 acol7 = TRANSPOSE_BLOCK_8_COL_FP16( _blockA, 7 ); \
+        _result = fma( _blockB.s0, acol0, _result ); \
+        _result = fma( _blockB.s1, acol1, _result ); \
+        _result = fma( _blockB.s2, acol2, _result ); \
+        _result = fma( _blockB.s3, acol3, _result ); \
+        _result = fma( _blockB.s4, acol4, _result ); \
+        _result = fma( _blockB.s5, acol5, _result ); \
+        _result = fma( _blockB.s6, acol6, _result ); \
+        _result = fma( _blockB.s7, acol7, _result ); \
+    }
+#else
+    #define MULTIPLY_BLOCKS_8x8(_result, _blockA, _blockB)  \
+    {   \
+        const float8 acol0 = TRANSPOSE_BLOCK_8_COL( _blockA, 0 ); \
+        const float8 acol1 = TRANSPOSE_BLOCK_8_COL( _blockA, 1 ); \
+        const float8 acol2 = TRANSPOSE_BLOCK_8_COL( _blockA, 2 ); \
+        const float8 acol3 = TRANSPOSE_BLOCK_8_COL( _blockA, 3 ); \
+        const float8 acol4 = TRANSPOSE_BLOCK_8_COL( _blockA, 4 ); \
+        const float8 acol5 = TRANSPOSE_BLOCK_8_COL( _blockA, 5 ); \
+        const float8 acol6 = TRANSPOSE_BLOCK_8_COL( _blockA, 6 ); \
+        const float8 acol7 = TRANSPOSE_BLOCK_8_COL( _blockA, 7 ); \
+        _result = mad( _blockB.s0, acol0, _result ); \
+        _result = mad( _blockB.s1, acol1, _result ); \
+        _result = mad( _blockB.s2, acol2, _result ); \
+        _result = mad( _blockB.s3, acol3, _result ); \
+        _result = mad( _blockB.s4, acol4, _result ); \
+        _result = mad( _blockB.s5, acol5, _result ); \
+        _result = mad( _blockB.s6, acol6, _result ); \
+        _result = mad( _blockB.s7, acol7, _result ); \
+    }
+#endif
+)__krnl"
+R"__krnl(
+#define SUB_GROUP_SIZE 8
+
+__attribute__((reqd_work_group_size(SUB_GROUP_SIZE, 1, 1)))
+KERNEL (fully_connected_gpu_xb_xb_b8_x8_vload)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output,
+    const __global UNIT_TYPE* weight
+#if BIAS_TERM
+    , __global UNIT_TYPE* bias)
+#else
+    )
+#endif
+{
+    const uint global_id = get_global_id(0);
+    const uint group_id = get_global_id(1); // which part of batches we are computing, for example for batch 64 we compute batches 0..31 for group_id == 0 and batches 32..65 for group_id == 1
+    uint sub_group_idx = (uint)get_local_id(0) % 8;
+
+    const uint out_id = (sub_group_idx * BATCHES_PER_WORK_ITEM * (uint)get_global_size(1)) / 8 + (global_id / 8) * BATCHES_PER_WORK_ITEM * NEURONS_PER_WORK_ITEM * (uint)get_global_size(1) + (BATCHES_PER_WORK_ITEM * group_id) / 8;
+
+    uint neuronIdx = sub_group_idx + (global_id / 8) * 8 * NEURONS_PER_WORK_ITEM;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC00 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC10 = UNIT_VAL_ZERO;
+
+#if BATCHES_PER_WORK_ITEM >= 16
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC01 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC11 = UNIT_VAL_ZERO;
+#endif
+
+#if BATCHES_PER_WORK_ITEM >= 32
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC02 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC12 = UNIT_VAL_ZERO;
+
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC03 = UNIT_VAL_ZERO;
+    MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockC13 = UNIT_VAL_ZERO;
+#endif
+
+    uint weight_offset = neuronIdx;
+#if NEURONS_PER_WORK_ITEM > 1
+
+    uint weight_offset2 = neuronIdx + 8;
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+
+    uint input_idx = sub_group_idx * (BATCHES_PER_WORK_ITEM / 8) * (uint)get_global_size(1) + (group_id * BATCHES_PER_WORK_ITEM) / 8;
+    for(uint h = 0; h < INPUT0_ELEMENTS_COUNT / 8; h++)
+    {
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA00 = vload8(input_idx, input);
+
+#if BATCHES_PER_WORK_ITEM >= 16
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA01 = vload8(input_idx + 1, input);
+#endif
+
+#if BATCHES_PER_WORK_ITEM >= 32
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA02 = vload8(input_idx + 2, input);
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockA03 = vload8(input_idx + 3, input);
+#endif
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockB00;
+        blockB00.s0 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s1 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s2 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s3 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s4 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s5 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s6 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        blockB00.s7 = weight[weight_offset]; weight_offset += FILTER_OFM_NUM;
+        MULTIPLY_BLOCKS_8x8(blockC00, blockA00, blockB00)
+
+#if BATCHES_PER_WORK_ITEM >= 16
+        MULTIPLY_BLOCKS_8x8(blockC01, blockA01, blockB00)
+#endif
+
+#if BATCHES_PER_WORK_ITEM >= 32
+        MULTIPLY_BLOCKS_8x8(blockC02, blockA02, blockB00)
+        MULTIPLY_BLOCKS_8x8(blockC03, blockA03, blockB00)
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+        MAKE_VECTOR_TYPE(UNIT_TYPE, 8) blockB10;
+        blockB10.s0 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s1 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s2 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s3 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s4 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s5 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s6 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        blockB10.s7 = weight[weight_offset2]; weight_offset2 += FILTER_OFM_NUM;
+        MULTIPLY_BLOCKS_8x8(blockC10, blockA00, blockB10)
+
+#if BATCHES_PER_WORK_ITEM >= 16
+        MULTIPLY_BLOCKS_8x8(blockC11, blockA01, blockB10)
+#endif
+#if BATCHES_PER_WORK_ITEM >= 32
+        MULTIPLY_BLOCKS_8x8(blockC12, blockA02, blockB10)
+        MULTIPLY_BLOCKS_8x8(blockC13, blockA03, blockB10)
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+        input_idx += INPUT0_BATCH_NUM; // we don't need to multiply by 8 because of vload8
+    }
+
+#if BIAS_TERM
+    blockC00 += bias[neuronIdx];
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC01 += bias[neuronIdx];
+#endif
+
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC02 += bias[neuronIdx];
+    blockC03 += bias[neuronIdx];
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+    blockC10 += bias[neuronIdx+8];
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC11 += bias[neuronIdx+8];
+#endif
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC12 += bias[neuronIdx+8];
+    blockC13 += bias[neuronIdx+8];
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+
+    blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC01 = ACTIVATION(blockC01, NL_M, NL_N);
+#endif
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC02 = ACTIVATION(blockC02, NL_M, NL_N);
+    blockC03 = ACTIVATION(blockC03, NL_M, NL_N);
+#endif
+
+#if NEURONS_PER_WORK_ITEM > 1
+
+    blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+#if BATCHES_PER_WORK_ITEM >= 16
+    blockC11 = ACTIVATION(blockC11, NL_M, NL_N);
+#endif
+#if BATCHES_PER_WORK_ITEM >= 32
+    blockC12 = ACTIVATION(blockC12, NL_M, NL_N);
+    blockC13 = ACTIVATION(blockC13, NL_M, NL_N);
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+
+    vstore8(blockC00, out_id, output);
+#if BATCHES_PER_WORK_ITEM >= 16
+    vstore8(blockC01, out_id + 1, output);
+#endif
+#if BATCHES_PER_WORK_ITEM >= 32
+    vstore8(blockC02, out_id + 2, output);
+    vstore8(blockC03, out_id + 3, output);
+#endif
+#endif // #if BIAS_TERM
+#if NEURONS_PER_WORK_ITEM > 1
+
+    vstore8(blockC10, out_id+INPUT0_BATCH_NUM, output);
+
+#if BATCHES_PER_WORK_ITEM >= 16
+    vstore8(blockC11, out_id+INPUT0_BATCH_NUM+1, output);
+#endif
+
+#if BATCHES_PER_WORK_ITEM >= 32
+    vstore8(blockC12, out_id+INPUT0_BATCH_NUM+2, output);
+    vstore8(blockC13, out_id+INPUT0_BATCH_NUM+3, output);
+#endif
+
+#endif // #if NEURONS_PER_WORK_ITEM > 1
+}
+
+#undef SUB_GROUP_SIZE
+#undef ALIGNED_BLOCK_READ8
+#undef MAKE_VECTOR_TYPE
+#undef CONCAT_TOKEN
+#undef CONCAT_TOKEN_HANDLER1
+#undef MULTIPLY_BLOCKS_8x8
+
+)__krnl"},
+
+{"reorder_data_to_yxfb_batched",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+)__krnl"
+R"__krnl(        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+///////////////////////// Input Index /////////////////////////
+inline uint FUNC(get_input_index)(uint b, uint f, uint y, uint x)
+{
+)__krnl"
+R"__krnl(#if   INPUT0_SIMPLE
+    return GET_DATA_INDEX(INPUT0, b, f, y, x);
+#elif defined INPUT0_LAYOUT_BS_F_BSV8__AF8  || \
+      defined INPUT0_LAYOUT_BS_F_BSV16__AF8
+    return GET_DATA_BS_FYX_BSV8_INDEX(INPUT0, b, f, y, x, SUB_GROUP_SIZE);
+#elif defined INPUT0_LAYOUT_BF8_XY16
+    return GET_DATA_BF8_XY16_INDEX(INPUT0, b, f, y, x);
+#elif defined INPUT0_LAYOUT_BYXF_AF32
+	return GET_DATA_BYXF_AF32_INDEX(INPUT0, b, f, y, x);
+#else
+#error reorder_data_to_yxfb_batched.cl: input format - not supported
+#endif
+}
+
+inline void FUNC(get_yxfb_coords_from_linear_idx_no_padding)(uint data_idx, uint* b, uint* f, uint* x, uint* y)
+{
+    uint tmp_data_idx = data_idx / INPUT0_BATCH_NUM;
+    *b = data_idx - tmp_data_idx * INPUT0_BATCH_NUM;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_FEATURE_NUM;
+    *f = data_idx - tmp_data_idx * INPUT0_FEATURE_NUM;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx  = data_idx / INPUT0_SIZE_X;
+    *x = data_idx - tmp_data_idx * INPUT0_SIZE_X;
+    data_idx = tmp_data_idx;
+
+    tmp_data_idx = data_idx / INPUT0_SIZE_Y;
+    *y = data_idx - tmp_data_idx * INPUT0_SIZE_Y;
+}
+
+__attribute__((intel_reqd_sub_group_size(8)))
+KERNEL (reorder_data_to_yxfb_batched)(
+    const __global INPUT_REORDER_TYPE* input,
+    __global OUTPUT_REORDER_TYPE* output
+    #ifdef MEAN_SUBTRACT_IN_BUFFER
+    , __global MEAN_SUBTRACT_TYPE* mean_subtract
+#endif
+    )
+{
+    uint group_idx = get_group_id(0) * OUTPUT_BATCH_NUM * 8;
+
+    for(uint i = 0; i < OUTPUT_BATCH_NUM; i++)
+    {
+        uint output_idx = group_idx + get_sub_group_local_id();
+        if(output_idx >= ELEMENTS_COUNT)
+            continue;
+
+        group_idx += 8;
+
+        uint x,y,f,b;
+        FUNC_CALL(get_yxfb_coords_from_linear_idx_no_padding)(output_idx, &b,&f,&x,&y);
+        const uint input_idx  = FUNC_CALL(get_input_index)(b, f, y, x);
+
+    #if defined MEAN_SUBTRACT_INSIDE_PARAMS
+        float res = TO_MEAN_TYPE(input[input_idx]);
+        res = MEAN_OP(res, VALUE_TO_SUBTRACT[f % VALUE_TO_SUBTRACT_SIZE]);
+    #elif defined MEAN_SUBTRACT_IN_BUFFER
+    #if defined MEAN_PER_FEATURE
+        MEAN_SUBTRACT_TYPE res = TO_MEAN_TYPE(input[input_idx]);
+        res = MEAN_OP(res, mean_subtract[f]);
+    #else
+        MEAN_SUBTRACT_TYPE res = TO_MEAN_TYPE(input[input_idx]);
+        uint4 msv = FUNC_CALL(reshape_dims)(b,f,y,x, INPUT0_SIZE_Y, INPUT0_SIZE_X, MEAN_SUBTRACT_SIZE_Y, MEAN_SUBTRACT_SIZE_X, INPUT0_DIMS, MEAN_SUBTRACT_DIMS);
+        res = MEAN_OP(res, mean_subtract[GET_DATA_INDEX_SAFE(MEAN_SUBTRACT, msv[0], msv[1], msv[2], msv[3])]);
+    #endif
+    #else
+        CALC_TYPE res = TO_CALC_TYPE(input[input_idx]);
+    #endif
+
+        output[output_idx] = ACTIVATION(TO_OUTPUT_REORDER_TYPE(res), NL_M ,NL_N);
+    }
+}
+
+)__krnl"},
+
+{"softmax_gpu_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(softmax)(__global INPUT0_TYPE* input, __global OUTPUT_TYPE* output)
+{
+    const uint other0 = get_global_id(0);
+    const uint other1 = get_global_id(1);
+    const uint batch  = get_global_id(2);
+
+    const uint in_depth_offset  = batch*INPUT0_BATCH_PITCH + other1*INPUT0_OTHER1_PITCH + other0*INPUT0_OTHER0_PITCH + INPUT0_OFFSET;
+    const uint out_depth_offset = batch*OUTPUT_BATCH_PITCH + other1*OUTPUT_OTHER1_PITCH + other0*OUTPUT_OTHER0_PITCH + OUTPUT_OFFSET;
+
+    UNIT_TYPE max_value = UNIT_VAL_MIN;
+    UNIT_TYPE data[INPUT0_CLASS_NUM];
+
+    for (uint cls = 0; cls < INPUT0_CLASS_NUM; ++cls)
+    {
+        const uint index = in_depth_offset + cls*INPUT0_CLASS_PITCH;
+        UNIT_TYPE in = input[index];
+        max_value = max(max_value, in);
+        data[cls] = in;
+    }
+
+    // TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+    ACCUMULATOR_TYPE denominator = 0.0;
+    for (uint cls = 0; cls < INPUT0_CLASS_NUM; ++cls)
+    {
+        data[cls] = native_exp(data[cls] - max_value);;
+        denominator += data[cls];
+    }
+
+    for (uint cls = 0; cls < INPUT0_CLASS_NUM; ++cls)
+    {
+        const UNIT_TYPE res = data[cls] / (UNIT_TYPE)denominator;
+        const uint output_idx = out_depth_offset + cls*OUTPUT_CLASS_PITCH;
+        output[output_idx] = ACTIVATION(res, NL_M, NL_N);
+    }
+}
+
+)__krnl"},
+
+{"pooling_gpu_int8_ref",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2016-2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+)__krnl"
+R"__krnl({   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+)__krnl"
+R"__krnl(    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if MAX_POOLING
+    #define INIT_VAL CHAR_MIN
+#elif AVG_POOLING
+    #define INIT_VAL 0
+#else
+#error
+#endif
+
+inline int FUNC(apply_pooling)(int tmp, int in)
+{
+#if MAX_POOLING
+    return max(tmp, in);
+#elif AVG_POOLING
+    return tmp + in;
+#endif
+}
+
+KERNEL(pooling_gpu_int8_ref)(
+    const __global UNIT_TYPE* input,
+    __global UNIT_TYPE* output)
+{
+#if OUTPUT_LAYOUT_BFYX  || OUTPUT_LAYOUT_BYXF
+    const uint x    = (uint)get_global_id(0);
+    const uint y    = (uint)get_global_id(1);
+    const uint bf   = (uint)get_global_id(2);
+    const uint f    = bf % INPUT0_FEATURE_NUM;
+    const uint b    = bf / INPUT0_FEATURE_NUM;
+
+    if (x >= OUTPUT_SIZE_X)
+    {
+        return;
+    }
+#elif OUTPUT_LAYOUT_YXFB
+    const uint x    = (uint)get_global_id(1);
+    const uint y    = (uint)get_global_id(2);
+    const uint bf   = (uint)get_global_id(0);
+    const uint f    = bf / INPUT0_BATCH_NUM;
+    const uint b    = bf % INPUT0_BATCH_NUM;
+#endif
+
+    const int offset_x = (int)x*STRIDE_SIZE_X - PADDING_SIZE_X;
+)__krnl"
+R"__krnl(    const int offset_y = (int)y*STRIDE_SIZE_Y - PADDING_SIZE_Y;
+
+    int result = INIT_VAL;
+
+#ifdef CHECK_BOUNDRY
+    if (offset_x + POOL_SIZE_X < 0 || offset_x >= INPUT0_SIZE_X ||
+        offset_y + POOL_SIZE_Y < 0 || offset_y >= INPUT0_SIZE_Y)
+    {
+        return;
+    }
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+    uint num_elementes = 0;
+#endif
+
+    const uint batch_and_feature_offset = GET_DATA_INDEX(INPUT0, b, f, 0, 0);
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        int input_offset_y = offset_y + j;
+        bool zero_y = input_offset_y >= INPUT0_SIZE_Y || input_offset_y < 0;
+        if(!zero_y)
+        {
+            for(uint i = 0; i < POOL_SIZE_X; i++)
+            {
+                int input_offset_x = offset_x + i;
+                bool zero = input_offset_x >= INPUT0_SIZE_X || input_offset_x < 0;
+                if(!zero)
+                {
+                    const uint input_idx = batch_and_feature_offset + input_offset_y*INPUT0_Y_PITCH + input_offset_x*INPUT0_X_PITCH;
+
+                    result = FUNC_CALL(apply_pooling)(result, (int)input[input_idx]);
+
+#ifdef DYNAMIC_KERNEL_DIVIDER
+                    num_elementes++;
+#endif
+                }
+            }
+        }
+    }
+#ifdef DYNAMIC_WITH_PADDING_KERNEL_DIVIDER
+    const int hend = min(offset_y + POOL_SIZE_Y, INPUT0_SIZE_Y + PADDING_SIZE_Y);
+    const int wend = min(offset_x + POOL_SIZE_X, INPUT0_SIZE_X + PADDING_SIZE_X);
+    const uint num_elementes = (hend - offset_y) * (wend - offset_x);
+#endif
+#else
+    uint input_idx = GET_DATA_INDEX(INPUT0, b, f, offset_y, offset_x);
+
+    for(uint j = 0; j < POOL_SIZE_Y; j++)
+    {
+        for(uint i = 0; i < POOL_SIZE_X; i++)
+        {
+            result = FUNC_CALL(apply_pooling)(result, (int)input[input_idx]);
+            input_idx += INPUT0_X_PITCH;
+        }
+        input_idx += (INPUT0_Y_PITCH - POOL_SIZE_X*INPUT0_X_PITCH);
+    }
+
+#if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+    const uint num_elementes = POOL_SIZE_X*POOL_SIZE_Y;
+#endif
+#endif
+
+#if defined AVG_POOLING
+    #if defined(DYNAMIC_KERNEL_DIVIDER) || defined(DYNAMIC_WITH_PADDING_KERNEL_DIVIDER)
+        result = convert_int(round(((float)result / max(num_elementes, (uint)1)));
+    #else
+        result = convert_int(round((float)result / (int)(POOL_SIZE_Y * POOL_SIZE_X)));
+    #endif
+#endif
+
+    const uint output_pos = GET_DATA_INDEX(OUTPUT, b, f, y, x);
+    output[output_pos] = ACTIVATION(convert_char(result), NL_M ,NL_N);
+}
+
+#undef INIT_VAL
+
+)__krnl"},
+
+{"convolution_gpu_bfyx_gemm_like_fp16",
+R"__krnl(
+)__krnl"
+R"__krnl(/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+)__krnl"
+R"__krnl(#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+        CAT(prefix, _BATCH_NUM))
+
+)__krnl"
+R"__krnl(#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+#if defined(cl_intel_subgroups_short)
+#define TILE_M          1
+#define TILE_K          FILTER_SIZE_X
+#define TILE_N          32
+
+__attribute__((intel_reqd_sub_group_size(16)))
+KERNEL(convolution_f16)(
+    const __global half *src0,
+    __global half *dst,
+    const __global half *src1,
+#if BIAS_TERM
+    const __global half *bias,
+#endif
+    uint split_idx)
+{
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+typedef struct half1  { half s0; }                                                               half1;
+typedef struct half5  { half s0; half s1; half s2; half s3; half s4; }                           half5;
+typedef struct half6  { half s0; half s1; half s2; half s3; half s4; half s5; }                  half6;
+typedef struct half7  { half s0; half s1; half s2; half s3; half s4; half s5; half s6; }         half7;
+typedef struct half9  { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; }                                                               half9;
+typedef struct half10 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8; half s9; }                                                      half10;
+typedef struct half11 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+)__krnl"
+R"__krnl(                        half s8; half s9; half sa; }                                             half11;
+typedef struct half12 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb;}                                    half12;
+typedef struct half13 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb; half sc;}                           half13;
+typedef struct half14 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                        half s8;  half s9; half sa; half sb; half sc; half se;}                  half14;
+typedef struct half15 { half s0; half s1; half s2; half s3; half s4; half s5; half s6; half s7;
+                       half s8;  half s9; half sa; half sb; half sc; half se; half sf;}          half15;
+typedef struct half0  { half s0; } half0; //never used but makes compiler happy.
+
+typedef struct float1 { float s0; } float1;
+typedef struct float5 { float s0; float s1; float s2; float s3; float s4; } float5;
+typedef struct float6 { float s0; float s1; float s2; float s3; float s4; float s5; } float6;
+typedef struct float7 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; } float7;
+typedef struct float9 { float s0; float s1; float s2; float s3; float s4; float s5; float s6; float s7; float s8; } float9;
+typedef struct float10 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9;} float10;
+typedef struct float11 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa;} float11;
+typedef struct float12 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; } float12;
+typedef struct float13 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc;} float13;
+typedef struct float14 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; } float14;
+typedef struct float15 { float s0; float s1; float s2; float s3; float s4; float s5;
+                         float s6; float s7; float s8; float s9; float sa; float sb; float sc; float sd; float se; } float15;
+typedef struct float0 { float s0; } float0; //never used but makes compiler happy.
+
+#if (KERNEL_WIDTH == 1)
+__constant half1 half_zeros= (half1){0};
+#elif (KERNEL_WIDTH == 2)
+    __constant half2 half_zeros = (half2)(0);
+#elif (KERNEL_WIDTH == 3)
+    __constant half3 half_zeros = (half3)(0);
+#elif (KERNEL_WIDTH == 4)
+    __constant half4 half_zeros = (half4)(0);
+#elif (KERNEL_WIDTH == 5)
+    __constant half5 half_zeros = (half5){0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 6)
+    __constant half6 half_zeros = (half6){0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 7)
+    __constant half7 half_zeros = (half7){0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 8)
+    __constant half8 half_zeros = (half8)(0);
+#elif (KERNEL_WIDTH == 9)
+    __constant half9 half_zeros = (half9){0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 10)
+    __constant half10 half_zeros = (half10){0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 11)
+    __constant half11 half_zeros = (half11){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 12)
+    __constant half12 half_zeros = (half12){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 13)
+    __constant half13 half_zeros = (half13){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 14)
+    __constant half14 half_zeros = (half14){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 15)
+    __constant half15 half_zeros = (half15){0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
+#elif (KERNEL_WIDTH == 16)
+    __constant half16 half_zeros = (half16)(0);
+#endif
+
+
+    const unsigned group_x = get_group_id(0);
+    const unsigned group_y = get_group_id(1);
+    const unsigned global_x = get_global_id(0);
+    const unsigned global_y = get_global_id(1);
+    const unsigned global_z = get_global_id(2);
+
+    unsigned interleaved_y;
+    unsigned kernel_y;
+    unsigned kernel_idx;
+
+    // Result ctile (*dst) is M rows x N columns
+    // LWG size is 1x16.  Thus each thread calculates 16*M rows x N cols of ctile.
+    half16  blockC00 = 0.f;
+    half16  blockC10 = 0.f;
+
+    const uint in_split_offset = split_idx * INPUT0_FEATURE_PITCH * INPUT0_FEATURE_NUM;
+    // Src0 (patch input) is directly used as atile.
+    // Each work item points to the start of a different patch.
+    // atile is M rows x K columns.
+#if defined(INPUT_BUFFER_WIDTH_PADDED) && defined(INPUT_BUFFER_HEIGHT_PADDED)
+    const uint src0_read_offset_const = INPUT0_OFFSET_WITH_PADDING + in_split_offset
+     + INPUT0_BATCH_PITCH * global_z                                                         // batch offset
+     + ( ( global_y / OUTPUT_SIZE_X ) * STRIDE_SIZE_Y * INPUT0_Y_PITCH )                     // y offset
+     + ( ( global_y % OUTPUT_SIZE_X ) * STRIDE_SIZE_X );                                     // x offset
+#elif !defined(INPUT_BUFFER_WIDTH_PADDED) && !defined(INPUT_BUFFER_HEIGHT_PADDED)
+    #pragma error - fix this path
+    const int y_offset = ( global_y / OUTPUT_SIZE_X ) * STRIDE_SIZE_Y - PADDING_SIZE_Y;
+    const int x_offset = ( global_y % OUTPUT_SIZE_X ) * STRIDE_SIZE_X - PADDING_SIZE_X;
+    uint src0_read_offset = INPUT_OFFSET + in_split_offset + INPUT0_BATCH_PITCH * global_z
+                            + y_offset * INPUT0_Y_PITCH;
+
+    int partial_left = 0, partial_right = 0;
+    if (x_offset < 0)
+    {
+        partial_left = min((int) FILTER_SIZE_X, (int) abs(x_offset));
+        src0_read_offset -= partial_left;
+    }
+    else
+    {
+        partial_left = 0;
+        src0_read_offset +=  x_offset;
+    }
+    if ((x_offset + FILTER_SIZE_X) >= INPUT_SIZE_X)
+        partial_right = min(FILTER_SIZE_X, INPUT_SIZE_X - x_offset);
+    else
+        partial_right = FILTER_SIZE_X;
+
+#elif defined(INPUT_BUFFER_WIDTH_PADDED)
+    #pragma error - fix this path
+    // TODO: Handle offset
+    const int y_offset = ( global_y / OUTPUT_SIZE_X ) * STRIDE_SIZE_Y -PADDING_SIZE_Y;
+    int src0_read_offset = in_split_offset + INPUT0_BATCH_PITCH * global_z        // batch offset
+     + y_offset * INPUT0_Y_PITCH                              // y offset
+     + ( ( global_y % OUTPUT_SIZE_X ) * STRIDE_SIZE_X );                // x offset
+#endif
+
+    // Src1 (filter) is directly used as btile.
+    // It starts at the top of src1 and walks down.
+    // btile is K rows x N columns.
+    uint src0_read_offset = src0_read_offset_const;
+    uint src1_read_offset = ( global_x * TILE_N * 2);
+
+#define DOT_PRODUCT_16( _result, _rowA, colB )    \
+    {   \
+        _result.s0 = mad( _rowA, sub_group_broadcast( colB,  0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, sub_group_broadcast( colB,  1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, sub_group_broadcast( colB,  2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, sub_group_broadcast( colB,  3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, sub_group_broadcast( colB,  4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, sub_group_broadcast( colB,  5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, sub_group_broadcast( colB,  6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, sub_group_broadcast( colB,  7 ), _result.s7 );  \
+        _result.s8 = mad( _rowA, sub_group_broadcast( colB,  8 ), _result.s8 );  \
+        _result.s9 = mad( _rowA, sub_group_broadcast( colB,  9 ), _result.s9 );  \
+        _result.sa = mad( _rowA, sub_group_broadcast( colB, 10 ), _result.sa );  \
+        _result.sb = mad( _rowA, sub_group_broadcast( colB, 11 ), _result.sb );  \
+        _result.sc = mad( _rowA, sub_group_broadcast( colB, 12 ), _result.sc );  \
+        _result.sd = mad( _rowA, sub_group_broadcast( colB, 13 ), _result.sd );  \
+        _result.se = mad( _rowA, sub_group_broadcast( colB, 14 ), _result.se );  \
+        _result.sf = mad( _rowA, sub_group_broadcast( colB, 15 ), _result.sf );  \
+    }
+    typedef CAT( half, FILTER_SIZE_X ) half_t;
+    // Walk DOWN src0 (patch 0, 1, 2, ...) and DOWN src1.
+    // Inner loop loads and FMADs one row (FILTER_SIZE_X) of each input patch
+    // and FILTER_SIZE_X/2 rows of interleaved filter.
+    unsigned patch_depth = 0;
+    __attribute__((opencl_unroll_hint(1)))
+    do
+    {
+        int patch_row = 0;
+        __attribute__((opencl_unroll_hint(1)))
+        do
+        {
+            // Load atile and btile.
+            // Kernel data is partially interleaved.  Every 2 rows are interleaved at half16 granularity.
+            // The exception is that if FILTER_SIZE_X is odd the last row is not interleaved.  The non
+            // interleaved row is padded with zero to ensure same size as interleaved rows. This
+            // interleaving is done to ensure 0% GDR bank conflicts.  For example, this is how the
+            // kernel data would be arranged before/after interleaving for FILTER_SIZE_X=3.
+            // (0, 0) (16, 0) (32, 0) (48, 0) ...     (0, 0) ( 0, 1) (16, 0) ( 0, 1) (32, 0) (0, 1) (48, 0) ...
+            // (0, 1) (16, 1) (32, 1) (48, 1) ... =>  (0, 2) (16, 2) (32, 2) (48, 2) ...
+            // (0, 2) (16, 2) (32, 2) (48, 2) ...     ...
+            // ...
+            const bool kernel_width_is_odd = FILTER_SIZE_X % 2 == 1;
+            #if defined(INPUT_BUFFER_WIDTH_PADDED) && defined(INPUT_BUFFER_HEIGHT_PADDED)
+
+            // in case the data is not aligned to sizeof(T)*FILTER_SIZE_X we need to use vload or set the data in a loop
+            half blockA00[FILTER_SIZE_X];
+            {
+                unsigned i = 0;
+                LOOP(FILTER_SIZE_X, i,
+                {
+#if LEFTOVERS == 1
+                    if(src0_read_offset_const + (FILTER_SIZE_Y - 1) * INPUT0_Y_PITCH + (INPUT0_FEATURE_NUM - 1) * (INPUT0_FEATURE_PITCH - ( FILTER_SIZE_Y * INPUT0_Y_PITCH )) >= INPUT0_BATCH_NUM * INPUT0_BATCH_PITCH)
+                    {
+                        if(src0_read_offset + i < INPUT0_BATCH_NUM * INPUT0_BATCH_PITCH)
+                            blockA00[i] = src0[src0_read_offset + i];
+                    }
+                    else
+#endif
+                        blockA00[i] = src0[src0_read_offset + i];
+                } )
+            }
+
+            half*  pblockA00 = (half*)(&blockA00);
+
+            #elif !defined(INPUT_BUFFER_WIDTH_PADDED) && !defined(INPUT_BUFFER_HEIGHT_PADDED)
+            // TODO: Fixed vload issue in this path.
+            #pragma error
+            half_t blockA00;
+            half*  pblockA00 = (half*)(&blockA00);
+            #if (PADDING_SIZE_X == 1) && (INPPUT_PADDING_Y == 1) && (FILTER_SIZE_X == 3) && (FILTER_SIZE_Y == 3)
+            if ((y_offset +  patch_row < 0) || ((y_offset + patch_row) >= INPUT_SIZE_Y))
+            {
+                blockA00 = half_zeros;
+)__krnl"
+R"__krnl(            }
+            else
+            {
+                 blockA00 = src0[src0_read_offset - partial_left];
+                 if (partial_left) pblockA00[0] = 0;
+                 if (partial_right != FILTER_SIZE_X) pblockA00[FILTER_SIZE_X - 1] = 0;
+            }
+            #else
+            if ((y_offset +  patch_row < 0) || ((y_offset + patch_row) >= INPUT_SIZE_Y))
+            {
+                blockA00 = half_zeros;
+            }
+            else
+            {
+                 blockA00 = src0[src0_read_offset - partial_left];
+                 for (unsigned i = 0; i < partial_left; ++i) pblockA00[i] = 0;
+                 for (unsigned i = partial_right; i < FILTER_SIZE_X; ++i) pblockA00[i] = 0;
+
+            }
+            #endif
+            #elif defined(INPUT_BUFFER_WIDTH_PADDED)
+            // TODO: Fixed vload issue in this path.
+            #pragma error
+            if ((y_offset +  patch_row < 0) || ((y_offset + patch_row) >= INPUT_SIZE_Y))
+            {
+                blockA00 = half_zeros;
+            }
+            else
+            {
+                blockA00 = src0[src0_read_offset];
+            }
+            #endif
+            src0_read_offset += INPUT0_Y_PITCH;
+
+            ushort blockB00[FILTER_SIZE_X * 2];
+            ushort4* p4BlockB00 = (ushort4*)blockB00;
+            ushort2* p2BlockB00 = (ushort2*)blockB00;
+            half* pBlockB00  = (half*)blockB00;
+
+            interleaved_y = 0;
+            LOOP(FILTER_SIZE_X_DIV2, interleaved_y,
+            {
+                p4BlockB00[interleaved_y] = intel_sub_group_block_read_us4( (const __global ushort*)src1 + src1_read_offset );
+                src1_read_offset += ALIGNED_OFM * 2;
+            } )
+            if ( kernel_width_is_odd )
+            {
+                p2BlockB00[FILTER_SIZE_X - 1] = intel_sub_group_block_read_us2( (const __global ushort*)src1 + src1_read_offset );
+                src1_read_offset += ALIGNED_OFM * 2;
+            }
+
+            // Perform MADs
+            kernel_idx = 0;
+            interleaved_y = 0;
+            LOOP(FILTER_SIZE_X_DIV2, interleaved_y,
+            {
+                kernel_y = interleaved_y * 2;
+                DOT_PRODUCT_16( blockC00, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_16( blockC00, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_16( blockC10, pblockA00[kernel_y    ], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_16( blockC10, pblockA00[kernel_y + 1], pBlockB00[kernel_idx] ); kernel_idx++;
+            } )
+            if ( kernel_width_is_odd )
+            {
+                kernel_y = interleaved_y * 2;
+                DOT_PRODUCT_16( blockC00, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;
+                DOT_PRODUCT_16( blockC10, pblockA00[kernel_y], pBlockB00[kernel_idx] ); kernel_idx++;
+            }
+        }
+        while( ++patch_row < FILTER_SIZE_Y );
+
+        src0_read_offset += INPUT0_FEATURE_PITCH - ( FILTER_SIZE_Y * INPUT0_Y_PITCH ); // reset to start of next slice of patch
+    }
+    while ( ++patch_depth < INPUT0_FEATURE_NUM );
+
+    #undef DOT_PRODUCT_16
+
+    const uint out_split_offset = split_idx * OUTPUT_FEATURE_PITCH * OUTPUT_FEATURE_NUM;
+    // Dst resembles a cube of width x height x (output channel * batches).  Each tile writes:
+    // (SIMD * TILE_M) x 1 x TILE_N.  Partial writes most likely generated if padding used.
+    __global half *out = dst + OUTPUT_OFFSET + out_split_offset
+     + global_z * OUTPUT_BATCH_PITCH                                                   // batch offset
+     + ( group_x * TILE_N ) * OUTPUT_FEATURE_PITCH                                     // channel offset
+     + ( ( global_y * TILE_M ) / OUTPUT_SIZE_X ) * OUTPUT_Y_PITCH                      // y offset
+     + ( ( global_y * TILE_M ) % OUTPUT_SIZE_X );                                      // x offset
+
+
+    if (global_y * TILE_M < OUTPUT_SIZE_X * OUTPUT_SIZE_Y )
+    {
+         #if BIAS_TERM
+         __global half16* biasPtr = (__global half16*) (bias + group_x * TILE_N);
+         #endif
+
+#if ( ( OUTPUT_FEATURE_NUM % TILE_N ) == 0 )
+
+        #if BIAS_TERM
+        blockC00 += *biasPtr;
+        blockC10 += *(biasPtr + 1);
+        #endif
+
+        blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+        blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+
+        for (unsigned i = 0; i < 16; i++)
+        {
+            out[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+            out[(16+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+        }
+
+#elif ( ( OUTPUT_FEATURE_NUM % 16 ) == 0 )
+        if ( ( global_x + 1 ) < get_global_size(0) )
+        {
+            #if BIAS_TERM
+            blockC00 += *biasPtr;
+            blockC10 += *(biasPtr + 1);
+            #endif
+
+            blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+            blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+
+            for ( unsigned i = 0; i < 16; i++ )
+            {
+                out[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                out[(16+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+            }
+        }
+        else
+        {
+            #if BIAS_TERM
+            blockC00 += *biasPtr;
+            #endif
+
+            blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+
+            for (unsigned i = 0; i < 16; i++)
+            {
+                out[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+            }
+        }
+#else
+        if ( ( global_x + 1 ) < get_global_size(0) )
+        {
+            #if BIAS_TERM
+            blockC00 += *biasPtr;
+            blockC10 += *(biasPtr + 1);
+            #endif
+
+            blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+            blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+
+            for ( unsigned i = 0; i < 16; i++ )
+            {
+                out[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+                out[(16+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+            }
+        }
+        else
+        {
+#if ( (OUTPUT_FEATURE_NUM % TILE_N) > 16 )
+
+            #if BIAS_TERM
+            blockC00 += *biasPtr;
+            blockC10 += *(biasPtr + 1);
+            #endif
+
+            blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+            blockC10 = ACTIVATION(blockC10, NL_M, NL_N);
+
+            for (unsigned i = 0; i < 16 ; i++)
+            {
+                out[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+            }
+            for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 16 ; i++)
+            {
+                out[(16+i) * OUTPUT_FEATURE_PITCH] = blockC10[i];
+            }
+#else
+            #if BIAS_TERM
+            blockC00 += *biasPtr;
+            #endif
+
+            blockC00 = ACTIVATION(blockC00, NL_M, NL_N);
+
+            for (unsigned i = 0; i < OUTPUT_FEATURE_NUM % 16 ; i++)
+            {
+                out[( 0+i) * OUTPUT_FEATURE_PITCH] = blockC00[i];
+            }
+#endif
+        }
+#endif
+    }
+
+}
+#endif
+
+)__krnl"},
+
+{"reorder_from_winograd_2x3_s1",
+R"__krnl(
+)__krnl"
+R"__krnl(// Copyright (c) 2017 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// --------------------------------------------------------------------------------------------------------------------------------
+// Convert the results using the inverse F(2,3) Winograd transform.
+// --------------------------------------------------------------------------------------------------------------------------------
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#define __CAT(x, y) x##y
+#define CAT(x, y) __CAT(x, y)
+
+#define __CAT_FUNC(x, y) FUNC(x##y)
+#define CAT_FUNC(x, y) __CAT_FUNC(x, y)
+
+#define __CAT_FUNC_CALL(x, y) FUNC_CALL(x##y)
+#define CAT_FUNC_CALL(x, y) __CAT_FUNC_CALL(x, y)
+
+#define OFFSET_GLOBAL_PTR(elem_type, ptr, byte_offset) ((__global elem_type*)((__global char*)(ptr) + (byte_offset)))
+#define MULTIPLY_OFFSET(elem_type, byte_offset) ((byte_offset) * sizeof(elem_type))
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_khr_fp16)
+#pragma OPENCL EXTENSION cl_khr_fp16 : enable
+#endif
+
+// TODO: currently we calculate on float32 because it's lot of "add" operation and it stuck on the value "8192.0f"
+#if !defined(ACCUMULATOR_TYPE)
+    #define ACCUMULATOR_TYPE float
+    #define TO_ACCUMULATOR_TYPE(v) (float)(v)
+    #define ACCUMULATOR_TYPE_ZERO 0.0f
+#endif
+
+// Creates vector type.
+#define MAKE_VECTOR_TYPE(elem_type, size) CAT(elem_type, size)
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+#if defined(cl_intel_subgroups)
+#pragma OPENCL EXTENSION  cl_intel_subgroups : enable
+#endif
+
+#if defined(cl_intel_subgroups_short)
+#pragma OPENCL EXTENSION  cl_intel_subgroups_short : enable
+#endif
+
+#define TRANSPOSE_BLOCK_8( _block )   \
+        (float8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_FP16( _block )   \
+        (half8)( intel_sub_group_shuffle( _block, 0 ), \
+                  intel_sub_group_shuffle( _block, 1 ), \
+                  intel_sub_group_shuffle( _block, 2 ), \
+                  intel_sub_group_shuffle( _block, 3 ), \
+                  intel_sub_group_shuffle( _block, 4 ), \
+                  intel_sub_group_shuffle( _block, 5 ), \
+                  intel_sub_group_shuffle( _block, 6 ), \
+                  intel_sub_group_shuffle( _block, 7 ) );
+
+#define TRANSPOSE_BLOCK_8_COL( _block, _col )   \
+        (float8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_8_COL_FP16( _block, _col )   \
+        (half8)( intel_sub_group_shuffle( _block.s0, _col ), \
+                  intel_sub_group_shuffle( _block.s1, _col ), \
+                  intel_sub_group_shuffle( _block.s2, _col ), \
+                  intel_sub_group_shuffle( _block.s3, _col ), \
+                  intel_sub_group_shuffle( _block.s4, _col ), \
+                  intel_sub_group_shuffle( _block.s5, _col ), \
+                  intel_sub_group_shuffle( _block.s6, _col ), \
+                  intel_sub_group_shuffle( _block.s7, _col ) );
+
+#define TRANSPOSE_BLOCK_16_FP16(_block)  \
+        (half16)(as_half2(intel_sub_group_shuffle(_block, 0)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 1)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 2)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 3)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 4)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 5)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 6)),  \
+                 as_half2(intel_sub_group_shuffle(_block, 7)));
+
+#define TRANSPOSE_BLOCK_16_FP16_HALF_TYPE(_block)  \
+        (half16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+                 intel_sub_group_shuffle(_block, 15));
+
+#define TRANSPOSE_BLOCK_16(_block)  \
+        (float16)(intel_sub_group_shuffle(_block, 0),  \
+                 intel_sub_group_shuffle(_block, 1),  \
+                 intel_sub_group_shuffle(_block, 2),  \
+                 intel_sub_group_shuffle(_block, 3),  \
+                 intel_sub_group_shuffle(_block, 4),  \
+                 intel_sub_group_shuffle(_block, 5),  \
+                 intel_sub_group_shuffle(_block, 6),  \
+                 intel_sub_group_shuffle(_block, 7),  \
+                 intel_sub_group_shuffle(_block, 8),  \
+                 intel_sub_group_shuffle(_block, 9),  \
+                 intel_sub_group_shuffle(_block, 10),  \
+                 intel_sub_group_shuffle(_block, 11),  \
+                 intel_sub_group_shuffle(_block, 12),  \
+                 intel_sub_group_shuffle(_block, 13),  \
+                 intel_sub_group_shuffle(_block, 14),  \
+)__krnl"
+R"__krnl(                 intel_sub_group_shuffle(_block, 15));
+
+#define DOT_PRODUCT_8( _result, _rowA, colB )    \
+{   \
+        _result.s0 = mad( _rowA, intel_sub_group_shuffle( colB, 0 ), _result.s0 );  \
+        _result.s1 = mad( _rowA, intel_sub_group_shuffle( colB, 1 ), _result.s1 );  \
+        _result.s2 = mad( _rowA, intel_sub_group_shuffle( colB, 2 ), _result.s2 );  \
+        _result.s3 = mad( _rowA, intel_sub_group_shuffle( colB, 3 ), _result.s3 );  \
+        _result.s4 = mad( _rowA, intel_sub_group_shuffle( colB, 4 ), _result.s4 );  \
+        _result.s5 = mad( _rowA, intel_sub_group_shuffle( colB, 5 ), _result.s5 );  \
+        _result.s6 = mad( _rowA, intel_sub_group_shuffle( colB, 6 ), _result.s6 );  \
+        _result.s7 = mad( _rowA, intel_sub_group_shuffle( colB, 7 ), _result.s7 );  \
+}
+
+#define ADD_BIAS_8( _result, _biasVal) \
+{ \
+    _result.s0 += intel_sub_group_shuffle( _biasVal, 0 ); \
+    _result.s1 += intel_sub_group_shuffle( _biasVal, 1 ); \
+    _result.s2 += intel_sub_group_shuffle( _biasVal, 2 ); \
+    _result.s3 += intel_sub_group_shuffle( _biasVal, 3 ); \
+    _result.s4 += intel_sub_group_shuffle( _biasVal, 4 ); \
+    _result.s5 += intel_sub_group_shuffle( _biasVal, 5 ); \
+    _result.s6 += intel_sub_group_shuffle( _biasVal, 6 ); \
+    _result.s7 += intel_sub_group_shuffle( _biasVal, 7 ); \
+}
+
+#define ADD_BIAS_16_FP16( _result, _biasVal) \
+{ \
+    _result.s01 += as_half2(intel_sub_group_shuffle(_biasVal, 0)); \
+    _result.s23 += as_half2(intel_sub_group_shuffle(_biasVal, 1)); \
+    _result.s45 += as_half2(intel_sub_group_shuffle(_biasVal, 2)); \
+    _result.s67 += as_half2(intel_sub_group_shuffle(_biasVal, 3)); \
+    _result.s89 += as_half2(intel_sub_group_shuffle(_biasVal, 4)); \
+    _result.sab += as_half2(intel_sub_group_shuffle(_biasVal, 5)); \
+    _result.scd += as_half2(intel_sub_group_shuffle(_biasVal, 6)); \
+    _result.sef += as_half2(intel_sub_group_shuffle(_biasVal, 7)); \
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+inline uint4 FUNC(reshape_2_to_4)(uint o, uint i, uint y, uint x, uint dst_size_y, uint dst_size_x)
+{
+    uint _i  = i / (dst_size_y*dst_size_x);
+    uint _yx = i % (dst_size_y*dst_size_x);
+    uint _y = _yx / dst_size_x;
+    uint _x = _yx % dst_size_x;
+    return (uint4)(o,_i,_y,_x);
+}
+
+inline uint4 FUNC(reshape_4_to_2)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x)
+{
+    uint _i = i*src_size_y*src_size_x + y*src_size_x + x;
+    return (uint4)(o,_i,0,0);
+}
+
+inline uint4 FUNC(reshape_dims)(uint o, uint i, uint y, uint x, uint src_size_y, uint src_size_x, uint dst_size_y, uint dst_size_x, uint src_dims, uint dst_dims)
+{
+    if (src_dims == 4 && dst_dims == 2)
+    {
+        return FUNC_CALL(reshape_4_to_2)(o,i,y,x,src_size_y,src_size_x);
+    }
+    else if (src_dims == 2 && dst_dims == 4)
+    {
+        return FUNC_CALL(reshape_2_to_4)(o,i,y,x,dst_size_y,dst_size_x);
+    }
+
+    return (uint4)(o,i,y,x);
+}
+
+/*
+// Copyright (c) 2016 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+*/
+
+
+#define GET_DATA_INDEX(prefix, b, f, y, x)  \
+    CAT(prefix, _OFFSET) +                  \
+    (x)*CAT(prefix, _X_PITCH) +             \
+    (y)*CAT(prefix, _Y_PITCH) +             \
+    (f)*CAT(prefix, _FEATURE_PITCH) +       \
+    (b)*CAT(prefix, _BATCH_PITCH)
+
+#define GET_DATA_INDEX_SAFE(prefix, b, f, y, x)                     \
+    CAT(prefix, _OFFSET) +                                          \
+    (x % CAT(prefix, _SIZE_X     ))*CAT(prefix, _X_PITCH) +         \
+    (y % CAT(prefix, _SIZE_Y     ))*CAT(prefix, _Y_PITCH) +         \
+    (f % CAT(prefix, _FEATURE_NUM))*CAT(prefix, _FEATURE_PITCH) +   \
+    (b % CAT(prefix, _BATCH_NUM  ))*CAT(prefix, _BATCH_PITCH)
+
+
+#define GET_DATA_BS_FYX_BSV8_INDEX(prefix, b, f, y, x, sub_group_size)  \
+    CAT(prefix, _OFFSET) +                                              \
+    ((b) % (sub_group_size)) +                                          \
+    (sub_group_size)*(                                                  \
+        (x)*CAT(prefix, _X_PITCH) +                                     \
+        (y)*CAT(prefix, _Y_PITCH) +                                     \
+        (f)*CAT(prefix, _FEATURE_PITCH) +                               \
+        ((b) / (sub_group_size))*CAT(prefix, _BATCH_PITCH)              \
+    )
+
+inline uint FUNC(get_bf8_xy16_index)(uint b, uint f, uint y, uint x, uint x_size, uint y_size, uint f_size, uint offset)
+{
+    const uint xy_idx = x + y * x_size;
+    const uint xy_offset = (xy_idx % 16) + (xy_idx / 16) * 16 * 8;
+    const uint xy_block_num = (x_size * y_size + 16 - 1) / 16;
+    const uint f_offset = (f % 8) * 16 + (f / 8) * xy_block_num * 16 * 8;
+    const uint f_block_num = (f_size + 8 - 1) / 8;
+    const uint b_offset = b * f_block_num * xy_block_num * 128;
+
+    const size_t idx = offset + xy_offset + f_offset + b_offset;
+
+    return idx;
+}
+
+inline uint FUNC(get_byxf_af32_index)(uint b, uint f, uint y, uint x, uint y_pitch, uint b_pitch, uint f_size, uint offset)
+{
+	const uint f_aligned_to_32 = ((f_size + 31) / 32) * 32;
+	const uint b_offset = b * b_pitch;
+	const uint xy_offset = f_aligned_to_32 * x + y_pitch * y;
+	const uint f_offset = f;
+	const size_t idx = offset + xy_offset + b_offset + f_offset;
+	return idx;
+}
+
+#define GET_DATA_BYXF_AF32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_byxf_af32_index)(                 \
+		b, f, y, x, CAT(prefix, _Y_PITCH),          \
+		CAT(prefix, _BATCH_PITCH),                      \
+		CAT(prefix, _FEATURE_NUM),                 \
+		CAT(prefix, _OFFSET))
+
+#define GET_DATA_BF8_XY16_INDEX(prefix, b, f, y, x)     \
+    FUNC_CALL(get_bf8_xy16_index)(                      \
+        b, f, y, x, CAT(prefix, _SIZE_X ),              \
+        CAT(prefix, _SIZE_Y),                           \
+        CAT(prefix, _FEATURE_NUM),                      \
+        CAT(prefix, _OFFSET))
+
+inline uint FUNC(get_fs_bs_yx_bsv4_fsv32_index)(uint b, uint f, uint y, uint x,
+    uint x_pad_before, uint x_size, uint x_pad_after,
+    uint y_pad_before, uint y_size, uint y_pad_after,
+    uint size_f, uint size_b)
+{
+    const uint f_32_aligned = ((size_f + 31)/32) * 32;
+    const uint b_4_aligned = ((size_b + 3)/4) * 4;
+    const uint fsv_idx = f % 32;
+    const uint bsv_idx = b % 4;
+    const uint fs_idx = f / 32;
+    const uint bs_idx = b / 4;
+
+    const uint x_pitch = 32 * 4;
+    const uint y_pitch = 32 * 4 * (x_pad_before + x_size + x_pad_after);
+    const uint bs_pitch = y_pitch * (y_pad_before + y_size + y_pad_after);
+    const uint fs_pitch = bs_pitch * (b_4_aligned / 4);
+    uint offset = x_pitch * x_pad_before + y_pitch * y_pad_before;
+
+    size_t idx = offset + fsv_idx + bsv_idx * 32;
+    idx += 32*4 * x;
+    idx += y * y_pitch;
+    idx += bs_idx * bs_pitch;
+    idx += fs_idx * fs_pitch;
+
+    return idx;
+}
+
+#define GET_DATA_FS_BS_YX_BSV4_FSV32_INDEX(prefix, b, f, y, x)\
+	FUNC_CALL(get_fs_bs_yx_bsv4_fsv32_index)(       \
+		b, f, y, x,                                 \
+        CAT(prefix, _PAD_BEFORE_SIZE_X),            \
+        CAT(prefix, _SIZE_X),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_X),             \
+        CAT(prefix, _PAD_BEFORE_SIZE_Y),            \
+        CAT(prefix, _SIZE_Y),                       \
+        CAT(prefix, _PAD_AFTER_SIZE_Y),             \
+		CAT(prefix, _FEATURE_NUM),                  \
+)__krnl"
+R"__krnl(        CAT(prefix, _BATCH_NUM))
+
+#define GET_FILTER_INDEX(prefix, o, i, y, x)    \
+    CAT(prefix, _OFFSET) +                      \
+    (x)*CAT(prefix, _X_PITCH) +                 \
+    (y)*CAT(prefix, _Y_PITCH) +                 \
+    (i)*CAT(prefix, _IFM_PITCH) +               \
+    (o)*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_INDEX_SAFE(prefix, o, i, y, x)           \
+    CAT(prefix, _OFFSET) +                                  \
+    (x % CAT(prefix, _SIZE_X ))*CAT(prefix, _X_PITCH) +     \
+    (y % CAT(prefix, _SIZE_Y ))*CAT(prefix, _Y_PITCH) +     \
+    (i % CAT(prefix, _IFM_NUM))*CAT(prefix, _IFM_PITCH) +   \
+    (o % CAT(prefix, _OFM_NUM))*CAT(prefix, _OFM_PITCH)
+
+#define GET_FILTER_OS_IYX_OSV8_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (x)*CAT(prefix, _X_PITCH) +                                         \
+        (y)*CAT(prefix, _Y_PITCH) +                                         \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+#define GET_FILTER_OS_IYX_OSV8_ROTATE_180_INDEX(prefix, o, i, y, x, sub_group_size)    \
+    CAT(prefix, _OFFSET) +                                                  \
+    ((o) % (sub_group_size)) +                                              \
+    (sub_group_size)*(                                                      \
+        (CAT(prefix, _SIZE_X ) - x - 1)*CAT(prefix, _X_PITCH) +             \
+        (CAT(prefix, _SIZE_Y ) - y - 1)*CAT(prefix, _Y_PITCH) +             \
+        (i)*CAT(prefix, _IFM_PITCH) +                                       \
+        ((o) / (sub_group_size))*CAT(prefix, _OFM_PITCH)                    \
+    )
+
+inline uint FUNC(get_i_yxs_os_yxsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint dst_height = i*ifm_height_pitch + y*x_size + x;
+    const uint base_filter_index = y*x_size + x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_I_YXS_OS_YXSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size) \
+    FUNC_CALL(get_i_yxs_os_yxsv2_osv_index)(                                    \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_iy_xs_os_xsv2_osv_index)(uint o, uint i, uint y, uint x, uint x_size, uint i_pitch, uint y_pitch, uint x_pitch, uint offset, uint sub_group_size)
+{
+    const uint aligned_ofm_line = x_pitch;
+    const uint ifm_height_pitch = (i_pitch/aligned_ofm_line);
+    const uint aligned_x_line = y_pitch / x_pitch;
+    const uint dst_height = i*ifm_height_pitch + y*aligned_x_line + x;
+    const uint base_filter_index = x;
+
+    const uint aligned_height = dst_height & 0xfffffffe;
+    const uint base_filter_odd = (base_filter_index & 0x1);
+
+    uint slice_id = o / sub_group_size;
+    uint id_in_slice = o % sub_group_size;
+    uint slice_pitch = 2*sub_group_size;
+    uint offset_in_slice = (int)(sub_group_size*base_filter_odd);
+
+    const bool last_line_in_base_filter = (x == (x_size - 1));
+    if (last_line_in_base_filter && base_filter_odd == 0)
+    {
+        const uint element_in_slice = 32;
+        slice_id = o / element_in_slice;
+        id_in_slice = o % element_in_slice;
+        slice_pitch = 2*element_in_slice;
+        offset_in_slice = 0;
+    }
+
+    const uint in_line = (slice_pitch*slice_id + offset_in_slice + id_in_slice);
+    const size_t idx = offset + aligned_height*aligned_ofm_line + in_line;
+
+    return idx;
+}
+
+#define GET_FILTER_IY_XS_OS_XSV2_OSV_INDEX(prefix, o, i, y, x, sub_group_size)  \
+    FUNC_CALL(get_iy_xs_os_xsv2_osv_index)(                                     \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _IFM_PITCH),                                                \
+        CAT(prefix, _Y_PITCH),                                                  \
+        CAT(prefix, _X_PITCH),                                                  \
+        CAT(prefix, _OFFSET),                                                   \
+        sub_group_size)
+
+inline uint FUNC(get_os_is_yx_isa8_osv8_isv4_index)(uint o, uint i, uint y, uint x, uint size_x, uint size_y, uint size_ifm, uint size_ofm, uint offset)
+{
+    const uint f_32_aligned = ((size_ifm + 31)/32) * 32;
+	const uint isv2_idx = i % 4;
+	const uint osv_idx = o % 8;
+	const uint isv1_idx = (i / 4) % 8;
+	const uint is_idx = i / 32;
+	const uint os_idx = o / 8;
+
+	size_t idx = offset + isv2_idx + 4 * (osv_idx + 8 * isv1_idx);
+	idx += x * 4 * 8 * 8;
+	idx += y * size_x * 4 * 8 * 8;
+	idx += is_idx * size_y * size_x * 4 * 8 * 8;
+	idx += os_idx * (f_32_aligned/32) * size_y * size_x * 4 * 8 * 8;
+
+    return idx;
+}
+
+#define GET_FILTER_OS_IS_YX_ISA8_OSV8_ISV4(prefix, o, i, y, x) \
+	FUNC_CALL(get_os_is_yx_isa8_osv8_isv4_index)(                               \
+        o, i, y, x, CAT(prefix, _SIZE_X ),                                      \
+        CAT(prefix, _SIZE_Y),                                                \
+        CAT(prefix, _IFM_NUM),                                                  \
+        CAT(prefix, _OFM_NUM),                                                  \
+        CAT(prefix, _OFFSET))
+
+
+inline uint FUNC(get_is_o_yx_isv32_index)(uint o, uint i, uint y, uint x, uint i_size, uint o_size, uint x_size, uint y_size)
+{
+    const uint i_aligned_to_32 = ((i_size + 31) / 32) * 32;
+    const uint i_val = i % 32;
+    const uint i_slice = i / 32;
+    const size_t idx = i_val + 32* (x + x_size * (y + y_size * (o + o_size * i_slice) ) );
+    return idx;
+}
+
+#define GET_FILTER_IS_O_YX_ISV32(prefix, o, i, y, x)\
+    FUNC_CALL(get_is_o_yx_isv32_index)(\
+        o, i, y, x, CAT(prefix, _IFM_NUM),\
+        CAT(prefix, _OFM_NUM),\
+        CAT(prefix, _SIZE_X),\
+        CAT(prefix, _SIZE_Y))
+
+#define DECLARE_SAMPLER const sampler_t imageSampler = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST
+
+#if FP16_UNIT_USED
+    #define IMAGE_READ(image, coord) read_imageh((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imageh((image), (coord), (val))
+#else
+    #define IMAGE_READ(image, coord) read_imagef((image), imageSampler, (coord))
+    #define IMAGE_WRITE(image, coord, val) write_imagef((image), (coord), (val))
+#endif
+
+
+
+KERNEL(reorder_from_winograd_2x3_s1)(global const UNIT_TYPE* input_winograd, global float* output)
+{
+    const int winograd_tile_width = 4;
+    const int winograd_tile_height = 1;
+    const int output_tile_width = 2;
+    const int output_tile_height = 1;
+
+    const int batch_idx = get_global_id(0) / INPUT0_FEATURE_NUM;
+    const int feature_idx = get_global_id(0) % INPUT0_FEATURE_NUM;
+    const int tile_idx_x = get_global_id(1);
+    const int tile_idx_y = get_global_id(2);
+
+    const int out_x_idx = (tile_idx_x * output_tile_width);
+
+    //input is in bxyf -- no paddings allowed in winograd domain
+    int input_idx = batch_idx * INPUT0_BATCH_PITCH +
+                    feature_idx * INPUT0_FEATURE_PITCH +
+                    tile_idx_y * winograd_tile_height * INPUT0_Y_PITCH +
+                    tile_idx_x * winograd_tile_width * INPUT0_X_PITCH;
+
+    //winograd tile is 4x1, during conversion to standard domain values should have already been multiplied so this tile is actually an 'm' tile from the original paper
+    UNIT_TYPE winograd_tile[winograd_tile_width];
+    winograd_tile[0] = input_winograd[input_idx]; input_idx += INPUT0_X_PITCH;
+    winograd_tile[1] = input_winograd[input_idx]; input_idx += INPUT0_X_PITCH;
+    winograd_tile[2] = input_winograd[input_idx]; input_idx += INPUT0_X_PITCH;
+    winograd_tile[3] = input_winograd[input_idx];
+
+    UNIT_TYPE out_tile[output_tile_width];
+
+    //transform back
+    out_tile[0] = ACTIVATION(winograd_tile[0] + winograd_tile[1] + winograd_tile[2], NL_M ,NL_N);
+    out_tile[1] = ACTIVATION(winograd_tile[1] - winograd_tile[2] - winograd_tile[3], NL_M ,NL_N);
+
+    int out_idx = (OUTPUT_PAD_BEFORE_BATCH_NUM + batch_idx) * OUTPUT_BATCH_PITCH +
+                  (OUTPUT_PAD_BEFORE_FEATURE_NUM + feature_idx) * OUTPUT_FEATURE_PITCH +
+                  (OUTPUT_PAD_BEFORE_SIZE_Y + (tile_idx_y * output_tile_height)) * OUTPUT_Y_PITCH +
+                  (OUTPUT_PAD_BEFORE_SIZE_X + (tile_idx_x * output_tile_width)) * OUTPUT_X_PITCH;
+
+)__krnl"
+R"__krnl(    output[out_idx] = out_tile[0];
+#if LEFTOVERS == 1
+    if (out_x_idx + 1 < OUTPUT_SIZE_X)
+#endif
+    {
+        out_idx += OUTPUT_X_PITCH;
+        output[out_idx] = out_tile[1];
+    }
+};
+
+)__krnl"},
+
diff --git a/inference-engine/src/cldnn_engine/cldnn_engine.cpp b/inference-engine/src/cldnn_engine/cldnn_engine.cpp
index 4b79fe6e..0f3fdf27 100644
--- a/inference-engine/src/cldnn_engine/cldnn_engine.cpp
+++ b/inference-engine/src/cldnn_engine/cldnn_engine.cpp
@@ -38,6 +38,11 @@ using InferenceEngine::Blob;
 using namespace InferenceEngine;
 using namespace details;
 
+#if defined(__ANDROID__)
+#define CI_BUILD_NUMBER "custom-master-android-nn"
+#endif
+
+
 namespace CLDNNPlugin {
 
 struct clDNNEngine::impl {
diff --git a/inference-engine/src/cldnn_engine/cldnn_infer_request.cpp b/inference-engine/src/cldnn_engine/cldnn_infer_request.cpp
index e36578cf..e7641a98 100644
--- a/inference-engine/src/cldnn_engine/cldnn_infer_request.cpp
+++ b/inference-engine/src/cldnn_engine/cldnn_infer_request.cpp
@@ -85,7 +85,11 @@ void CLDNNInferRequest::copyOutputData(const cldnn::memory& outputMemory,
 
     switch (bptr->precision()) {
     case Precision::FP32: {
-        TBlob<float>::Ptr out_f = std::dynamic_pointer_cast<TBlob<float>>(bptr);
+#if defined(__ANDROID__)
+        TBlob<float>::Ptr out_f = std::static_pointer_cast<TBlob<float> >(bptr);
+#else
+        TBlob<float>::Ptr out_f = std::dynamic_pointer_cast<TBlob<float> >(bptr);
+#endif
         if (out_f == nullptr) {
             THROW_IE_EXCEPTION << "Invalid output blob";
         }
-- 
2.23.0

