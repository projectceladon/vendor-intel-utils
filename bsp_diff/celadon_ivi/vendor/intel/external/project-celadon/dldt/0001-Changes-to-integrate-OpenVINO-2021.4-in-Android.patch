From ef8aa622f1815bb44590624324d2e859ffdf84b7 Mon Sep 17 00:00:00 2001
From: "nagamani.chennuboina@intel.com" <nagamani.chennuboina@intel.com>
Date: Tue, 24 Aug 2021 18:22:54 +0530
Subject: [PATCH] Changes to integrate OpenVINO 2021.4 in Android

- Added Android.bp to generated libinference_engine and its dependencies
- Fixed rtti dynamic_cast compilation issues with respect to NNHAL
- Added "CVS-61200" PR to fix issue related to node with "unique friendly name"
- Added files that are generated during cmake

Signed-off-by: Nagamani Chennuboina <nagamani.chennuboina@intel.com>
---
 Android.bp                                    | 1102 +++++++++++++++++
 inference-engine/include/ie_blob.h            |   20 +
 inference-engine/include/ie_parameter.hpp     |   16 +-
 inference-engine/include/ie_precision.hpp     |    9 +
 .../cnn_network_ngraph_impl.cpp               |    8 +-
 .../src/inference_engine/ie_rtti.cpp          |  193 +++
 .../cross-compiled/ANY/proposal_imp.cpp       |  388 ++++++
 .../cross-compiled/AVX2/proposal_imp.cpp      |  388 ++++++
 .../cross-compiled/proposal_imp_disp.cpp      |   43 +
 .../cross-compiled/proposal_imp_holder.txt    |   16 +
 ngraph/core/include/ngraph/version.hpp        |    5 +
 .../src/runtime/reference/jit_generator.cpp   |    4 +
 .../src/runtime/reference/jit_generator.hpp   |    4 +
 13 files changed, 2189 insertions(+), 7 deletions(-)
 create mode 100644 Android.bp
 create mode 100644 inference-engine/src/inference_engine/ie_rtti.cpp
 create mode 100644 inference-engine/src/mkldnn_plugin/cross-compiled/ANY/proposal_imp.cpp
 create mode 100644 inference-engine/src/mkldnn_plugin/cross-compiled/AVX2/proposal_imp.cpp
 create mode 100644 inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_disp.cpp
 create mode 100644 inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_holder.txt
 create mode 100644 ngraph/core/include/ngraph/version.hpp

diff --git a/Android.bp b/Android.bp
new file mode 100644
index 000000000000..d308be1c21f4
--- /dev/null
+++ b/Android.bp
@@ -0,0 +1,1102 @@
+//####################COMMON CFLAGS######################
+common_cflags = [
+    "-O3",
+    "-std=c++11",
+    "-DANDROID",
+    "-D__ANDROID__",
+    "-D_FORTIFY_SOURCE=2",
+    "-DNDEBUG",
+    "-Wall",
+    "-Wformat",
+    "-Wformat-security",
+    "-Wuninitialized",
+    "-Wno-undef",
+    "-Wno-unused-function",
+    "-Winit-self",
+    "-Wmaybe-uninitialized",
+    "-Wreturn-type",
+    "-Wno-unused-variable",
+    "-Wno-unused-parameter",
+    "-Wno-error=redundant-move",
+    "-Wno-unknown-pragmas",
+    "-Wno-profile-instr-missing",
+    "-Wno-sign-conversion",
+    "-Wno-reorder-ctor",
+    "-Wno-deprecated-declarations",
+    "-Wno-non-virtual-dtor",
+    "-Wno-overloaded-virtual",
+    "-Wno-strict-overflow",
+    "-fPIC",
+    "-fexceptions",
+    "-frtti",
+    "-ffunction-sections",
+    "-fdata-sections",
+    "-fdiagnostics-show-option",
+    "-ffunction-sections",
+    "-fstack-protector-all",
+    "-fsigned-char",
+]
+
+//#################################################
+
+cc_library_headers {
+    name: "plugin_api_headers",
+    vendor: true,
+    export_include_dirs: [
+        "inference-engine/src/plugin_api",
+        "inference-engine/src/plugin_api/cpp_interfaces",
+        "inference-engine/src/plugin_api/threading",
+    ],
+}
+
+//#################################################
+
+cc_library_headers {
+    name: "libinference_headers",
+    vendor: true,
+    export_include_dirs: [
+        "inference-engine/src/plugin_api",
+        "inference-engine/src/preprocessing",
+        "inference-engine/src/inference_engine",
+        "inference-engine/include",
+        "openvino/itt/include",
+        "openvino/pp/include",
+        "inference-engine/src/legacy_api/include",
+        "inference-engine/thirdparty/pugixml/src",
+        "ngraph/core/include",
+        "inference-engine/src/transformations/include",
+        "inference-engine/src/readers/reader_api",
+        "inference-engine/src/readers/ir_reader",
+        "inference-engine/include/cpp",
+        "inference-engine/include/details",
+    ]
+}
+
+//#################################################
+
+cc_library_shared {
+
+    name: "libinference_engine",
+    proprietary: true,
+    owner: "intel",
+    compile_multilib: "64",
+
+    srcs: [
+        "inference-engine/src/inference_engine/blob_factory.cpp",
+        "inference-engine/src/inference_engine/blob_transform.cpp",
+        "inference-engine/src/inference_engine/cnn_network_ngraph_impl.cpp",
+        "inference-engine/src/inference_engine/compilation_context.cpp",
+        "inference-engine/src/inference_engine/cpp/ie_cnn_network.cpp",
+        "inference-engine/src/inference_engine/cpp/ie_executable_network.cpp",
+        "inference-engine/src/inference_engine/cpp/ie_infer_request.cpp",
+        "inference-engine/src/inference_engine/cpp/ie_variable_state.cpp",
+        "inference-engine/src/inference_engine/cpp_interfaces/interface/ie_iexecutable_network_internal.cpp",
+        "inference-engine/src/inference_engine/cpp_interfaces/interface/ie_iinfer_request_internal.cpp",
+        "inference-engine/src/inference_engine/cpp_interfaces/interface/ie_iplugin_internal.cpp",
+        "inference-engine/src/inference_engine/cpp_interfaces/interface/ie_ivariable_state_internal.cpp",
+        "inference-engine/src/inference_engine/ie_blob_common.cpp",
+        "inference-engine/src/inference_engine/ie_cache_guard.cpp",
+        "inference-engine/src/inference_engine/ie_common.cpp",
+        "inference-engine/src/inference_engine/ie_compound_blob.cpp",
+        "inference-engine/src/inference_engine/ie_core.cpp",
+        "inference-engine/src/inference_engine/ie_data.cpp",
+        "inference-engine/src/inference_engine/ie_layouts.cpp",
+        "inference-engine/src/inference_engine/ie_memcpy.cpp",
+        "inference-engine/src/inference_engine/ie_network_reader.cpp",
+        "inference-engine/src/inference_engine/ie_ngraph_utils.cpp",
+        "inference-engine/src/inference_engine/ie_parameter.cpp",
+        "inference-engine/src/inference_engine/ie_system_conf.cpp",
+        "inference-engine/src/inference_engine/ie_transformations.cpp",
+        "inference-engine/src/inference_engine/ie_version.cpp",
+        "inference-engine/src/inference_engine/os/lin/lin_shared_object_loader.cpp",
+        "inference-engine/src/inference_engine/os/lin/lin_system_conf.cpp",
+        "inference-engine/src/inference_engine/precision_utils.cpp",
+        "inference-engine/src/inference_engine/system_allocator.cpp",
+        "inference-engine/src/inference_engine/threading/ie_cpu_streams_executor.cpp",
+        "inference-engine/src/inference_engine/threading/ie_executor_manager.cpp",
+        "inference-engine/src/inference_engine/threading/ie_istreams_executor.cpp",
+        "inference-engine/src/inference_engine/threading/ie_itask_executor.cpp",
+        "inference-engine/src/inference_engine/threading/ie_parallel_custom_arena.cpp",
+        "inference-engine/src/inference_engine/threading/ie_thread_affinity.cpp",
+        "inference-engine/src/inference_engine/xml_parse_utils.cpp",
+        "inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp",
+        "inference-engine/src/inference_engine/file_utils.cpp",
+        "inference-engine/src/legacy_api/src/transformations/convert_opset1_to_legacy/convert_one_hot_to_one_hot_ie.cpp",
+        "inference-engine/src/legacy_api/src/transformations/convert_opset1_to_legacy/convert_nms_5_to_legacy.cpp",
+        "inference-engine/src/legacy_api/src/ngraph_ops/nms_ie.cpp",
+        "inference-engine/src/legacy_api/src/ngraph_ops/onehot_ie.cpp",
+        "inference-engine/src/inference_engine/ie_rtti.cpp",
+    ],
+
+    header_libs: [
+        "libinference_headers",
+        "libdnnl_headers",
+        "libtbb_headers",
+    ],
+
+    cflags: common_cflags + [
+        "-DENABLE_MKL_DNN=1",
+        "-DHAVE_SSE=1",
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-DIE_THREAD=IE_THREAD_TBB",
+        "-DIMPLEMENT_INFERENCE_ENGINE_API",
+        "-Dinference_engine_EXPORTS",
+        "-DUSE_STATIC_IE",
+        "-Dmsse4.2",
+        "-DCI_BUILD_NUMBER=\"custom_releases/2021/4_b76c90374501ac461d89a8fba44335741d066ccc\"",
+    ],
+
+    shared_libs: [
+        "liblog",
+        "libtbb",
+        "libtbbmalloc",
+        "libinference_engine_transformations",
+        "libngraph"
+    ],
+
+    static_libs: [
+        "libpugixml",
+        "libitt"
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+
+}
+
+//#################################################
+
+cc_library_headers {
+    name: "libinference_engine_transformations_headers",
+    vendor: true,
+    export_include_dirs: [
+        "inference-engine/src/transformations/include",
+        "inference-engine/src/transformations/src",
+        "ngraph/core/include",
+        "ngraph/core/reference/include",
+        "openvino/itt/include",
+        "openvino/pp/include",
+        "ngraph/core/builder/include",
+        "inference-engine/thirdparty/pugixml/src",
+        "openvino/conditional_compilation/include"
+    ]
+}
+
+//#################################################
+
+cc_library_shared {
+
+    name: "libinference_engine_transformations",
+    proprietary: true,
+    owner: "intel",
+    compile_multilib: "64",
+
+    srcs: [
+        "inference-engine/src/transformations/src/ngraph_ops/convolution_ie.cpp",
+        "inference-engine/src/transformations/src/ngraph_ops/deconvolution_ie.cpp",
+        "inference-engine/src/transformations/src/ngraph_ops/framework_node.cpp",
+        "inference-engine/src/transformations/src/ngraph_ops/nms_ie_internal.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/add_fake_quantize_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/algebraic_simplification.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/batch_to_space_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/binarize_weights.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/broadcast_elementwise_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/clamp_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/common_optimizations.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/conv_bias_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/conv_mul_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/conv_to_binary_conv.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/convert_quantize_dequantize.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/depth_to_space_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/dilated_convolution_converter.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/eliminate_unsqueeze_gather.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/fq_mul_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/fq_reshape_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/gelu_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/hsigmoid_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/hswish_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/lin_op_sequence_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/mish_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/mul_fake_quantize_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/mvn_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/nop_elimination.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/normalize_l2_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/optimize_strided_slice.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/pad_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/pull_transpose_through_fq.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/relu_fake_quantize_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/remove_filtering_boxes_by_size.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/shuffle_channels_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/softmax_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/softplus_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/softplus_to_mish_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/space_to_batch_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/split_squeeze_concat_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/swish_fusion.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/transpose_sinking.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/transpose_to_reshape.cpp",
+        "inference-engine/src/transformations/src/transformations/common_optimizations/weights_dequantize_to_fake_quantize.cpp",
+        "inference-engine/src/transformations/src/transformations/control_flow/unroll_tensor_iterator.cpp",
+        "inference-engine/src/transformations/src/transformations/init_node_info.cpp",
+        "inference-engine/src/transformations/src/transformations/low_precision/disable_convert_constant_folding_on_const_path.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/batch_norm_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/bidirectional_sequences_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_batch_to_space.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_broadcast3.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_broadcast_to_tiles.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_convolutions.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_depth_to_space.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_divide.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_gather_0d.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_gather_v1_to_gather_v7.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_gather_v7_to_gather_v1.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_gelu.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_interpolate1_to_interpolate4.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_minimum_to_power_and_max.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_mod.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_mvn1_to_mvn6.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_negative.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_nms_to_nms_ie_internal.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_pad_to_group_conv.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_previous_nms_to_nms_5.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_reduce_to_pooling.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_scatter_elements_to_scatter.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_sequences_to_tensor_iterator.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_shapeof3.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_shuffle_channels3.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_space_to_batch.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_space_to_depth.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_subtract.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_ti_to_sequences.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/convert_topk3.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/einsum_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/fq_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/gather_normalize_negative_indices.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/gelu7_downgrade.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/gru_cell_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/hsigmoid_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/hswish_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/log_softmax_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/lstm_cell_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/mvn6_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/reduce_l1_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/reduce_l2_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/rnn_cell_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/simplify_ctc_greedy_decoder_seq_len.cpp",
+        "inference-engine/src/transformations/src/transformations/op_conversions/softplus_decomposition.cpp",
+        "inference-engine/src/transformations/src/transformations/opset_conversions/convert_opset2_to_opset1.cpp",
+        "inference-engine/src/transformations/src/transformations/opset_conversions/convert_opset3_to_opset2.cpp",
+        "inference-engine/src/transformations/src/transformations/rt_info/dequantization_attribute.cpp",
+        "inference-engine/src/transformations/src/transformations/rt_info/fused_names_attribute.cpp",
+        "inference-engine/src/transformations/src/transformations/rt_info/primitives_priority_attribute.cpp",
+        "inference-engine/src/transformations/src/transformations/serialize.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/matmul_sr.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/mimic_set_batch_size.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/proposal_scales_stridedslice.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/reshape_to_1D.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/set_batch_size.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/smart_reshape.cpp",
+        "inference-engine/src/transformations/src/transformations/smart_reshape/strided_slice_squeeze.cpp",
+        "inference-engine/src/transformations/src/transformations/utils/utils.cpp",
+    ],
+
+    header_libs: [
+        "libinference_engine_transformations_headers"
+    ],
+
+    cflags: common_cflags + [
+        "-DENABLE_MKL_DNN=1",
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-Dinference_engine_transformations_EXPORTS"
+    ],
+
+    shared_libs: [
+        "liblog",
+        "libngraph"
+    ],
+
+    static_libs: [
+        "libngraph_reference",
+        "libngraph_builders",
+        "libpugixml",
+        "libitt"
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_headers {
+    name: "libinference_engine_lp_transformations_headers",
+    vendor: true,
+    export_include_dirs: [
+        "inference-engine/src/low_precision_transformations/include",
+        "inference-engine/src/transformations/include",
+        "ngraph/core/include",
+        "openvino/itt/include",
+        "openvino/pp/include",
+    ],
+}
+
+//#################################################
+
+cc_library_shared {
+
+    name: "libinference_engine_lp_transformations",
+    proprietary: true,
+    owner: "intel",
+    compile_multilib: "64",
+
+    srcs: [
+        "inference-engine/src/low_precision_transformations/src/add.cpp",
+        "inference-engine/src/low_precision_transformations/src/avg_pool.cpp",
+        "inference-engine/src/low_precision_transformations/src/clamp.cpp",
+        "inference-engine/src/low_precision_transformations/src/concat.cpp",
+        "inference-engine/src/low_precision_transformations/src/concat_multi_channels.cpp",
+        "inference-engine/src/low_precision_transformations/src/convert.cpp",
+        "inference-engine/src/low_precision_transformations/src/convert_subtract_constant.cpp",
+        "inference-engine/src/low_precision_transformations/src/convolution.cpp",
+        "inference-engine/src/low_precision_transformations/src/convolution_backprop_data.cpp",
+        "inference-engine/src/low_precision_transformations/src/depth_to_space.cpp",
+        "inference-engine/src/low_precision_transformations/src/eltwise_base_transformation.cpp",
+        "inference-engine/src/low_precision_transformations/src/fake_quantize.cpp",
+        "inference-engine/src/low_precision_transformations/src/fake_quantize_decomposition.cpp",
+        "inference-engine/src/low_precision_transformations/src/fake_quantize_dequantization.cpp",
+        "inference-engine/src/low_precision_transformations/src/fold_convert.cpp",
+        "inference-engine/src/low_precision_transformations/src/fuse_convert.cpp",
+        "inference-engine/src/low_precision_transformations/src/fuse_fake_quantize.cpp",
+        "inference-engine/src/low_precision_transformations/src/fuse_multiply_to_fake_quantize.cpp",
+        "inference-engine/src/low_precision_transformations/src/fuse_subtract_to_fake_quantize.cpp",
+        "inference-engine/src/low_precision_transformations/src/group_convolution.cpp",
+        "inference-engine/src/low_precision_transformations/src/interpolate.cpp",
+        "inference-engine/src/low_precision_transformations/src/layer_transformation.cpp",
+        "inference-engine/src/low_precision_transformations/src/mat_mul.cpp",
+        "inference-engine/src/low_precision_transformations/src/max_pool.cpp",
+        "inference-engine/src/low_precision_transformations/src/multiply.cpp",
+        "inference-engine/src/low_precision_transformations/src/multiply_to_group_convolution.cpp",
+        "inference-engine/src/low_precision_transformations/src/mvn.cpp",
+        "inference-engine/src/low_precision_transformations/src/network_helper.cpp",
+        "inference-engine/src/low_precision_transformations/src/normalize_l2.cpp",
+        "inference-engine/src/low_precision_transformations/src/prelu.cpp",
+        "inference-engine/src/low_precision_transformations/src/pull_reshape_through_dequantization.cpp",
+        "inference-engine/src/low_precision_transformations/src/pull_transpose_through_dequantization.cpp",
+        "inference-engine/src/low_precision_transformations/src/quantization_details.cpp",
+        "inference-engine/src/low_precision_transformations/src/reduce_base_transformation.cpp",
+        "inference-engine/src/low_precision_transformations/src/reduce_max.cpp",
+        "inference-engine/src/low_precision_transformations/src/reduce_mean.cpp",
+        "inference-engine/src/low_precision_transformations/src/reduce_min.cpp",
+        "inference-engine/src/low_precision_transformations/src/reduce_sum.cpp",
+        "inference-engine/src/low_precision_transformations/src/relu.cpp",
+        "inference-engine/src/low_precision_transformations/src/reshape.cpp",
+        "inference-engine/src/low_precision_transformations/src/shuffle_channels.cpp",
+        "inference-engine/src/low_precision_transformations/src/split.cpp",
+        "inference-engine/src/low_precision_transformations/src/squeeze.cpp",
+        "inference-engine/src/low_precision_transformations/src/strided_slice.cpp",
+        "inference-engine/src/low_precision_transformations/src/subgraph.cpp",
+        "inference-engine/src/low_precision_transformations/src/subtract.cpp",
+        "inference-engine/src/low_precision_transformations/src/subtract_multiply_to_multiply_add.cpp",
+        "inference-engine/src/low_precision_transformations/src/transformation_context.cpp",
+        "inference-engine/src/low_precision_transformations/src/transformer.cpp",
+        "inference-engine/src/low_precision_transformations/src/transparent_base_transformation.cpp",
+        "inference-engine/src/low_precision_transformations/src/transpose.cpp",
+        "inference-engine/src/low_precision_transformations/src/unsqueeze.cpp",
+        "inference-engine/src/low_precision_transformations/src/variadic_split.cpp",
+        "inference-engine/src/low_precision_transformations/src/weightable_layer_transformation.cpp"
+    ],
+
+    header_libs: [
+        "libinference_engine_lp_transformations_headers"
+    ],
+
+    cflags: common_cflags + [
+        "-DENABLE_MKL_DNN=1",
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-Dinference_engine_lp_transformations_EXPORTS",
+        "-Dinference_engine_transformations_EXPORTS"
+    ],
+
+    shared_libs: [
+        "liblog",
+        "libinference_engine_transformations",
+        "libngraph"
+    ],
+
+    static_libs: [
+        "libitt"
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_headers {
+    name: "libngraph_headers",
+    vendor: true,
+    export_include_dirs: [
+        "ngraph/core/include",
+        "ngraph/core/src",
+        "ngraph/core/builder/include",
+        "ngraph/core/reference/include",
+        "openvino/conditional_compilation/include",
+        "openvino/itt/include",
+        "openvino/pp/include",
+        "inference-engine/src/transformations/include"
+    ],
+}
+
+//#################################################
+
+cc_library_shared {
+
+    name: "libngraph",
+    proprietary: true,
+    owner: "intel",
+    compile_multilib: "64",
+
+    srcs: [
+        "ngraph/core/src/attribute_adapter.cpp",
+        "ngraph/core/src/attribute_visitor.cpp",
+        "ngraph/core/src/axis_set.cpp",
+        "ngraph/core/src/axis_vector.cpp",
+        "ngraph/core/src/check.cpp",
+        "ngraph/core/src/coordinate.cpp",
+        "ngraph/core/src/coordinate_diff.cpp",
+        "ngraph/core/src/descriptor/input.cpp",
+        "ngraph/core/src/descriptor/output.cpp",
+        "ngraph/core/src/descriptor/tensor.cpp",
+        "ngraph/core/src/dimension.cpp",
+        "ngraph/core/src/distributed.cpp",
+        "ngraph/core/src/env_util.cpp",
+        "ngraph/core/src/factory.cpp",
+        "ngraph/core/src/file_util.cpp",
+        "ngraph/core/src/function.cpp",
+        "ngraph/core/src/graph_util.cpp",
+        "ngraph/core/src/interval.cpp",
+        "ngraph/core/src/log.cpp",
+        "ngraph/core/src/ngraph.cpp",
+        "ngraph/core/src/node.cpp",
+        "ngraph/core/src/node_input.cpp",
+        "ngraph/core/src/node_output.cpp",
+        "ngraph/core/src/op/abs.cpp",
+        "ngraph/core/src/op/acos.cpp",
+        "ngraph/core/src/op/acosh.cpp",
+        "ngraph/core/src/op/add.cpp",
+        "ngraph/core/src/op/and.cpp",
+        "ngraph/core/src/op/asin.cpp",
+        "ngraph/core/src/op/asinh.cpp",
+        "ngraph/core/src/op/assign.cpp",
+        "ngraph/core/src/op/atan.cpp",
+        "ngraph/core/src/op/atanh.cpp",
+        "ngraph/core/src/op/avg_pool.cpp",
+        "ngraph/core/src/op/batch_norm.cpp",
+        "ngraph/core/src/op/batch_to_space.cpp",
+        "ngraph/core/src/op/binary_convolution.cpp",
+        "ngraph/core/src/op/broadcast.cpp",
+        "ngraph/core/src/op/bucketize.cpp",
+        "ngraph/core/src/op/ceiling.cpp",
+        "ngraph/core/src/op/clamp.cpp",
+        "ngraph/core/src/op/concat.cpp",
+        "ngraph/core/src/op/constant.cpp",
+        "ngraph/core/src/op/convert.cpp",
+        "ngraph/core/src/op/convert_like.cpp",
+        "ngraph/core/src/op/convolution.cpp",
+        "ngraph/core/src/op/cos.cpp",
+        "ngraph/core/src/op/cosh.cpp",
+        "ngraph/core/src/op/ctc_greedy_decoder.cpp",
+        "ngraph/core/src/op/ctc_greedy_decoder_seq_len.cpp",
+        "ngraph/core/src/op/ctc_loss.cpp",
+        "ngraph/core/src/op/cum_sum.cpp",
+        "ngraph/core/src/op/deformable_convolution.cpp",
+        "ngraph/core/src/op/deformable_psroi_pooling.cpp",
+        "ngraph/core/src/op/depth_to_space.cpp",
+        "ngraph/core/src/op/detection_output.cpp",
+        "ngraph/core/src/op/dft.cpp",
+        "ngraph/core/src/op/divide.cpp",
+        "ngraph/core/src/op/einsum.cpp",
+        "ngraph/core/src/op/elu.cpp",
+        "ngraph/core/src/op/embedding_segments_sum.cpp",
+        "ngraph/core/src/op/embeddingbag_offsets_sum.cpp",
+        "ngraph/core/src/op/embeddingbag_packedsum.cpp",
+        "ngraph/core/src/op/equal.cpp",
+        "ngraph/core/src/op/erf.cpp",
+        "ngraph/core/src/op/exp.cpp",
+        "ngraph/core/src/op/experimental_detectron_detection_output.cpp",
+        "ngraph/core/src/op/experimental_detectron_generate_proposals.cpp",
+        "ngraph/core/src/op/experimental_detectron_prior_grid_generator.cpp",
+        "ngraph/core/src/op/experimental_detectron_roi_feature.cpp",
+        "ngraph/core/src/op/experimental_detectron_topkrois.cpp",
+        "ngraph/core/src/op/extractimagepatches.cpp",
+        "ngraph/core/src/op/fake_quantize.cpp",
+        "ngraph/core/src/op/floor.cpp",
+        "ngraph/core/src/op/floor_mod.cpp",
+        "ngraph/core/src/op/gather.cpp",
+        "ngraph/core/src/op/gather_elements.cpp",
+        "ngraph/core/src/op/gather_nd.cpp",
+        "ngraph/core/src/op/gather_tree.cpp",
+        "ngraph/core/src/op/gelu.cpp",
+        "ngraph/core/src/op/greater.cpp",
+        "ngraph/core/src/op/greater_eq.cpp",
+        "ngraph/core/src/op/grn.cpp",
+        "ngraph/core/src/op/group_conv.cpp",
+        "ngraph/core/src/op/gru_cell.cpp",
+        "ngraph/core/src/op/gru_sequence.cpp",
+        "ngraph/core/src/op/hard_sigmoid.cpp",
+        "ngraph/core/src/op/hsigmoid.cpp",
+        "ngraph/core/src/op/hswish.cpp",
+        "ngraph/core/src/op/idft.cpp",
+        "ngraph/core/src/op/interpolate.cpp",
+        "ngraph/core/src/op/less.cpp",
+        "ngraph/core/src/op/less_eq.cpp",
+        "ngraph/core/src/op/log.cpp",
+        "ngraph/core/src/op/log_softmax.cpp",
+        "ngraph/core/src/op/loop.cpp",
+        "ngraph/core/src/op/lrn.cpp",
+        "ngraph/core/src/op/lstm_cell.cpp",
+        "ngraph/core/src/op/lstm_sequence.cpp",
+        "ngraph/core/src/op/matmul.cpp",
+        "ngraph/core/src/op/max.cpp",
+        "ngraph/core/src/op/max_pool.cpp",
+        "ngraph/core/src/op/maximum.cpp",
+        "ngraph/core/src/op/min.cpp",
+        "ngraph/core/src/op/minimum.cpp",
+        "ngraph/core/src/op/mish.cpp",
+        "ngraph/core/src/op/mod.cpp",
+        "ngraph/core/src/op/multiply.cpp",
+        "ngraph/core/src/op/mvn.cpp",
+        "ngraph/core/src/op/negative.cpp",
+        "ngraph/core/src/op/non_max_suppression.cpp",
+        "ngraph/core/src/op/non_zero.cpp",
+        "ngraph/core/src/op/normalize_l2.cpp",
+        "ngraph/core/src/op/not.cpp",
+        "ngraph/core/src/op/not_equal.cpp",
+        "ngraph/core/src/op/one_hot.cpp",
+        "ngraph/core/src/op/op.cpp",
+        "ngraph/core/src/op/or.cpp",
+        "ngraph/core/src/op/pad.cpp",
+        "ngraph/core/src/op/parameter.cpp",
+        "ngraph/core/src/op/power.cpp",
+        "ngraph/core/src/op/prelu.cpp",
+        "ngraph/core/src/op/prior_box.cpp",
+        "ngraph/core/src/op/prior_box_clustered.cpp",
+        "ngraph/core/src/op/proposal.cpp",
+        "ngraph/core/src/op/psroi_pooling.cpp",
+        "ngraph/core/src/op/range.cpp",
+        "ngraph/core/src/op/read_value.cpp",
+        "ngraph/core/src/op/reduce_l1.cpp",
+        "ngraph/core/src/op/reduce_l2.cpp",
+        "ngraph/core/src/op/reduce_logical_and.cpp",
+        "ngraph/core/src/op/reduce_logical_or.cpp",
+        "ngraph/core/src/op/reduce_mean.cpp",
+        "ngraph/core/src/op/reduce_prod.cpp",
+        "ngraph/core/src/op/reduce_sum.cpp",
+        "ngraph/core/src/op/region_yolo.cpp",
+        "ngraph/core/src/op/relu.cpp",
+        "ngraph/core/src/op/reorg_yolo.cpp",
+        "ngraph/core/src/op/reshape.cpp",
+        "ngraph/core/src/op/result.cpp",
+        "ngraph/core/src/op/reverse.cpp",
+        "ngraph/core/src/op/reverse_sequence.cpp",
+        "ngraph/core/src/op/rnn_cell.cpp",
+        "ngraph/core/src/op/rnn_sequence.cpp",
+        "ngraph/core/src/op/roi_align.cpp",
+        "ngraph/core/src/op/roi_pooling.cpp",
+        "ngraph/core/src/op/roll.cpp",
+        "ngraph/core/src/op/round.cpp",
+        "ngraph/core/src/op/scatter_elements_update.cpp",
+        "ngraph/core/src/op/scatter_nd_update.cpp",
+        "ngraph/core/src/op/scatter_update.cpp",
+        "ngraph/core/src/op/select.cpp",
+        "ngraph/core/src/op/selu.cpp",
+        "ngraph/core/src/op/shape_of.cpp",
+        "ngraph/core/src/op/shuffle_channels.cpp",
+        "ngraph/core/src/op/sigmoid.cpp",
+        "ngraph/core/src/op/sign.cpp",
+        "ngraph/core/src/op/sin.cpp",
+        "ngraph/core/src/op/sinh.cpp",
+        "ngraph/core/src/op/sink.cpp",
+        "ngraph/core/src/op/softmax.cpp",
+        "ngraph/core/src/op/softplus.cpp",
+        "ngraph/core/src/op/space_to_batch.cpp",
+        "ngraph/core/src/op/space_to_depth.cpp",
+        "ngraph/core/src/op/split.cpp",
+        "ngraph/core/src/op/sqrt.cpp",
+        "ngraph/core/src/op/squared_difference.cpp",
+        "ngraph/core/src/op/squeeze.cpp",
+        "ngraph/core/src/op/strided_slice.cpp",
+        "ngraph/core/src/op/subtract.cpp",
+        "ngraph/core/src/op/swish.cpp",
+        "ngraph/core/src/op/tan.cpp",
+        "ngraph/core/src/op/tanh.cpp",
+        "ngraph/core/src/op/tensor_iterator.cpp",
+        "ngraph/core/src/op/tile.cpp",
+        "ngraph/core/src/op/topk.cpp",
+        "ngraph/core/src/op/transpose.cpp",
+        "ngraph/core/src/op/type_relaxed.cpp",
+        "ngraph/core/src/op/unsqueeze.cpp",
+        "ngraph/core/src/op/util/activation_functions.cpp",
+        "ngraph/core/src/op/util/arithmetic_reduction.cpp",
+        "ngraph/core/src/op/util/arithmetic_reductions_keep_dims.cpp",
+        "ngraph/core/src/op/util/attr_types.cpp",
+        "ngraph/core/src/op/util/binary_elementwise_arithmetic.cpp",
+        "ngraph/core/src/op/util/binary_elementwise_comparison.cpp",
+        "ngraph/core/src/op/util/binary_elementwise_logical.cpp",
+        "ngraph/core/src/op/util/broadcast_base.cpp",
+        "ngraph/core/src/op/util/elementwise_args.cpp",
+        "ngraph/core/src/op/util/embeddingbag_offsets_base.cpp",
+        "ngraph/core/src/op/util/embeddingbag_packed_base.cpp",
+        "ngraph/core/src/op/util/evaluation_context.cpp",
+        "ngraph/core/src/op/util/fft_base.cpp",
+        "ngraph/core/src/op/util/fused_op.cpp",
+        "ngraph/core/src/op/util/gather_base.cpp",
+        "ngraph/core/src/op/util/index_reduction.cpp",
+        "ngraph/core/src/op/util/logical_reduction.cpp",
+        "ngraph/core/src/op/util/logical_reduction_keep_dims.cpp",
+        "ngraph/core/src/op/util/op_types.cpp",
+        "ngraph/core/src/op/util/recurrent_sequence.cpp",
+        "ngraph/core/src/op/util/rnn_cell_base.cpp",
+        "ngraph/core/src/op/util/scatter_base.cpp",
+        "ngraph/core/src/op/util/scatter_nd_base.cpp",
+        "ngraph/core/src/op/util/sub_graph_base.cpp",
+        "ngraph/core/src/op/util/unary_elementwise_arithmetic.cpp",
+        "ngraph/core/src/op/util/variable.cpp",
+        "ngraph/core/src/op/variadic_split.cpp",
+        "ngraph/core/src/op/xor.cpp",
+        "ngraph/core/src/opsets/opset.cpp",
+        "ngraph/core/src/partial_shape.cpp",
+        "ngraph/core/src/pass/constant_folding.cpp",
+        "ngraph/core/src/pass/convert_fp32_to_fp16.cpp",
+        "ngraph/core/src/pass/convert_precision.cpp",
+        "ngraph/core/src/pass/graph_rewrite.cpp",
+        "ngraph/core/src/pass/low_latency.cpp",
+        "ngraph/core/src/pass/manager.cpp",
+        "ngraph/core/src/pass/pass.cpp",
+        "ngraph/core/src/pass/pass_config.cpp",
+        "ngraph/core/src/pass/perf_counters.cpp",
+        "ngraph/core/src/pass/validate.cpp",
+        "ngraph/core/src/pass/visualize_tree.cpp",
+        "ngraph/core/src/pattern/matcher.cpp",
+        "ngraph/core/src/pattern/op/any.cpp",
+        "ngraph/core/src/pattern/op/any_of.cpp",
+        "ngraph/core/src/pattern/op/any_output.cpp",
+        "ngraph/core/src/pattern/op/branch.cpp",
+        "ngraph/core/src/pattern/op/capture.cpp",
+        "ngraph/core/src/pattern/op/label.cpp",
+        "ngraph/core/src/pattern/op/or.cpp",
+        "ngraph/core/src/pattern/op/pattern.cpp",
+        "ngraph/core/src/pattern/op/skip.cpp",
+        "ngraph/core/src/pattern/op/true.cpp",
+        "ngraph/core/src/pattern/op/wrap_type.cpp",
+        "ngraph/core/src/provenance.cpp",
+        "ngraph/core/src/rt_info.cpp",
+        "ngraph/core/src/runtime/aligned_buffer.cpp",
+        "ngraph/core/src/runtime/host_tensor.cpp",
+        "ngraph/core/src/runtime/tensor.cpp",
+        "ngraph/core/src/shape.cpp",
+        "ngraph/core/src/shape_util.cpp",
+        "ngraph/core/src/slice_plan.cpp",
+        "ngraph/core/src/specialize_function.cpp",
+        "ngraph/core/src/strides.cpp",
+        "ngraph/core/src/type.cpp",
+        "ngraph/core/src/type/bfloat16.cpp",
+        "ngraph/core/src/type/element_type.cpp",
+        "ngraph/core/src/type/float16.cpp",
+        "ngraph/core/src/util.cpp",
+        "ngraph/core/src/validation_util.cpp",
+        "ngraph/core/src/variant.cpp"
+    ],
+
+    header_libs: [
+        "libngraph_headers"
+    ],
+
+    cflags: common_cflags + [
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-DIN_NGRAPH_LIBRARY",
+        "-DPROJECT_ROOT_DIR=\"ngraph\"",
+        "-Dngraph_EXPORTS"
+    ],
+
+    shared_libs: [
+        "liblog"
+    ],
+
+    static_libs: [
+        "libngraph_reference",
+        "libngraph_builders",
+        "libitt"
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_static {
+    name: "libngraph_builders",
+    proprietary: true,
+    compile_multilib: "64",
+
+    srcs: [
+        "ngraph/core/builder/src/builder/autobroadcast.cpp",
+        "ngraph/core/builder/src/builder/make_constant.cpp",
+        "ngraph/core/builder/src/builder/norm.cpp",
+        "ngraph/core/builder/src/builder/reduce_ops.cpp",
+        "ngraph/core/builder/src/builder/reshape.cpp",
+        "ngraph/core/builder/src/builder/split.cpp",
+    ],
+
+    local_include_dirs: [
+        "ngraph/core/builder/include",
+        "ngraph/core/include"
+    ],
+
+    cflags: common_cflags + [
+        "-DNGRAPH_DYNAMIC_COMPONENTS_ENABLE",
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-DPROJECT_ROOT_DIR=\"ngraph\""
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_static {
+    name: "libngraph_reference",
+    proprietary: true,
+    compile_multilib: "64",
+
+    srcs: [
+        "ngraph/core/reference/src/coordinate_index.cpp",
+        "ngraph/core/reference/src/coordinate_range.cpp",
+        "ngraph/core/reference/src/coordinate_transform.cpp",
+        "ngraph/core/reference/src/runtime/opt_kernel/reshape.cpp",
+        "ngraph/core/reference/src/runtime/reference/broadcast.cpp",
+        "ngraph/core/reference/src/runtime/reference/concat.cpp",
+        "ngraph/core/reference/src/runtime/reference/convert.cpp",
+        "ngraph/core/reference/src/runtime/reference/einsum.cpp",
+        "ngraph/core/reference/src/runtime/reference/eval_helpers.cpp",
+        "ngraph/core/reference/src/runtime/reference/experimental_detectron_detection_output.cpp",
+        "ngraph/core/reference/src/runtime/reference/fft.cpp",
+        "ngraph/core/reference/src/runtime/reference/function.cpp",
+        "ngraph/core/reference/src/runtime/reference/gather_tree.cpp",
+        "ngraph/core/reference/src/runtime/reference/interpolate.cpp",
+        "ngraph/core/reference/src/runtime/reference/jit_generator.cpp",
+        "ngraph/core/reference/src/runtime/reference/loop.cpp",
+        "ngraph/core/reference/src/runtime/reference/matmul.cpp",
+        "ngraph/core/reference/src/runtime/reference/non_max_suppression.cpp",
+        "ngraph/core/reference/src/runtime/reference/pad.cpp",
+        "ngraph/core/reference/src/runtime/reference/reorg_yolo.cpp",
+        "ngraph/core/reference/src/runtime/reference/reshape.cpp",
+        "ngraph/core/reference/src/runtime/reference/reverse.cpp",
+        "ngraph/core/reference/src/runtime/reference/slice.cpp",
+        "ngraph/core/reference/src/runtime/reference/split.cpp",
+        "ngraph/core/reference/src/runtime/reference/strided_slice.cpp",
+        "ngraph/core/reference/src/runtime/reference/tensor_iterator.cpp",
+        "ngraph/core/reference/src/runtime/reference/tile.cpp",
+        "ngraph/core/reference/src/runtime/reference/transpose.cpp"
+    ],
+
+    local_include_dirs: [
+        "ngraph/core/reference/include",
+        "ngraph/core/include"
+    ],
+
+    header_libs: [
+        "xbyak_headers"
+    ],
+
+    cflags: common_cflags + [
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-DIN_NGRAPH_LIBRARY",
+        "-DPROJECT_ROOT_DIR=\"ngraph\"",
+        "-DXBYAK64",
+        "-DXBYAK_NO_OP_NAMES"
+    ],
+
+    shared_libs: [
+        "liblog"
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_headers {
+    name: "libMKLDNNPlugin_headers",
+    vendor: true,
+    export_include_dirs: [
+        "inference-engine/src/plugin_api",
+        "inference-engine/src/preprocessing",
+        "inference-engine/include",
+        "openvino/itt/include",
+        "openvino/pp/include",
+        "inference-engine/src/mkldnn_plugin/mkldnn",
+        "inference-engine/src/mkldnn_plugin",
+        "openvino/conditional_compilation/include",
+        "inference-engine/src/transformations/include",
+        "ngraph/core/include",
+        "inference-engine/src/low_precision_transformations/include",
+        "inference-engine/src/mkldnn_plugin/nodes"
+    ],
+}
+
+//#################################################
+
+cc_library_shared {
+
+    name: "libMKLDNNPlugin",
+    proprietary: true,
+    owner: "intel",
+    compile_multilib: "64",
+
+    srcs: [
+        "inference-engine/src/mkldnn_plugin/config.cpp",
+        "inference-engine/src/mkldnn_plugin/emitters/jit_eltwise_emitters.cpp",
+        "inference-engine/src/mkldnn_plugin/emitters/jit_emitter.cpp",
+        "inference-engine/src/mkldnn_plugin/emitters/jit_load_store_emitters.cpp",
+        "inference-engine/src/mkldnn_plugin/emitters/jit_mkldnn_emitters.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn/ie_mkldnn.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn/iml_type_mapper.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_async_infer_request.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_descriptor.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_edge.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_exec_network.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_extension_mngr.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_extension_utils.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_graph.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_graph_dumper.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_graph_optimizer.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_memory.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_memory_solver.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_memory_state.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_node.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp",
+        "inference-engine/src/mkldnn_plugin/mkldnn_weights_cache.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/convert_broadcast_to_tiles.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/convert_matmul_to_fc_or_gemm.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/convert_tile_to_seq_tiles.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/convert_to_leaky_relu.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/convert_to_power_static.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/convert_to_swish_cpu.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/fc_bias_fusion.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/op/fully_connected.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/op/leaky_relu.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/op/power_static.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/op/swish_cpu.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/reshape_1d_ops.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/reshape_fc_fusion.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/reshape_fully_connected.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/reshape_prelu.cpp",
+        "inference-engine/src/mkldnn_plugin/ngraph_transformations/rnn_sequences_optimization.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/bucketize.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/common/cpu_convert.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/common/permute_kernel.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/common/softmax.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/common/tensor_desc_creator.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/ctc_greedy_decoder.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/ctc_greedy_decoder_seq_len.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/ctc_loss.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/cum_sum.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/detectionoutput.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/detectionoutput_onnx.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/extract_image_patches.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/gather_tree.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/grn.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/list.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/log_softmax.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_batch_to_space_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_bin_conv_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_broadcast_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_concat_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_conv_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_convert_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_deconv_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_def_conv_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_depth_to_space_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_dft_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_eltwise_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_embedding_bag_offset_sum_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_embedding_bag_packed_sum_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_embedding_bag_sum_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_embedding_segments_sum_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_fake_quantize_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_fullyconnected_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_gather_elements_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_gather_nd_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_gather_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_generic_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_input_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_interpolate_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_lrn_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_math_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_matmul_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_memory_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_mvn_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_normalize_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_one_hot_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_pad_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_pooling_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_psroi_pooling_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_reduce_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_reference_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_region_yolo_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_reorder_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_reshape_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_rnn.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_roi_align_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_roi_pooling_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_roll_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_scatter_update_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_select_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_shuffle_channels_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_softmax_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_space_to_batch_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_space_to_depth_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_split_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_strided_slice_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_tensoriterator_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_tile_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/mkldnn_transpose_node.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/non_max_suppression.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/priorgridgenerator_onnx.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/proposal.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/proposal_onnx.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/range.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/reorg_yolo.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/reverse_sequence.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/roifeatureextractor_onnx.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/topk.cpp",
+        "inference-engine/src/mkldnn_plugin/nodes/topkrois_onnx.cpp",
+        "inference-engine/src/mkldnn_plugin/normalize_preprocess.cpp",
+        "inference-engine/src/mkldnn_plugin/utils/blob_dump.cpp",
+        "inference-engine/src/mkldnn_plugin/utils/node_dumper.cpp",
+        "inference-engine/src/mkldnn_plugin/utils/rt_info/memory_formats_attribute.cpp",
+        "inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_disp.cpp"
+    ],
+
+    header_libs: [
+        "libMKLDNNPlugin_headers",
+        "libdnnl_headers",
+        "libtbb_headers"
+    ],
+
+    cflags: common_cflags + [
+        "-DENABLE_MKL_DNN=1",
+        "-DIE_BUILD_POSTFIX=\"\"",
+        "-DIE_THREAD=IE_THREAD_TBB",
+        "-DIMPLEMENT_INFERENCE_ENGINE_PLUGIN",
+        "-DMKLDNNPlugin_EXPORTS",
+        "-DCI_BUILD_NUMBER=\"custom_releases/2021/4_b76c90374501ac461d89a8fba44335741d066ccc\"",
+        "-DXARCH=ANY"
+        
+    ],
+
+    shared_libs: [
+        "liblog",
+        "libtbb",
+        "libtbbmalloc",
+        "libinference_engine",
+        "libinference_engine_lp_transformations",
+        "libinference_engine_transformations",
+        "libngraph"
+    ],
+
+    static_libs: [
+        "libitt",
+        "libdnnl"
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_headers {
+    name: "libpugixml_headers",
+    vendor: true,
+    export_include_dirs: ["inference-engine/thirdparty/pugixml/src"],
+}
+
+//#################################################
+
+cc_library_static {
+
+    name: "libpugixml",
+    proprietary: true,
+    compile_multilib: "64",
+
+    srcs: [
+        "inference-engine/thirdparty/pugixml/src/pugixml.cpp"
+    ],
+
+    cflags: common_cflags + [
+        "-DENABLE_MKL_DNN=1",
+        "-DIE_BUILD_POSTFIX=\"\""
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
+
+//#################################################
+
+cc_library_static {
+    name: "libitt",
+    proprietary: true,
+    compile_multilib: "64",
+
+    srcs: [
+        "openvino/itt/src/itt.cpp",
+    ],
+
+    local_include_dirs: [
+        "openvino/itt/include",
+        "openvino/pp/include"
+    ],
+
+    cflags: common_cflags + [
+        "-DIE_BUILD_POSTFIX=\"\""
+    ],
+
+    rtti: true,
+
+    strip: {
+        none: true,
+    }
+}
diff --git a/inference-engine/include/ie_blob.h b/inference-engine/include/ie_blob.h
index db7c29c95087..f083f7529f8b 100644
--- a/inference-engine/include/ie_blob.h
+++ b/inference-engine/include/ie_blob.h
@@ -65,12 +65,19 @@ public:
      * @tparam T Type to be checked. Must represent a class derived from the Blob
      * @return true if this object can be dynamically cast to the type T*. Otherwise, false
      */
+#ifdef __clang__
+    template <typename T,
+              typename std::enable_if<!std::is_pointer<T>::value && !std::is_reference<T>::value, int>::type = 0,
+              typename std::enable_if<std::is_base_of<Blob, T>::value, int>::type = 0>
+    bool is() noexcept;
+#else
     template <typename T,
               typename std::enable_if<!std::is_pointer<T>::value && !std::is_reference<T>::value, int>::type = 0,
               typename std::enable_if<std::is_base_of<Blob, T>::value, int>::type = 0>
     bool is() noexcept {
         return dynamic_cast<T*>(this) != nullptr;
     }
+#endif
 
     /**
      * @brief Checks if the Blob object can be cast to the type const T*
@@ -78,12 +85,19 @@ public:
      * @tparam T Type to be checked. Must represent a class derived from the Blob
      * @return true if this object can be dynamically cast to the type const T*. Otherwise, false
      */
+#ifdef __clang__
+    template <typename T,
+              typename std::enable_if<!std::is_pointer<T>::value && !std::is_reference<T>::value, int>::type = 0,
+              typename std::enable_if<std::is_base_of<Blob, T>::value, int>::type = 0>
+    bool is() const noexcept;
+#else
     template <typename T,
               typename std::enable_if<!std::is_pointer<T>::value && !std::is_reference<T>::value, int>::type = 0,
               typename std::enable_if<std::is_base_of<Blob, T>::value, int>::type = 0>
     bool is() const noexcept {
         return dynamic_cast<const T*>(this) != nullptr;
     }
+#endif
 
     /**
      * @brief Casts this Blob object to the type T*.
@@ -576,7 +590,13 @@ public:
     /**
      *@brief Virtual destructor.
      */
+#ifdef __clang__
     virtual ~TBlob();
+#else
+    virtual ~TBlob() {
+        free();
+    }
+#endif  // __clang__
 
     /**
      * @brief Gets the size of the given type.
diff --git a/inference-engine/include/ie_parameter.hpp b/inference-engine/include/ie_parameter.hpp
index 1343f89db329..299618c1055f 100644
--- a/inference-engine/include/ie_parameter.hpp
+++ b/inference-engine/include/ie_parameter.hpp
@@ -139,10 +139,14 @@ public:
      * @tparam T Type of value
      * @return true if type of value is correct
      */
+#ifdef __clang__
+    template <class T> bool is() const;
+#else
     template <class T>
     bool is() const {
         return empty() ? false : ptr->is(typeid(T));
     }
+#endif // __clang__
 
     /**
      * Dynamic cast to specified type
@@ -287,10 +291,13 @@ private:
     template <class T>
     struct RealData : Any, std::tuple<T> {
         using std::tuple<T>::tuple;
-
+#ifdef __clang__
+        bool is(const std::type_info& id) const override;
+#else
         bool is(const std::type_info& id) const override {
             return id == typeid(T);
         }
+#endif //__clang__
         Any* copy() const override {
             return new RealData {get()};
         }
@@ -314,10 +321,13 @@ private:
         equal(const Any& left, const Any& rhs) const {
             return dyn_cast<U>(&left) == dyn_cast<U>(&rhs);
         }
-
+#ifdef __clang__
+        bool operator==(const Any& rhs) const override;
+#else
         bool operator==(const Any& rhs) const override {
             return rhs.is(typeid(T)) && equal<T>(*this, rhs);
         }
+#endif //__clang__
     };
 
     template <typename T>
@@ -340,7 +350,7 @@ private:
   */
 using ParamMap = std::map<std::string, Parameter>;
 
-#ifdef __ANDROID__
+#ifdef __clang__
 extern template struct INFERENCE_ENGINE_API_CLASS(InferenceEngine::Parameter::RealData<InferenceEngine::Blob::Ptr>);
 extern template struct INFERENCE_ENGINE_API_CLASS(InferenceEngine::Parameter::RealData<int>);
 extern template struct INFERENCE_ENGINE_API_CLASS(InferenceEngine::Parameter::RealData<bool>);
diff --git a/inference-engine/include/ie_precision.hpp b/inference-engine/include/ie_precision.hpp
index e72a8248f008..6a9b9ce97498 100644
--- a/inference-engine/include/ie_precision.hpp
+++ b/inference-engine/include/ie_precision.hpp
@@ -90,7 +90,15 @@ public:
         }
         precisionInfo.value = CUSTOM;
     }
+#ifdef __clang__
+    /** @brief Creates custom precision with specific underlined type */
+    template <class T>
+    static INFERENCE_ENGINE_API_CPP(Precision) fromType(const char* typeName = nullptr);
 
+    /** @brief checks whether given storage class T can be used to store objects of current precision */
+    template <class T>
+    INFERENCE_ENGINE_API_CPP(bool) hasStorageType(const char* typeName = nullptr) const noexcept;
+#else
     /** @brief Creates custom precision with specific underlined type */
     template <class T>
     static Precision fromType(const char* typeName = nullptr) {
@@ -140,6 +148,7 @@ public:
             return false;
         }
     }
+#endif //__clang__
 
     /**
      * @brief Equality operator with Precision object
diff --git a/inference-engine/src/inference_engine/cnn_network_ngraph_impl.cpp b/inference-engine/src/inference_engine/cnn_network_ngraph_impl.cpp
index 1252c1b856af..b4aa5d095b6f 100644
--- a/inference-engine/src/inference_engine/cnn_network_ngraph_impl.cpp
+++ b/inference-engine/src/inference_engine/cnn_network_ngraph_impl.cpp
@@ -84,12 +84,12 @@ void CNNNetworkNGraphImpl::createDataForResult(const ::ngraph::Output<::ngraph::
 
 void CNNNetworkNGraphImpl::validateFunctionNames() const {
     // nGraph function parameters and pre-Results operations should have unique names
-    std::unordered_set<std::string> unique_names;
+    std::unordered_map<std::string, std::shared_ptr<ngraph::Node>> unique_names;
     for (const auto& param : _ngraph_function->get_parameters()) {
         if (unique_names.count(param->get_friendly_name())) {
             IE_THROW() << "Function contains several inputs with one friendly name!";
         }
-        unique_names.insert(param->get_friendly_name());
+        unique_names.insert({param->get_friendly_name(), param});
     }
     for (const auto& result : _ngraph_function->get_results()) {
         const auto& parent = result->get_input_node_shared_ptr(0);
@@ -97,10 +97,10 @@ void CNNNetworkNGraphImpl::validateFunctionNames() const {
         if (parent->get_output_size() > 1) {
             name += "." + std::to_string(result->get_input_source_output(0).get_index());
         }
-        if (unique_names.count(name) && !ngraph::op::is_parameter(parent)) {
+        if (unique_names.count(name) && !ngraph::op::is_parameter(parent) && parent != unique_names.at(name)) {
             IE_THROW() << "Function contains several inputs and outputs with one friendly name!";
         }
-        unique_names.insert(name);
+        unique_names.insert({name, parent});
     }
 }
 
diff --git a/inference-engine/src/inference_engine/ie_rtti.cpp b/inference-engine/src/inference_engine/ie_rtti.cpp
new file mode 100644
index 000000000000..e5bfa52b6cac
--- /dev/null
+++ b/inference-engine/src/inference_engine/ie_rtti.cpp
@@ -0,0 +1,193 @@
+// Copyright (C) 2018-2020 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include <string>
+#include <vector>
+#include <tuple>
+#include <memory>
+#include <map>
+
+#include <details/ie_exception.hpp>
+#include <ie_blob.h>
+#include <ie_parameter.hpp>
+#include <ie_iextension.h>
+#include <ie_extension.h>
+#include <ie_preprocess.hpp>
+#include <inference_engine.hpp>
+#include <ie_precision.hpp>
+#include <exec_graph_info.hpp>
+
+#include <ngraph/opsets/opset.hpp>
+#if defined(ENABLE_GNA)
+#include <gna-api-types-xnn.h>
+#endif
+
+using namespace InferenceEngine;
+
+#ifdef __clang__
+
+template <typename T,
+          typename std::enable_if<!std::is_pointer<T>::value && !std::is_reference<T>::value, int>::type,
+          typename std::enable_if<std::is_base_of<Blob, T>::value, int>::type>
+    bool Blob::is() noexcept {
+        return dynamic_cast<T*>(this) != nullptr;
+    }
+
+template <typename T,
+          typename std::enable_if<!std::is_pointer<T>::value && !std::is_reference<T>::value, int>::type,
+          typename std::enable_if<std::is_base_of<Blob, T>::value, int>::type>
+    bool Blob::is() const noexcept {
+        return dynamic_cast<const T*>(this) != nullptr;
+    }
+template bool InferenceEngine::Blob::is<InferenceEngine::MemoryBlob, 0, 0>();
+template bool InferenceEngine::Blob::is<InferenceEngine::CompoundBlob, 0, 0>();
+template bool InferenceEngine::Blob::is<InferenceEngine::RemoteBlob, 0, 0>();
+
+template <class T>
+inline bool Parameter::is() const {
+    return empty() ? false : ptr->is(typeid(T));
+}
+
+template <class T>
+inline bool Parameter::RealData<T>::is(const std::type_info& id) const {
+    return id == typeid(T);
+}
+
+template <class T>
+inline bool Parameter::RealData<T>::operator==(const Parameter::Any& rhs) const {
+    return rhs.is(typeid(T)) && equal<T>(*this, rhs);
+}
+
+
+template struct InferenceEngine::Parameter::RealData<int>;
+template struct InferenceEngine::Parameter::RealData<bool>;
+template struct InferenceEngine::Parameter::RealData<float>;
+template struct InferenceEngine::Parameter::RealData<double>;
+template struct InferenceEngine::Parameter::RealData<uint32_t>;
+template struct InferenceEngine::Parameter::RealData<std::string>;
+template struct InferenceEngine::Parameter::RealData<unsigned long>;
+template struct InferenceEngine::Parameter::RealData<std::vector<int>>;
+template struct InferenceEngine::Parameter::RealData<std::vector<std::string>>;
+template struct InferenceEngine::Parameter::RealData<std::vector<unsigned long>>;
+template struct InferenceEngine::Parameter::RealData<std::tuple<unsigned int, unsigned int>>;
+template struct InferenceEngine::Parameter::RealData<std::tuple<unsigned int, unsigned int, unsigned int>>;
+template struct InferenceEngine::Parameter::RealData<InferenceEngine::Blob::Ptr>;
+template bool InferenceEngine::Parameter::is<std::shared_ptr<InferenceEngine::Blob> >() const;
+template bool InferenceEngine::Parameter::is<std::shared_ptr<InferenceEngine::Blob const> >() const;
+template bool InferenceEngine::Parameter::is<std::basic_string<char, std::char_traits<char>, std::allocator<char> > >() const;
+template bool InferenceEngine::Parameter::is<int>() const;
+template bool InferenceEngine::Parameter::is<unsigned int>() const;
+template bool InferenceEngine::Parameter::is<float>() const;
+template bool InferenceEngine::Parameter::is<bool>() const;
+template bool InferenceEngine::Parameter::is<std::vector<std::basic_string<char, std::char_traits<char>, std::allocator<char> >,
+                             std::allocator<std::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >() const;
+template bool InferenceEngine::Parameter::is<std::vector<int, std::allocator<int> > >() const;
+template bool InferenceEngine::Parameter::is<std::vector<float, std::allocator<float> > >() const;
+template bool InferenceEngine::Parameter::is<std::vector<unsigned int, std::allocator<unsigned int> > >() const;
+template bool InferenceEngine::Parameter::is<std::tuple<unsigned int, unsigned int, unsigned int> >() const;
+template bool InferenceEngine::Parameter::is<std::tuple<unsigned int, unsigned int> >() const;
+template bool InferenceEngine::Parameter::is<InferenceEngine::PreProcessInfo>() const;
+template bool InferenceEngine::Parameter::is<unsigned long>() const;
+template bool InferenceEngine::Parameter::is<std::vector<unsigned long, std::allocator<unsigned long> > >() const;
+template bool InferenceEngine::Parameter::is<std::vector<bool, std::allocator<bool> > >() const;
+
+template bool InferenceEngine::Parameter::RealData<std::shared_ptr<InferenceEngine::Blob const> >::is(std::type_info const&) const;
+template bool InferenceEngine::Parameter::RealData<InferenceEngine::PreProcessInfo>::is(std::type_info const&)  const;
+
+template bool InferenceEngine::Parameter::RealData<std::vector<unsigned int, std::allocator<unsigned int> > >::is(std::type_info const&) const;
+template bool InferenceEngine::Parameter::RealData<std::vector<float, std::allocator<float> > >::is(std::type_info const&) const;
+template bool InferenceEngine::Parameter::RealData<std::__bit_reference<std::vector<bool, std::allocator<bool> >, true> >::is(std::type_info const&) const;
+
+template bool InferenceEngine::Parameter::RealData<std::vector<float, std::allocator<float> > >::operator==(InferenceEngine::Parameter::Any const&) const;
+template bool InferenceEngine::Parameter::RealData<std::vector<unsigned int, std::allocator<unsigned int> > >::operator==(InferenceEngine::Parameter::Any const&) const;
+template bool InferenceEngine::Parameter::RealData<std::shared_ptr<InferenceEngine::Blob const> >::operator==(InferenceEngine::Parameter::Any const&)  const;
+template bool InferenceEngine::Parameter::RealData<InferenceEngine::PreProcessInfo>::operator==(InferenceEngine::Parameter::Any const&)  const;
+template bool InferenceEngine::Parameter::RealData<std::__bit_reference<std::vector<bool, std::allocator<bool> >, true> >::operator==(InferenceEngine::Parameter::Any const&) const;
+#endif  // __clang__
+//
+// ie_blob.h
+//
+
+#ifdef __clang__
+template <typename T, typename U>
+TBlob<T, U>::~TBlob() {
+    free();
+}
+
+template class InferenceEngine::TBlob<float>;
+template class InferenceEngine::TBlob<double>;
+template class InferenceEngine::TBlob<int8_t>;
+template class InferenceEngine::TBlob<uint8_t>;
+template class InferenceEngine::TBlob<int16_t>;
+template class InferenceEngine::TBlob<uint16_t>;
+template class InferenceEngine::TBlob<int32_t>;
+template class InferenceEngine::TBlob<uint32_t>;
+template class InferenceEngine::TBlob<long>;
+template class InferenceEngine::TBlob<long long>;
+template class InferenceEngine::TBlob<unsigned long>;
+template class InferenceEngine::TBlob<unsigned long long>;
+#endif  // __clang__
+#ifdef __clang__
+template <class T>
+Precision Precision::fromType(const char* typeName) {
+    return Precision(8 * sizeof(T), typeName == nullptr ? typeid(T).name() : typeName);
+}
+
+/** @brief checks whether given storage class T can be used to store objects of current precision */
+template <class T>
+bool Precision::hasStorageType(const char* typeName) const noexcept {
+    try {
+        if (precisionInfo.value != BIN) {
+            if (sizeof(T) != size()) {
+                return false;
+            }
+        }
+#define CASE(x, y) \
+    case x:        \
+        return std::is_same<T, y>()
+#define CASE2(x, y1, y2) \
+    case x:              \
+        return std::is_same<T, y1>() || std::is_same<T, y2>()
+        switch (precisionInfo.value) {
+            CASE(FP32, float);
+            CASE2(FP16, int16_t, uint16_t);
+            CASE(I16, int16_t);
+            CASE(I32, int32_t);
+            CASE(I64, int64_t);
+            CASE(U16, uint16_t);
+            CASE(U8, uint8_t);
+            CASE(I8, int8_t);
+            CASE(BOOL, uint8_t);
+            CASE2(Q78, int16_t, uint16_t);
+            CASE2(BIN, int8_t, uint8_t);
+       default:
+           return areSameStrings(name(), typeName == nullptr ? typeid(T).name() : typeName);
+#undef CASE
+#undef CASE2
+            }
+    } catch (...) {
+        return false;
+    }
+}
+template Precision Precision::fromType<int>(const char* typeName);
+template Precision Precision::fromType<signed char>(const char* typeName);
+template Precision Precision::fromType<short>(const char* typeName);
+#if defined(ENABLE_GNA)
+template Precision Precision::fromType<_compound_bias_t>(const char* typeName);
+#endif // ENABLE_GNA
+template bool Precision::hasStorageType<float>(const char* typeName) const;
+template bool Precision::hasStorageType<int>(const char* typeName) const;
+template bool Precision::hasStorageType<unsigned char>(const char* typeName) const;
+template bool Precision::hasStorageType<short>(const char* typeName) const;
+template bool Precision::hasStorageType<unsigned short>(const char* typeName) const;
+template bool Precision::hasStorageType<signed char>(const char* typeName) const;
+template bool Precision::hasStorageType<long>(const char* typeName) const;
+template bool Precision::hasStorageType<unsigned long>(const char* typeName) const;
+template bool Precision::hasStorageType<unsigned int>(const char* typeName) const;
+template bool Precision::hasStorageType<double>(const char* typeName) const;
+#if defined(ENABLE_GNA)
+template bool Precision::hasStorageType<_compound_bias_t>(const char* typeName) const;
+#endif // ENABLE_GNA
+
+#endif // __clang__
diff --git a/inference-engine/src/mkldnn_plugin/cross-compiled/ANY/proposal_imp.cpp b/inference-engine/src/mkldnn_plugin/cross-compiled/ANY/proposal_imp.cpp
new file mode 100644
index 000000000000..49f38eb2719e
--- /dev/null
+++ b/inference-engine/src/mkldnn_plugin/cross-compiled/ANY/proposal_imp.cpp
@@ -0,0 +1,388 @@
+// Copyright (C) 2018-2021 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "proposal_imp.hpp"
+
+#include <cstring>
+#include <cmath>
+#include <string>
+#include <vector>
+#include <utility>
+#include <algorithm>
+#if defined(HAVE_AVX2)
+#include <immintrin.h>
+#endif
+#include "ie_parallel.hpp"
+
+namespace InferenceEngine {
+namespace Extensions {
+namespace Cpu {
+namespace XARCH {
+
+static
+void enumerate_proposals_cpu(const float* bottom4d, const float* d_anchor4d, const float* anchors,
+                             float* proposals, const int num_anchors, const int bottom_H,
+                             const int bottom_W, const float img_H, const float img_W,
+                             const float min_box_H, const float min_box_W, const int feat_stride,
+                             const float box_coordinate_scale, const float box_size_scale,
+                             float coordinates_offset, bool initial_clip, bool swap_xy, bool clip_before_nms) {
+    const int bottom_area = bottom_H * bottom_W;
+
+    const float* p_anchors_wm = anchors + 0 * num_anchors;
+    const float* p_anchors_hm = anchors + 1 * num_anchors;
+    const float* p_anchors_wp = anchors + 2 * num_anchors;
+    const float* p_anchors_hp = anchors + 3 * num_anchors;
+
+    parallel_for2d(bottom_H, bottom_W, [&](size_t h, size_t w) {
+        const float x = static_cast<float>((swap_xy ? h : w) * feat_stride);
+        const float y = static_cast<float>((swap_xy ? w : h) * feat_stride);
+
+        const float* p_box   = d_anchor4d + h * bottom_W + w;
+        const float* p_score = bottom4d   + h * bottom_W + w;
+
+        float* p_proposal = proposals + (h * bottom_W + w) * num_anchors * 5;
+
+        for (int anchor = 0; anchor < num_anchors; ++anchor) {
+            const float dx = p_box[(anchor * 4 + 0) * bottom_area] / box_coordinate_scale;
+            const float dy = p_box[(anchor * 4 + 1) * bottom_area] / box_coordinate_scale;
+
+            const float d_log_w = p_box[(anchor * 4 + 2) * bottom_area] / box_size_scale;
+            const float d_log_h = p_box[(anchor * 4 + 3) * bottom_area] / box_size_scale;
+
+            const float score = p_score[anchor * bottom_area];
+
+            float x0 = x + p_anchors_wm[anchor];
+            float y0 = y + p_anchors_hm[anchor];
+            float x1 = x + p_anchors_wp[anchor];
+            float y1 = y + p_anchors_hp[anchor];
+
+            if (initial_clip) {
+                // adjust new corner locations to be within the image region
+                x0 = std::max<float>(0.0f, std::min<float>(x0, img_W));
+                y0 = std::max<float>(0.0f, std::min<float>(y0, img_H));
+                x1 = std::max<float>(0.0f, std::min<float>(x1, img_W));
+                y1 = std::max<float>(0.0f, std::min<float>(y1, img_H));
+            }
+
+            // width & height of box
+            const float ww = x1 - x0 + coordinates_offset;
+            const float hh = y1 - y0 + coordinates_offset;
+            // center location of box
+            const float ctr_x = x0 + 0.5f * ww;
+            const float ctr_y = y0 + 0.5f * hh;
+
+            // new center location according to gradient (dx, dy)
+            const float pred_ctr_x = dx * ww + ctr_x;
+            const float pred_ctr_y = dy * hh + ctr_y;
+            // new width & height according to gradient d(log w), d(log h)
+            const float pred_w = std::exp(d_log_w) * ww;
+            const float pred_h = std::exp(d_log_h) * hh;
+
+            // update upper-left corner location
+            x0 = pred_ctr_x - 0.5f * pred_w;
+            y0 = pred_ctr_y - 0.5f * pred_h;
+            // update lower-right corner location
+            x1 = pred_ctr_x + 0.5f * pred_w;
+            y1 = pred_ctr_y + 0.5f * pred_h;
+
+            // adjust new corner locations to be within the image region,
+            if (clip_before_nms) {
+                x0 = std::max<float>(0.0f, std::min<float>(x0, img_W - coordinates_offset));
+                y0 = std::max<float>(0.0f, std::min<float>(y0, img_H - coordinates_offset));
+                x1 = std::max<float>(0.0f, std::min<float>(x1, img_W - coordinates_offset));
+                y1 = std::max<float>(0.0f, std::min<float>(y1, img_H - coordinates_offset));
+            }
+
+            // recompute new width & height
+            const float box_w = x1 - x0 + coordinates_offset;
+            const float box_h = y1 - y0 + coordinates_offset;
+
+            p_proposal[5*anchor + 0] = x0;
+            p_proposal[5*anchor + 1] = y0;
+            p_proposal[5*anchor + 2] = x1;
+            p_proposal[5*anchor + 3] = y1;
+            p_proposal[5*anchor + 4] = (min_box_W <= box_w) * (min_box_H <= box_h) * score;
+        }
+    });
+}
+
+static void unpack_boxes(const float* p_proposals, float* unpacked_boxes, int pre_nms_topn, bool store_prob) {
+    if (store_prob) {
+        parallel_for(pre_nms_topn, [&](size_t i) {
+            unpacked_boxes[0 * pre_nms_topn + i] = p_proposals[5 * i + 0];
+            unpacked_boxes[1 * pre_nms_topn + i] = p_proposals[5 * i + 1];
+            unpacked_boxes[2 * pre_nms_topn + i] = p_proposals[5 * i + 2];
+            unpacked_boxes[3 * pre_nms_topn + i] = p_proposals[5 * i + 3];
+            unpacked_boxes[4 * pre_nms_topn + i] = p_proposals[5 * i + 4];
+        });
+    } else {
+        parallel_for(pre_nms_topn, [&](size_t i) {
+            unpacked_boxes[0 * pre_nms_topn + i] = p_proposals[5 * i + 0];
+            unpacked_boxes[1 * pre_nms_topn + i] = p_proposals[5 * i + 1];
+            unpacked_boxes[2 * pre_nms_topn + i] = p_proposals[5 * i + 2];
+            unpacked_boxes[3 * pre_nms_topn + i] = p_proposals[5 * i + 3];
+        });
+    }
+}
+
+static void nms_cpu(const int num_boxes, int is_dead[],
+             const float* boxes, int index_out[], int* const num_out,
+             const int base_index, const float nms_thresh, const int max_num_out,
+             float coordinates_offset) {
+    const int num_proposals = num_boxes;
+    int count = 0;
+
+    const float* x0 = boxes + 0 * num_proposals;
+    const float* y0 = boxes + 1 * num_proposals;
+    const float* x1 = boxes + 2 * num_proposals;
+    const float* y1 = boxes + 3 * num_proposals;
+
+    std::memset(is_dead, 0, num_boxes * sizeof(int));
+
+#if defined(HAVE_AVX2)
+    __m256  vc_fone = _mm256_set1_ps(coordinates_offset);
+    __m256i vc_ione = _mm256_set1_epi32(1);
+    __m256  vc_zero = _mm256_set1_ps(0.0f);
+
+    __m256 vc_nms_thresh = _mm256_set1_ps(nms_thresh);
+#endif
+
+    for (int box = 0; box < num_boxes; ++box) {
+        if (is_dead[box])
+            continue;
+
+        index_out[count++] = base_index + box;
+        if (count == max_num_out)
+            break;
+
+        int tail = box + 1;
+
+#if defined(HAVE_AVX2)
+        __m256 vx0i = _mm256_set1_ps(x0[box]);
+        __m256 vy0i = _mm256_set1_ps(y0[box]);
+        __m256 vx1i = _mm256_set1_ps(x1[box]);
+        __m256 vy1i = _mm256_set1_ps(y1[box]);
+
+        __m256 vA_width  = _mm256_sub_ps(vx1i, vx0i);
+        __m256 vA_height = _mm256_sub_ps(vy1i, vy0i);
+        __m256 vA_area   = _mm256_mul_ps(_mm256_add_ps(vA_width, vc_fone), _mm256_add_ps(vA_height, vc_fone));
+
+        for (; tail <= num_boxes - 8; tail += 8) {
+            __m256i *pdst = reinterpret_cast<__m256i*>(is_dead + tail);
+            __m256i  vdst = _mm256_loadu_si256(pdst);
+
+            __m256 vx0j = _mm256_loadu_ps(x0 + tail);
+            __m256 vy0j = _mm256_loadu_ps(y0 + tail);
+            __m256 vx1j = _mm256_loadu_ps(x1 + tail);
+            __m256 vy1j = _mm256_loadu_ps(y1 + tail);
+
+            __m256 vx0 = _mm256_max_ps(vx0i, vx0j);
+            __m256 vy0 = _mm256_max_ps(vy0i, vy0j);
+            __m256 vx1 = _mm256_min_ps(vx1i, vx1j);
+            __m256 vy1 = _mm256_min_ps(vy1i, vy1j);
+
+            __m256 vwidth  = _mm256_add_ps(_mm256_sub_ps(vx1, vx0), vc_fone);
+            __m256 vheight = _mm256_add_ps(_mm256_sub_ps(vy1, vy0), vc_fone);
+            __m256 varea = _mm256_mul_ps(_mm256_max_ps(vc_zero, vwidth), _mm256_max_ps(vc_zero, vheight));
+
+            __m256 vB_width  = _mm256_sub_ps(vx1j, vx0j);
+            __m256 vB_height = _mm256_sub_ps(vy1j, vy0j);
+            __m256 vB_area   = _mm256_mul_ps(_mm256_add_ps(vB_width, vc_fone), _mm256_add_ps(vB_height, vc_fone));
+
+            __m256 vdivisor = _mm256_sub_ps(_mm256_add_ps(vA_area, vB_area), varea);
+            __m256 vintersection_area = _mm256_div_ps(varea, vdivisor);
+
+            __m256 vcmp_0 = _mm256_cmp_ps(vx0i, vx1j, _CMP_LE_OS);
+            __m256 vcmp_1 = _mm256_cmp_ps(vy0i, vy1j, _CMP_LE_OS);
+            __m256 vcmp_2 = _mm256_cmp_ps(vx0j, vx1i, _CMP_LE_OS);
+            __m256 vcmp_3 = _mm256_cmp_ps(vy0j, vy1i, _CMP_LE_OS);
+            __m256 vcmp_4 = _mm256_cmp_ps(vc_nms_thresh, vintersection_area, _CMP_LT_OS);
+
+            vcmp_0 = _mm256_and_ps(vcmp_0, vcmp_1);
+            vcmp_2 = _mm256_and_ps(vcmp_2, vcmp_3);
+            vcmp_4 = _mm256_and_ps(vcmp_4, vcmp_0);
+            vcmp_4 = _mm256_and_ps(vcmp_4, vcmp_2);
+
+            _mm256_storeu_si256(pdst, _mm256_blendv_epi8(vdst, vc_ione, _mm256_castps_si256(vcmp_4)));
+        }
+#endif
+
+        for (; tail < num_boxes; ++tail) {
+            float res = 0.0f;
+
+            const float x0i = x0[box];
+            const float y0i = y0[box];
+            const float x1i = x1[box];
+            const float y1i = y1[box];
+
+            const float x0j = x0[tail];
+            const float y0j = y0[tail];
+            const float x1j = x1[tail];
+            const float y1j = y1[tail];
+
+            if (x0i <= x1j && y0i <= y1j && x0j <= x1i && y0j <= y1i) {
+                // overlapped region (= box)
+                const float x0 = std::max<float>(x0i, x0j);
+                const float y0 = std::max<float>(y0i, y0j);
+                const float x1 = std::min<float>(x1i, x1j);
+                const float y1 = std::min<float>(y1i, y1j);
+
+                // intersection area
+                const float width  = std::max<float>(0.0f,  x1 - x0 + coordinates_offset);
+                const float height = std::max<float>(0.0f,  y1 - y0 + coordinates_offset);
+                const float area   = width * height;
+
+                // area of A, B
+                const float A_area = (x1i - x0i + coordinates_offset) * (y1i - y0i + coordinates_offset);
+                const float B_area = (x1j - x0j + coordinates_offset) * (y1j - y0j + coordinates_offset);
+
+                // IoU
+                res = area / (A_area + B_area - area);
+            }
+
+            if (nms_thresh < res)
+                is_dead[tail] = 1;
+        }
+    }
+
+    *num_out = count;
+}
+
+static void retrieve_rois_cpu(const int num_rois, const int item_index,
+                              const int num_proposals,
+                              const float* proposals, const int roi_indices[],
+                              float* rois, int post_nms_topn_,
+                              bool normalize, float img_h, float img_w, bool clip_after_nms, float* probs) {
+    const float *src_x0 = proposals + 0 * num_proposals;
+    const float *src_y0 = proposals + 1 * num_proposals;
+    const float *src_x1 = proposals + 2 * num_proposals;
+    const float *src_y1 = proposals + 3 * num_proposals;
+    const float *src_probs = proposals + 4 * num_proposals;
+
+    parallel_for(num_rois, [&](size_t roi) {
+        int index = roi_indices[roi];
+
+        float x0 = src_x0[index];
+        float y0 = src_y0[index];
+        float x1 = src_x1[index];
+        float y1 = src_y1[index];
+
+        if (clip_after_nms) {
+            x0 = std::max<float>(0.0f, std::min<float>(x0, img_w));
+            y0 = std::max<float>(0.0f, std::min<float>(y0, img_h));
+            x1 = std::max<float>(0.0f, std::min<float>(x1, img_w));
+            y1 = std::max<float>(0.0f, std::min<float>(y1, img_h));
+        }
+
+        if (normalize) {
+            x0 /= img_w;
+            y0 /= img_h;
+            x1 /= img_w;
+            y1 /= img_h;
+        }
+
+        rois[roi * 5 + 0] = static_cast<float>(item_index);
+        rois[roi * 5 + 1] = x0;
+        rois[roi * 5 + 2] = y0;
+        rois[roi * 5 + 3] = x1;
+        rois[roi * 5 + 4] = y1;
+
+        if (probs)
+            probs[roi] = src_probs[index];
+    });
+
+    if (num_rois < post_nms_topn_) {
+        for (int i = 5 * num_rois; i < 5 * post_nms_topn_; i++) {
+            rois[i] = 0.f;
+        }
+
+        // marker at end of boxes list
+        rois[num_rois * 5 + 0] = -1;
+    }
+}
+
+void proposal_exec(const float* input0, const float* input1,
+             std::vector<size_t> dims0, std::array<float, 4> img_info,
+             const float* anchors, int* roi_indices,
+             float* output0, float* output1, proposal_conf &conf) {
+    // Prepare memory
+    const float *p_bottom_item = input0;
+    const float *p_d_anchor_item = input1;
+
+    float *p_roi_item = output0;
+    float *p_prob_item = output1;
+    auto store_prob = p_prob_item != nullptr;
+
+    // bottom shape: (2 x num_anchors) x H x W
+    const int bottom_H = dims0[2];
+    const int bottom_W = dims0[3];
+
+    // input image height & width
+    const float img_H = img_info[conf.swap_xy ? 1 : 0];
+    const float img_W = img_info[conf.swap_xy ? 0 : 1];
+
+    // scale factor for height & width
+    const float scale_H = img_info[2];
+    const float scale_W = img_info[3];
+
+    // minimum box width & height
+    const float min_box_H = conf.min_size_ * scale_H;
+    const float min_box_W = conf.min_size_ * scale_W;
+
+    // number of all proposals = num_anchors * H * W
+    const int num_proposals = conf.anchors_shape_0 * bottom_H * bottom_W;
+
+    // number of top-n proposals before NMS
+    const int pre_nms_topn = std::min<int>(num_proposals, conf.pre_nms_topn_);
+
+    // number of final RoIs
+    int num_rois = 0;
+
+    // enumerate all proposals
+    //   num_proposals = num_anchors * H * W
+    //   (x1, y1, x2, y2, score) for each proposal
+    // NOTE: for bottom, only foreground scores are passed
+    struct ProposalBox {
+        float x0;
+        float y0;
+        float x1;
+        float y1;
+        float score;
+    };
+    std::vector<ProposalBox> proposals_(num_proposals);
+    const int unpacked_boxes_buffer_size = store_prob ? 5 * pre_nms_topn : 4 * pre_nms_topn;
+    std::vector<float> unpacked_boxes(unpacked_boxes_buffer_size);
+    std::vector<int> is_dead(pre_nms_topn);
+
+    // Execute
+    int nn = dims0[0];
+    for (int n = 0; n < nn; ++n) {
+        enumerate_proposals_cpu(p_bottom_item + num_proposals + n * num_proposals * 2,
+                                p_d_anchor_item + n * num_proposals * 4,
+                                anchors, reinterpret_cast<float *>(&proposals_[0]),
+                                conf.anchors_shape_0, bottom_H, bottom_W, img_H, img_W,
+                                min_box_H, min_box_W, conf.feat_stride_,
+                                conf.box_coordinate_scale_, conf.box_size_scale_,
+                                conf.coordinates_offset, conf.initial_clip, conf.swap_xy, conf.clip_before_nms);
+        std::partial_sort(proposals_.begin(), proposals_.begin() + pre_nms_topn, proposals_.end(),
+                          [](const ProposalBox &struct1, const ProposalBox &struct2) {
+                              return (struct1.score > struct2.score);
+                          });
+
+        unpack_boxes(reinterpret_cast<float *>(&proposals_[0]), &unpacked_boxes[0], pre_nms_topn, store_prob);
+        nms_cpu(pre_nms_topn, &is_dead[0], &unpacked_boxes[0], roi_indices, &num_rois, 0, conf.nms_thresh_,
+                conf.post_nms_topn_, conf.coordinates_offset);
+
+        float* p_probs = store_prob ? p_prob_item + n * conf.post_nms_topn_ : nullptr;
+        retrieve_rois_cpu(num_rois, n, pre_nms_topn, &unpacked_boxes[0], roi_indices,
+                          p_roi_item + n * conf.post_nms_topn_ * 5,
+                          conf.post_nms_topn_, conf.normalize_, img_H, img_W, conf.clip_after_nms, p_probs);
+    }
+}
+
+
+}  // namespace XARCH
+}  // namespace Cpu
+}  // namespace Extensions
+}  // namespace InferenceEngine
diff --git a/inference-engine/src/mkldnn_plugin/cross-compiled/AVX2/proposal_imp.cpp b/inference-engine/src/mkldnn_plugin/cross-compiled/AVX2/proposal_imp.cpp
new file mode 100644
index 000000000000..49f38eb2719e
--- /dev/null
+++ b/inference-engine/src/mkldnn_plugin/cross-compiled/AVX2/proposal_imp.cpp
@@ -0,0 +1,388 @@
+// Copyright (C) 2018-2021 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+#include "proposal_imp.hpp"
+
+#include <cstring>
+#include <cmath>
+#include <string>
+#include <vector>
+#include <utility>
+#include <algorithm>
+#if defined(HAVE_AVX2)
+#include <immintrin.h>
+#endif
+#include "ie_parallel.hpp"
+
+namespace InferenceEngine {
+namespace Extensions {
+namespace Cpu {
+namespace XARCH {
+
+static
+void enumerate_proposals_cpu(const float* bottom4d, const float* d_anchor4d, const float* anchors,
+                             float* proposals, const int num_anchors, const int bottom_H,
+                             const int bottom_W, const float img_H, const float img_W,
+                             const float min_box_H, const float min_box_W, const int feat_stride,
+                             const float box_coordinate_scale, const float box_size_scale,
+                             float coordinates_offset, bool initial_clip, bool swap_xy, bool clip_before_nms) {
+    const int bottom_area = bottom_H * bottom_W;
+
+    const float* p_anchors_wm = anchors + 0 * num_anchors;
+    const float* p_anchors_hm = anchors + 1 * num_anchors;
+    const float* p_anchors_wp = anchors + 2 * num_anchors;
+    const float* p_anchors_hp = anchors + 3 * num_anchors;
+
+    parallel_for2d(bottom_H, bottom_W, [&](size_t h, size_t w) {
+        const float x = static_cast<float>((swap_xy ? h : w) * feat_stride);
+        const float y = static_cast<float>((swap_xy ? w : h) * feat_stride);
+
+        const float* p_box   = d_anchor4d + h * bottom_W + w;
+        const float* p_score = bottom4d   + h * bottom_W + w;
+
+        float* p_proposal = proposals + (h * bottom_W + w) * num_anchors * 5;
+
+        for (int anchor = 0; anchor < num_anchors; ++anchor) {
+            const float dx = p_box[(anchor * 4 + 0) * bottom_area] / box_coordinate_scale;
+            const float dy = p_box[(anchor * 4 + 1) * bottom_area] / box_coordinate_scale;
+
+            const float d_log_w = p_box[(anchor * 4 + 2) * bottom_area] / box_size_scale;
+            const float d_log_h = p_box[(anchor * 4 + 3) * bottom_area] / box_size_scale;
+
+            const float score = p_score[anchor * bottom_area];
+
+            float x0 = x + p_anchors_wm[anchor];
+            float y0 = y + p_anchors_hm[anchor];
+            float x1 = x + p_anchors_wp[anchor];
+            float y1 = y + p_anchors_hp[anchor];
+
+            if (initial_clip) {
+                // adjust new corner locations to be within the image region
+                x0 = std::max<float>(0.0f, std::min<float>(x0, img_W));
+                y0 = std::max<float>(0.0f, std::min<float>(y0, img_H));
+                x1 = std::max<float>(0.0f, std::min<float>(x1, img_W));
+                y1 = std::max<float>(0.0f, std::min<float>(y1, img_H));
+            }
+
+            // width & height of box
+            const float ww = x1 - x0 + coordinates_offset;
+            const float hh = y1 - y0 + coordinates_offset;
+            // center location of box
+            const float ctr_x = x0 + 0.5f * ww;
+            const float ctr_y = y0 + 0.5f * hh;
+
+            // new center location according to gradient (dx, dy)
+            const float pred_ctr_x = dx * ww + ctr_x;
+            const float pred_ctr_y = dy * hh + ctr_y;
+            // new width & height according to gradient d(log w), d(log h)
+            const float pred_w = std::exp(d_log_w) * ww;
+            const float pred_h = std::exp(d_log_h) * hh;
+
+            // update upper-left corner location
+            x0 = pred_ctr_x - 0.5f * pred_w;
+            y0 = pred_ctr_y - 0.5f * pred_h;
+            // update lower-right corner location
+            x1 = pred_ctr_x + 0.5f * pred_w;
+            y1 = pred_ctr_y + 0.5f * pred_h;
+
+            // adjust new corner locations to be within the image region,
+            if (clip_before_nms) {
+                x0 = std::max<float>(0.0f, std::min<float>(x0, img_W - coordinates_offset));
+                y0 = std::max<float>(0.0f, std::min<float>(y0, img_H - coordinates_offset));
+                x1 = std::max<float>(0.0f, std::min<float>(x1, img_W - coordinates_offset));
+                y1 = std::max<float>(0.0f, std::min<float>(y1, img_H - coordinates_offset));
+            }
+
+            // recompute new width & height
+            const float box_w = x1 - x0 + coordinates_offset;
+            const float box_h = y1 - y0 + coordinates_offset;
+
+            p_proposal[5*anchor + 0] = x0;
+            p_proposal[5*anchor + 1] = y0;
+            p_proposal[5*anchor + 2] = x1;
+            p_proposal[5*anchor + 3] = y1;
+            p_proposal[5*anchor + 4] = (min_box_W <= box_w) * (min_box_H <= box_h) * score;
+        }
+    });
+}
+
+static void unpack_boxes(const float* p_proposals, float* unpacked_boxes, int pre_nms_topn, bool store_prob) {
+    if (store_prob) {
+        parallel_for(pre_nms_topn, [&](size_t i) {
+            unpacked_boxes[0 * pre_nms_topn + i] = p_proposals[5 * i + 0];
+            unpacked_boxes[1 * pre_nms_topn + i] = p_proposals[5 * i + 1];
+            unpacked_boxes[2 * pre_nms_topn + i] = p_proposals[5 * i + 2];
+            unpacked_boxes[3 * pre_nms_topn + i] = p_proposals[5 * i + 3];
+            unpacked_boxes[4 * pre_nms_topn + i] = p_proposals[5 * i + 4];
+        });
+    } else {
+        parallel_for(pre_nms_topn, [&](size_t i) {
+            unpacked_boxes[0 * pre_nms_topn + i] = p_proposals[5 * i + 0];
+            unpacked_boxes[1 * pre_nms_topn + i] = p_proposals[5 * i + 1];
+            unpacked_boxes[2 * pre_nms_topn + i] = p_proposals[5 * i + 2];
+            unpacked_boxes[3 * pre_nms_topn + i] = p_proposals[5 * i + 3];
+        });
+    }
+}
+
+static void nms_cpu(const int num_boxes, int is_dead[],
+             const float* boxes, int index_out[], int* const num_out,
+             const int base_index, const float nms_thresh, const int max_num_out,
+             float coordinates_offset) {
+    const int num_proposals = num_boxes;
+    int count = 0;
+
+    const float* x0 = boxes + 0 * num_proposals;
+    const float* y0 = boxes + 1 * num_proposals;
+    const float* x1 = boxes + 2 * num_proposals;
+    const float* y1 = boxes + 3 * num_proposals;
+
+    std::memset(is_dead, 0, num_boxes * sizeof(int));
+
+#if defined(HAVE_AVX2)
+    __m256  vc_fone = _mm256_set1_ps(coordinates_offset);
+    __m256i vc_ione = _mm256_set1_epi32(1);
+    __m256  vc_zero = _mm256_set1_ps(0.0f);
+
+    __m256 vc_nms_thresh = _mm256_set1_ps(nms_thresh);
+#endif
+
+    for (int box = 0; box < num_boxes; ++box) {
+        if (is_dead[box])
+            continue;
+
+        index_out[count++] = base_index + box;
+        if (count == max_num_out)
+            break;
+
+        int tail = box + 1;
+
+#if defined(HAVE_AVX2)
+        __m256 vx0i = _mm256_set1_ps(x0[box]);
+        __m256 vy0i = _mm256_set1_ps(y0[box]);
+        __m256 vx1i = _mm256_set1_ps(x1[box]);
+        __m256 vy1i = _mm256_set1_ps(y1[box]);
+
+        __m256 vA_width  = _mm256_sub_ps(vx1i, vx0i);
+        __m256 vA_height = _mm256_sub_ps(vy1i, vy0i);
+        __m256 vA_area   = _mm256_mul_ps(_mm256_add_ps(vA_width, vc_fone), _mm256_add_ps(vA_height, vc_fone));
+
+        for (; tail <= num_boxes - 8; tail += 8) {
+            __m256i *pdst = reinterpret_cast<__m256i*>(is_dead + tail);
+            __m256i  vdst = _mm256_loadu_si256(pdst);
+
+            __m256 vx0j = _mm256_loadu_ps(x0 + tail);
+            __m256 vy0j = _mm256_loadu_ps(y0 + tail);
+            __m256 vx1j = _mm256_loadu_ps(x1 + tail);
+            __m256 vy1j = _mm256_loadu_ps(y1 + tail);
+
+            __m256 vx0 = _mm256_max_ps(vx0i, vx0j);
+            __m256 vy0 = _mm256_max_ps(vy0i, vy0j);
+            __m256 vx1 = _mm256_min_ps(vx1i, vx1j);
+            __m256 vy1 = _mm256_min_ps(vy1i, vy1j);
+
+            __m256 vwidth  = _mm256_add_ps(_mm256_sub_ps(vx1, vx0), vc_fone);
+            __m256 vheight = _mm256_add_ps(_mm256_sub_ps(vy1, vy0), vc_fone);
+            __m256 varea = _mm256_mul_ps(_mm256_max_ps(vc_zero, vwidth), _mm256_max_ps(vc_zero, vheight));
+
+            __m256 vB_width  = _mm256_sub_ps(vx1j, vx0j);
+            __m256 vB_height = _mm256_sub_ps(vy1j, vy0j);
+            __m256 vB_area   = _mm256_mul_ps(_mm256_add_ps(vB_width, vc_fone), _mm256_add_ps(vB_height, vc_fone));
+
+            __m256 vdivisor = _mm256_sub_ps(_mm256_add_ps(vA_area, vB_area), varea);
+            __m256 vintersection_area = _mm256_div_ps(varea, vdivisor);
+
+            __m256 vcmp_0 = _mm256_cmp_ps(vx0i, vx1j, _CMP_LE_OS);
+            __m256 vcmp_1 = _mm256_cmp_ps(vy0i, vy1j, _CMP_LE_OS);
+            __m256 vcmp_2 = _mm256_cmp_ps(vx0j, vx1i, _CMP_LE_OS);
+            __m256 vcmp_3 = _mm256_cmp_ps(vy0j, vy1i, _CMP_LE_OS);
+            __m256 vcmp_4 = _mm256_cmp_ps(vc_nms_thresh, vintersection_area, _CMP_LT_OS);
+
+            vcmp_0 = _mm256_and_ps(vcmp_0, vcmp_1);
+            vcmp_2 = _mm256_and_ps(vcmp_2, vcmp_3);
+            vcmp_4 = _mm256_and_ps(vcmp_4, vcmp_0);
+            vcmp_4 = _mm256_and_ps(vcmp_4, vcmp_2);
+
+            _mm256_storeu_si256(pdst, _mm256_blendv_epi8(vdst, vc_ione, _mm256_castps_si256(vcmp_4)));
+        }
+#endif
+
+        for (; tail < num_boxes; ++tail) {
+            float res = 0.0f;
+
+            const float x0i = x0[box];
+            const float y0i = y0[box];
+            const float x1i = x1[box];
+            const float y1i = y1[box];
+
+            const float x0j = x0[tail];
+            const float y0j = y0[tail];
+            const float x1j = x1[tail];
+            const float y1j = y1[tail];
+
+            if (x0i <= x1j && y0i <= y1j && x0j <= x1i && y0j <= y1i) {
+                // overlapped region (= box)
+                const float x0 = std::max<float>(x0i, x0j);
+                const float y0 = std::max<float>(y0i, y0j);
+                const float x1 = std::min<float>(x1i, x1j);
+                const float y1 = std::min<float>(y1i, y1j);
+
+                // intersection area
+                const float width  = std::max<float>(0.0f,  x1 - x0 + coordinates_offset);
+                const float height = std::max<float>(0.0f,  y1 - y0 + coordinates_offset);
+                const float area   = width * height;
+
+                // area of A, B
+                const float A_area = (x1i - x0i + coordinates_offset) * (y1i - y0i + coordinates_offset);
+                const float B_area = (x1j - x0j + coordinates_offset) * (y1j - y0j + coordinates_offset);
+
+                // IoU
+                res = area / (A_area + B_area - area);
+            }
+
+            if (nms_thresh < res)
+                is_dead[tail] = 1;
+        }
+    }
+
+    *num_out = count;
+}
+
+static void retrieve_rois_cpu(const int num_rois, const int item_index,
+                              const int num_proposals,
+                              const float* proposals, const int roi_indices[],
+                              float* rois, int post_nms_topn_,
+                              bool normalize, float img_h, float img_w, bool clip_after_nms, float* probs) {
+    const float *src_x0 = proposals + 0 * num_proposals;
+    const float *src_y0 = proposals + 1 * num_proposals;
+    const float *src_x1 = proposals + 2 * num_proposals;
+    const float *src_y1 = proposals + 3 * num_proposals;
+    const float *src_probs = proposals + 4 * num_proposals;
+
+    parallel_for(num_rois, [&](size_t roi) {
+        int index = roi_indices[roi];
+
+        float x0 = src_x0[index];
+        float y0 = src_y0[index];
+        float x1 = src_x1[index];
+        float y1 = src_y1[index];
+
+        if (clip_after_nms) {
+            x0 = std::max<float>(0.0f, std::min<float>(x0, img_w));
+            y0 = std::max<float>(0.0f, std::min<float>(y0, img_h));
+            x1 = std::max<float>(0.0f, std::min<float>(x1, img_w));
+            y1 = std::max<float>(0.0f, std::min<float>(y1, img_h));
+        }
+
+        if (normalize) {
+            x0 /= img_w;
+            y0 /= img_h;
+            x1 /= img_w;
+            y1 /= img_h;
+        }
+
+        rois[roi * 5 + 0] = static_cast<float>(item_index);
+        rois[roi * 5 + 1] = x0;
+        rois[roi * 5 + 2] = y0;
+        rois[roi * 5 + 3] = x1;
+        rois[roi * 5 + 4] = y1;
+
+        if (probs)
+            probs[roi] = src_probs[index];
+    });
+
+    if (num_rois < post_nms_topn_) {
+        for (int i = 5 * num_rois; i < 5 * post_nms_topn_; i++) {
+            rois[i] = 0.f;
+        }
+
+        // marker at end of boxes list
+        rois[num_rois * 5 + 0] = -1;
+    }
+}
+
+void proposal_exec(const float* input0, const float* input1,
+             std::vector<size_t> dims0, std::array<float, 4> img_info,
+             const float* anchors, int* roi_indices,
+             float* output0, float* output1, proposal_conf &conf) {
+    // Prepare memory
+    const float *p_bottom_item = input0;
+    const float *p_d_anchor_item = input1;
+
+    float *p_roi_item = output0;
+    float *p_prob_item = output1;
+    auto store_prob = p_prob_item != nullptr;
+
+    // bottom shape: (2 x num_anchors) x H x W
+    const int bottom_H = dims0[2];
+    const int bottom_W = dims0[3];
+
+    // input image height & width
+    const float img_H = img_info[conf.swap_xy ? 1 : 0];
+    const float img_W = img_info[conf.swap_xy ? 0 : 1];
+
+    // scale factor for height & width
+    const float scale_H = img_info[2];
+    const float scale_W = img_info[3];
+
+    // minimum box width & height
+    const float min_box_H = conf.min_size_ * scale_H;
+    const float min_box_W = conf.min_size_ * scale_W;
+
+    // number of all proposals = num_anchors * H * W
+    const int num_proposals = conf.anchors_shape_0 * bottom_H * bottom_W;
+
+    // number of top-n proposals before NMS
+    const int pre_nms_topn = std::min<int>(num_proposals, conf.pre_nms_topn_);
+
+    // number of final RoIs
+    int num_rois = 0;
+
+    // enumerate all proposals
+    //   num_proposals = num_anchors * H * W
+    //   (x1, y1, x2, y2, score) for each proposal
+    // NOTE: for bottom, only foreground scores are passed
+    struct ProposalBox {
+        float x0;
+        float y0;
+        float x1;
+        float y1;
+        float score;
+    };
+    std::vector<ProposalBox> proposals_(num_proposals);
+    const int unpacked_boxes_buffer_size = store_prob ? 5 * pre_nms_topn : 4 * pre_nms_topn;
+    std::vector<float> unpacked_boxes(unpacked_boxes_buffer_size);
+    std::vector<int> is_dead(pre_nms_topn);
+
+    // Execute
+    int nn = dims0[0];
+    for (int n = 0; n < nn; ++n) {
+        enumerate_proposals_cpu(p_bottom_item + num_proposals + n * num_proposals * 2,
+                                p_d_anchor_item + n * num_proposals * 4,
+                                anchors, reinterpret_cast<float *>(&proposals_[0]),
+                                conf.anchors_shape_0, bottom_H, bottom_W, img_H, img_W,
+                                min_box_H, min_box_W, conf.feat_stride_,
+                                conf.box_coordinate_scale_, conf.box_size_scale_,
+                                conf.coordinates_offset, conf.initial_clip, conf.swap_xy, conf.clip_before_nms);
+        std::partial_sort(proposals_.begin(), proposals_.begin() + pre_nms_topn, proposals_.end(),
+                          [](const ProposalBox &struct1, const ProposalBox &struct2) {
+                              return (struct1.score > struct2.score);
+                          });
+
+        unpack_boxes(reinterpret_cast<float *>(&proposals_[0]), &unpacked_boxes[0], pre_nms_topn, store_prob);
+        nms_cpu(pre_nms_topn, &is_dead[0], &unpacked_boxes[0], roi_indices, &num_rois, 0, conf.nms_thresh_,
+                conf.post_nms_topn_, conf.coordinates_offset);
+
+        float* p_probs = store_prob ? p_prob_item + n * conf.post_nms_topn_ : nullptr;
+        retrieve_rois_cpu(num_rois, n, pre_nms_topn, &unpacked_boxes[0], roi_indices,
+                          p_roi_item + n * conf.post_nms_topn_ * 5,
+                          conf.post_nms_topn_, conf.normalize_, img_H, img_W, conf.clip_after_nms, p_probs);
+    }
+}
+
+
+}  // namespace XARCH
+}  // namespace Cpu
+}  // namespace Extensions
+}  // namespace InferenceEngine
diff --git a/inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_disp.cpp b/inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_disp.cpp
new file mode 100644
index 000000000000..cf10b492bf09
--- /dev/null
+++ b/inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_disp.cpp
@@ -0,0 +1,43 @@
+
+//
+// Auto generated file by CMake macros cross_compiled_file()
+// !! do not modify it !!!
+//
+#include "proposal_imp.hpp"
+#include "ie_system_conf.h"
+
+namespace InferenceEngine {
+namespace Extensions {
+namespace Cpu {
+namespace AVX2 {
+    void proposal_exec(const float* input0, const float* input1,
+        std::vector<size_t> dims0, std::array<float, 4> img_info,
+        const float* anchors, int* roi_indices,
+        float* output0, float* output1, proposal_conf &conf); 
+}
+namespace ANY {
+    void proposal_exec(const float* input0, const float* input1,
+        std::vector<size_t> dims0, std::array<float, 4> img_info,
+        const float* anchors, int* roi_indices,
+        float* output0, float* output1, proposal_conf &conf); 
+}
+namespace XARCH {
+
+void proposal_exec(const float* input0, const float* input1,
+        std::vector<size_t> dims0, std::array<float, 4> img_info,
+        const float* anchors, int* roi_indices,
+        float* output0, float* output1, proposal_conf &conf) {
+#ifndef __clang__
+    if (with_cpu_x86_avx2()) {
+        return AVX2::proposal_exec(input0, input1, dims0, img_info, anchors, roi_indices, output0, output1, conf);
+    }
+#endif
+    if (true) {
+        return ANY::proposal_exec(input0, input1, dims0, img_info, anchors, roi_indices, output0, output1, conf);
+    }
+}
+
+}
+}  // namespace InferenceEngine
+}  // namespace Extensions
+}  // namespace Cpu
diff --git a/inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_holder.txt b/inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_holder.txt
new file mode 100644
index 000000000000..1adf2153d03f
--- /dev/null
+++ b/inference-engine/src/mkldnn_plugin/cross-compiled/proposal_imp_holder.txt
@@ -0,0 +1,16 @@
+# Copyright (C) 2020 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+#
+
+# =================================================================
+#
+# This file is used to add dependency on option value. If the args
+# was changes the configure file will be updated. And the dependent
+# add_custom_command will rerun.
+#
+# Otherwise the changing of CMake options will not have affect on
+# generated file.
+#
+# =================================================================
+
+
diff --git a/ngraph/core/include/ngraph/version.hpp b/ngraph/core/include/ngraph/version.hpp
new file mode 100644
index 000000000000..90b58bb440e0
--- /dev/null
+++ b/ngraph/core/include/ngraph/version.hpp
@@ -0,0 +1,5 @@
+// Copyright (C) 2018-2021 Intel Corporation
+// SPDX-License-Identifier: Apache-2.0
+//
+
+const char* NGRAPH_VERSION_NUMBER = "0.0.0+b76c903";
diff --git a/ngraph/core/reference/src/runtime/reference/jit_generator.cpp b/ngraph/core/reference/src/runtime/reference/jit_generator.cpp
index 0f4d5a26f539..9bd60040ce91 100644
--- a/ngraph/core/reference/src/runtime/reference/jit_generator.cpp
+++ b/ngraph/core/reference/src/runtime/reference/jit_generator.cpp
@@ -5,7 +5,11 @@
 #include "jit_generator.hpp"
 #include "ngraph/type/float16.hpp"
 
+#ifdef __clang__
+#include <xbyak_util.h>
+#else
 #include <xbyak/xbyak_util.h>
+#endif
 
 namespace ngraph
 {
diff --git a/ngraph/core/reference/src/runtime/reference/jit_generator.hpp b/ngraph/core/reference/src/runtime/reference/jit_generator.hpp
index b855253ed044..855a953d5c97 100644
--- a/ngraph/core/reference/src/runtime/reference/jit_generator.hpp
+++ b/ngraph/core/reference/src/runtime/reference/jit_generator.hpp
@@ -5,7 +5,11 @@
 #pragma once
 
 #include <functional>
+#ifdef __clang__
+#include <xbyak.h>
+#else
 #include <xbyak/xbyak.h>
+#endif
 
 namespace ngraph
 {
-- 
2.33.0

