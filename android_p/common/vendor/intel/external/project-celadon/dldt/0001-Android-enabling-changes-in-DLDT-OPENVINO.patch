From 6e94de182230a9978a8178d54fb2c75affca517a Mon Sep 17 00:00:00 2001
From: Atul Vaish <atul.vaish@intel.com>
Date: Tue, 12 Feb 2019 21:43:07 +0530
Subject: [PATCH] Android enabling changes in DLDT/OPENVINO

Tracked-On: https://jira.devtools.intel.com/browse/OAM-76130
Signed-off-by: Atul Vaish <atul.vaish@intel.com>
---
 inference-engine/include/details/ie_so_pointer.hpp |  10 +-
 inference-engine/include/ie_blob.h                 |  16 ++-
 inference-engine/include/ie_common.h               |   5 +-
 inference-engine/include/ie_layers.h               | 139 ++++++++++++++++++++-
 inference-engine/include/ie_precision.hpp          |  13 +-
 inference-engine/include/inference_engine.hpp      |  29 ++++-
 inference-engine/install_dependencies.sh           |   0
 .../cpu_x86_sse42/blob_transform_sse42.cpp         |   4 +
 .../cpu_x86_sse42/ie_preprocess_data_sse42.cpp     |   6 +-
 inference-engine/src/inference_engine/ie_blob.cpp  |  54 ++++++++
 .../src/inference_engine/ie_device.cpp             |   2 +-
 .../src/inference_engine/ie_layers.cpp             | 137 ++++++++++++++++++++
 .../src/inference_engine/ie_preprocess_data.cpp    |   4 +-
 .../src/inference_engine/ie_util_internal.cpp      |   4 +
 inference-engine/src/inference_engine/ie_utils.cpp |   4 +
 .../src/inference_engine/ie_version.cpp            |   4 +-
 .../src/inference_engine/layer_transform.hpp       |   4 +
 .../inference_engine/shape_infer/ie_reshaper.cpp   |   1 -
 .../src/mkldnn_plugin/mkldnn_infer_request.cpp     |   7 +-
 .../src/mkldnn_plugin/mkldnn_plugin.cpp            |   3 +
 .../src/mkldnn_plugin/mkldnn_primitive.cpp         |   8 +-
 .../src/mkldnn_plugin/mkldnn_streams.cpp           |   2 +-
 .../thirdparty/mkl-dnn/src/common/engine.cpp       |   2 +-
 .../thirdparty/mkl-dnn/src/common/utils.cpp        |   3 +-
 .../thirdparty/mkl-dnn/src/cpu/cpu_concat.cpp      |   6 +-
 .../thirdparty/mkl-dnn/src/cpu/cpu_engine.cpp      |  94 +++++++-------
 .../thirdparty/mkl-dnn/src/cpu/cpu_reorder.cpp     |   6 +-
 .../thirdparty/mkl-dnn/src/cpu/cpu_sum.cpp         |   6 +-
 .../mkl-dnn/src/cpu/gemm_convolution.cpp           |   2 +-
 .../mkl-dnn/src/cpu/jit_avx2_1x1_convolution.hpp   |   2 +-
 .../mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp   |   2 +-
 31 files changed, 495 insertions(+), 84 deletions(-)
 mode change 100755 => 100644 inference-engine/install_dependencies.sh
 create mode 100644 inference-engine/src/inference_engine/ie_blob.cpp
 create mode 100644 inference-engine/src/inference_engine/ie_layers.cpp

diff --git a/inference-engine/include/details/ie_so_pointer.hpp b/inference-engine/include/details/ie_so_pointer.hpp
index a4973ff..96f6429 100644
--- a/inference-engine/include/details/ie_so_pointer.hpp
+++ b/inference-engine/include/details/ie_so_pointer.hpp
@@ -95,12 +95,20 @@ public:
     * @brief The copy-like constructor, can create So Pointer that dereferenced into child type if T is derived of U
     * @param that copied SOPointer object
     */
+#if defined(__ANDROID__)
+    //FIX ME by providing below implementation without using dynamic_pointer_cast
+    template<class U, class W>
+    SOPointer(const SOPointer<U, W> & that) :
+        _so_loader(std::static_pointer_cast<Loader>(that._so_loader)),
+        _pointedObj(std::static_pointer_cast<T>(that._pointedObj)) {
+    }
+#else
     template<class U, class W>
     SOPointer(const SOPointer<U, W> & that) :
         _so_loader(std::dynamic_pointer_cast<Loader>(that._so_loader)),
         _pointedObj(std::dynamic_pointer_cast<T>(that._pointedObj)) {
     }
-
+#endif
     /**
     * @brief Standard pointer operator
     * @return underlined interface with disabled Release method
diff --git a/inference-engine/include/ie_blob.h b/inference-engine/include/ie_blob.h
index 21267a3..bdc3d99 100644
--- a/inference-engine/include/ie_blob.h
+++ b/inference-engine/include/ie_blob.h
@@ -76,7 +76,11 @@ public:
     /**
      * @brief Blob virtual destructor
      */
-    virtual ~Blob()  = default;
+	#if defined(__ANDROID__)
+        virtual ~Blob();
+    #else
+		virtual ~Blob()  = default;
+	#endif
 
     /**
      * @brief Constructor. Creates an empty Blob object with the specified precision.
@@ -166,7 +170,14 @@ public:
     const SizeVector dims() const noexcept {
         return SizeVector(tensorDesc.getDims().rbegin(), tensorDesc.getDims().rend());
     }
-
+#if defined(__ANDROID__)
+    /**
+     * @brief Returns the tensor desctiption
+     */
+    TensorDesc &getTensorDesc() {   // otherwose, how would I do reshape() ??
+        return tensorDesc;
+    }
+#endif
     /**
      * @brief Returns the tensor description
      */
@@ -174,6 +185,7 @@ public:
         return tensorDesc;
     }
 
+
     /**
      * @brief Returns the total number of elements (a product of all the dims)
      */
diff --git a/inference-engine/include/ie_common.h b/inference-engine/include/ie_common.h
index e08c265..a9d5ac1 100644
--- a/inference-engine/include/ie_common.h
+++ b/inference-engine/include/ie_common.h
@@ -90,7 +90,10 @@ enum Layout : uint8_t {
 
     // Single image layout (for mean image)
     CHW = 128,
-
+ //for depth conv 2d
+#if defined(__ANDROID__)
+    IHWO = 160,
+#endif
     // 2D
     HW = 192,
     NC = 193,
diff --git a/inference-engine/include/ie_layers.h b/inference-engine/include/ie_layers.h
index 4582842..fb32e84 100644
--- a/inference-engine/include/ie_layers.h
+++ b/inference-engine/include/ie_layers.h
@@ -88,7 +88,11 @@ public:
     /**
      * @brief A virtual destructor
      */
+#if defined(__ANDROID__)
+    virtual ~CNNLayer();
+#else
     virtual ~CNNLayer() = default;
+#endif
 
     /**
      * @brief Sets a layer to be fused with
@@ -457,6 +461,10 @@ public:
      * @brief Constructs a WeightableLayer instance and initiates layer parameters with the given values
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~WeightableLayer();
+#endif
 };
 
 /**
@@ -483,7 +491,13 @@ public:
     /**
      * @brief A convolution paddings end array [X, Y, Z, ...]
      */
-    PropertyVector<unsigned int> _pads_end;
+
+  #if defined(__ANDROID__)
+    DEFINE_PROP(_pads_end);
+  #else
+    PropertyVector<unsigned int> _pads_end; //org
+  #endif
+
     /**
      * @brief A convolution strides array [X, Y, Z, ...]
      */
@@ -540,6 +554,11 @@ public:
      * @brief move constructor
      */
     ConvolutionLayer(ConvolutionLayer &&) = default;
+
+#if defined(__ANDROID__)
+    virtual ~ConvolutionLayer();
+#endif
+
 };
 
 /**
@@ -549,6 +568,11 @@ class DeconvolutionLayer : public ConvolutionLayer {
  public:
     using ConvolutionLayer::ConvolutionLayer;
     using ConvolutionLayer::operator=;
+
+#if defined(__ANDROID__)
+    virtual ~DeconvolutionLayer();
+#endif
+
 };
 
 /**
@@ -636,6 +660,11 @@ public:
      * @brief move constructor
      */
     PoolingLayer(PoolingLayer &&) = default;
+
+#if defined(__ANDROID__)
+    virtual ~PoolingLayer();
+#endif
+
 };
 
 #undef DEFINE_PROP
@@ -654,6 +683,11 @@ public:
     * @brief Creates a new FullyConnectedLayer instance and initializes layer parameters with the given values.
     */
     using WeightableLayer::WeightableLayer;
+
+#if defined(__ANDROID__)
+    virtual ~FullyConnectedLayer();
+#endif
+
 };
 
 /**
@@ -673,6 +707,10 @@ public:
     * In current implementation 1 means channels, 0 - batch
     */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~ConcatLayer();
+#endif
 };
 
 /**
@@ -689,6 +727,10 @@ public:
     * @brief Creates a new SplitLayer instance.
     */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~SplitLayer();
+#endif
 };
 
 /**
@@ -721,6 +763,10 @@ public:
      * @brief Creates a new NormLayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+     virtual ~NormLayer();
+#endif
 };
 
 /**
@@ -728,6 +774,10 @@ public:
  */
 class SoftMaxLayer : public CNNLayer {
 public:
+#if defined(__ANDROID__)
+    virtual ~SoftMaxLayer();
+#endif
+
     /**
      * @brief Axis number for a softmax operation
      */
@@ -754,6 +804,10 @@ public:
      * @brief Bias for squares sum
      */
     float bias = 0.f;
+
+#if defined(__ANDROID__)
+    virtual ~GRNLayer();
+#endif
 };
 
 /**
@@ -777,6 +831,10 @@ public:
     * @brief Indicate that the result needs to be normalized
     */
     int normalize = 1;
+
+#if defined(__ANDROID__)
+    virtual ~MVNLayer();
+#endif
 };
 
 /**
@@ -793,6 +851,10 @@ public:
      * @brief Creates a new ReLULayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~ReLULayer();
+#endif
 };
 
 /**
@@ -814,6 +876,10 @@ public:
      * @brief Creates a new ClampLayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~ClampLayer();
+#endif
 };
 
 /**
@@ -843,6 +909,10 @@ public:
     * @brief Creates a new EltwiseLayer instance.
     */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~EltwiseLayer();
+#endif
 };
 
 /**
@@ -867,6 +937,10 @@ public:
      * @brief Creates a new CropLayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~CropLayer();
+#endif
+
 };
 
 /**
@@ -891,6 +965,9 @@ public:
      * @brief Creates a new ReshapeLayer instance.
      */
     using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~ReshapeLayer();
+#endif
 };
 
 /**
@@ -911,6 +988,10 @@ public:
      * @brief Creates a new TileLayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~TileLayer();
+#endif
 };
 
 
@@ -928,6 +1009,9 @@ public:
      * @brief Creates a new ScaleShiftLayer instance.
      */
     using WeightableLayer::WeightableLayer;
+#if defined(__ANDROID__)
+    virtual ~ScaleShiftLayer();
+#endif
 };
 
 /**
@@ -979,6 +1063,10 @@ public:
     * @param prms Initial layer parameters
     */
     explicit PReLULayer(const LayerParams &prms) : WeightableLayer(prms), _channel_shared(false) {}
+
+#if defined(__ANDROID__)
+    virtual ~PReLULayer();
+#endif
 };
 
 /**
@@ -1004,6 +1092,10 @@ public:
      * @brief Creates a new PowerLayer instance.
      */
     using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~PowerLayer();
+#endif
 };
 
 /**
@@ -1020,8 +1112,53 @@ public:
      * @brief Creates a new BatchNormalizationLayer instance.
      */
     using WeightableLayer::WeightableLayer;
+
+#if defined(__ANDROID__)
+    virtual ~BatchNormalizationLayer();
+#endif
+};
+
+#if defined(__ANDROID__)
+class TanHLayer : public CNNLayer {
+public:
+    /**
+    * @brief A default constructor. Creates a new ReLULayer instance and initializes layer parameters with the given values.
+    * @param prms Initial layer parameters
+    */
+    //explicit TanHLayer(const LayerParams &prms) : CNNLayer(prms), negative_slope(0.0f) {}
+
+    using CNNLayer::CNNLayer;
+#if defined(__ANDROID__)
+    virtual ~TanHLayer();
+#endif
+
+    /**
+     * @brief Negative slope is used to takle negative inputs instead of setting them to 0
+     */
+    float negative_slope;
+};
+
+class SigmoidLayer : public CNNLayer {
+public:
+    /**
+    * @brief A default constructor. Creates a new ReLULayer instance and initializes layer parameters with the given values.
+    * @param prms Initial layer parameters
+    */
+    //explicit SigmoidLayer(const LayerParams &prms) : CNNLayer(prms), negative_slope(0.0f) {}
+
+    using CNNLayer::CNNLayer;
+
+#if defined(__ANDROID__)
+    virtual ~SigmoidLayer();
+#endif
+
+    /**
+     * @brief Negative slope is used to takle negative inputs instead of setting them to 0
+     */
+    float negative_slope;
 };
 
+#endif
 /**
  * @brief This class represents a general matrix multiplication operation layer
  * Formula is: dst := alpha*src1*src2 + beta*src3
diff --git a/inference-engine/include/ie_precision.hpp b/inference-engine/include/ie_precision.hpp
index d50fe5c..2c67310 100644
--- a/inference-engine/include/ie_precision.hpp
+++ b/inference-engine/include/ie_precision.hpp
@@ -72,13 +72,18 @@ public:
         }
         precisionInfo.value = CUSTOM;
     }
-
+#if defined(__ANDROID__)
+    template <class T>
+    static Precision fromType(const char * typeName) {
+        return Precision(8 * sizeof(T), typeName);
+    }
+#else
     /** @brief Creates custom precision with specific underlined type */
     template <class T>
     static Precision fromType(const char * typeName = nullptr) {
         return Precision(8 * sizeof(T), typeName == nullptr ? typeid(T).name() : typeName);
     }
-
+#endif
     /** @brief checks whether given storage class T can be used for store objects of current precision */
     template <class T>
     bool hasStorageType(const char * typeName = nullptr) const noexcept {
@@ -97,7 +102,11 @@ public:
             CASE(U8, uint8_t);
             CASE(I8, int8_t);
             CASE2(Q78, int16_t, uint16_t);
+#if defined(__ANDROID__)
+            default : return areSameStrings(name(), typeName);
+#else
             default : return areSameStrings(name(), typeName == nullptr ? typeid(T).name() : typeName);
+#endif
 #undef CASE
 #undef CASE2
         }
diff --git a/inference-engine/include/inference_engine.hpp b/inference-engine/include/inference_engine.hpp
index 352d943..80065bf 100644
--- a/inference-engine/include/inference_engine.hpp
+++ b/inference-engine/include/inference_engine.hpp
@@ -61,7 +61,15 @@ inline void TopResults(unsigned int n, TBlob<T> &input, std::vector<unsigned> &o
         }
     }
 }
-
+#if defined(__ANDROID__)
+#define TBLOB_TOP_RESULT(precision)\
+    case InferenceEngine::Precision::precision  : {\
+        using myBlobType = InferenceEngine::PrecisionTrait<Precision::precision>::value_type;\
+        TBlob<myBlobType> &tblob = static_cast<TBlob<myBlobType> &>(input);\
+        TopResults(n, tblob, output);\
+        break;\
+    }
+#else
 #define TBLOB_TOP_RESULT(precision)\
     case InferenceEngine::Precision::precision  : {\
         using myBlobType = InferenceEngine::PrecisionTrait<Precision::precision>::value_type;\
@@ -69,7 +77,7 @@ inline void TopResults(unsigned int n, TBlob<T> &input, std::vector<unsigned> &o
         TopResults(n, tblob, output);\
         break;\
     }
-
+#endif
 /**
  * @brief Gets the top n results from a blob
  * @param n Top n count
@@ -146,7 +154,17 @@ void copyFromRGB8(uint8_t *RGB8, size_t RGB8_size, InferenceEngine::TBlob<data_t
  * @param input Blob to contain the split image (to 3 channels)
  */
 inline void ConvertImageToInput(unsigned char *imgBufRGB8, size_t lengthbytesSize, Blob &input) {
-    TBlob<float> *float_input = dynamic_cast<TBlob<float> *>(&input);
+#if defined(__ANDROID__)
+    TBlob<float> *float_input = static_cast<TBlob<float> *>(&input);
+    if (float_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, float_input);
+
+    TBlob<short> *short_input = static_cast<TBlob<short> *>(&input);
+    if (short_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, short_input);
+
+    TBlob<uint8_t> *byte_input = static_cast<TBlob<uint8_t> *>(&input);
+    if (byte_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, byte_input);
+#else
+	TBlob<float> *float_input = dynamic_cast<TBlob<float> *>(&input);
     if (float_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, float_input);
 
     TBlob<short> *short_input = dynamic_cast<TBlob<short> *>(&input);
@@ -154,6 +172,7 @@ inline void ConvertImageToInput(unsigned char *imgBufRGB8, size_t lengthbytesSiz
 
     TBlob<uint8_t> *byte_input = dynamic_cast<TBlob<uint8_t> *>(&input);
     if (byte_input != nullptr) copyFromRGB8(imgBufRGB8, lengthbytesSize, byte_input);
+#endif
 }
 
 /**
@@ -168,7 +187,11 @@ void copyToFloat(float *dst, const InferenceEngine::Blob *src) {
     }
     const InferenceEngine::TBlob<T> *t_blob = dynamic_cast<const InferenceEngine::TBlob<T> *>(src);
     if (t_blob == nullptr) {
+	#if defined(__ANDROID__)
+	// input type mismatch with actual input
+	#else
         THROW_IE_EXCEPTION << "input type is " << src->precision() << " but input is not " << typeid(T).name();
+	#endif
     }
 
     const T *srcPtr = t_blob->readOnly();
diff --git a/inference-engine/install_dependencies.sh b/inference-engine/install_dependencies.sh
old mode 100755
new mode 100644
diff --git a/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp b/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp
index f8c16a4..88f5a6b 100644
--- a/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp
+++ b/inference-engine/src/inference_engine/cpu_x86_sse42/blob_transform_sse42.cpp
@@ -4,7 +4,11 @@
 
 #include "blob_transform_sse42.hpp"
 
+#if defined(__ANDROID__)
+#include <immintrin.h> // SSE 4.2
+#else
 #include <nmmintrin.h>  // SSE 4.2
+#endif
 
 namespace InferenceEngine {
 
diff --git a/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp b/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp
index 7d40157..9ef121a 100644
--- a/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp
+++ b/inference-engine/src/inference_engine/cpu_x86_sse42/ie_preprocess_data_sse42.cpp
@@ -4,9 +4,11 @@
 
 #include "ie_preprocess_data.hpp"
 #include "ie_preprocess_data_sse42.hpp"
-
+#if defined(__ANDROID__)
+#include <immintrin.h> // SSE 4.2
+#else
 #include <nmmintrin.h>  // SSE 4.2
-
+#endif
 #include <stdint.h>
 
 namespace InferenceEngine {
diff --git a/inference-engine/src/inference_engine/ie_blob.cpp b/inference-engine/src/inference_engine/ie_blob.cpp
new file mode 100644
index 0000000..ccf99b2
--- /dev/null
+++ b/inference-engine/src/inference_engine/ie_blob.cpp
@@ -0,0 +1,54 @@
+/*
+ * INTEL CONFIDENTIAL
+ * Copyright 2016 Intel Corporation.
+ *
+ * The source code contained or described herein and all documents
+ * related to the source code ("Material") are owned by Intel Corporation
+ * or its suppliers or licensors. Title to the Material remains with
+ * Intel Corporation or its suppliers and licensors. The Material may
+ * contain trade secrets and proprietary and confidential information
+ * of Intel Corporation and its suppliers and licensors, and is protected
+ * by worldwide copyright and trade secret laws and treaty provisions.
+ * No part of the Material may be used, copied, reproduced, modified,
+ * published, uploaded, posted, transmitted, distributed, or disclosed
+ * in any way without Intel's prior express written permission.
+ *
+ * No license under any patent, copyright, trade secret or other
+ * intellectual property right is granted to or conferred upon you by
+ * disclosure or delivery of the Materials, either expressly, by implication,
+ * inducement, estoppel or otherwise. Any license under such intellectual
+ * property rights must be express and approved by Intel in writing.
+ *
+ * Include any supplier copyright notices as supplier requires Intel to use.
+ *
+ * Include supplier trademarks or logos as supplier requires Intel to use,
+ * preceded by an asterisk. An asterisked footnote can be added as follows:
+ * *Third Party trademarks are the property of their respective owners.
+ *
+ * Unless otherwise agreed by Intel in writing, you may not remove or alter
+ * this notice or any other notice embedded in Materials by Intel or Intel's
+ * suppliers or licensors in any way.
+ */
+
+#include "ie_blob.h"
+
+
+using namespace std;
+using namespace InferenceEngine;
+
+#if defined(__ANDROID__)
+
+Blob::~Blob() {
+
+}
+
+/*
+//template <typename T>
+//template <class T>
+template<typename T, typename = std::enable_if<std::is_pod<T>::value>>
+TBlob<T>::~TBlob() {
+    free();
+}
+*/
+
+#endif
diff --git a/inference-engine/src/inference_engine/ie_device.cpp b/inference-engine/src/inference_engine/ie_device.cpp
index 3094414..20c2cd8 100644
--- a/inference-engine/src/inference_engine/ie_device.cpp
+++ b/inference-engine/src/inference_engine/ie_device.cpp
@@ -7,7 +7,7 @@
 #include <ie_device.hpp>
 #include <details/ie_exception.hpp>
 #include "description_buffer.hpp"
-
+#define IE_BUILD_POSTFIX ""
 using namespace InferenceEngine;
 
 FindPluginResponse InferenceEngine::findPlugin(const FindPluginRequest& req) {
diff --git a/inference-engine/src/inference_engine/ie_layers.cpp b/inference-engine/src/inference_engine/ie_layers.cpp
new file mode 100644
index 0000000..00972ce
--- /dev/null
+++ b/inference-engine/src/inference_engine/ie_layers.cpp
@@ -0,0 +1,137 @@
+/*
+ * INTEL CONFIDENTIAL
+ * Copyright 2016 Intel Corporation.
+ *
+ * The source code contained or described herein and all documents
+ * related to the source code ("Material") are owned by Intel Corporation
+ * or its suppliers or licensors. Title to the Material remains with
+ * Intel Corporation or its suppliers and licensors. The Material may
+ * contain trade secrets and proprietary and confidential information
+ * of Intel Corporation and its suppliers and licensors, and is protected
+ * by worldwide copyright and trade secret laws and treaty provisions.
+ * No part of the Material may be used, copied, reproduced, modified,
+ * published, uploaded, posted, transmitted, distributed, or disclosed
+ * in any way without Intel's prior express written permission.
+ *
+ * No license under any patent, copyright, trade secret or other
+ * intellectual property right is granted to or conferred upon you by
+ * disclosure or delivery of the Materials, either expressly, by implication,
+ * inducement, estoppel or otherwise. Any license under such intellectual
+ * property rights must be express and approved by Intel in writing.
+ *
+ * Include any supplier copyright notices as supplier requires Intel to use.
+ *
+ * Include supplier trademarks or logos as supplier requires Intel to use,
+ * preceded by an asterisk. An asterisked footnote can be added as follows:
+ * *Third Party trademarks are the property of their respective owners.
+ *
+ * Unless otherwise agreed by Intel in writing, you may not remove or alter
+ * this notice or any other notice embedded in Materials by Intel or Intel's
+ * suppliers or licensors in any way.
+ */
+
+#include "ie_layers.h"
+#include "ie_data.h"
+#include <memory>
+#include <string>
+#include <map>
+
+using namespace InferenceEngine;
+#if defined(__ANDROID__)
+CNNLayer::~CNNLayer() {
+
+}
+
+WeightableLayer::~WeightableLayer(){
+
+}
+
+ConvolutionLayer::~ConvolutionLayer(){
+
+}
+
+DeconvolutionLayer::~DeconvolutionLayer(){
+
+}
+
+PoolingLayer::~PoolingLayer(){
+
+}
+
+FullyConnectedLayer::~FullyConnectedLayer(){
+
+}
+
+ConcatLayer::~ConcatLayer(){
+
+}
+
+SplitLayer::~SplitLayer(){
+
+}
+
+NormLayer::~NormLayer(){
+
+}
+
+SoftMaxLayer::~SoftMaxLayer(){
+
+}
+
+ReLULayer::~ReLULayer(){
+
+}
+
+TanHLayer::~TanHLayer(){
+
+}
+
+SigmoidLayer::~SigmoidLayer(){
+
+}
+
+ClampLayer::~ClampLayer(){
+
+}
+
+EltwiseLayer::~EltwiseLayer(){
+
+}
+
+CropLayer::~CropLayer(){
+
+}
+
+ReshapeLayer::~ReshapeLayer(){
+
+}
+
+TileLayer::~TileLayer(){
+
+}
+
+ScaleShiftLayer::~ScaleShiftLayer(){
+
+}
+
+PowerLayer::~PowerLayer(){
+
+}
+
+BatchNormalizationLayer::~BatchNormalizationLayer(){
+
+}
+
+PReLULayer::~PReLULayer(){
+
+}
+
+GRNLayer::~GRNLayer(){
+
+}
+
+MVNLayer::~MVNLayer(){
+
+}
+
+#endif
diff --git a/inference-engine/src/inference_engine/ie_preprocess_data.cpp b/inference-engine/src/inference_engine/ie_preprocess_data.cpp
index 11c3f9e..8cc7d1d 100644
--- a/inference-engine/src/inference_engine/ie_preprocess_data.cpp
+++ b/inference-engine/src/inference_engine/ie_preprocess_data.cpp
@@ -761,14 +761,14 @@ void PreProcessData::execute(Blob::Ptr &outBlob, const ResizeAlgorithm &algorith
     if (_roiBlob == nullptr) {
         THROW_IE_EXCEPTION << "Input pre-processing is called without ROI blob set";
     }
-
+#if !defined (GAPI_STANDALONE) //OPENVINO change by Android team
     if (!_preproc) {
         _preproc.reset(new PreprocEngine);
     }
     if (_preproc->preprocessWithGAPI(_roiBlob, outBlob, algorithm, serial)) {
         return;
     }
-
+#endif
     Blob::Ptr res_in, res_out;
     if (_roiBlob->getTensorDesc().getLayout() == NHWC) {
         if (!_tmp1 || _tmp1->size() != _roiBlob->size()) {
diff --git a/inference-engine/src/inference_engine/ie_util_internal.cpp b/inference-engine/src/inference_engine/ie_util_internal.cpp
index 44be1b5..eb7796c 100644
--- a/inference-engine/src/inference_engine/ie_util_internal.cpp
+++ b/inference-engine/src/inference_engine/ie_util_internal.cpp
@@ -150,6 +150,10 @@ CNNLayerPtr clonelayer(const CNNLayer& source) {
         &layerCloneImpl<DeconvolutionLayer     >,
         &layerCloneImpl<ConvolutionLayer       >,
         &layerCloneImpl<WeightableLayer        >,
+#if defined(__ANDROID__)
+        &layerCloneImpl<TanHLayer              >,
+        &layerCloneImpl<SigmoidLayer           >,
+#endif
         &layerCloneImpl<CNNLayer               >
     };
     for (auto cloner : cloners) {
diff --git a/inference-engine/src/inference_engine/ie_utils.cpp b/inference-engine/src/inference_engine/ie_utils.cpp
index aa8e009..f993145 100644
--- a/inference-engine/src/inference_engine/ie_utils.cpp
+++ b/inference-engine/src/inference_engine/ie_utils.cpp
@@ -104,6 +104,10 @@ InferenceEngine::LayerComplexity getComplexity(const InferenceEngine::CNNLayerPt
 
         // roughly count exp as 1 flop
         {"SoftMax", std::bind(eltwise_complexity, _1, 4, 0)},
+#if defined(__ANDROID__)
+        {"TanH", std::bind(eltwise_complexity, _1, 1, 0)},
+        {"Sigmoid", std::bind(eltwise_complexity, _1, 1, 0)},
+#endif
     };
 
     if (layerComplexityLookup.count(type) > 0) {
diff --git a/inference-engine/src/inference_engine/ie_version.cpp b/inference-engine/src/inference_engine/ie_version.cpp
index cca54cc..a35f356 100644
--- a/inference-engine/src/inference_engine/ie_version.cpp
+++ b/inference-engine/src/inference_engine/ie_version.cpp
@@ -5,7 +5,9 @@
 #include "ie_version.hpp"
 
 namespace InferenceEngine {
-
+#if defined(__ANDROID__)
+#define CI_BUILD_NUMBER "custom-master-android-nn"
+#endif
 INFERENCE_ENGINE_API(const Version*) GetInferenceEngineVersion() noexcept {
     // Use local static variable to make sure it is always properly initialized
     // even if called from global constructor
diff --git a/inference-engine/src/inference_engine/layer_transform.hpp b/inference-engine/src/inference_engine/layer_transform.hpp
index fd51793..d0f78d7 100644
--- a/inference-engine/src/inference_engine/layer_transform.hpp
+++ b/inference-engine/src/inference_engine/layer_transform.hpp
@@ -51,6 +51,10 @@ using AllLayers = std::tuple <
     LSTMCell*,
     RNNLayer*,
     WeightableLayer*,
+#if defined(__ANDROID__)
+    TanHLayer*,
+    SigmoidLayer*,
+#endif
     CNNLayer*
 >;
 
diff --git a/inference-engine/src/inference_engine/shape_infer/ie_reshaper.cpp b/inference-engine/src/inference_engine/shape_infer/ie_reshaper.cpp
index 89dd72e..ff583d5 100644
--- a/inference-engine/src/inference_engine/shape_infer/ie_reshaper.cpp
+++ b/inference-engine/src/inference_engine/shape_infer/ie_reshaper.cpp
@@ -272,4 +272,3 @@ LauncherCreator::createInputLauncher(const CNNLayer* layer, const std::vector<IS
     THROW_IE_EXCEPTION << "Failed to reshape: Layer with type `" << layerType
                        << "` can't be input. Supported input types: Input, Const and Memory(with index=1)";
 }
-
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp
index 95e8039..43a212c 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_infer_request.cpp
@@ -18,8 +18,11 @@ MKLDNNPlugin::MKLDNNInferRequest::MKLDNNInferRequest(InferenceEngine::InputsData
 
 
 template <typename T> void MKLDNNPlugin::MKLDNNInferRequest::pushInput(const std::string& inputName, InferenceEngine::Blob::Ptr& inputBlob) {
-    InferenceEngine::TBlob<T> *in_f = dynamic_cast<InferenceEngine::TBlob<T> *>(inputBlob.get());
-
+#if defined(__ANDROID__)
+    InferenceEngine::TBlob<T> *in_f = static_cast<InferenceEngine::TBlob<T> *>(inputBlob.get());
+#else
+	InferenceEngine::TBlob<T> *in_f = dynamic_cast<InferenceEngine::TBlob<T> *>(inputBlob.get());
+#endif
     if (in_f == nullptr) {
         THROW_IE_EXCEPTION << "Input data precision not supported. Expected float.";
     }
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp
index 35a965a..db447c4 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_plugin.cpp
@@ -7,6 +7,9 @@
 #include <cpp_interfaces/base/ie_plugin_base.hpp>
 #include <memory>
 
+#if defined(__ANDROID__)
+#define CI_BUILD_NUMBER "custom-master-android-nn"
+#endif
 using namespace MKLDNNPlugin;
 using namespace InferenceEngine;
 
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
index f9e59f2..4474b0a 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_primitive.cpp
@@ -32,7 +32,11 @@ MKLDNNPrimitive &MKLDNNPrimitive::operator=(const std::shared_ptr<mkldnn::primit
 void MKLDNNPrimitive::setBatchLimit(int batch, size_t inputNum, size_t outputNum) {
     bool success = true;
     auto * primDesc = prim->get_primitive_desc();
-    auto * concatPrimDesc = dynamic_cast<const mkldnn::impl::cpu::cpu_concat_pd_t *>(primDesc);
+#ifdef __ANDROID__
+auto * concatPrimDesc = static_cast<const mkldnn::impl::cpu::cpu_concat_pd_t *>(primDesc);
+#else
+auto * concatPrimDesc = dynamic_cast<const mkldnn::impl::cpu::cpu_concat_pd_t *>(primDesc);
+#endif
     for (int i = 0; success && i < primDesc->n_inputs() && i < inputNum; i++) {
         // Depthwise layers contains weights as input
         if (primDesc->input_pd()->desc()->ndims != primDesc->input_pd(i)->desc()->ndims)
@@ -81,4 +85,4 @@ void MKLDNNPrimitive::setBatchLimit(int batch, size_t inputNum, size_t outputNum
     }
 
     THROW_IE_EXCEPTION << "Dynamic batch cannot be changed!";
-}
\ No newline at end of file
+}
diff --git a/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp b/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp
index a519837..481f9cc 100644
--- a/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp
+++ b/inference-engine/src/mkldnn_plugin/mkldnn_streams.cpp
@@ -31,7 +31,7 @@ bool check_env_variables() {
 #endif
 }
 
-#if !(defined(__APPLE__) || defined(_WIN32))
+#if !(defined(__APPLE__) || defined(_WIN32) || (__ANDROID__))
 /* Get the cores affinity mask for the current process */
 bool get_process_mask(int& ncpus, cpu_set_t*& mask) {
     for (ncpus = sizeof(cpu_set_t) / CHAR_BIT; ncpus < 1024 /* reasonable limit of #cores*/; ncpus <<= 1) {
diff --git a/inference-engine/thirdparty/mkl-dnn/src/common/engine.cpp b/inference-engine/thirdparty/mkl-dnn/src/common/engine.cpp
index 967bfc3..cafd4e8 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/common/engine.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/common/engine.cpp
@@ -19,7 +19,7 @@
 #include "nstl.hpp"
 
 #include "c_types_map.hpp"
-#include "../cpu/cpu_engine.hpp"
+#include "cpu_engine.hpp"
 
 namespace mkldnn {
 namespace impl {
diff --git a/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp b/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp
index 055681f..75d588d 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/common/utils.cpp
@@ -19,7 +19,7 @@
 #define XBYAK64
 #define XBYAK_NO_OP_NAMES
 
-#include <cpu/xbyak/xbyak_util.h>
+#include "xbyak_util.h"
 
 #ifdef _WIN32
 #include <malloc.h>
@@ -158,4 +158,3 @@ unsigned int get_cache_size(int level, bool per_core) {
 unsigned int mkldnn_get_cache_size(int level, int per_core) {
     return mkldnn::impl::get_cache_size(level, per_core != 0);
 }
-
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_concat.cpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_concat.cpp
index eb9bc81..bf12367 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_concat.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_concat.cpp
@@ -20,9 +20,9 @@
 #include "cpu_memory.hpp"
 #include "type_helpers.hpp"
 
-#include "cpu/cpu_concat.hpp"
-#include "cpu/ref_concat.hpp"
-#include "cpu/simple_concat.hpp"
+#include "cpu_concat.hpp"
+#include "ref_concat.hpp"
+#include "simple_concat.hpp"
 
 namespace mkldnn {
 namespace impl {
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_engine.cpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_engine.cpp
index 104ce88..2abaad3 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_engine.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_engine.cpp
@@ -24,54 +24,54 @@
 #include "cpu_concat.hpp"
 #include "cpu_sum.hpp"
 
-#include "cpu/ref_rnn.hpp"
+#include "ref_rnn.hpp"
 
-#include "cpu/jit_avx512_core_x8s8s32x_1x1_convolution.hpp"
-#include "cpu/jit_avx512_common_1x1_convolution.hpp"
-#include "cpu/jit_avx512_core_fp32_wino_conv_4x3.hpp"
-#include "cpu/jit_avx512_common_convolution_winograd.hpp"
-#include "cpu/jit_avx512_core_x8s8s32x_convolution.hpp"
-#include "cpu/jit_avx512_common_convolution.hpp"
-#include "cpu/jit_avx2_1x1_convolution.hpp"
-#include "cpu/jit_sse42_1x1_convolution.hpp"
-#include "cpu/jit_avx2_convolution.hpp"
-#include "cpu/jit_sse42_convolution.hpp"
-#include "cpu/gemm_convolution.hpp"
-#include "cpu/gemm_x8s8s32x_convolution.hpp"
-#include "cpu/ref_convolution.hpp"
-#include "cpu/jit_avx512_core_u8s8s32x_deconvolution.hpp"
-#include "cpu/ref_deconvolution.hpp"
-#include "cpu/ref_shuffle.hpp"
-#include "cpu/jit_uni_eltwise.hpp"
-#include "cpu/ref_eltwise.hpp"
-#include "cpu/ref_softmax.hpp"
-#include "cpu/jit_uni_pooling.hpp"
-#include "cpu/jit_avx512_core_i8i8_pooling.hpp"
-#include "cpu/ref_pooling.hpp"
-#include "cpu/nchw_pooling.hpp"
-#include "cpu/nhwc_pooling.hpp"
-#include "cpu/jit_avx512_common_lrn.hpp"
-#include "cpu/jit_uni_lrn.hpp"
-#include "cpu/ref_lrn.hpp"
-#include "cpu/jit_uni_batch_normalization.hpp"
-#include "cpu/ref_batch_normalization.hpp"
-#include "cpu/ncsp_batch_normalization.hpp"
-#include "cpu/nspc_batch_normalization.hpp"
-#include "cpu/ref_inner_product.hpp"
-#include "cpu/gemm_inner_product.hpp"
-#include "cpu/gemm_u8s8s32x_inner_product.hpp"
-#include "cpu/jit_uni_dw_convolution.hpp"
-#include "cpu/jit_avx512_core_u8s8s32x_wino_convolution.hpp"
-#include "cpu/jit_avx512_core_fp32_wino_conv_2x3.hpp"
-#include "cpu/jit_uni_roi_pooling.hpp"
-#include "cpu/jit_uni_softmax.hpp"
-#include "cpu/ref_roi_pooling.hpp"
-#include "cpu/jit_uni_depthwise.hpp"
-#include "cpu/ref_depthwise.hpp"
-#include "cpu/jit_uni_x8s8s32x_convolution.hpp"
-#include "cpu/jit_uni_x8s8s32x_1x1_convolution.hpp"
-#include "cpu/jit_uni_x8s8s32x_dw_convolution.hpp"
-#include "cpu/jit_uni_i8i8_pooling.hpp"
+#include "jit_avx512_core_x8s8s32x_1x1_convolution.hpp"
+#include "jit_avx512_common_1x1_convolution.hpp"
+#include "jit_avx512_core_fp32_wino_conv_4x3.hpp"
+#include "jit_avx512_common_convolution_winograd.hpp"
+#include "jit_avx512_core_x8s8s32x_convolution.hpp"
+#include "jit_avx512_common_convolution.hpp"
+#include "jit_avx2_1x1_convolution.hpp"
+#include "jit_sse42_1x1_convolution.hpp"
+#include "jit_avx2_convolution.hpp"
+#include "jit_sse42_convolution.hpp"
+#include "gemm_convolution.hpp"
+#include "gemm_x8s8s32x_convolution.hpp"
+#include "ref_convolution.hpp"
+#include "jit_avx512_core_u8s8s32x_deconvolution.hpp"
+#include "ref_deconvolution.hpp"
+#include "ref_shuffle.hpp"
+#include "jit_uni_eltwise.hpp"
+#include "ref_eltwise.hpp"
+#include "ref_softmax.hpp"
+#include "jit_uni_pooling.hpp"
+#include "jit_avx512_core_i8i8_pooling.hpp"
+#include "ref_pooling.hpp"
+#include "nchw_pooling.hpp"
+#include "nhwc_pooling.hpp"
+#include "jit_avx512_common_lrn.hpp"
+#include "jit_uni_lrn.hpp"
+#include "ref_lrn.hpp"
+#include "jit_uni_batch_normalization.hpp"
+#include "ref_batch_normalization.hpp"
+#include "ncsp_batch_normalization.hpp"
+#include "nspc_batch_normalization.hpp"
+#include "ref_inner_product.hpp"
+#include "gemm_inner_product.hpp"
+#include "gemm_u8s8s32x_inner_product.hpp"
+#include "jit_uni_dw_convolution.hpp"
+#include "jit_avx512_core_u8s8s32x_wino_convolution.hpp"
+#include "jit_avx512_core_fp32_wino_conv_2x3.hpp"
+#include "jit_uni_roi_pooling.hpp"
+#include "jit_uni_softmax.hpp"
+#include "ref_roi_pooling.hpp"
+#include "jit_uni_depthwise.hpp"
+#include "ref_depthwise.hpp"
+#include "jit_uni_x8s8s32x_convolution.hpp"
+#include "jit_uni_x8s8s32x_1x1_convolution.hpp"
+#include "jit_uni_x8s8s32x_dw_convolution.hpp"
+#include "jit_uni_i8i8_pooling.hpp"
 
 namespace mkldnn {
 namespace impl {
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_reorder.cpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_reorder.cpp
index eee668b..d0b1252 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_reorder.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_reorder.cpp
@@ -20,9 +20,9 @@
 #include "cpu_memory.hpp"
 #include "type_helpers.hpp"
 
-#include "cpu/jit_uni_reorder.hpp"
-#include "cpu/simple_reorder.hpp"
-#include "cpu/wino_reorder.hpp"
+#include "jit_uni_reorder.hpp"
+#include "simple_reorder.hpp"
+#include "wino_reorder.hpp"
 
 namespace mkldnn {
 namespace impl {
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_sum.cpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_sum.cpp
index f443ddc..2ee7c50 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_sum.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/cpu_sum.cpp
@@ -18,9 +18,9 @@
 #include "cpu_memory.hpp"
 #include "type_helpers.hpp"
 
-#include "cpu/cpu_sum.hpp"
-#include "cpu/ref_sum.hpp"
-#include "cpu/simple_sum.hpp"
+#include "cpu_sum.hpp"
+#include "ref_sum.hpp"
+#include "simple_sum.hpp"
 
 namespace mkldnn {
 namespace impl {
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/gemm_convolution.cpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/gemm_convolution.cpp
index c403e45..8d6629d 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/gemm_convolution.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/gemm_convolution.cpp
@@ -14,7 +14,7 @@
 * limitations under the License.
 *******************************************************************************/
 
-#include <common/primitive_attr.hpp>
+#include "primitive_attr.hpp"
 #include "mkldnn_types.h"
 
 #include "c_types_map.hpp"
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_1x1_convolution.hpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_1x1_convolution.hpp
index 7846252..1473fa1 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_1x1_convolution.hpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_1x1_convolution.hpp
@@ -17,7 +17,7 @@
 #ifndef CPU_JIT_AVX2_1x1_CONVOLUTION_HPP
 #define CPU_JIT_AVX2_1x1_CONVOLUTION_HPP
 
-#include <common/primitive_attr.hpp>
+#include "primitive_attr.hpp"
 #include "c_types_map.hpp"
 #include "cpu_convolution_pd.hpp"
 #include "cpu_engine.hpp"
diff --git a/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp b/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp
index 392622a..50a44a2 100644
--- a/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp
+++ b/inference-engine/thirdparty/mkl-dnn/src/cpu/jit_avx2_conv_kernel_f32.cpp
@@ -15,7 +15,7 @@
 * limitations under the License.
 *******************************************************************************/
 
-#include <common/primitive_attr.hpp>
+#include "primitive_attr.hpp"
 #include "c_types_map.hpp"
 #include "nstl.hpp"
 #include "type_helpers.hpp"
-- 
2.7.4

