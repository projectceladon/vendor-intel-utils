From 2eca98dd6cafd886979f9ecf075c312959499e05 Mon Sep 17 00:00:00 2001
From: Jeevaka Prabu Badrappan <jeevaka.badrappan@intel.com>
Date: Mon, 10 Jul 2023 18:20:16 +0530
Subject: [PATCH] Revert "FROMLIST: KVM: mmu: introduce new gfn_to_pfn_page
 functions"

This reverts commit ab733e258051592b47daf828cc7f9bf7ddcb87ad as it
causes device reboot during Android VM boot.

Signed-off-by: Jeevaka Prabu Badrappan <jeevaka.badrappan@intel.com>
---
 include/linux/kvm_host.h |  17 ----
 virt/kvm/kvm_main.c      | 186 +++++++++++----------------------------
 2 files changed, 51 insertions(+), 152 deletions(-)

diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5d5910a6834f..7408644aa2d0 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -981,19 +981,6 @@ kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable, hva_t *hva);
 
-kvm_pfn_t gfn_to_pfn_page(struct kvm *kvm, gfn_t gfn, struct page **page);
-kvm_pfn_t gfn_to_pfn_page_prot(struct kvm *kvm, gfn_t gfn,
-			       bool write_fault, bool *writable,
-			       struct page **page);
-kvm_pfn_t gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
-				  gfn_t gfn, struct page **page);
-kvm_pfn_t gfn_to_pfn_page_memslot_atomic(struct kvm_memory_slot *slot,
-					 gfn_t gfn, struct page **page);
-kvm_pfn_t __gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
-				    gfn_t gfn, bool atomic, bool *async,
-				    bool write_fault, bool *writable,
-				    hva_t *hva, struct page **page);
-
 void kvm_release_pfn_clean(kvm_pfn_t pfn);
 void kvm_release_pfn_dirty(kvm_pfn_t pfn);
 void kvm_set_pfn_dirty(kvm_pfn_t pfn);
@@ -1074,10 +1061,6 @@ struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
-kvm_pfn_t kvm_vcpu_gfn_to_pfn_page_atomic(struct kvm_vcpu *vcpu, gfn_t gfn,
-					  struct page **page);
-kvm_pfn_t kvm_vcpu_gfn_to_pfn_page(struct kvm_vcpu *vcpu, gfn_t gfn,
-				   struct page **page);
 int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);
 int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
 		struct gfn_to_pfn_cache *cache, bool atomic);
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index a5cbb1130f3e..993ff7019a30 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -2454,9 +2454,9 @@ static inline int check_user_page_hwpoison(unsigned long addr)
  * only part that runs if we can in atomic context.
  */
 static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
-			    bool *writable, kvm_pfn_t *pfn,
-			    struct page **page)
+			    bool *writable, kvm_pfn_t *pfn)
 {
+	struct page *page[1];
 
 	/*
 	 * Fast pin a writable pfn only if it is a write fault request
@@ -2467,7 +2467,7 @@ static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
 		return false;
 
 	if (get_user_page_fast_only(addr, FOLL_WRITE, page)) {
-		*pfn = page_to_pfn(*page);
+		*pfn = page_to_pfn(page[0]);
 
 		if (writable)
 			*writable = true;
@@ -2482,9 +2482,10 @@ static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
  * 1 indicates success, -errno is returned if error is detected.
  */
 static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
-			   bool *writable, kvm_pfn_t *pfn, struct page **page)
+			   bool *writable, kvm_pfn_t *pfn)
 {
 	unsigned int flags = FOLL_HWPOISON;
+	struct page *page;
 	int npages = 0;
 
 	might_sleep();
@@ -2497,7 +2498,7 @@ static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
 	if (async)
 		flags |= FOLL_NOWAIT;
 
-	npages = get_user_pages_unlocked(addr, 1, page, flags);
+	npages = get_user_pages_unlocked(addr, 1, &page, flags);
 	if (npages != 1)
 		return npages;
 
@@ -2507,11 +2508,11 @@ static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
 
 		if (get_user_page_fast_only(addr, FOLL_WRITE, &wpage)) {
 			*writable = true;
-			put_page(*page);
-			*page = wpage;
+			put_page(page);
+			page = wpage;
 		}
 	}
-	*pfn = page_to_pfn(*page);
+	*pfn = page_to_pfn(page);
 	return npages;
 }
 
@@ -2526,6 +2527,13 @@ static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)
 	return true;
 }
 
+static int kvm_try_get_pfn(kvm_pfn_t pfn)
+{
+	if (kvm_is_reserved_pfn(pfn))
+		return 1;
+	return get_page_unless_zero(pfn_to_page(pfn));
+}
+
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool *async,
 			       bool write_fault, bool *writable,
@@ -2565,6 +2573,26 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 		*writable = pte_write(*ptep);
 	pfn = pte_pfn(*ptep);
 
+	/*
+	 * Get a reference here because callers of *hva_to_pfn* and
+	 * *gfn_to_pfn* ultimately call kvm_release_pfn_clean on the
+	 * returned pfn.  This is only needed if the VMA has VM_MIXEDMAP
+	 * set, but the kvm_try_get_pfn/kvm_release_pfn_clean pair will
+	 * simply do nothing for reserved pfns.
+	 *
+	 * Whoever called remap_pfn_range is also going to call e.g.
+	 * unmap_mapping_range before the underlying pages are freed,
+	 * causing a call to our MMU notifier.
+	 *
+	 * Certain IO or PFNMAP mappings can be backed with valid
+	 * struct pages, but be allocated without refcounting e.g.,
+	 * tail pages of non-compound higher order allocations, which
+	 * would then underflow the refcount when the caller does the
+	 * required put_page. Don't allow those pages here.
+	 */ 
+	if (!kvm_try_get_pfn(pfn))
+		r = -EFAULT;
+
 out:
 	pte_unmap_unlock(ptep, ptl);
 	*p_pfn = pfn;
@@ -2586,9 +2614,8 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
-static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic,
-			    bool *async, bool write_fault, bool *writable,
-			    struct page **page)
+static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
+			bool write_fault, bool *writable)
 {
 	struct vm_area_struct *vma;
 	kvm_pfn_t pfn = 0;
@@ -2597,14 +2624,13 @@ static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic,
 	/* we can do it either atomically or asynchronously, not both */
 	BUG_ON(atomic && async);
 
-	if (hva_to_pfn_fast(addr, write_fault, writable, &pfn, page))
+	if (hva_to_pfn_fast(addr, write_fault, writable, &pfn))
 		return pfn;
 
 	if (atomic)
 		return KVM_PFN_ERR_FAULT;
 
-	npages = hva_to_pfn_slow(addr, async, write_fault, writable,
-				 &pfn, page);
+	npages = hva_to_pfn_slow(addr, async, write_fault, writable, &pfn);
 	if (npages == 1)
 		return pfn;
 
@@ -2636,14 +2662,12 @@ static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic,
 	return pfn;
 }
 
-kvm_pfn_t __gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
-				    gfn_t gfn, bool atomic, bool *async,
-				    bool write_fault, bool *writable,
-				    hva_t *hva, struct page **page)
+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool *async, bool write_fault,
+			       bool *writable, hva_t *hva)
 {
 	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
-	*page = NULL;
 	if (hva)
 		*hva = addr;
 
@@ -2666,153 +2690,45 @@ kvm_pfn_t __gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
 	}
 
 	return hva_to_pfn(addr, atomic, async, write_fault,
-			  writable, page);
-}
-EXPORT_SYMBOL_GPL(__gfn_to_pfn_page_memslot);
-
-kvm_pfn_t gfn_to_pfn_page_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
-			       bool *writable, struct page **page)
-{
-	return __gfn_to_pfn_page_memslot(gfn_to_memslot(kvm, gfn), gfn, false,
-					 NULL, write_fault, writable, NULL,
-					 page);
-}
-EXPORT_SYMBOL_GPL(gfn_to_pfn_page_prot);
-
-kvm_pfn_t gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
-				  struct page **page)
-{
-	return __gfn_to_pfn_page_memslot(slot, gfn, false, NULL, true,
-					 NULL, NULL, page);
-}
-EXPORT_SYMBOL_GPL(gfn_to_pfn_page_memslot);
-
-kvm_pfn_t gfn_to_pfn_page_memslot_atomic(struct kvm_memory_slot *slot,
-					 gfn_t gfn, struct page **page)
-{
-	return __gfn_to_pfn_page_memslot(slot, gfn, true, NULL, true, NULL,
-					 NULL, page);
-}
-EXPORT_SYMBOL_GPL(gfn_to_pfn_page_memslot_atomic);
-
-kvm_pfn_t kvm_vcpu_gfn_to_pfn_page_atomic(struct kvm_vcpu *vcpu, gfn_t gfn,
-					  struct page **page)
-{
-	return gfn_to_pfn_page_memslot_atomic(
-			kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, page);
-}
-EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_page_atomic);
-
-kvm_pfn_t gfn_to_pfn_page(struct kvm *kvm, gfn_t gfn, struct page **page)
-{
-	return gfn_to_pfn_page_memslot(gfn_to_memslot(kvm, gfn), gfn, page);
-}
-EXPORT_SYMBOL_GPL(gfn_to_pfn_page);
-
-kvm_pfn_t kvm_vcpu_gfn_to_pfn_page(struct kvm_vcpu *vcpu, gfn_t gfn,
-				   struct page **page)
-{
-	return gfn_to_pfn_page_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn),
-				       gfn, page);
-}
-EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_page);
-
-static kvm_pfn_t ensure_pfn_ref(struct page *page, kvm_pfn_t pfn)
-{
-	if (page || is_error_pfn(pfn) || kvm_is_reserved_pfn(pfn))
-		return pfn;
-
-	/*
-	 * Certain IO or PFNMAP mappings can be backed with valid
-	 * struct pages, but be allocated without refcounting e.g.,
-	 * tail pages of non-compound higher order allocations, which
-	 * would then underflow the refcount when the caller does the
-	 * required put_page. Don't allow those pages here.
-	 */
-	if (get_page_unless_zero(pfn_to_page(pfn)))
-		return pfn;
-
-	return KVM_PFN_ERR_FAULT;
-}
-
-kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
-			       bool atomic, bool *async, bool write_fault,
-			       bool *writable, hva_t *hva)
-{
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = __gfn_to_pfn_page_memslot(slot, gfn, atomic, async,
-					write_fault, writable, hva, &page);
-
-	return ensure_pfn_ref(page, pfn);
+			  writable);
 }
 EXPORT_SYMBOL_GPL(__gfn_to_pfn_memslot);
 
 kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable)
 {
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = gfn_to_pfn_page_prot(kvm, gfn, write_fault, writable, &page);
-
-	return ensure_pfn_ref(page, pfn);
+	return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, NULL,
+				    write_fault, writable, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);
 
 kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
 {
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = gfn_to_pfn_page_memslot(slot, gfn, &page);
-
-	return ensure_pfn_ref(page, pfn);
+	return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);
 
 kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)
 {
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = gfn_to_pfn_page_memslot_atomic(slot, gfn, &page);
-
-	return ensure_pfn_ref(page, pfn);
+	return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);
 
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = kvm_vcpu_gfn_to_pfn_page_atomic(vcpu, gfn, &page);
-
-	return ensure_pfn_ref(page, pfn);
+	return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_atomic);
 
 kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)
 {
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = gfn_to_pfn_page(kvm, gfn, &page);
-
-	return ensure_pfn_ref(page, pfn);
+	return gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn);
 
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
-	struct page *page;
-	kvm_pfn_t pfn;
-
-	pfn = kvm_vcpu_gfn_to_pfn_page(vcpu, gfn, &page);
-
-	return ensure_pfn_ref(page, pfn);
+	return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn);
 
-- 
2.41.0

